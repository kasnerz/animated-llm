{
  "id": "generated_20251113_155656",
  "prompt": "Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?",
  "language": "de",
  "temperature": 5.0,
  "top_k": 10,
  "model_info": {
    "name": "CohereLabs/aya-expanse-8b",
    "num_layers": 32,
    "hidden_size": 4096,
    "num_attention_heads": 32,
    "vocab_size": 256000
  },
  "generation_steps": [
    {
      "step": 0,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Im",
            "token_id": 4928,
            "logprob": -0.0005,
            "prob": 0.9995
          },
          {
            "token": "Das",
            "token_id": 21930,
            "logprob": -7.875,
            "prob": 0.0004
          },
          {
            "token": "L",
            "token_id": 51,
            "logprob": -10.7969,
            "prob": 0.0
          },
          {
            "token": "Wenn",
            "token_id": 247488,
            "logprob": -11.0156,
            "prob": 0.0
          },
          {
            "token": "Um",
            "token_id": 26254,
            "logprob": -11.5781,
            "prob": 0.0
          },
          {
            "token": "im",
            "token_id": 1748,
            "logprob": -11.6406,
            "prob": 0.0
          },
          {
            "token": "Ins",
            "token_id": 17543,
            "logprob": -11.9688,
            "prob": 0.0
          },
          {
            "token": "Z",
            "token_id": 65,
            "logprob": -12.5156,
            "prob": 0.0
          },
          {
            "token": "Nel",
            "token_id": 17040,
            "logprob": -12.8438,
            "prob": 0.0
          },
          {
            "token": "In",
            "token_id": 2158,
            "logprob": -13.6953,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Im",
        "token_id": 4928,
        "selection_method": "sampling"
      }
    },
    {
      "step": 1,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "ĠWort",
            "token_id": 80704,
            "logprob": -0.0,
            "prob": 1.0
          },
          {
            "token": "ĠWord",
            "token_id": 17019,
            "logprob": -12.1406,
            "prob": 0.0
          },
          {
            "token": "ĠWörter",
            "token_id": 187284,
            "logprob": -14.7656,
            "prob": 0.0
          },
          {
            "token": "ĠWör",
            "token_id": 104945,
            "logprob": -16.8438,
            "prob": 0.0
          },
          {
            "token": "<SEP>",
            "token_id": 3,
            "logprob": -34.25,
            "prob": 0.0
          },
          {
            "token": "<CLS>",
            "token_id": 2,
            "logprob": -33.0,
            "prob": 0.0
          },
          {
            "token": "<MASK_TOKEN>",
            "token_id": 4,
            "logprob": -33.4688,
            "prob": 0.0
          },
          {
            "token": "<BOS_TOKEN>",
            "token_id": 5,
            "logprob": -30.0781,
            "prob": 0.0
          },
          {
            "token": "<PAD>",
            "token_id": 0,
            "logprob": -33.4062,
            "prob": 0.0
          },
          {
            "token": "<UNK>",
            "token_id": 1,
            "logprob": -33.0,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "ĠWort",
        "token_id": 80704,
        "selection_method": "sampling"
      }
    },
    {
      "step": 2,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġ\"",
            "token_id": 1789,
            "logprob": -0.0,
            "prob": 1.0
          },
          {
            "token": "Ġ„",
            "token_id": 4352,
            "logprob": -14.4219,
            "prob": 0.0
          },
          {
            "token": "<PAD>",
            "token_id": 0,
            "logprob": -41.6875,
            "prob": 0.0
          },
          {
            "token": "<UNK>",
            "token_id": 1,
            "logprob": -41.0312,
            "prob": 0.0
          },
          {
            "token": "<EOS_TOKEN>",
            "token_id": 6,
            "logprob": -38.9375,
            "prob": 0.0
          },
          {
            "token": "<EOP_TOKEN>",
            "token_id": 7,
            "logprob": -41.9375,
            "prob": 0.0
          },
          {
            "token": "<BOS_TOKEN>",
            "token_id": 5,
            "logprob": -38.0,
            "prob": 0.0
          },
          {
            "token": "<MASK_TOKEN>",
            "token_id": 4,
            "logprob": -41.8438,
            "prob": 0.0
          },
          {
            "token": "<CLS>",
            "token_id": 2,
            "logprob": -42.0625,
            "prob": 0.0
          },
          {
            "token": "<SEP>",
            "token_id": 3,
            "logprob": -41.75,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġ\"",
        "token_id": 1789,
        "selection_method": "sampling"
      }
    },
    {
      "step": 3,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\""
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Gesch",
            "token_id": 23361,
            "logprob": -0.0,
            "prob": 1.0
          },
          {
            "token": "Ges",
            "token_id": 63053,
            "logprob": -13.1875,
            "prob": 0.0
          },
          {
            "token": "ĠGesch",
            "token_id": 17592,
            "logprob": -13.3281,
            "prob": 0.0
          },
          {
            "token": "Gest",
            "token_id": 76148,
            "logprob": -16.1406,
            "prob": 0.0
          },
          {
            "token": "Geb",
            "token_id": 21650,
            "logprob": -15.9531,
            "prob": 0.0
          },
          {
            "token": "Gift",
            "token_id": 152383,
            "logprob": -16.4375,
            "prob": 0.0
          },
          {
            "token": "engesch",
            "token_id": 186367,
            "logprob": -17.3281,
            "prob": 0.0
          },
          {
            "token": "gesch",
            "token_id": 19238,
            "logprob": -16.4375,
            "prob": 0.0
          },
          {
            "token": "Besch",
            "token_id": 83660,
            "logprob": -16.6562,
            "prob": 0.0
          },
          {
            "token": "<PAD>",
            "token_id": 0,
            "logprob": -32.6562,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Gesch",
        "token_id": 23361,
        "selection_method": "sampling"
      }
    },
    {
      "step": 4,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Gesch",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "wind",
            "token_id": 25965,
            "logprob": -0.0,
            "prob": 1.0
          },
          {
            "token": "windigkeit",
            "token_id": 139420,
            "logprob": -14.5312,
            "prob": 0.0
          },
          {
            "token": "Wind",
            "token_id": 77097,
            "logprob": -15.0156,
            "prob": 0.0
          },
          {
            "token": "winds",
            "token_id": 147686,
            "logprob": -15.0625,
            "prob": 0.0
          },
          {
            "token": "Ġwind",
            "token_id": 15701,
            "logprob": -15.2656,
            "prob": 0.0
          },
          {
            "token": "wend",
            "token_id": 26803,
            "logprob": -15.7812,
            "prob": 0.0
          },
          {
            "token": "ĠWind",
            "token_id": 21198,
            "logprob": -16.8438,
            "prob": 0.0
          },
          {
            "token": "wand",
            "token_id": 38163,
            "logprob": -16.8281,
            "prob": 0.0
          },
          {
            "token": "風",
            "token_id": 15108,
            "logprob": -16.2812,
            "prob": 0.0
          },
          {
            "token": "win",
            "token_id": 7904,
            "logprob": -17.0312,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "wind",
        "token_id": 25965,
        "selection_method": "sampling"
      }
    },
    {
      "step": 5,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwind",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "igkeits",
            "token_id": 173625,
            "logprob": -0.0,
            "prob": 1.0
          },
          {
            "token": "keits",
            "token_id": 106569,
            "logprob": -11.0312,
            "prob": 0.0
          },
          {
            "token": "heits",
            "token_id": 59710,
            "logprob": -12.0,
            "prob": 0.0
          },
          {
            "token": "itäts",
            "token_id": 125605,
            "logprob": -12.9062,
            "prob": 0.0
          },
          {
            "token": "heids",
            "token_id": 62547,
            "logprob": -12.8906,
            "prob": 0.0
          },
          {
            "token": "igungs",
            "token_id": 123652,
            "logprob": -13.1406,
            "prob": 0.0
          },
          {
            "token": "igkeiten",
            "token_id": 95949,
            "logprob": -14.5,
            "prob": 0.0
          },
          {
            "token": "igs",
            "token_id": 38783,
            "logprob": -15.1562,
            "prob": 0.0
          },
          {
            "token": "ungs",
            "token_id": 10484,
            "logprob": -16.2031,
            "prob": 0.0
          },
          {
            "token": "igkeit",
            "token_id": 29109,
            "logprob": -15.9531,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "igkeits",
        "token_id": 173625,
        "selection_method": "sampling"
      }
    },
    {
      "step": 6,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeits",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "beg",
            "token_id": 43882,
            "logprob": -0.0,
            "prob": 1.0
          },
          {
            "token": "begr",
            "token_id": 163311,
            "logprob": -13.875,
            "prob": 0.0
          },
          {
            "token": "besch",
            "token_id": 78652,
            "logprob": -14.9062,
            "prob": 0.0
          },
          {
            "token": "bek",
            "token_id": 24763,
            "logprob": -16.7031,
            "prob": 0.0
          },
          {
            "token": "<SEP>",
            "token_id": 3,
            "logprob": -36.0938,
            "prob": 0.0
          },
          {
            "token": "<CLS>",
            "token_id": 2,
            "logprob": -36.1562,
            "prob": 0.0
          },
          {
            "token": "<MASK_TOKEN>",
            "token_id": 4,
            "logprob": -36.0938,
            "prob": 0.0
          },
          {
            "token": "<BOS_TOKEN>",
            "token_id": 5,
            "logprob": -37.5312,
            "prob": 0.0
          },
          {
            "token": "<PAD>",
            "token_id": 0,
            "logprob": -35.9688,
            "prob": 0.0
          },
          {
            "token": "<UNK>",
            "token_id": 1,
            "logprob": -34.8125,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "beg",
        "token_id": 43882,
        "selection_method": "sampling"
      }
    },
    {
      "step": 7,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbeg",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "ren",
            "token_id": 2764,
            "logprob": -0.0,
            "prob": 1.0
          },
          {
            "token": "renz",
            "token_id": 93821,
            "logprob": -14.5625,
            "prob": 0.0
          },
          {
            "token": "ran",
            "token_id": 2138,
            "logprob": -17.2656,
            "prob": 0.0
          },
          {
            "token": "<PAD>",
            "token_id": 0,
            "logprob": -35.5625,
            "prob": 0.0
          },
          {
            "token": "<EOS_TOKEN>",
            "token_id": 6,
            "logprob": -32.375,
            "prob": 0.0
          },
          {
            "token": "<SEP>",
            "token_id": 3,
            "logprob": -35.0625,
            "prob": 0.0
          },
          {
            "token": "<BOS_TOKEN>",
            "token_id": 5,
            "logprob": -36.0938,
            "prob": 0.0
          },
          {
            "token": "<MASK_TOKEN>",
            "token_id": 4,
            "logprob": -35.6875,
            "prob": 0.0
          },
          {
            "token": "<UNK>",
            "token_id": 1,
            "logprob": -34.75,
            "prob": 0.0
          },
          {
            "token": "<CLS>",
            "token_id": 2,
            "logprob": -35.4375,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "ren",
        "token_id": 2764,
        "selection_method": "sampling"
      }
    },
    {
      "step": 8,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegren",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "zung",
            "token_id": 60525,
            "logprob": -0.0,
            "prob": 1.0
          },
          {
            "token": "nung",
            "token_id": 28931,
            "logprob": -11.3438,
            "prob": 0.0
          },
          {
            "token": "zing",
            "token_id": 24415,
            "logprob": -12.9062,
            "prob": 0.0
          },
          {
            "token": "ung",
            "token_id": 2359,
            "logprob": -13.7344,
            "prob": 0.0
          },
          {
            "token": "kung",
            "token_id": 176493,
            "logprob": -13.8906,
            "prob": 0.0
          },
          {
            "token": "chung",
            "token_id": 137384,
            "logprob": -13.9062,
            "prob": 0.0
          },
          {
            "token": "bung",
            "token_id": 128752,
            "logprob": -14.0156,
            "prob": 0.0
          },
          {
            "token": "igung",
            "token_id": 31481,
            "logprob": -14.0625,
            "prob": 0.0
          },
          {
            "token": "zug",
            "token_id": 51594,
            "logprob": -14.7969,
            "prob": 0.0
          },
          {
            "token": "zungen",
            "token_id": 121932,
            "logprob": -14.9219,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "ung",
        "token_id": 2359,
        "selection_method": "sampling"
      }
    },
    {
      "step": 9,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "\"",
            "token_id": 9,
            "logprob": 0.0,
            "prob": 1.0
          },
          {
            "token": "<UNK>",
            "token_id": 1,
            "logprob": -43.2188,
            "prob": 0.0
          },
          {
            "token": "<SEP>",
            "token_id": 3,
            "logprob": -44.0312,
            "prob": 0.0
          },
          {
            "token": "<EOS_TOKEN>",
            "token_id": 6,
            "logprob": -35.4375,
            "prob": 0.0
          },
          {
            "token": "<EOP_TOKEN>",
            "token_id": 7,
            "logprob": -43.9375,
            "prob": 0.0
          },
          {
            "token": "<PAD>",
            "token_id": 0,
            "logprob": -43.75,
            "prob": 0.0
          },
          {
            "token": "<CLS>",
            "token_id": 2,
            "logprob": -44.3125,
            "prob": 0.0
          },
          {
            "token": "<BOS_TOKEN>",
            "token_id": 5,
            "logprob": -41.1875,
            "prob": 0.0
          },
          {
            "token": "!",
            "token_id": 8,
            "logprob": -39.2188,
            "prob": 0.0
          },
          {
            "token": "<MASK_TOKEN>",
            "token_id": 4,
            "logprob": -43.8125,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "\"",
        "token_id": 9,
        "selection_method": "sampling"
      }
    },
    {
      "step": 10,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\"",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\""
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġgibt",
            "token_id": 38561,
            "logprob": -0.0058,
            "prob": 0.9941
          },
          {
            "token": "Ġsind",
            "token_id": 11382,
            "logprob": -5.2695,
            "prob": 0.0051
          },
          {
            "token": "Ġkommen",
            "token_id": 89552,
            "logprob": -8.1641,
            "prob": 0.0003
          },
          {
            "token": "Ġstehen",
            "token_id": 87437,
            "logprob": -8.3828,
            "prob": 0.0002
          },
          {
            "token": "Ġbefinden",
            "token_id": 85036,
            "logprob": -9.6484,
            "prob": 0.0001
          },
          {
            "token": "Ġgab",
            "token_id": 25163,
            "logprob": -10.3672,
            "prob": 0.0
          },
          {
            "token": "Ġkommt",
            "token_id": 62615,
            "logprob": -12.0625,
            "prob": 0.0
          },
          {
            "token": "Ġtreten",
            "token_id": 225941,
            "logprob": -12.3203,
            "prob": 0.0
          },
          {
            "token": "Ġfinden",
            "token_id": 71457,
            "logprob": -12.4531,
            "prob": 0.0
          },
          {
            "token": "Ġliegen",
            "token_id": 94271,
            "logprob": -12.6875,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġgab",
        "token_id": 25163,
        "selection_method": "sampling"
      }
    },
    {
      "step": 11,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġes",
            "token_id": 2278,
            "logprob": -0.0,
            "prob": 1.0
          },
          {
            "token": "Ġund",
            "token_id": 2280,
            "logprob": -12.1094,
            "prob": 0.0
          },
          {
            "token": "t",
            "token_id": 91,
            "logprob": -14.4531,
            "prob": 0.0
          },
          {
            "token": "/",
            "token_id": 22,
            "logprob": -14.4219,
            "prob": 0.0
          },
          {
            "token": "Ġ(",
            "token_id": 1737,
            "logprob": -14.9922,
            "prob": 0.0
          },
          {
            "token": "Ġoder",
            "token_id": 13624,
            "logprob": -16.0938,
            "prob": 0.0
          },
          {
            "token": "ĠEs",
            "token_id": 6154,
            "logprob": -16.2344,
            "prob": 0.0
          },
          {
            "token": "Es",
            "token_id": 13522,
            "logprob": -17.2188,
            "prob": 0.0
          },
          {
            "token": "<PAD>",
            "token_id": 0,
            "logprob": -33.9688,
            "prob": 0.0
          },
          {
            "token": "<UNK>",
            "token_id": 1,
            "logprob": -34.2812,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġes",
        "token_id": 2278,
        "selection_method": "sampling"
      }
    },
    {
      "step": 12,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġ",
            "token_id": 228,
            "logprob": -1.1211,
            "prob": 0.3259
          },
          {
            "token": "Ġzählen",
            "token_id": 166844,
            "logprob": -1.9883,
            "prob": 0.137
          },
          {
            "token": ",",
            "token_id": 19,
            "logprob": -2.1055,
            "prob": 0.1218
          },
          {
            "token": "Ġin",
            "token_id": 1709,
            "logprob": -2.2148,
            "prob": 0.1092
          },
          {
            "token": "Ġvier",
            "token_id": 16675,
            "logprob": -2.6836,
            "prob": 0.0684
          },
          {
            "token": "Ġbisher",
            "token_id": 77352,
            "logprob": -3.2617,
            "prob": 0.0383
          },
          {
            "token": "Ġinsgesamt",
            "token_id": 66426,
            "logprob": -3.5977,
            "prob": 0.0274
          },
          {
            "token": "Ġbis",
            "token_id": 6390,
            "logprob": -3.6758,
            "prob": 0.0253
          },
          {
            "token": "Ġim",
            "token_id": 2101,
            "logprob": -4.2461,
            "prob": 0.0143
          },
          {
            "token": "Ġbei",
            "token_id": 9174,
            "logprob": -4.3945,
            "prob": 0.0124
          }
        ]
      },
      "selected_token": {
        "token": "Ġin",
        "token_id": 1709,
        "selection_method": "sampling"
      }
    },
    {
      "step": 13,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es in",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġder",
            "token_id": 2479,
            "logprob": -0.0137,
            "prob": 0.9863
          },
          {
            "token": "Ġme",
            "token_id": 2307,
            "logprob": -4.9961,
            "prob": 0.0068
          },
          {
            "token": "Ġunserer",
            "token_id": 253371,
            "logprob": -5.3086,
            "prob": 0.0049
          },
          {
            "token": "Ġunser",
            "token_id": 145079,
            "logprob": -6.8867,
            "prob": 0.001
          },
          {
            "token": "Ġdem",
            "token_id": 3297,
            "logprob": -8.7266,
            "prob": 0.0002
          },
          {
            "token": "Ġdiesem",
            "token_id": 38439,
            "logprob": -8.9062,
            "prob": 0.0001
          },
          {
            "token": "Ġeiner",
            "token_id": 10295,
            "logprob": -9.2891,
            "prob": 0.0001
          },
          {
            "token": "Ġeinem",
            "token_id": 12315,
            "logprob": -9.3516,
            "prob": 0.0001
          },
          {
            "token": "kl",
            "token_id": 9817,
            "logprob": -9.4141,
            "prob": 0.0001
          },
          {
            "token": "Ġdieser",
            "token_id": 23691,
            "logprob": -9.5859,
            "prob": 0.0001
          }
        ]
      },
      "selected_token": {
        "token": "kl",
        "token_id": 9817,
        "selection_method": "sampling"
      }
    },
    {
      "step": 14,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inkl",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "usive",
            "token_id": 44269,
            "logprob": -0.0,
            "prob": 1.0
          },
          {
            "token": "ud",
            "token_id": 1879,
            "logprob": -12.7969,
            "prob": 0.0
          },
          {
            "token": "usion",
            "token_id": 10479,
            "logprob": -14.4609,
            "prob": 0.0
          },
          {
            "token": "uding",
            "token_id": 90932,
            "logprob": -16.5625,
            "prob": 0.0
          },
          {
            "token": "usions",
            "token_id": 59172,
            "logprob": -16.75,
            "prob": 0.0
          },
          {
            "token": "uis",
            "token_id": 4532,
            "logprob": -16.8438,
            "prob": 0.0
          },
          {
            "token": "ativ",
            "token_id": 4021,
            "logprob": -16.3906,
            "prob": 0.0
          },
          {
            "token": "<CLS>",
            "token_id": 2,
            "logprob": -29.5,
            "prob": 0.0
          },
          {
            "token": "<UNK>",
            "token_id": 1,
            "logprob": -29.7656,
            "prob": 0.0
          },
          {
            "token": "<PAD>",
            "token_id": 0,
            "logprob": -29.7344,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "usive",
        "token_id": 44269,
        "selection_method": "sampling"
      }
    },
    {
      "step": 15,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġaller",
            "token_id": 41292,
            "logprob": -1.3682,
            "prob": 0.2544
          },
          {
            "token": "Ġalle",
            "token_id": 10528,
            "logprob": -1.5322,
            "prob": 0.2159
          },
          {
            "token": "ĠLeer",
            "token_id": 164347,
            "logprob": -2.4004,
            "prob": 0.0908
          },
          {
            "token": "Ġallen",
            "token_id": 43318,
            "logprob": -2.666,
            "prob": 0.0696
          },
          {
            "token": "Ġder",
            "token_id": 2479,
            "logprob": -2.6816,
            "prob": 0.0685
          },
          {
            "token": "Ġdoppel",
            "token_id": 198317,
            "logprob": -3.0801,
            "prob": 0.046
          },
          {
            "token": "ĠNach",
            "token_id": 13475,
            "logprob": -3.1582,
            "prob": 0.0425
          },
          {
            "token": "ĠAlle",
            "token_id": 40696,
            "logprob": -3.5645,
            "prob": 0.0283
          },
          {
            "token": "ĠDup",
            "token_id": 22582,
            "logprob": -3.7832,
            "prob": 0.0228
          },
          {
            "token": "ĠDoppel",
            "token_id": 88725,
            "logprob": -4.0938,
            "prob": 0.0166
          }
        ]
      },
      "selected_token": {
        "token": "ĠNach",
        "token_id": 13475,
        "selection_method": "sampling"
      }
    },
    {
      "step": 16,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nach",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "komm",
            "token_id": 79979,
            "logprob": -0.9556,
            "prob": 0.3845
          },
          {
            "token": "namen",
            "token_id": 60247,
            "logprob": -1.917,
            "prob": 0.1471
          },
          {
            "token": "sil",
            "token_id": 22700,
            "logprob": -2.9941,
            "prob": 0.05
          },
          {
            "token": "kom",
            "token_id": 15293,
            "logprob": -3.791,
            "prob": 0.0226
          },
          {
            "token": "de",
            "token_id": 1966,
            "logprob": -3.8066,
            "prob": 0.0222
          },
          {
            "token": "druck",
            "token_id": 84017,
            "logprob": -4.0352,
            "prob": 0.0177
          },
          {
            "token": "str",
            "token_id": 2507,
            "logprob": -4.168,
            "prob": 0.0155
          },
          {
            "token": "le",
            "token_id": 1693,
            "logprob": -4.5508,
            "prob": 0.0106
          },
          {
            "token": "dr",
            "token_id": 7221,
            "logprob": -4.5664,
            "prob": 0.0104
          },
          {
            "token": "ged",
            "token_id": 7994,
            "logprob": -4.5664,
            "prob": 0.0104
          }
        ]
      },
      "selected_token": {
        "token": "de",
        "token_id": 1966,
        "selection_method": "sampling"
      }
    },
    {
      "step": 17,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachde",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "hn",
            "token_id": 7940,
            "logprob": -1.043,
            "prob": 0.3523
          },
          {
            "token": "ut",
            "token_id": 1754,
            "logprob": -1.1992,
            "prob": 0.3015
          },
          {
            "token": "utung",
            "token_id": 172819,
            "logprob": -2.1523,
            "prob": 0.1162
          },
          {
            "token": "hnung",
            "token_id": 87584,
            "logprob": -2.4805,
            "prob": 0.0837
          },
          {
            "token": "hm",
            "token_id": 11097,
            "logprob": -3.8477,
            "prob": 0.0213
          },
          {
            "token": "hnen",
            "token_id": 87881,
            "logprob": -4.2148,
            "prob": 0.0148
          },
          {
            "token": "ch",
            "token_id": 1750,
            "logprob": -4.2383,
            "prob": 0.0144
          },
          {
            "token": "uts",
            "token_id": 16022,
            "logprob": -4.4102,
            "prob": 0.0122
          },
          {
            "token": "ken",
            "token_id": 4278,
            "logprob": -4.5898,
            "prob": 0.0102
          },
          {
            "token": "uten",
            "token_id": 10700,
            "logprob": -4.6211,
            "prob": 0.0098
          }
        ]
      },
      "selected_token": {
        "token": "hnen",
        "token_id": 87881,
        "selection_method": "sampling"
      }
    },
    {
      "step": 18,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġund",
            "token_id": 2280,
            "logprob": -0.0771,
            "prob": 0.9258
          },
          {
            "token": "Ġ(",
            "token_id": 1737,
            "logprob": -2.6387,
            "prob": 0.0714
          },
          {
            "token": "Ġkeine",
            "token_id": 41487,
            "logprob": -6.4219,
            "prob": 0.0016
          },
          {
            "token": "Ġder",
            "token_id": 2479,
            "logprob": -7.6875,
            "prob": 0.0005
          },
          {
            "token": "Ġ",
            "token_id": 228,
            "logprob": -7.6875,
            "prob": 0.0005
          },
          {
            "token": "Ġdes",
            "token_id": 2003,
            "logprob": -9.1328,
            "prob": 0.0001
          },
          {
            "token": "Ġvon",
            "token_id": 3615,
            "logprob": -9.6562,
            "prob": 0.0001
          },
          {
            "token": "Ġkein",
            "token_id": 77135,
            "logprob": -9.75,
            "prob": 0.0001
          },
          {
            "token": ",",
            "token_id": 19,
            "logprob": -11.4844,
            "prob": 0.0
          },
          {
            "token": "Ġoder",
            "token_id": 13624,
            "logprob": -11.5156,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġ(",
        "token_id": 1737,
        "selection_method": "sampling"
      }
    },
    {
      "step": 19,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ("
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "da",
            "token_id": 4205,
            "logprob": -0.8833,
            "prob": 0.4136
          },
          {
            "token": "um",
            "token_id": 1783,
            "logprob": -1.5859,
            "prob": 0.2047
          },
          {
            "token": "also",
            "token_id": 19705,
            "logprob": -1.8047,
            "prob": 0.1644
          },
          {
            "token": "auch",
            "token_id": 44051,
            "logprob": -3.9297,
            "prob": 0.0196
          },
          {
            "token": "w",
            "token_id": 94,
            "logprob": -4.0703,
            "prob": 0.0171
          },
          {
            "token": "ob",
            "token_id": 2047,
            "logprob": -4.25,
            "prob": 0.0143
          },
          {
            "token": "das",
            "token_id": 57980,
            "logprob": -4.3828,
            "prob": 0.0125
          },
          {
            "token": "d",
            "token_id": 75,
            "logprob": -4.3906,
            "prob": 0.0124
          },
          {
            "token": "wie",
            "token_id": 12031,
            "logprob": -4.4219,
            "prob": 0.012
          },
          {
            "token": "z",
            "token_id": 97,
            "logprob": -4.4531,
            "prob": 0.0116
          }
        ]
      },
      "selected_token": {
        "token": "um",
        "token_id": 1783,
        "selection_method": "sampling"
      }
    },
    {
      "step": 20,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737, 1783
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġsicher",
            "token_id": 171491,
            "logprob": -0.4109,
            "prob": 0.6631
          },
          {
            "token": "Ġalle",
            "token_id": 10528,
            "logprob": -2.2539,
            "prob": 0.1049
          },
          {
            "token": "Ġdie",
            "token_id": 2915,
            "logprob": -2.3164,
            "prob": 0.0986
          },
          {
            "token": "Ġdoppel",
            "token_id": 198317,
            "logprob": -3.3164,
            "prob": 0.0363
          },
          {
            "token": "ĠDoppel",
            "token_id": 88725,
            "logprob": -3.9336,
            "prob": 0.0196
          },
          {
            "token": "ĠLa",
            "token_id": 2795,
            "logprob": -4.1367,
            "prob": 0.016
          },
          {
            "token": "Ġeine",
            "token_id": 7147,
            "logprob": -4.6523,
            "prob": 0.0095
          },
          {
            "token": "Ġauch",
            "token_id": 9216,
            "logprob": -4.6914,
            "prob": 0.0092
          },
          {
            "token": "ĠDup",
            "token_id": 22582,
            "logprob": -5.168,
            "prob": 0.0057
          },
          {
            "token": "ĠDip",
            "token_id": 16826,
            "logprob": -5.4258,
            "prob": 0.0044
          }
        ]
      },
      "selected_token": {
        "token": "Ġsicher",
        "token_id": 171491,
        "selection_method": "sampling"
      }
    },
    {
      "step": 21,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicher",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "zustellen",
            "token_id": 193025,
            "logprob": -0.0125,
            "prob": 0.9878
          },
          {
            "token": "zuge",
            "token_id": 208867,
            "logprob": -4.5117,
            "prob": 0.011
          },
          {
            "token": "Ġzu",
            "token_id": 5937,
            "logprob": -6.5586,
            "prob": 0.0014
          },
          {
            "token": "gehend",
            "token_id": 91463,
            "logprob": -10.7812,
            "prob": 0.0
          },
          {
            "token": "gehen",
            "token_id": 196994,
            "logprob": -11.5938,
            "prob": 0.0
          },
          {
            "token": "zus",
            "token_id": 111714,
            "logprob": -11.7266,
            "prob": 0.0
          },
          {
            "token": "zust",
            "token_id": 118637,
            "logprob": -11.7422,
            "prob": 0.0
          },
          {
            "token": "ge",
            "token_id": 2035,
            "logprob": -12.9141,
            "prob": 0.0
          },
          {
            "token": "Ġsicher",
            "token_id": 171491,
            "logprob": -13.3281,
            "prob": 0.0
          },
          {
            "token": "stell",
            "token_id": 18868,
            "logprob": -13.4453,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "zuge",
        "token_id": 208867,
        "selection_method": "sampling"
      }
    },
    {
      "step": 22,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzuge",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "hen",
            "token_id": 11942,
            "logprob": -0.0,
            "prob": 1.0
          },
          {
            "token": "gehen",
            "token_id": 196994,
            "logprob": -12.5781,
            "prob": 0.0
          },
          {
            "token": "hn",
            "token_id": 7940,
            "logprob": -13.0469,
            "prob": 0.0
          },
          {
            "token": "ben",
            "token_id": 5881,
            "logprob": -13.375,
            "prob": 0.0
          },
          {
            "token": "h",
            "token_id": 79,
            "logprob": -15.375,
            "prob": 0.0
          },
          {
            "token": "ht",
            "token_id": 1876,
            "logprob": -15.5469,
            "prob": 0.0
          },
          {
            "token": "hnen",
            "token_id": 87881,
            "logprob": -16.2031,
            "prob": 0.0
          },
          {
            "token": "he",
            "token_id": 1678,
            "logprob": -16.0312,
            "prob": 0.0
          },
          {
            "token": "hens",
            "token_id": 76923,
            "logprob": -15.7891,
            "prob": 0.0
          },
          {
            "token": "hmen",
            "token_id": 28089,
            "logprob": -16.7188,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "hmen",
        "token_id": 28089,
        "selection_method": "sampling"
      }
    },
    {
      "step": 23,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": ",",
            "token_id": 19,
            "logprob": -0.0,
            "prob": 1.0
          },
          {
            "token": ")",
            "token_id": 16,
            "logprob": -10.1719,
            "prob": 0.0
          },
          {
            "token": "Ġalle",
            "token_id": 10528,
            "logprob": -13.2656,
            "prob": 0.0
          },
          {
            "token": "Ġdass",
            "token_id": 15760,
            "logprob": -15.2109,
            "prob": 0.0
          },
          {
            "token": "Ġauch",
            "token_id": 9216,
            "logprob": -15.5781,
            "prob": 0.0
          },
          {
            "token": "Ġund",
            "token_id": 2280,
            "logprob": -15.6797,
            "prob": 0.0
          },
          {
            "token": "Ġdie",
            "token_id": 2915,
            "logprob": -16.9844,
            "prob": 0.0
          },
          {
            "token": "<CLS>",
            "token_id": 2,
            "logprob": -33.3438,
            "prob": 0.0
          },
          {
            "token": "<UNK>",
            "token_id": 1,
            "logprob": -33.8125,
            "prob": 0.0
          },
          {
            "token": "<PAD>",
            "token_id": 0,
            "logprob": -33.4062,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": ",",
        "token_id": 19,
        "selection_method": "sampling"
      }
    },
    {
      "step": 24,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen,",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ","
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġdass",
            "token_id": 15760,
            "logprob": -0.0049,
            "prob": 0.9951
          },
          {
            "token": "Ġob",
            "token_id": 2394,
            "logprob": -5.3633,
            "prob": 0.0047
          },
          {
            "token": "Ġwelche",
            "token_id": 64536,
            "logprob": -9.3984,
            "prob": 0.0001
          },
          {
            "token": "Ġauch",
            "token_id": 9216,
            "logprob": -10.0703,
            "prob": 0.0
          },
          {
            "token": "Ġalle",
            "token_id": 10528,
            "logprob": -10.9922,
            "prob": 0.0
          },
          {
            "token": "Ġwelches",
            "token_id": 99748,
            "logprob": -12.0,
            "prob": 0.0
          },
          {
            "token": "Ġdas",
            "token_id": 4031,
            "logprob": -12.0938,
            "prob": 0.0
          },
          {
            "token": "Ġdie",
            "token_id": 2915,
            "logprob": -12.3125,
            "prob": 0.0
          },
          {
            "token": "Ġda",
            "token_id": 2213,
            "logprob": -12.8047,
            "prob": 0.0
          },
          {
            "token": "Ġwenn",
            "token_id": 54426,
            "logprob": -13.1953,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġdass",
        "token_id": 15760,
        "selection_method": "sampling"
      }
    },
    {
      "step": 25,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġalle",
            "token_id": 10528,
            "logprob": -0.0829,
            "prob": 0.9204
          },
          {
            "token": "Ġjedes",
            "token_id": 174473,
            "logprob": -2.6602,
            "prob": 0.0699
          },
          {
            "token": "Ġjede",
            "token_id": 158569,
            "logprob": -4.9258,
            "prob": 0.0073
          },
          {
            "token": "Ġauch",
            "token_id": 9216,
            "logprob": -6.9414,
            "prob": 0.001
          },
          {
            "token": "Ġalles",
            "token_id": 80365,
            "logprob": -7.7852,
            "prob": 0.0004
          },
          {
            "token": "Ġjeder",
            "token_id": 115351,
            "logprob": -8.0234,
            "prob": 0.0003
          },
          {
            "token": "Ġbeide",
            "token_id": 45137,
            "logprob": -8.8047,
            "prob": 0.0002
          },
          {
            "token": "Ġkeine",
            "token_id": 41487,
            "logprob": -9.1641,
            "prob": 0.0001
          },
          {
            "token": "Ġsowohl",
            "token_id": 79881,
            "logprob": -9.1953,
            "prob": 0.0001
          },
          {
            "token": "Ġkein",
            "token_id": 77135,
            "logprob": -9.3359,
            "prob": 0.0001
          }
        ]
      },
      "selected_token": {
        "token": "Ġalle",
        "token_id": 10528,
        "selection_method": "sampling"
      }
    },
    {
      "step": 26,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass alle",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass",
        "Ġalle"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760, 10528
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġ\"",
            "token_id": 1789,
            "logprob": -0.3474,
            "prob": 0.7065
          },
          {
            "token": "ĠV",
            "token_id": 1908,
            "logprob": -2.3398,
            "prob": 0.0964
          },
          {
            "token": "ĠVok",
            "token_id": 192220,
            "logprob": -2.5195,
            "prob": 0.0805
          },
          {
            "token": "Ġ'",
            "token_id": 2155,
            "logprob": -3.793,
            "prob": 0.0225
          },
          {
            "token": "ĠSchreib",
            "token_id": 189501,
            "logprob": -4.2773,
            "prob": 0.0139
          },
          {
            "token": "ĠBuch",
            "token_id": 19893,
            "logprob": -4.4102,
            "prob": 0.0122
          },
          {
            "token": "Ġvork",
            "token_id": 215633,
            "logprob": -4.4727,
            "prob": 0.0114
          },
          {
            "token": "ĠVork",
            "token_id": 234484,
            "logprob": -4.9258,
            "prob": 0.0073
          },
          {
            "token": "ĠVokal",
            "token_id": 179375,
            "logprob": -5.3398,
            "prob": 0.0048
          },
          {
            "token": "ĠVari",
            "token_id": 18323,
            "logprob": -5.457,
            "prob": 0.0043
          }
        ]
      },
      "selected_token": {
        "token": "Ġ\"",
        "token_id": 1789,
        "selection_method": "sampling"
      }
    },
    {
      "step": 27,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass alle \"",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass",
        "Ġalle",
        "Ġ\""
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760, 10528, 1789
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "e",
            "token_id": 76,
            "logprob": -0.0,
            "prob": 1.0
          },
          {
            "token": "i",
            "token_id": 80,
            "logprob": -14.7188,
            "prob": 0.0
          },
          {
            "token": "en",
            "token_id": 1673,
            "logprob": -14.8906,
            "prob": 0.0
          },
          {
            "token": "ei",
            "token_id": 17174,
            "logprob": -16.8906,
            "prob": 0.0
          },
          {
            "token": "E",
            "token_id": 44,
            "logprob": -16.9688,
            "prob": 0.0
          },
          {
            "token": "<SEP>",
            "token_id": 3,
            "logprob": -34.7812,
            "prob": 0.0
          },
          {
            "token": "<MASK_TOKEN>",
            "token_id": 4,
            "logprob": -35.2188,
            "prob": 0.0
          },
          {
            "token": "<CLS>",
            "token_id": 2,
            "logprob": -35.0625,
            "prob": 0.0
          },
          {
            "token": "<PAD>",
            "token_id": 0,
            "logprob": -35.2188,
            "prob": 0.0
          },
          {
            "token": "<UNK>",
            "token_id": 1,
            "logprob": -34.8438,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "e",
        "token_id": 76,
        "selection_method": "sampling"
      }
    },
    {
      "step": 28,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass alle \"e",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass",
        "Ġalle",
        "Ġ\"",
        "e"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760, 10528, 1789, 76
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "\"",
            "token_id": 9,
            "logprob": -0.0,
            "prob": 1.0
          },
          {
            "token": "\"-",
            "token_id": 64403,
            "logprob": -12.625,
            "prob": 0.0
          },
          {
            "token": "<PAD>",
            "token_id": 0,
            "logprob": -37.4062,
            "prob": 0.0
          },
          {
            "token": "<UNK>",
            "token_id": 1,
            "logprob": -37.375,
            "prob": 0.0
          },
          {
            "token": "<EOS_TOKEN>",
            "token_id": 6,
            "logprob": -33.4062,
            "prob": 0.0
          },
          {
            "token": "<EOP_TOKEN>",
            "token_id": 7,
            "logprob": -37.5312,
            "prob": 0.0
          },
          {
            "token": "<BOS_TOKEN>",
            "token_id": 5,
            "logprob": -34.4688,
            "prob": 0.0
          },
          {
            "token": "<MASK_TOKEN>",
            "token_id": 4,
            "logprob": -37.4688,
            "prob": 0.0
          },
          {
            "token": "<CLS>",
            "token_id": 2,
            "logprob": -37.3438,
            "prob": 0.0
          },
          {
            "token": "<SEP>",
            "token_id": 3,
            "logprob": -37.9688,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "\"",
        "token_id": 9,
        "selection_method": "sampling"
      }
    },
    {
      "step": 29,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass alle \"e\"",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass",
        "Ġalle",
        "Ġ\"",
        "e",
        "\""
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760, 10528, 1789, 76, 9
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġgez",
            "token_id": 23180,
            "logprob": -0.0051,
            "prob": 0.9951
          },
          {
            "token": "Ġmit",
            "token_id": 4665,
            "logprob": -5.957,
            "prob": 0.0026
          },
          {
            "token": "Ġberücksicht",
            "token_id": 233356,
            "logprob": -6.6289,
            "prob": 0.0013
          },
          {
            "token": "Ġzählen",
            "token_id": 166844,
            "logprob": -7.082,
            "prob": 0.0008
          },
          {
            "token": "Ġzählt",
            "token_id": 152779,
            "logprob": -9.4141,
            "prob": 0.0001
          },
          {
            "token": "Ġer",
            "token_id": 2347,
            "logprob": -9.6641,
            "prob": 0.0001
          },
          {
            "token": "Ġbetrachtet",
            "token_id": 193894,
            "logprob": -10.2578,
            "prob": 0.0
          },
          {
            "token": "gez",
            "token_id": 77530,
            "logprob": -10.5156,
            "prob": 0.0
          },
          {
            "token": "Ġgere",
            "token_id": 27346,
            "logprob": -10.7812,
            "prob": 0.0
          },
          {
            "token": "Ġzählte",
            "token_id": 188401,
            "logprob": -10.9219,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġgere",
        "token_id": 27346,
        "selection_method": "sampling"
      }
    },
    {
      "step": 30,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass alle \"e\" gere",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass",
        "Ġalle",
        "Ġ\"",
        "e",
        "\"",
        "Ġgere"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760, 10528, 1789, 76, 9, 27346
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "chnet",
            "token_id": 159065,
            "logprob": -0.0001,
            "prob": 1.0
          },
          {
            "token": "chent",
            "token_id": 96166,
            "logprob": -10.2656,
            "prob": 0.0
          },
          {
            "token": "cht",
            "token_id": 7378,
            "logprob": -10.5938,
            "prob": 0.0
          },
          {
            "token": "chn",
            "token_id": 3220,
            "logprob": -11.0781,
            "prob": 0.0
          },
          {
            "token": "cnt",
            "token_id": 46131,
            "logprob": -12.6719,
            "prob": 0.0
          },
          {
            "token": "chet",
            "token_id": 32868,
            "logprob": -12.9688,
            "prob": 0.0
          },
          {
            "token": "count",
            "token_id": 8601,
            "logprob": -13.0781,
            "prob": 0.0
          },
          {
            "token": "ch",
            "token_id": 1750,
            "logprob": -13.5859,
            "prob": 0.0
          },
          {
            "token": "chen",
            "token_id": 7788,
            "logprob": -13.7969,
            "prob": 0.0
          },
          {
            "token": "chnung",
            "token_id": 141660,
            "logprob": -14.3672,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "chnet",
        "token_id": 159065,
        "selection_method": "sampling"
      }
    },
    {
      "step": 31,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass alle \"e\" gerechnet",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass",
        "Ġalle",
        "Ġ\"",
        "e",
        "\"",
        "Ġgere",
        "chnet"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760, 10528, 1789, 76, 9, 27346, 159065
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġwerden",
            "token_id": 9178,
            "logprob": -0.0,
            "prob": 1.0
          },
          {
            "token": "Ġwurden",
            "token_id": 14935,
            "logprob": -12.9531,
            "prob": 0.0
          },
          {
            "token": "Ġsind",
            "token_id": 11382,
            "logprob": -13.0625,
            "prob": 0.0
          },
          {
            "token": "Ġwird",
            "token_id": 13252,
            "logprob": -15.25,
            "prob": 0.0
          },
          {
            "token": "Ġworden",
            "token_id": 12174,
            "logprob": -15.3125,
            "prob": 0.0
          },
          {
            "token": "Ġwerde",
            "token_id": 152398,
            "logprob": -16.3594,
            "prob": 0.0
          },
          {
            "token": "<CLS>",
            "token_id": 2,
            "logprob": -36.3438,
            "prob": 0.0
          },
          {
            "token": "<SEP>",
            "token_id": 3,
            "logprob": -36.5938,
            "prob": 0.0
          },
          {
            "token": "<UNK>",
            "token_id": 1,
            "logprob": -36.5625,
            "prob": 0.0
          },
          {
            "token": "<PAD>",
            "token_id": 0,
            "logprob": -36.2812,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġwerden",
        "token_id": 9178,
        "selection_method": "sampling"
      }
    },
    {
      "step": 32,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass alle \"e\" gerechnet werden",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass",
        "Ġalle",
        "Ġ\"",
        "e",
        "\"",
        "Ġgere",
        "chnet",
        "Ġwerden"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760, 10528, 1789, 76, 9, 27346, 159065, 9178
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": ")",
            "token_id": 16,
            "logprob": -0.0,
            "prob": 1.0
          },
          {
            "token": ",",
            "token_id": 19,
            "logprob": -13.3125,
            "prob": 0.0
          },
          {
            "token": "),",
            "token_id": 1969,
            "logprob": -14.2656,
            "prob": 0.0
          },
          {
            "token": "<PAD>",
            "token_id": 0,
            "logprob": -37.7188,
            "prob": 0.0
          },
          {
            "token": "<EOS_TOKEN>",
            "token_id": 6,
            "logprob": -29.1562,
            "prob": 0.0
          },
          {
            "token": "<SEP>",
            "token_id": 3,
            "logprob": -37.4688,
            "prob": 0.0
          },
          {
            "token": "<BOS_TOKEN>",
            "token_id": 5,
            "logprob": -38.25,
            "prob": 0.0
          },
          {
            "token": "<MASK_TOKEN>",
            "token_id": 4,
            "logprob": -37.7812,
            "prob": 0.0
          },
          {
            "token": "<UNK>",
            "token_id": 1,
            "logprob": -38.6875,
            "prob": 0.0
          },
          {
            "token": "<CLS>",
            "token_id": 2,
            "logprob": -37.5,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": ")",
        "token_id": 16,
        "selection_method": "sampling"
      }
    },
    {
      "step": 33,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass alle \"e\" gerechnet werden)",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass",
        "Ġalle",
        "Ġ\"",
        "e",
        "\"",
        "Ġgere",
        "chnet",
        "Ġwerden",
        ")"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760, 10528, 1789, 76, 9, 27346, 159065, 9178, 16
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġ",
            "token_id": 228,
            "logprob": -0.7744,
            "prob": 0.4609
          },
          {
            "token": "Ġ**",
            "token_id": 4434,
            "logprob": -1.2275,
            "prob": 0.293
          },
          {
            "token": "Ġvier",
            "token_id": 16675,
            "logprob": -2.2441,
            "prob": 0.1061
          },
          {
            "token": "Ġinsgesamt",
            "token_id": 66426,
            "logprob": -2.3848,
            "prob": 0.0922
          },
          {
            "token": "Ġfünf",
            "token_id": 44522,
            "logprob": -3.7441,
            "prob": 0.0237
          },
          {
            "token": "Ġdrei",
            "token_id": 26089,
            "logprob": -4.5078,
            "prob": 0.011
          },
          {
            "token": "Ġsechs",
            "token_id": 59235,
            "logprob": -4.8203,
            "prob": 0.0081
          },
          {
            "token": "Ġsieben",
            "token_id": 73723,
            "logprob": -5.8203,
            "prob": 0.003
          },
          {
            "token": "Ġzwei",
            "token_id": 20293,
            "logprob": -6.8906,
            "prob": 0.001
          },
          {
            "token": "Ġein",
            "token_id": 4001,
            "logprob": -8.6953,
            "prob": 0.0002
          }
        ]
      },
      "selected_token": {
        "token": "Ġfünf",
        "token_id": 44522,
        "selection_method": "sampling"
      }
    },
    {
      "step": 34,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass alle \"e\" gerechnet werden) fünf",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass",
        "Ġalle",
        "Ġ\"",
        "e",
        "\"",
        "Ġgere",
        "chnet",
        "Ġwerden",
        ")",
        "Ġfünf"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760, 10528, 1789, 76, 9, 27346, 159065, 9178, 16, 44522
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġ\"",
            "token_id": 1789,
            "logprob": -0.0254,
            "prob": 0.9751
          },
          {
            "token": "ĠBuch",
            "token_id": 19893,
            "logprob": -3.6973,
            "prob": 0.0248
          },
          {
            "token": "ĠSil",
            "token_id": 6823,
            "logprob": -10.2344,
            "prob": 0.0
          },
          {
            "token": "ĠZeichen",
            "token_id": 148772,
            "logprob": -10.4062,
            "prob": 0.0
          },
          {
            "token": "buch",
            "token_id": 22513,
            "logprob": -10.9375,
            "prob": 0.0
          },
          {
            "token": "Ġбуквы",
            "token_id": 216948,
            "logprob": -11.2344,
            "prob": 0.0
          },
          {
            "token": "ĠLetter",
            "token_id": 39638,
            "logprob": -11.3594,
            "prob": 0.0
          },
          {
            "token": "ĠAnf",
            "token_id": 80303,
            "logprob": -11.3594,
            "prob": 0.0
          },
          {
            "token": "ĠWörter",
            "token_id": 187284,
            "logprob": -11.4062,
            "prob": 0.0
          },
          {
            "token": "Ġбук",
            "token_id": 55780,
            "logprob": -11.4297,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "ĠBuch",
        "token_id": 19893,
        "selection_method": "sampling"
      }
    },
    {
      "step": 35,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass alle \"e\" gerechnet werden) fünf Buch",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass",
        "Ġalle",
        "Ġ\"",
        "e",
        "\"",
        "Ġgere",
        "chnet",
        "Ġwerden",
        ")",
        "Ġfünf",
        "ĠBuch"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760, 10528, 1789, 76, 9, 27346, 159065, 9178, 16, 44522,
        19893
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "staben",
            "token_id": 224913,
            "logprob": -0.0006,
            "prob": 0.9995
          },
          {
            "token": "stab",
            "token_id": 98460,
            "logprob": -7.9844,
            "prob": 0.0003
          },
          {
            "token": "st",
            "token_id": 1725,
            "logprob": -8.5,
            "prob": 0.0002
          },
          {
            "token": "sten",
            "token_id": 9310,
            "logprob": -10.3438,
            "prob": 0.0
          },
          {
            "token": "stellen",
            "token_id": 64690,
            "logprob": -11.625,
            "prob": 0.0
          },
          {
            "token": "stä",
            "token_id": 66846,
            "logprob": -11.75,
            "prob": 0.0
          },
          {
            "token": "stan",
            "token_id": 10869,
            "logprob": -12.7656,
            "prob": 0.0
          },
          {
            "token": "stoffe",
            "token_id": 174722,
            "logprob": -13.6797,
            "prob": 0.0
          },
          {
            "token": "sen",
            "token_id": 8488,
            "logprob": -13.8438,
            "prob": 0.0
          },
          {
            "token": "strecken",
            "token_id": 187777,
            "logprob": -14.25,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "st",
        "token_id": 1725,
        "selection_method": "sampling"
      }
    },
    {
      "step": 36,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass alle \"e\" gerechnet werden) fünf Buchst",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass",
        "Ġalle",
        "Ġ\"",
        "e",
        "\"",
        "Ġgere",
        "chnet",
        "Ġwerden",
        ")",
        "Ġfünf",
        "ĠBuch",
        "st"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760, 10528, 1789, 76, 9, 27346, 159065, 9178, 16, 44522,
        19893, 1725
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "abe",
            "token_id": 9424,
            "logprob": -0.0006,
            "prob": 0.9995
          },
          {
            "token": "aben",
            "token_id": 35346,
            "logprob": -7.75,
            "prob": 0.0004
          },
          {
            "token": "aber",
            "token_id": 37151,
            "logprob": -9.4219,
            "prob": 0.0001
          },
          {
            "token": "abb",
            "token_id": 24343,
            "logprob": -11.8438,
            "prob": 0.0
          },
          {
            "token": "abre",
            "token_id": 81358,
            "logprob": -11.875,
            "prob": 0.0
          },
          {
            "token": "aba",
            "token_id": 5538,
            "logprob": -12.3125,
            "prob": 0.0
          },
          {
            "token": "abin",
            "token_id": 51189,
            "logprob": -12.3906,
            "prob": 0.0
          },
          {
            "token": "abel",
            "token_id": 3789,
            "logprob": -12.5469,
            "prob": 0.0
          },
          {
            "token": "abet",
            "token_id": 26563,
            "logprob": -12.625,
            "prob": 0.0
          },
          {
            "token": "abor",
            "token_id": 4643,
            "logprob": -13.0078,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "aben",
        "token_id": 35346,
        "selection_method": "sampling"
      }
    },
    {
      "step": 37,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass alle \"e\" gerechnet werden) fünf Buchstaben",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass",
        "Ġalle",
        "Ġ\"",
        "e",
        "\"",
        "Ġgere",
        "chnet",
        "Ġwerden",
        ")",
        "Ġfünf",
        "ĠBuch",
        "st",
        "aben"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760, 10528, 1789, 76, 9, 27346, 159065, 9178, 16, 44522,
        19893, 1725, 35346
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġ\"",
            "token_id": 1789,
            "logprob": -0.0,
            "prob": 1.0
          },
          {
            "token": "Ġ„",
            "token_id": 4352,
            "logprob": -16.3438,
            "prob": 0.0
          },
          {
            "token": "Ġ'",
            "token_id": 2155,
            "logprob": -17.0156,
            "prob": 0.0
          },
          {
            "token": "<PAD>",
            "token_id": 0,
            "logprob": -38.4375,
            "prob": 0.0
          },
          {
            "token": "<EOS_TOKEN>",
            "token_id": 6,
            "logprob": -36.0938,
            "prob": 0.0
          },
          {
            "token": "<SEP>",
            "token_id": 3,
            "logprob": -38.5625,
            "prob": 0.0
          },
          {
            "token": "<BOS_TOKEN>",
            "token_id": 5,
            "logprob": -36.0312,
            "prob": 0.0
          },
          {
            "token": "<MASK_TOKEN>",
            "token_id": 4,
            "logprob": -38.4062,
            "prob": 0.0
          },
          {
            "token": "<UNK>",
            "token_id": 1,
            "logprob": -36.8438,
            "prob": 0.0
          },
          {
            "token": "<CLS>",
            "token_id": 2,
            "logprob": -38.25,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġ\"",
        "token_id": 1789,
        "selection_method": "sampling"
      }
    },
    {
      "step": 38,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass alle \"e\" gerechnet werden) fünf Buchstaben \"",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass",
        "Ġalle",
        "Ġ\"",
        "e",
        "\"",
        "Ġgere",
        "chnet",
        "Ġwerden",
        ")",
        "Ġfünf",
        "ĠBuch",
        "st",
        "aben",
        "Ġ\""
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760, 10528, 1789, 76, 9, 27346, 159065, 9178, 16, 44522,
        19893, 1725, 35346, 1789
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "e",
            "token_id": 76,
            "logprob": 0.0,
            "prob": 1.0
          },
          {
            "token": "<UNK>",
            "token_id": 1,
            "logprob": -42.8125,
            "prob": 0.0
          },
          {
            "token": "<SEP>",
            "token_id": 3,
            "logprob": -43.9688,
            "prob": 0.0
          },
          {
            "token": "<EOS_TOKEN>",
            "token_id": 6,
            "logprob": -36.5938,
            "prob": 0.0
          },
          {
            "token": "<EOP_TOKEN>",
            "token_id": 7,
            "logprob": -44.0625,
            "prob": 0.0
          },
          {
            "token": "<PAD>",
            "token_id": 0,
            "logprob": -44.0312,
            "prob": 0.0
          },
          {
            "token": "<CLS>",
            "token_id": 2,
            "logprob": -43.4375,
            "prob": 0.0
          },
          {
            "token": "<BOS_TOKEN>",
            "token_id": 5,
            "logprob": -42.625,
            "prob": 0.0
          },
          {
            "token": "!",
            "token_id": 8,
            "logprob": -39.8438,
            "prob": 0.0
          },
          {
            "token": "<MASK_TOKEN>",
            "token_id": 4,
            "logprob": -44.0625,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "e",
        "token_id": 76,
        "selection_method": "sampling"
      }
    },
    {
      "step": 39,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass alle \"e\" gerechnet werden) fünf Buchstaben \"e",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass",
        "Ġalle",
        "Ġ\"",
        "e",
        "\"",
        "Ġgere",
        "chnet",
        "Ġwerden",
        ")",
        "Ġfünf",
        "ĠBuch",
        "st",
        "aben",
        "Ġ\"",
        "e"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760, 10528, 1789, 76, 9, 27346, 159065, 9178, 16, 44522,
        19893, 1725, 35346, 1789, 76
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "\".",
            "token_id": 3193,
            "logprob": -0.0024,
            "prob": 0.9976
          },
          {
            "token": "\":",
            "token_id": 2209,
            "logprob": -6.0195,
            "prob": 0.0024
          },
          {
            "token": "\"",
            "token_id": 9,
            "logprob": -12.7344,
            "prob": 0.0
          },
          {
            "token": "Ġ\".",
            "token_id": 24029,
            "logprob": -13.0625,
            "prob": 0.0
          },
          {
            "token": "\",",
            "token_id": 2040,
            "logprob": -14.7812,
            "prob": 0.0
          },
          {
            "token": "\").",
            "token_id": 9044,
            "logprob": -15.1094,
            "prob": 0.0
          },
          {
            "token": "\"。",
            "token_id": 188135,
            "logprob": -15.3281,
            "prob": 0.0
          },
          {
            "token": "\"!",
            "token_id": 188622,
            "logprob": -15.5,
            "prob": 0.0
          },
          {
            "token": "“.",
            "token_id": 23670,
            "logprob": -15.5781,
            "prob": 0.0
          },
          {
            "token": ".\"",
            "token_id": 2875,
            "logprob": -15.4375,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "\".",
        "token_id": 3193,
        "selection_method": "sampling"
      }
    },
    {
      "step": 40,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass alle \"e\" gerechnet werden) fünf Buchstaben \"e\".",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass",
        "Ġalle",
        "Ġ\"",
        "e",
        "\"",
        "Ġgere",
        "chnet",
        "Ġwerden",
        ")",
        "Ġfünf",
        "ĠBuch",
        "st",
        "aben",
        "Ġ\"",
        "e",
        "\"."
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760, 10528, 1789, 76, 9, 27346, 159065, 9178, 16, 44522,
        19893, 1725, 35346, 1789, 76, 3193
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "<|END_OF_TURN_TOKEN|>",
            "token_id": 255001,
            "logprob": -0.7612,
            "prob": 0.467
          },
          {
            "token": "\n",
            "token_id": 206,
            "logprob": -1.3711,
            "prob": 0.2539
          },
          {
            "token": "Ġ\n",
            "token_id": 1973,
            "logprob": -2.2461,
            "prob": 0.1058
          },
          {
            "token": "ĠSie",
            "token_id": 14317,
            "logprob": -2.2617,
            "prob": 0.1042
          },
          {
            "token": "ĠDie",
            "token_id": 5393,
            "logprob": -3.2305,
            "prob": 0.0396
          },
          {
            "token": "ĠHier",
            "token_id": 23396,
            "logprob": -3.9648,
            "prob": 0.019
          },
          {
            "token": "Ġ \n",
            "token_id": 8261,
            "logprob": -5.168,
            "prob": 0.0057
          },
          {
            "token": "ĠDiese",
            "token_id": 40658,
            "logprob": -6.7539,
            "prob": 0.0012
          },
          {
            "token": "ĠVer",
            "token_id": 4954,
            "logprob": -6.9102,
            "prob": 0.001
          },
          {
            "token": "ĠEs",
            "token_id": 6154,
            "logprob": -7.0977,
            "prob": 0.0008
          }
        ]
      },
      "selected_token": {
        "token": "Ġ\n",
        "token_id": 1973,
        "selection_method": "sampling"
      }
    },
    {
      "step": 41,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass alle \"e\" gerechnet werden) fünf Buchstaben \"e\". \n",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass",
        "Ġalle",
        "Ġ\"",
        "e",
        "\"",
        "Ġgere",
        "chnet",
        "Ġwerden",
        ")",
        "Ġfünf",
        "ĠBuch",
        "st",
        "aben",
        "Ġ\"",
        "e",
        "\".",
        "Ġ\n"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760, 10528, 1789, 76, 9, 27346, 159065, 9178, 16, 44522,
        19893, 1725, 35346, 1789, 76, 3193, 1973
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "\n",
            "token_id": 206,
            "logprob": 0.0,
            "prob": 1.0
          },
          {
            "token": "<UNK>",
            "token_id": 1,
            "logprob": -36.7188,
            "prob": 0.0
          },
          {
            "token": "<SEP>",
            "token_id": 3,
            "logprob": -36.6875,
            "prob": 0.0
          },
          {
            "token": "<EOS_TOKEN>",
            "token_id": 6,
            "logprob": -24.0938,
            "prob": 0.0
          },
          {
            "token": "<EOP_TOKEN>",
            "token_id": 7,
            "logprob": -37.5312,
            "prob": 0.0
          },
          {
            "token": "<PAD>",
            "token_id": 0,
            "logprob": -37.5312,
            "prob": 0.0
          },
          {
            "token": "<CLS>",
            "token_id": 2,
            "logprob": -37.3438,
            "prob": 0.0
          },
          {
            "token": "<BOS_TOKEN>",
            "token_id": 5,
            "logprob": -34.2188,
            "prob": 0.0
          },
          {
            "token": "!",
            "token_id": 8,
            "logprob": -33.5625,
            "prob": 0.0
          },
          {
            "token": "<MASK_TOKEN>",
            "token_id": 4,
            "logprob": -37.5312,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "\n",
        "token_id": 206,
        "selection_method": "sampling"
      }
    },
    {
      "step": 42,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass alle \"e\" gerechnet werden) fünf Buchstaben \"e\". \n\n",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass",
        "Ġalle",
        "Ġ\"",
        "e",
        "\"",
        "Ġgere",
        "chnet",
        "Ġwerden",
        ")",
        "Ġfünf",
        "ĠBuch",
        "st",
        "aben",
        "Ġ\"",
        "e",
        "\".",
        "Ġ\n",
        "\n"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760, 10528, 1789, 76, 9, 27346, 159065, 9178, 16, 44522,
        19893, 1725, 35346, 1789, 76, 3193, 1973, 206
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Sie",
            "token_id": 34677,
            "logprob": -0.3005,
            "prob": 0.7402
          },
          {
            "token": "Die",
            "token_id": 10749,
            "logprob": -2.2227,
            "prob": 0.1083
          },
          {
            "token": "Hier",
            "token_id": 52729,
            "logprob": -3.082,
            "prob": 0.0459
          },
          {
            "token": "Z",
            "token_id": 65,
            "logprob": -3.2695,
            "prob": 0.038
          },
          {
            "token": "-",
            "token_id": 20,
            "logprob": -3.8164,
            "prob": 0.022
          },
          {
            "token": "Ver",
            "token_id": 7970,
            "logprob": -3.8164,
            "prob": 0.022
          },
          {
            "token": "*",
            "token_id": 17,
            "logprob": -5.582,
            "prob": 0.0038
          },
          {
            "token": "I",
            "token_id": 48,
            "logprob": -5.7305,
            "prob": 0.0032
          },
          {
            "token": "Auf",
            "token_id": 51214,
            "logprob": -5.8867,
            "prob": 0.0028
          },
          {
            "token": "Das",
            "token_id": 21930,
            "logprob": -6.082,
            "prob": 0.0023
          }
        ]
      },
      "selected_token": {
        "token": "Auf",
        "token_id": 51214,
        "selection_method": "sampling"
      }
    },
    {
      "step": 43,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass alle \"e\" gerechnet werden) fünf Buchstaben \"e\". \n\nAuf",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass",
        "Ġalle",
        "Ġ\"",
        "e",
        "\"",
        "Ġgere",
        "chnet",
        "Ġwerden",
        ")",
        "Ġfünf",
        "ĠBuch",
        "st",
        "aben",
        "Ġ\"",
        "e",
        "\".",
        "Ġ\n",
        "\n",
        "Auf"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760, 10528, 1789, 76, 9, 27346, 159065, 9178, 16, 44522,
        19893, 1725, 35346, 1789, 76, 3193, 1973, 206, 51214
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "gesch",
            "token_id": 19238,
            "logprob": -0.6514,
            "prob": 0.5215
          },
          {
            "token": "z",
            "token_id": 97,
            "logprob": -1.9014,
            "prob": 0.1493
          },
          {
            "token": "geteilt",
            "token_id": 227539,
            "logprob": -2.043,
            "prob": 0.1298
          },
          {
            "token": "gez",
            "token_id": 77530,
            "logprob": -2.6055,
            "prob": 0.0739
          },
          {
            "token": "ge",
            "token_id": 2035,
            "logprob": -3.3086,
            "prob": 0.0366
          },
          {
            "token": "ges",
            "token_id": 5325,
            "logprob": -3.3398,
            "prob": 0.0355
          },
          {
            "token": "ged",
            "token_id": 7994,
            "logprob": -4.0586,
            "prob": 0.0173
          },
          {
            "token": "get",
            "token_id": 2214,
            "logprob": -4.9805,
            "prob": 0.0069
          },
          {
            "token": "g",
            "token_id": 78,
            "logprob": -5.1992,
            "prob": 0.0055
          },
          {
            "token": "gel",
            "token_id": 15746,
            "logprob": -5.5586,
            "prob": 0.0039
          }
        ]
      },
      "selected_token": {
        "token": "z",
        "token_id": 97,
        "selection_method": "sampling"
      }
    },
    {
      "step": 44,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass alle \"e\" gerechnet werden) fünf Buchstaben \"e\". \n\nAufz",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass",
        "Ġalle",
        "Ġ\"",
        "e",
        "\"",
        "Ġgere",
        "chnet",
        "Ġwerden",
        ")",
        "Ġfünf",
        "ĠBuch",
        "st",
        "aben",
        "Ġ\"",
        "e",
        "\".",
        "Ġ\n",
        "\n",
        "Auf",
        "z"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760, 10528, 1789, 76, 9, 27346, 159065, 9178, 16, 44522,
        19893, 1725, 35346, 1789, 76, 3193, 1973, 206, 51214, 97
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "ähl",
            "token_id": 41330,
            "logprob": -0.3269,
            "prob": 0.7212
          },
          {
            "token": "ählen",
            "token_id": 113076,
            "logprob": -1.4053,
            "prob": 0.2454
          },
          {
            "token": "uz",
            "token_id": 4147,
            "logprob": -4.25,
            "prob": 0.0143
          },
          {
            "token": "ählt",
            "token_id": 41963,
            "logprob": -4.9375,
            "prob": 0.0072
          },
          {
            "token": "äh",
            "token_id": 21073,
            "logprob": -5.0938,
            "prob": 0.0061
          },
          {
            "token": "ä",
            "token_id": 2689,
            "logprob": -5.9062,
            "prob": 0.0027
          },
          {
            "token": "äl",
            "token_id": 30894,
            "logprob": -6.5859,
            "prob": 0.0014
          },
          {
            "token": "ähler",
            "token_id": 137112,
            "logprob": -7.9219,
            "prob": 0.0004
          },
          {
            "token": "ählte",
            "token_id": 100120,
            "logprob": -8.9141,
            "prob": 0.0001
          },
          {
            "token": "ü",
            "token_id": 2073,
            "logprob": -8.9531,
            "prob": 0.0001
          }
        ]
      },
      "selected_token": {
        "token": "ähler",
        "token_id": 137112,
        "selection_method": "sampling"
      }
    },
    {
      "step": 45,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass alle \"e\" gerechnet werden) fünf Buchstaben \"e\". \n\nAufzähler",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass",
        "Ġalle",
        "Ġ\"",
        "e",
        "\"",
        "Ġgere",
        "chnet",
        "Ġwerden",
        ")",
        "Ġfünf",
        "ĠBuch",
        "st",
        "aben",
        "Ġ\"",
        "e",
        "\".",
        "Ġ\n",
        "\n",
        "Auf",
        "z",
        "ähler"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760, 10528, 1789, 76, 9, 27346, 159065, 9178, 16, 44522,
        19893, 1725, 35346, 1789, 76, 3193, 1973, 206, 51214, 97, 137112
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": ":",
            "token_id": 33,
            "logprob": -0.4744,
            "prob": 0.6226
          },
          {
            "token": "isch",
            "token_id": 3418,
            "logprob": -1.0059,
            "prob": 0.366
          },
          {
            "token": "ische",
            "token_id": 5160,
            "logprob": -4.6758,
            "prob": 0.0093
          },
          {
            "token": "Ġsind",
            "token_id": 11382,
            "logprob": -7.3945,
            "prob": 0.0006
          },
          {
            "token": "ischer",
            "token_id": 8712,
            "logprob": -7.6836,
            "prob": 0.0005
          },
          {
            "token": "ischen",
            "token_id": 5456,
            "logprob": -8.3516,
            "prob": 0.0002
          },
          {
            "token": "ich",
            "token_id": 1972,
            "logprob": -8.6953,
            "prob": 0.0002
          },
          {
            "token": "n",
            "token_id": 85,
            "logprob": -8.9766,
            "prob": 0.0001
          },
          {
            "token": "isches",
            "token_id": 24804,
            "logprob": -8.9844,
            "prob": 0.0001
          },
          {
            "token": "weise",
            "token_id": 12539,
            "logprob": -9.2656,
            "prob": 0.0001
          }
        ]
      },
      "selected_token": {
        "token": "isches",
        "token_id": 24804,
        "selection_method": "sampling"
      }
    },
    {
      "step": 46,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass alle \"e\" gerechnet werden) fünf Buchstaben \"e\". \n\nAufzählerisches",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass",
        "Ġalle",
        "Ġ\"",
        "e",
        "\"",
        "Ġgere",
        "chnet",
        "Ġwerden",
        ")",
        "Ġfünf",
        "ĠBuch",
        "st",
        "aben",
        "Ġ\"",
        "e",
        "\".",
        "Ġ\n",
        "\n",
        "Auf",
        "z",
        "ähler",
        "isches"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760, 10528, 1789, 76, 9, 27346, 159065, 9178, 16, 44522,
        19893, 1725, 35346, 1789, 76, 3193, 1973, 206, 51214, 97, 137112, 24804
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "ĠZ",
            "token_id": 2358,
            "logprob": -0.3596,
            "prob": 0.6978
          },
          {
            "token": "ĠV",
            "token_id": 1908,
            "logprob": -1.9062,
            "prob": 0.1486
          },
          {
            "token": "ĠDurch",
            "token_id": 39450,
            "logprob": -2.5156,
            "prob": 0.0808
          },
          {
            "token": "ĠÜber",
            "token_id": 32815,
            "logprob": -3.8672,
            "prob": 0.0209
          },
          {
            "token": "ĠBeispiel",
            "token_id": 69619,
            "logprob": -4.3828,
            "prob": 0.0125
          },
          {
            "token": "ĠEr",
            "token_id": 4120,
            "logprob": -4.9219,
            "prob": 0.0073
          },
          {
            "token": "ĠVer",
            "token_id": 4954,
            "logprob": -5.6094,
            "prob": 0.0037
          },
          {
            "token": ":",
            "token_id": 33,
            "logprob": -5.7891,
            "prob": 0.0031
          },
          {
            "token": "ĠZer",
            "token_id": 80524,
            "logprob": -6.1719,
            "prob": 0.0021
          },
          {
            "token": "ĠVerständ",
            "token_id": 251869,
            "logprob": -6.6016,
            "prob": 0.0014
          }
        ]
      },
      "selected_token": {
        "token": ":",
        "token_id": 33,
        "selection_method": "sampling"
      }
    },
    {
      "step": 47,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass alle \"e\" gerechnet werden) fünf Buchstaben \"e\". \n\nAufzählerisches:",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass",
        "Ġalle",
        "Ġ\"",
        "e",
        "\"",
        "Ġgere",
        "chnet",
        "Ġwerden",
        ")",
        "Ġfünf",
        "ĠBuch",
        "st",
        "aben",
        "Ġ\"",
        "e",
        "\".",
        "Ġ\n",
        "\n",
        "Auf",
        "z",
        "ähler",
        "isches",
        ":"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760, 10528, 1789, 76, 9, 27346, 159065, 9178, 16, 44522,
        19893, 1725, 35346, 1789, 76, 3193, 1973, 206, 51214, 97, 137112, 24804, 33
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "\n",
            "token_id": 206,
            "logprob": -0.0,
            "prob": 1.0
          },
          {
            "token": "\n\n",
            "token_id": 2126,
            "logprob": -11.5156,
            "prob": 0.0
          },
          {
            "token": "ĠGesch",
            "token_id": 17592,
            "logprob": -12.3906,
            "prob": 0.0
          },
          {
            "token": "Ġ\n",
            "token_id": 1973,
            "logprob": -12.5938,
            "prob": 0.0
          },
          {
            "token": "Ġ",
            "token_id": 228,
            "logprob": -13.125,
            "prob": 0.0
          },
          {
            "token": "Ġ\"",
            "token_id": 1789,
            "logprob": -14.3281,
            "prob": 0.0
          },
          {
            "token": "\n ",
            "token_id": 1868,
            "logprob": -15.0312,
            "prob": 0.0
          },
          {
            "token": "Ġgesch",
            "token_id": 18831,
            "logprob": -15.375,
            "prob": 0.0
          },
          {
            "token": "<PAD>",
            "token_id": 0,
            "logprob": -35.0312,
            "prob": 0.0
          },
          {
            "token": "<UNK>",
            "token_id": 1,
            "logprob": -34.1562,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġ\n",
        "token_id": 1973,
        "selection_method": "sampling"
      }
    },
    {
      "step": 48,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass alle \"e\" gerechnet werden) fünf Buchstaben \"e\". \n\nAufzählerisches: \n",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass",
        "Ġalle",
        "Ġ\"",
        "e",
        "\"",
        "Ġgere",
        "chnet",
        "Ġwerden",
        ")",
        "Ġfünf",
        "ĠBuch",
        "st",
        "aben",
        "Ġ\"",
        "e",
        "\".",
        "Ġ\n",
        "\n",
        "Auf",
        "z",
        "ähler",
        "isches",
        ":",
        "Ġ\n"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760, 10528, 1789, 76, 9, 27346, 159065, 9178, 16, 44522,
        19893, 1725, 35346, 1789, 76, 3193, 1973, 206, 51214, 97, 137112, 24804, 33, 1973
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "\n",
            "token_id": 206,
            "logprob": -0.0004,
            "prob": 0.9995
          },
          {
            "token": "1",
            "token_id": 24,
            "logprob": -7.8906,
            "prob": 0.0004
          },
          {
            "token": "<PAD>",
            "token_id": 0,
            "logprob": -40.4375,
            "prob": 0.0
          },
          {
            "token": "<UNK>",
            "token_id": 1,
            "logprob": -39.0312,
            "prob": 0.0
          },
          {
            "token": "<EOS_TOKEN>",
            "token_id": 6,
            "logprob": -24.8281,
            "prob": 0.0
          },
          {
            "token": "<EOP_TOKEN>",
            "token_id": 7,
            "logprob": -40.4688,
            "prob": 0.0
          },
          {
            "token": "<BOS_TOKEN>",
            "token_id": 5,
            "logprob": -37.9375,
            "prob": 0.0
          },
          {
            "token": "<MASK_TOKEN>",
            "token_id": 4,
            "logprob": -40.4688,
            "prob": 0.0
          },
          {
            "token": "<CLS>",
            "token_id": 2,
            "logprob": -39.5625,
            "prob": 0.0
          },
          {
            "token": "<SEP>",
            "token_id": 3,
            "logprob": -39.5312,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "\n",
        "token_id": 206,
        "selection_method": "sampling"
      }
    },
    {
      "step": 49,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass alle \"e\" gerechnet werden) fünf Buchstaben \"e\". \n\nAufzählerisches: \n\n",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass",
        "Ġalle",
        "Ġ\"",
        "e",
        "\"",
        "Ġgere",
        "chnet",
        "Ġwerden",
        ")",
        "Ġfünf",
        "ĠBuch",
        "st",
        "aben",
        "Ġ\"",
        "e",
        "\".",
        "Ġ\n",
        "\n",
        "Auf",
        "z",
        "ähler",
        "isches",
        ":",
        "Ġ\n",
        "\n"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760, 10528, 1789, 76, 9, 27346, 159065, 9178, 16, 44522,
        19893, 1725, 35346, 1789, 76, 3193, 1973, 206, 51214, 97, 137112, 24804, 33, 1973, 206
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "-",
            "token_id": 20,
            "logprob": -0.3484,
            "prob": 0.7056
          },
          {
            "token": "*",
            "token_id": 17,
            "logprob": -1.2236,
            "prob": 0.2942
          },
          {
            "token": "Gesch",
            "token_id": 23361,
            "logprob": -15.0547,
            "prob": 0.0
          },
          {
            "token": "\"",
            "token_id": 9,
            "logprob": -16.4375,
            "prob": 0.0
          },
          {
            "token": "**",
            "token_id": 2093,
            "logprob": -16.5938,
            "prob": 0.0
          },
          {
            "token": "<SEP>",
            "token_id": 3,
            "logprob": -35.125,
            "prob": 0.0
          },
          {
            "token": "<MASK_TOKEN>",
            "token_id": 4,
            "logprob": -34.4375,
            "prob": 0.0
          },
          {
            "token": "<CLS>",
            "token_id": 2,
            "logprob": -34.7812,
            "prob": 0.0
          },
          {
            "token": "<PAD>",
            "token_id": 0,
            "logprob": -34.4688,
            "prob": 0.0
          },
          {
            "token": "<UNK>",
            "token_id": 1,
            "logprob": -35.4375,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "-",
        "token_id": 20,
        "selection_method": "sampling"
      }
    },
    {
      "step": 50,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass alle \"e\" gerechnet werden) fünf Buchstaben \"e\". \n\nAufzählerisches: \n\n-",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass",
        "Ġalle",
        "Ġ\"",
        "e",
        "\"",
        "Ġgere",
        "chnet",
        "Ġwerden",
        ")",
        "Ġfünf",
        "ĠBuch",
        "st",
        "aben",
        "Ġ\"",
        "e",
        "\".",
        "Ġ\n",
        "\n",
        "Auf",
        "z",
        "ähler",
        "isches",
        ":",
        "Ġ\n",
        "\n",
        "-"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760, 10528, 1789, 76, 9, 27346, 159065, 9178, 16, 44522,
        19893, 1725, 35346, 1789, 76, 3193, 1973, 206, 51214, 97, 137112, 24804, 33, 1973, 206, 20
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "ĠGesch",
            "token_id": 17592,
            "logprob": -0.0012,
            "prob": 0.999
          },
          {
            "token": "Ġ\"",
            "token_id": 1789,
            "logprob": -7.1406,
            "prob": 0.0008
          },
          {
            "token": "Ġgesch",
            "token_id": 18831,
            "logprob": -8.5781,
            "prob": 0.0002
          },
          {
            "token": "ĠGes",
            "token_id": 17062,
            "logprob": -9.9531,
            "prob": 0.0
          },
          {
            "token": "ĠSch",
            "token_id": 2881,
            "logprob": -10.0938,
            "prob": 0.0
          },
          {
            "token": "Gesch",
            "token_id": 23361,
            "logprob": -10.3594,
            "prob": 0.0
          },
          {
            "token": "Ġzwei",
            "token_id": 20293,
            "logprob": -10.75,
            "prob": 0.0
          },
          {
            "token": "Ġe",
            "token_id": 1739,
            "logprob": -10.8594,
            "prob": 0.0
          },
          {
            "token": "ĠBesch",
            "token_id": 53128,
            "logprob": -11.6094,
            "prob": 0.0
          },
          {
            "token": "Ġ",
            "token_id": 228,
            "logprob": -11.7109,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġzwei",
        "token_id": 20293,
        "selection_method": "sampling"
      }
    },
    {
      "step": 51,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass alle \"e\" gerechnet werden) fünf Buchstaben \"e\". \n\nAufzählerisches: \n\n- zwei",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass",
        "Ġalle",
        "Ġ\"",
        "e",
        "\"",
        "Ġgere",
        "chnet",
        "Ġwerden",
        ")",
        "Ġfünf",
        "ĠBuch",
        "st",
        "aben",
        "Ġ\"",
        "e",
        "\".",
        "Ġ\n",
        "\n",
        "Auf",
        "z",
        "ähler",
        "isches",
        ":",
        "Ġ\n",
        "\n",
        "-",
        "Ġzwei"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760, 10528, 1789, 76, 9, 27346, 159065, 9178, 16, 44522,
        19893, 1725, 35346, 1789, 76, 3193, 1973, 206, 51214, 97, 137112, 24804, 33, 1973, 206, 20,
        20293
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġ\"",
            "token_id": 1789,
            "logprob": -0.0009,
            "prob": 0.999
          },
          {
            "token": "Ġin",
            "token_id": 1709,
            "logprob": -7.625,
            "prob": 0.0005
          },
          {
            "token": "Ġ",
            "token_id": 228,
            "logprob": -9.4453,
            "prob": 0.0001
          },
          {
            "token": "ĠMal",
            "token_id": 5126,
            "logprob": -9.4531,
            "prob": 0.0001
          },
          {
            "token": "Ġdire",
            "token_id": 5245,
            "logprob": -9.4844,
            "prob": 0.0001
          },
          {
            "token": "Ġmal",
            "token_id": 7241,
            "logprob": -9.8125,
            "prob": 0.0001
          },
          {
            "token": "Ġdirekt",
            "token_id": 38725,
            "logprob": -10.2266,
            "prob": 0.0
          },
          {
            "token": "Ġ„",
            "token_id": 4352,
            "logprob": -11.1719,
            "prob": 0.0
          },
          {
            "token": "Ġim",
            "token_id": 2101,
            "logprob": -11.4062,
            "prob": 0.0
          },
          {
            "token": "Ġ'",
            "token_id": 2155,
            "logprob": -12.1016,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġ\"",
        "token_id": 1789,
        "selection_method": "sampling"
      }
    },
    {
      "step": 52,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass alle \"e\" gerechnet werden) fünf Buchstaben \"e\". \n\nAufzählerisches: \n\n- zwei \"",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass",
        "Ġalle",
        "Ġ\"",
        "e",
        "\"",
        "Ġgere",
        "chnet",
        "Ġwerden",
        ")",
        "Ġfünf",
        "ĠBuch",
        "st",
        "aben",
        "Ġ\"",
        "e",
        "\".",
        "Ġ\n",
        "\n",
        "Auf",
        "z",
        "ähler",
        "isches",
        ":",
        "Ġ\n",
        "\n",
        "-",
        "Ġzwei",
        "Ġ\""
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760, 10528, 1789, 76, 9, 27346, 159065, 9178, 16, 44522,
        19893, 1725, 35346, 1789, 76, 3193, 1973, 206, 51214, 97, 137112, 24804, 33, 1973, 206, 20,
        20293, 1789
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "e",
            "token_id": 76,
            "logprob": 0.0,
            "prob": 1.0
          },
          {
            "token": "<UNK>",
            "token_id": 1,
            "logprob": -40.0938,
            "prob": 0.0
          },
          {
            "token": "<SEP>",
            "token_id": 3,
            "logprob": -40.5,
            "prob": 0.0
          },
          {
            "token": "<EOS_TOKEN>",
            "token_id": 6,
            "logprob": -36.3125,
            "prob": 0.0
          },
          {
            "token": "<EOP_TOKEN>",
            "token_id": 7,
            "logprob": -40.6875,
            "prob": 0.0
          },
          {
            "token": "<PAD>",
            "token_id": 0,
            "logprob": -40.5625,
            "prob": 0.0
          },
          {
            "token": "<CLS>",
            "token_id": 2,
            "logprob": -40.2812,
            "prob": 0.0
          },
          {
            "token": "<BOS_TOKEN>",
            "token_id": 5,
            "logprob": -38.5938,
            "prob": 0.0
          },
          {
            "token": "!",
            "token_id": 8,
            "logprob": -38.1562,
            "prob": 0.0
          },
          {
            "token": "<MASK_TOKEN>",
            "token_id": 4,
            "logprob": -40.6875,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "e",
        "token_id": 76,
        "selection_method": "sampling"
      }
    },
    {
      "step": 53,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass alle \"e\" gerechnet werden) fünf Buchstaben \"e\". \n\nAufzählerisches: \n\n- zwei \"e",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass",
        "Ġalle",
        "Ġ\"",
        "e",
        "\"",
        "Ġgere",
        "chnet",
        "Ġwerden",
        ")",
        "Ġfünf",
        "ĠBuch",
        "st",
        "aben",
        "Ġ\"",
        "e",
        "\".",
        "Ġ\n",
        "\n",
        "Auf",
        "z",
        "ähler",
        "isches",
        ":",
        "Ġ\n",
        "\n",
        "-",
        "Ġzwei",
        "Ġ\"",
        "e"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760, 10528, 1789, 76, 9, 27346, 159065, 9178, 16, 44522,
        19893, 1725, 35346, 1789, 76, 3193, 1973, 206, 51214, 97, 137112, 24804, 33, 1973, 206, 20,
        20293, 1789, 76
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "\"",
            "token_id": 9,
            "logprob": -0.0,
            "prob": 1.0
          },
          {
            "token": "\"-",
            "token_id": 64403,
            "logprob": -14.6406,
            "prob": 0.0
          },
          {
            "token": "\",",
            "token_id": 2040,
            "logprob": -16.0,
            "prob": 0.0
          },
          {
            "token": "<PAD>",
            "token_id": 0,
            "logprob": -39.4062,
            "prob": 0.0
          },
          {
            "token": "<EOS_TOKEN>",
            "token_id": 6,
            "logprob": -32.5,
            "prob": 0.0
          },
          {
            "token": "<SEP>",
            "token_id": 3,
            "logprob": -40.0938,
            "prob": 0.0
          },
          {
            "token": "<BOS_TOKEN>",
            "token_id": 5,
            "logprob": -38.4688,
            "prob": 0.0
          },
          {
            "token": "<MASK_TOKEN>",
            "token_id": 4,
            "logprob": -39.4375,
            "prob": 0.0
          },
          {
            "token": "<UNK>",
            "token_id": 1,
            "logprob": -38.8125,
            "prob": 0.0
          },
          {
            "token": "<CLS>",
            "token_id": 2,
            "logprob": -39.6875,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "\"",
        "token_id": 9,
        "selection_method": "sampling"
      }
    },
    {
      "step": 54,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass alle \"e\" gerechnet werden) fünf Buchstaben \"e\". \n\nAufzählerisches: \n\n- zwei \"e\"",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass",
        "Ġalle",
        "Ġ\"",
        "e",
        "\"",
        "Ġgere",
        "chnet",
        "Ġwerden",
        ")",
        "Ġfünf",
        "ĠBuch",
        "st",
        "aben",
        "Ġ\"",
        "e",
        "\".",
        "Ġ\n",
        "\n",
        "Auf",
        "z",
        "ähler",
        "isches",
        ":",
        "Ġ\n",
        "\n",
        "-",
        "Ġzwei",
        "Ġ\"",
        "e",
        "\""
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760, 10528, 1789, 76, 9, 27346, 159065, 9178, 16, 44522,
        19893, 1725, 35346, 1789, 76, 3193, 1973, 206, 51214, 97, 137112, 24804, 33, 1973, 206, 20,
        20293, 1789, 76, 9
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġin",
            "token_id": 1709,
            "logprob": -0.0005,
            "prob": 0.9995
          },
          {
            "token": "Ġim",
            "token_id": 2101,
            "logprob": -7.6406,
            "prob": 0.0005
          },
          {
            "token": "Ġ",
            "token_id": 228,
            "logprob": -11.125,
            "prob": 0.0
          },
          {
            "token": "Ġals",
            "token_id": 4561,
            "logprob": -13.1172,
            "prob": 0.0
          },
          {
            "token": "Ġbei",
            "token_id": 9174,
            "logprob": -13.6953,
            "prob": 0.0
          },
          {
            "token": "Ġsind",
            "token_id": 11382,
            "logprob": -13.8125,
            "prob": 0.0
          },
          {
            "token": "Ġan",
            "token_id": 1844,
            "logprob": -14.1641,
            "prob": 0.0
          },
          {
            "token": "Ġam",
            "token_id": 2437,
            "logprob": -14.8672,
            "prob": 0.0
          },
          {
            "token": "Ġauf",
            "token_id": 5709,
            "logprob": -14.9609,
            "prob": 0.0
          },
          {
            "token": "Ġfür",
            "token_id": 6081,
            "logprob": -15.1562,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġin",
        "token_id": 1709,
        "selection_method": "sampling"
      }
    },
    {
      "step": 55,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass alle \"e\" gerechnet werden) fünf Buchstaben \"e\". \n\nAufzählerisches: \n\n- zwei \"e\" in",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass",
        "Ġalle",
        "Ġ\"",
        "e",
        "\"",
        "Ġgere",
        "chnet",
        "Ġwerden",
        ")",
        "Ġfünf",
        "ĠBuch",
        "st",
        "aben",
        "Ġ\"",
        "e",
        "\".",
        "Ġ\n",
        "\n",
        "Auf",
        "z",
        "ähler",
        "isches",
        ":",
        "Ġ\n",
        "\n",
        "-",
        "Ġzwei",
        "Ġ\"",
        "e",
        "\"",
        "Ġin"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760, 10528, 1789, 76, 9, 27346, 159065, 9178, 16, 44522,
        19893, 1725, 35346, 1789, 76, 3193, 1973, 206, 51214, 97, 137112, 24804, 33, 1973, 206, 20,
        20293, 1789, 76, 9, 1709
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġ\"",
            "token_id": 1789,
            "logprob": -0.0,
            "prob": 1.0
          },
          {
            "token": "Ġ",
            "token_id": 228,
            "logprob": -11.0312,
            "prob": 0.0
          },
          {
            "token": "Ġ„",
            "token_id": 4352,
            "logprob": -11.4219,
            "prob": 0.0
          },
          {
            "token": "Ġ*",
            "token_id": 1987,
            "logprob": -12.1406,
            "prob": 0.0
          },
          {
            "token": "ĠGesch",
            "token_id": 17592,
            "logprob": -13.0156,
            "prob": 0.0
          },
          {
            "token": "Ġ**",
            "token_id": 4434,
            "logprob": -13.3438,
            "prob": 0.0
          },
          {
            "token": "Ġ'",
            "token_id": 2155,
            "logprob": -14.1094,
            "prob": 0.0
          },
          {
            "token": ":",
            "token_id": 33,
            "logprob": -15.7031,
            "prob": 0.0
          },
          {
            "token": "Ġdem",
            "token_id": 3297,
            "logprob": -15.5,
            "prob": 0.0
          },
          {
            "token": "Ġ`",
            "token_id": 2606,
            "logprob": -15.9688,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġ\"",
        "token_id": 1789,
        "selection_method": "sampling"
      }
    },
    {
      "step": 56,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass alle \"e\" gerechnet werden) fünf Buchstaben \"e\". \n\nAufzählerisches: \n\n- zwei \"e\" in \"",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass",
        "Ġalle",
        "Ġ\"",
        "e",
        "\"",
        "Ġgere",
        "chnet",
        "Ġwerden",
        ")",
        "Ġfünf",
        "ĠBuch",
        "st",
        "aben",
        "Ġ\"",
        "e",
        "\".",
        "Ġ\n",
        "\n",
        "Auf",
        "z",
        "ähler",
        "isches",
        ":",
        "Ġ\n",
        "\n",
        "-",
        "Ġzwei",
        "Ġ\"",
        "e",
        "\"",
        "Ġin",
        "Ġ\""
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760, 10528, 1789, 76, 9, 27346, 159065, 9178, 16, 44522,
        19893, 1725, 35346, 1789, 76, 3193, 1973, 206, 51214, 97, 137112, 24804, 33, 1973, 206, 20,
        20293, 1789, 76, 9, 1709, 1789
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Gesch",
            "token_id": 23361,
            "logprob": -0.0001,
            "prob": 1.0
          },
          {
            "token": "Beg",
            "token_id": 49913,
            "logprob": -10.4219,
            "prob": 0.0
          },
          {
            "token": "Ges",
            "token_id": 63053,
            "logprob": -10.9375,
            "prob": 0.0
          },
          {
            "token": "Besch",
            "token_id": 83660,
            "logprob": -10.9531,
            "prob": 0.0
          },
          {
            "token": "ĠGesch",
            "token_id": 17592,
            "logprob": -11.0156,
            "prob": 0.0
          },
          {
            "token": "Schn",
            "token_id": 234655,
            "logprob": -13.3125,
            "prob": 0.0
          },
          {
            "token": "Gest",
            "token_id": 76148,
            "logprob": -13.4844,
            "prob": 0.0
          },
          {
            "token": "gesch",
            "token_id": 19238,
            "logprob": -13.6875,
            "prob": 0.0
          },
          {
            "token": "Geb",
            "token_id": 21650,
            "logprob": -14.3281,
            "prob": 0.0
          },
          {
            "token": "Ge",
            "token_id": 7844,
            "logprob": -14.7578,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Besch",
        "token_id": 83660,
        "selection_method": "sampling"
      }
    },
    {
      "step": 57,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass alle \"e\" gerechnet werden) fünf Buchstaben \"e\". \n\nAufzählerisches: \n\n- zwei \"e\" in \"Besch",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass",
        "Ġalle",
        "Ġ\"",
        "e",
        "\"",
        "Ġgere",
        "chnet",
        "Ġwerden",
        ")",
        "Ġfünf",
        "ĠBuch",
        "st",
        "aben",
        "Ġ\"",
        "e",
        "\".",
        "Ġ\n",
        "\n",
        "Auf",
        "z",
        "ähler",
        "isches",
        ":",
        "Ġ\n",
        "\n",
        "-",
        "Ġzwei",
        "Ġ\"",
        "e",
        "\"",
        "Ġin",
        "Ġ\"",
        "Besch"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760, 10528, 1789, 76, 9, 27346, 159065, 9178, 16, 44522,
        19893, 1725, 35346, 1789, 76, 3193, 1973, 206, 51214, 97, 137112, 24804, 33, 1973, 206, 20,
        20293, 1789, 76, 9, 1709, 1789, 83660
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "leun",
            "token_id": 250042,
            "logprob": -0.0831,
            "prob": 0.9204
          },
          {
            "token": "wind",
            "token_id": 25965,
            "logprob": -3.2246,
            "prob": 0.0398
          },
          {
            "token": "w",
            "token_id": 94,
            "logprob": -3.9746,
            "prob": 0.0188
          },
          {
            "token": "windigkeit",
            "token_id": 139420,
            "logprob": -4.2695,
            "prob": 0.014
          },
          {
            "token": "rän",
            "token_id": 94005,
            "logprob": -5.8477,
            "prob": 0.0029
          },
          {
            "token": "wing",
            "token_id": 16046,
            "logprob": -7.5742,
            "prob": 0.0005
          },
          {
            "token": "rych",
            "token_id": 109403,
            "logprob": -7.7852,
            "prob": 0.0004
          },
          {
            "token": "will",
            "token_id": 22947,
            "logprob": -7.8477,
            "prob": 0.0004
          },
          {
            "token": "wer",
            "token_id": 7925,
            "logprob": -7.9883,
            "prob": 0.0003
          },
          {
            "token": "r",
            "token_id": 89,
            "logprob": -8.2656,
            "prob": 0.0003
          }
        ]
      },
      "selected_token": {
        "token": "w",
        "token_id": 94,
        "selection_method": "sampling"
      }
    },
    {
      "step": 58,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass alle \"e\" gerechnet werden) fünf Buchstaben \"e\". \n\nAufzählerisches: \n\n- zwei \"e\" in \"Beschw",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass",
        "Ġalle",
        "Ġ\"",
        "e",
        "\"",
        "Ġgere",
        "chnet",
        "Ġwerden",
        ")",
        "Ġfünf",
        "ĠBuch",
        "st",
        "aben",
        "Ġ\"",
        "e",
        "\".",
        "Ġ\n",
        "\n",
        "Auf",
        "z",
        "ähler",
        "isches",
        ":",
        "Ġ\n",
        "\n",
        "-",
        "Ġzwei",
        "Ġ\"",
        "e",
        "\"",
        "Ġin",
        "Ġ\"",
        "Besch",
        "w"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760, 10528, 1789, 76, 9, 27346, 159065, 9178, 16, 44522,
        19893, 1725, 35346, 1789, 76, 3193, 1973, 206, 51214, 97, 137112, 24804, 33, 1973, 206, 20,
        20293, 1789, 76, 9, 1709, 1789, 83660, 94
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "indi",
            "token_id": 22816,
            "logprob": -0.6992,
            "prob": 0.4968
          },
          {
            "token": "inde",
            "token_id": 3513,
            "logprob": -1.8477,
            "prob": 0.1576
          },
          {
            "token": "ind",
            "token_id": 2059,
            "logprob": -2.3945,
            "prob": 0.0912
          },
          {
            "token": "inn",
            "token_id": 2949,
            "logprob": -3.5195,
            "prob": 0.0296
          },
          {
            "token": "indo",
            "token_id": 9654,
            "logprob": -3.668,
            "prob": 0.0255
          },
          {
            "token": "inni",
            "token_id": 145182,
            "logprob": -3.9727,
            "prob": 0.0188
          },
          {
            "token": "inding",
            "token_id": 9762,
            "logprob": -4.418,
            "prob": 0.0121
          },
          {
            "token": "inden",
            "token_id": 9543,
            "logprob": -4.6055,
            "prob": 0.01
          },
          {
            "token": "indir",
            "token_id": 110828,
            "logprob": -4.8477,
            "prob": 0.0078
          },
          {
            "token": "idi",
            "token_id": 10318,
            "logprob": -4.9648,
            "prob": 0.007
          }
        ]
      },
      "selected_token": {
        "token": "indi",
        "token_id": 22816,
        "selection_method": "sampling"
      }
    },
    {
      "step": 59,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass alle \"e\" gerechnet werden) fünf Buchstaben \"e\". \n\nAufzählerisches: \n\n- zwei \"e\" in \"Beschwindi",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass",
        "Ġalle",
        "Ġ\"",
        "e",
        "\"",
        "Ġgere",
        "chnet",
        "Ġwerden",
        ")",
        "Ġfünf",
        "ĠBuch",
        "st",
        "aben",
        "Ġ\"",
        "e",
        "\".",
        "Ġ\n",
        "\n",
        "Auf",
        "z",
        "ähler",
        "isches",
        ":",
        "Ġ\n",
        "\n",
        "-",
        "Ġzwei",
        "Ġ\"",
        "e",
        "\"",
        "Ġin",
        "Ġ\"",
        "Besch",
        "w",
        "indi"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760, 10528, 1789, 76, 9, 27346, 159065, 9178, 16, 44522,
        19893, 1725, 35346, 1789, 76, 3193, 1973, 206, 51214, 97, 137112, 24804, 33, 1973, 206, 20,
        20293, 1789, 76, 9, 1709, 1789, 83660, 94, 22816
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "(",
            "token_id": 15,
            "logprob": -0.856,
            "prob": 0.4248
          },
          {
            "token": "-",
            "token_id": 20,
            "logprob": -1.6523,
            "prob": 0.1915
          },
          {
            "token": "\"",
            "token_id": 9,
            "logprob": -1.8555,
            "prob": 0.1564
          },
          {
            "token": "\"-",
            "token_id": 64403,
            "logprob": -2.5898,
            "prob": 0.075
          },
          {
            "token": "-\"",
            "token_id": 62562,
            "logprob": -2.7148,
            "prob": 0.0662
          },
          {
            "token": "**",
            "token_id": 2093,
            "logprob": -3.4336,
            "prob": 0.0323
          },
          {
            "token": "*\"",
            "token_id": 133689,
            "logprob": -4.6523,
            "prob": 0.0095
          },
          {
            "token": "گ",
            "token_id": 3227,
            "logprob": -5.1367,
            "prob": 0.0059
          },
          {
            "token": "ग",
            "token_id": 4368,
            "logprob": -5.4492,
            "prob": 0.0043
          },
          {
            "token": "G",
            "token_id": 46,
            "logprob": -5.9102,
            "prob": 0.0027
          }
        ]
      },
      "selected_token": {
        "token": "-",
        "token_id": 20,
        "selection_method": "sampling"
      }
    },
    {
      "step": 60,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass alle \"e\" gerechnet werden) fünf Buchstaben \"e\". \n\nAufzählerisches: \n\n- zwei \"e\" in \"Beschwindi-",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass",
        "Ġalle",
        "Ġ\"",
        "e",
        "\"",
        "Ġgere",
        "chnet",
        "Ġwerden",
        ")",
        "Ġfünf",
        "ĠBuch",
        "st",
        "aben",
        "Ġ\"",
        "e",
        "\".",
        "Ġ\n",
        "\n",
        "Auf",
        "z",
        "ähler",
        "isches",
        ":",
        "Ġ\n",
        "\n",
        "-",
        "Ġzwei",
        "Ġ\"",
        "e",
        "\"",
        "Ġin",
        "Ġ\"",
        "Besch",
        "w",
        "indi",
        "-"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760, 10528, 1789, 76, 9, 27346, 159065, 9178, 16, 44522,
        19893, 1725, 35346, 1789, 76, 3193, 1973, 206, 51214, 97, 137112, 24804, 33, 1973, 206, 20,
        20293, 1789, 76, 9, 1709, 1789, 83660, 94, 22816, 20
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "igkeit",
            "token_id": 29109,
            "logprob": -0.4165,
            "prob": 0.6592
          },
          {
            "token": "g",
            "token_id": 78,
            "logprob": -2.1504,
            "prob": 0.1164
          },
          {
            "token": "\",",
            "token_id": 2040,
            "logprob": -2.4785,
            "prob": 0.0838
          },
          {
            "token": "igkeits",
            "token_id": 173625,
            "logprob": -2.6191,
            "prob": 0.0728
          },
          {
            "token": "\".",
            "token_id": 3193,
            "logprob": -4.0352,
            "prob": 0.0177
          },
          {
            "token": "keit",
            "token_id": 20243,
            "logprob": -4.582,
            "prob": 0.0102
          },
          {
            "token": "\n",
            "token_id": 206,
            "logprob": -5.0664,
            "prob": 0.0063
          },
          {
            "token": "\":",
            "token_id": 2209,
            "logprob": -5.1211,
            "prob": 0.006
          },
          {
            "token": "\";",
            "token_id": 5379,
            "logprob": -5.4336,
            "prob": 0.0044
          },
          {
            "token": "keits",
            "token_id": 106569,
            "logprob": -6.1602,
            "prob": 0.0021
          }
        ]
      },
      "selected_token": {
        "token": "\",",
        "token_id": 2040,
        "selection_method": "sampling"
      }
    },
    {
      "step": 61,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass alle \"e\" gerechnet werden) fünf Buchstaben \"e\". \n\nAufzählerisches: \n\n- zwei \"e\" in \"Beschwindi-\",",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass",
        "Ġalle",
        "Ġ\"",
        "e",
        "\"",
        "Ġgere",
        "chnet",
        "Ġwerden",
        ")",
        "Ġfünf",
        "ĠBuch",
        "st",
        "aben",
        "Ġ\"",
        "e",
        "\".",
        "Ġ\n",
        "\n",
        "Auf",
        "z",
        "ähler",
        "isches",
        ":",
        "Ġ\n",
        "\n",
        "-",
        "Ġzwei",
        "Ġ\"",
        "e",
        "\"",
        "Ġin",
        "Ġ\"",
        "Besch",
        "w",
        "indi",
        "-",
        "\","
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760, 10528, 1789, 76, 9, 27346, 159065, 9178, 16, 44522,
        19893, 1725, 35346, 1789, 76, 3193, 1973, 206, 51214, 97, 137112, 24804, 33, 1973, 206, 20,
        20293, 1789, 76, 9, 1709, 1789, 83660, 94, 22816, 20, 2040
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġ",
            "token_id": 228,
            "logprob": -0.0546,
            "prob": 0.9468
          },
          {
            "token": "\n",
            "token_id": 206,
            "logprob": -2.9609,
            "prob": 0.0518
          },
          {
            "token": "Ġ\n",
            "token_id": 1973,
            "logprob": -7.3125,
            "prob": 0.0007
          },
          {
            "token": "Ġ ",
            "token_id": 1667,
            "logprob": -9.2969,
            "prob": 0.0001
          },
          {
            "token": "Ġeinem",
            "token_id": 12315,
            "logprob": -9.5234,
            "prob": 0.0001
          },
          {
            "token": "Ġein",
            "token_id": 4001,
            "logprob": -10.1172,
            "prob": 0.0
          },
          {
            "token": "Ġda",
            "token_id": 2213,
            "logprob": -10.2734,
            "prob": 0.0
          },
          {
            "token": "Ġim",
            "token_id": 2101,
            "logprob": -10.2969,
            "prob": 0.0
          },
          {
            "token": "Ġin",
            "token_id": 1709,
            "logprob": -10.3359,
            "prob": 0.0
          },
          {
            "token": "Ġplus",
            "token_id": 4254,
            "logprob": -10.6328,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġeinem",
        "token_id": 12315,
        "selection_method": "sampling"
      }
    },
    {
      "step": 62,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass alle \"e\" gerechnet werden) fünf Buchstaben \"e\". \n\nAufzählerisches: \n\n- zwei \"e\" in \"Beschwindi-\", einem",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass",
        "Ġalle",
        "Ġ\"",
        "e",
        "\"",
        "Ġgere",
        "chnet",
        "Ġwerden",
        ")",
        "Ġfünf",
        "ĠBuch",
        "st",
        "aben",
        "Ġ\"",
        "e",
        "\".",
        "Ġ\n",
        "\n",
        "Auf",
        "z",
        "ähler",
        "isches",
        ":",
        "Ġ\n",
        "\n",
        "-",
        "Ġzwei",
        "Ġ\"",
        "e",
        "\"",
        "Ġin",
        "Ġ\"",
        "Besch",
        "w",
        "indi",
        "-",
        "\",",
        "Ġeinem"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760, 10528, 1789, 76, 9, 27346, 159065, 9178, 16, 44522,
        19893, 1725, 35346, 1789, 76, 3193, 1973, 206, 51214, 97, 137112, 24804, 33, 1973, 206, 20,
        20293, 1789, 76, 9, 1709, 1789, 83660, 94, 22816, 20, 2040, 12315
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "ĠTeil",
            "token_id": 22815,
            "logprob": -0.3477,
            "prob": 0.7065
          },
          {
            "token": "Ġzusamm",
            "token_id": 161375,
            "logprob": -1.4258,
            "prob": 0.2404
          },
          {
            "token": "ĠZusamm",
            "token_id": 162089,
            "logprob": -3.5508,
            "prob": 0.0287
          },
          {
            "token": "ĠBestandteil",
            "token_id": 199868,
            "logprob": -4.5352,
            "prob": 0.0107
          },
          {
            "token": "ĠWort",
            "token_id": 80704,
            "logprob": -5.7461,
            "prob": 0.0032
          },
          {
            "token": "Ġun",
            "token_id": 1880,
            "logprob": -6.6758,
            "prob": 0.0013
          },
          {
            "token": "ĠKom",
            "token_id": 9020,
            "logprob": -7.3711,
            "prob": 0.0006
          },
          {
            "token": "ĠBestand",
            "token_id": 115163,
            "logprob": -7.4336,
            "prob": 0.0006
          },
          {
            "token": "Ġgemeinsamen",
            "token_id": 179756,
            "logprob": -7.4727,
            "prob": 0.0006
          },
          {
            "token": "Ġzusammen",
            "token_id": 34807,
            "logprob": -7.6055,
            "prob": 0.0005
          }
        ]
      },
      "selected_token": {
        "token": "Ġzusamm",
        "token_id": 161375,
        "selection_method": "sampling"
      }
    },
    {
      "step": 63,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass alle \"e\" gerechnet werden) fünf Buchstaben \"e\". \n\nAufzählerisches: \n\n- zwei \"e\" in \"Beschwindi-\", einem zusamm",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass",
        "Ġalle",
        "Ġ\"",
        "e",
        "\"",
        "Ġgere",
        "chnet",
        "Ġwerden",
        ")",
        "Ġfünf",
        "ĠBuch",
        "st",
        "aben",
        "Ġ\"",
        "e",
        "\".",
        "Ġ\n",
        "\n",
        "Auf",
        "z",
        "ähler",
        "isches",
        ":",
        "Ġ\n",
        "\n",
        "-",
        "Ġzwei",
        "Ġ\"",
        "e",
        "\"",
        "Ġin",
        "Ġ\"",
        "Besch",
        "w",
        "indi",
        "-",
        "\",",
        "Ġeinem",
        "Ġzusamm"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760, 10528, 1789, 76, 9, 27346, 159065, 9178, 16, 44522,
        19893, 1725, 35346, 1789, 76, 3193, 1973, 206, 51214, 97, 137112, 24804, 33, 1973, 206, 20,
        20293, 1789, 76, 9, 1709, 1789, 83660, 94, 22816, 20, 2040, 12315, 161375
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "enges",
            "token_id": 53378,
            "logprob": -0.0005,
            "prob": 0.9995
          },
          {
            "token": "engesch",
            "token_id": 186367,
            "logprob": -7.8906,
            "prob": 0.0004
          },
          {
            "token": "ens",
            "token_id": 2028,
            "logprob": -9.4219,
            "prob": 0.0001
          },
          {
            "token": "eng",
            "token_id": 2344,
            "logprob": -10.1875,
            "prob": 0.0
          },
          {
            "token": "ges",
            "token_id": 5325,
            "logprob": -12.0312,
            "prob": 0.0
          },
          {
            "token": "enge",
            "token_id": 8525,
            "logprob": -12.6719,
            "prob": 0.0
          },
          {
            "token": "engen",
            "token_id": 41479,
            "logprob": -12.9531,
            "prob": 0.0
          },
          {
            "token": "erges",
            "token_id": 155680,
            "logprob": -13.5312,
            "prob": 0.0
          },
          {
            "token": "enger",
            "token_id": 12439,
            "logprob": -13.9219,
            "prob": 0.0
          },
          {
            "token": "gesetz",
            "token_id": 106065,
            "logprob": -14.0156,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "engesch",
        "token_id": 186367,
        "selection_method": "sampling"
      }
    },
    {
      "step": 64,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass alle \"e\" gerechnet werden) fünf Buchstaben \"e\". \n\nAufzählerisches: \n\n- zwei \"e\" in \"Beschwindi-\", einem zusammengesch",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass",
        "Ġalle",
        "Ġ\"",
        "e",
        "\"",
        "Ġgere",
        "chnet",
        "Ġwerden",
        ")",
        "Ġfünf",
        "ĠBuch",
        "st",
        "aben",
        "Ġ\"",
        "e",
        "\".",
        "Ġ\n",
        "\n",
        "Auf",
        "z",
        "ähler",
        "isches",
        ":",
        "Ġ\n",
        "\n",
        "-",
        "Ġzwei",
        "Ġ\"",
        "e",
        "\"",
        "Ġin",
        "Ġ\"",
        "Besch",
        "w",
        "indi",
        "-",
        "\",",
        "Ġeinem",
        "Ġzusamm",
        "engesch"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760, 10528, 1789, 76, 9, 27346, 159065, 9178, 16, 44522,
        19893, 1725, 35346, 1789, 76, 3193, 1973, 206, 51214, 97, 137112, 24804, 33, 1973, 206, 20,
        20293, 1789, 76, 9, 1709, 1789, 83660, 94, 22816, 20, 2040, 12315, 161375, 186367
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "rieben",
            "token_id": 44927,
            "logprob": -0.001,
            "prob": 0.999
          },
          {
            "token": "reiben",
            "token_id": 95916,
            "logprob": -7.9922,
            "prob": 0.0003
          },
          {
            "token": "lossenen",
            "token_id": 206408,
            "logprob": -9.0625,
            "prob": 0.0001
          },
          {
            "token": "lag",
            "token_id": 8057,
            "logprob": -9.3359,
            "prob": 0.0001
          },
          {
            "token": "reven",
            "token_id": 34578,
            "logprob": -9.7109,
            "prob": 0.0001
          },
          {
            "token": "iebenen",
            "token_id": 231317,
            "logprob": -10.1172,
            "prob": 0.0
          },
          {
            "token": "ri",
            "token_id": 1833,
            "logprob": -10.2578,
            "prob": 0.0
          },
          {
            "token": "Ġgeschrieben",
            "token_id": 154746,
            "logprob": -10.3828,
            "prob": 0.0
          },
          {
            "token": "reten",
            "token_id": 60152,
            "logprob": -10.6641,
            "prob": 0.0
          },
          {
            "token": "reibung",
            "token_id": 59725,
            "logprob": -10.9219,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "rieben",
        "token_id": 44927,
        "selection_method": "sampling"
      }
    },
    {
      "step": 65,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass alle \"e\" gerechnet werden) fünf Buchstaben \"e\". \n\nAufzählerisches: \n\n- zwei \"e\" in \"Beschwindi-\", einem zusammengeschrieben",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass",
        "Ġalle",
        "Ġ\"",
        "e",
        "\"",
        "Ġgere",
        "chnet",
        "Ġwerden",
        ")",
        "Ġfünf",
        "ĠBuch",
        "st",
        "aben",
        "Ġ\"",
        "e",
        "\".",
        "Ġ\n",
        "\n",
        "Auf",
        "z",
        "ähler",
        "isches",
        ":",
        "Ġ\n",
        "\n",
        "-",
        "Ġzwei",
        "Ġ\"",
        "e",
        "\"",
        "Ġin",
        "Ġ\"",
        "Besch",
        "w",
        "indi",
        "-",
        "\",",
        "Ġeinem",
        "Ġzusamm",
        "engesch",
        "rieben"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760, 10528, 1789, 76, 9, 27346, 159065, 9178, 16, 44522,
        19893, 1725, 35346, 1789, 76, 3193, 1973, 206, 51214, 97, 137112, 24804, 33, 1973, 206, 20,
        20293, 1789, 76, 9, 1709, 1789, 83660, 94, 22816, 20, 2040, 12315, 161375, 186367, 44927
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "en",
            "token_id": 1673,
            "logprob": -0.0011,
            "prob": 0.999
          },
          {
            "token": "ĠWort",
            "token_id": 80704,
            "logprob": -6.9062,
            "prob": 0.001
          },
          {
            "token": "em",
            "token_id": 1759,
            "logprob": -9.4062,
            "prob": 0.0001
          },
          {
            "token": "ĠTeil",
            "token_id": 22815,
            "logprob": -12.1562,
            "prob": 0.0
          },
          {
            "token": "es",
            "token_id": 1674,
            "logprob": -14.6094,
            "prob": 0.0
          },
          {
            "token": "ĠVerb",
            "token_id": 44459,
            "logprob": -17.3125,
            "prob": 0.0
          },
          {
            "token": "ĠWör",
            "token_id": 104945,
            "logprob": -17.0312,
            "prob": 0.0
          },
          {
            "token": "ĠWord",
            "token_id": 17019,
            "logprob": -16.7344,
            "prob": 0.0
          },
          {
            "token": "ĠPart",
            "token_id": 3993,
            "logprob": -17.0469,
            "prob": 0.0
          },
          {
            "token": "enen",
            "token_id": 15066,
            "logprob": -16.2656,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "ĠWort",
        "token_id": 80704,
        "selection_method": "sampling"
      }
    },
    {
      "step": 66,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass alle \"e\" gerechnet werden) fünf Buchstaben \"e\". \n\nAufzählerisches: \n\n- zwei \"e\" in \"Beschwindi-\", einem zusammengeschrieben Wort",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass",
        "Ġalle",
        "Ġ\"",
        "e",
        "\"",
        "Ġgere",
        "chnet",
        "Ġwerden",
        ")",
        "Ġfünf",
        "ĠBuch",
        "st",
        "aben",
        "Ġ\"",
        "e",
        "\".",
        "Ġ\n",
        "\n",
        "Auf",
        "z",
        "ähler",
        "isches",
        ":",
        "Ġ\n",
        "\n",
        "-",
        "Ġzwei",
        "Ġ\"",
        "e",
        "\"",
        "Ġin",
        "Ġ\"",
        "Besch",
        "w",
        "indi",
        "-",
        "\",",
        "Ġeinem",
        "Ġzusamm",
        "engesch",
        "rieben",
        "ĠWort"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760, 10528, 1789, 76, 9, 27346, 159065, 9178, 16, 44522,
        19893, 1725, 35346, 1789, 76, 3193, 1973, 206, 51214, 97, 137112, 24804, 33, 1973, 206, 20,
        20293, 1789, 76, 9, 1709, 1789, 83660, 94, 22816, 20, 2040, 12315, 161375, 186367, 44927,
        80704
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "\n",
            "token_id": 206,
            "logprob": -0.5854,
            "prob": 0.5566
          },
          {
            "token": "Ġaus",
            "token_id": 6523,
            "logprob": -1.0078,
            "prob": 0.3652
          },
          {
            "token": ",",
            "token_id": 19,
            "logprob": -3.7578,
            "prob": 0.0233
          },
          {
            "token": "Ġ",
            "token_id": 228,
            "logprob": -3.7891,
            "prob": 0.0226
          },
          {
            "token": ".",
            "token_id": 21,
            "logprob": -4.4297,
            "prob": 0.0119
          },
          {
            "token": "Ġvon",
            "token_id": 3615,
            "logprob": -4.6328,
            "prob": 0.0097
          },
          {
            "token": "Ġ(",
            "token_id": 1737,
            "logprob": -5.3984,
            "prob": 0.0045
          },
          {
            "token": "ĠTeil",
            "token_id": 22815,
            "logprob": -6.5703,
            "prob": 0.0014
          },
          {
            "token": "Ġmit",
            "token_id": 4665,
            "logprob": -7.0469,
            "prob": 0.0009
          },
          {
            "token": "Ġ\n",
            "token_id": 1973,
            "logprob": -7.1797,
            "prob": 0.0008
          }
        ]
      },
      "selected_token": {
        "token": ",",
        "token_id": 19,
        "selection_method": "sampling"
      }
    },
    {
      "step": 67,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass alle \"e\" gerechnet werden) fünf Buchstaben \"e\". \n\nAufzählerisches: \n\n- zwei \"e\" in \"Beschwindi-\", einem zusammengeschrieben Wort,",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass",
        "Ġalle",
        "Ġ\"",
        "e",
        "\"",
        "Ġgere",
        "chnet",
        "Ġwerden",
        ")",
        "Ġfünf",
        "ĠBuch",
        "st",
        "aben",
        "Ġ\"",
        "e",
        "\".",
        "Ġ\n",
        "\n",
        "Auf",
        "z",
        "ähler",
        "isches",
        ":",
        "Ġ\n",
        "\n",
        "-",
        "Ġzwei",
        "Ġ\"",
        "e",
        "\"",
        "Ġin",
        "Ġ\"",
        "Besch",
        "w",
        "indi",
        "-",
        "\",",
        "Ġeinem",
        "Ġzusamm",
        "engesch",
        "rieben",
        "ĠWort",
        ","
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760, 10528, 1789, 76, 9, 27346, 159065, 9178, 16, 44522,
        19893, 1725, 35346, 1789, 76, 3193, 1973, 206, 51214, 97, 137112, 24804, 33, 1973, 206, 20,
        20293, 1789, 76, 9, 1709, 1789, 83660, 94, 22816, 20, 2040, 12315, 161375, 186367, 44927,
        80704, 19
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġdas",
            "token_id": 4031,
            "logprob": -0.0319,
            "prob": 0.9688
          },
          {
            "token": "Ġwelches",
            "token_id": 99748,
            "logprob": -4.5469,
            "prob": 0.0106
          },
          {
            "token": "Ġwo",
            "token_id": 21081,
            "logprob": -4.7656,
            "prob": 0.0085
          },
          {
            "token": "\n",
            "token_id": 206,
            "logprob": -5.8438,
            "prob": 0.0029
          },
          {
            "token": "Ġwobei",
            "token_id": 61188,
            "logprob": -6.3203,
            "prob": 0.0018
          },
          {
            "token": "Ġ",
            "token_id": 228,
            "logprob": -6.3281,
            "prob": 0.0018
          },
          {
            "token": "Ġin",
            "token_id": 1709,
            "logprob": -6.4844,
            "prob": 0.0015
          },
          {
            "token": "Ġ\n",
            "token_id": 1973,
            "logprob": -7.625,
            "prob": 0.0005
          },
          {
            "token": "Ġdass",
            "token_id": 15760,
            "logprob": -7.6484,
            "prob": 0.0005
          },
          {
            "token": "Ġwas",
            "token_id": 1898,
            "logprob": -7.9453,
            "prob": 0.0004
          }
        ]
      },
      "selected_token": {
        "token": "Ġdas",
        "token_id": 4031,
        "selection_method": "sampling"
      }
    },
    {
      "step": 68,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass alle \"e\" gerechnet werden) fünf Buchstaben \"e\". \n\nAufzählerisches: \n\n- zwei \"e\" in \"Beschwindi-\", einem zusammengeschrieben Wort, das",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass",
        "Ġalle",
        "Ġ\"",
        "e",
        "\"",
        "Ġgere",
        "chnet",
        "Ġwerden",
        ")",
        "Ġfünf",
        "ĠBuch",
        "st",
        "aben",
        "Ġ\"",
        "e",
        "\".",
        "Ġ\n",
        "\n",
        "Auf",
        "z",
        "ähler",
        "isches",
        ":",
        "Ġ\n",
        "\n",
        "-",
        "Ġzwei",
        "Ġ\"",
        "e",
        "\"",
        "Ġin",
        "Ġ\"",
        "Besch",
        "w",
        "indi",
        "-",
        "\",",
        "Ġeinem",
        "Ġzusamm",
        "engesch",
        "rieben",
        "ĠWort",
        ",",
        "Ġdas"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760, 10528, 1789, 76, 9, 27346, 159065, 9178, 16, 44522,
        19893, 1725, 35346, 1789, 76, 3193, 1973, 206, 51214, 97, 137112, 24804, 33, 1973, 206, 20,
        20293, 1789, 76, 9, 1709, 1789, 83660, 94, 22816, 20, 2040, 12315, 161375, 186367, 44927,
        80704, 19, 4031
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġ\"",
            "token_id": 1789,
            "logprob": -0.3752,
            "prob": 0.687
          },
          {
            "token": "ĠTeil",
            "token_id": 22815,
            "logprob": -2.3438,
            "prob": 0.0959
          },
          {
            "token": "Ġden",
            "token_id": 3636,
            "logprob": -3.3281,
            "prob": 0.0359
          },
          {
            "token": "Ġals",
            "token_id": 4561,
            "logprob": -3.5312,
            "prob": 0.0293
          },
          {
            "token": "Ġhier",
            "token_id": 14371,
            "logprob": -3.7344,
            "prob": 0.0239
          },
          {
            "token": "Ġdas",
            "token_id": 4031,
            "logprob": -3.8906,
            "prob": 0.0204
          },
          {
            "token": "Ġzwei",
            "token_id": 20293,
            "logprob": -4.3906,
            "prob": 0.0124
          },
          {
            "token": "Ġdie",
            "token_id": 2915,
            "logprob": -4.4844,
            "prob": 0.0113
          },
          {
            "token": "Ġan",
            "token_id": 1844,
            "logprob": -4.5391,
            "prob": 0.0107
          },
          {
            "token": "Ġaus",
            "token_id": 6523,
            "logprob": -4.5781,
            "prob": 0.0103
          }
        ]
      },
      "selected_token": {
        "token": "Ġden",
        "token_id": 3636,
        "selection_method": "sampling"
      }
    },
    {
      "step": 69,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass alle \"e\" gerechnet werden) fünf Buchstaben \"e\". \n\nAufzählerisches: \n\n- zwei \"e\" in \"Beschwindi-\", einem zusammengeschrieben Wort, das den",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass",
        "Ġalle",
        "Ġ\"",
        "e",
        "\"",
        "Ġgere",
        "chnet",
        "Ġwerden",
        ")",
        "Ġfünf",
        "ĠBuch",
        "st",
        "aben",
        "Ġ\"",
        "e",
        "\".",
        "Ġ\n",
        "\n",
        "Auf",
        "z",
        "ähler",
        "isches",
        ":",
        "Ġ\n",
        "\n",
        "-",
        "Ġzwei",
        "Ġ\"",
        "e",
        "\"",
        "Ġin",
        "Ġ\"",
        "Besch",
        "w",
        "indi",
        "-",
        "\",",
        "Ġeinem",
        "Ġzusamm",
        "engesch",
        "rieben",
        "ĠWort",
        ",",
        "Ġdas",
        "Ġden"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760, 10528, 1789, 76, 9, 27346, 159065, 9178, 16, 44522,
        19893, 1725, 35346, 1789, 76, 3193, 1973, 206, 51214, 97, 137112, 24804, 33, 1973, 206, 20,
        20293, 1789, 76, 9, 1709, 1789, 83660, 94, 22816, 20, 2040, 12315, 161375, 186367, 44927,
        80704, 19, 4031, 3636
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "ĠWort",
            "token_id": 80704,
            "logprob": -1.1973,
            "prob": 0.3022
          },
          {
            "token": "ĠBegriff",
            "token_id": 98060,
            "logprob": -1.5332,
            "prob": 0.2159
          },
          {
            "token": "ĠTeil",
            "token_id": 22815,
            "logprob": -2.3301,
            "prob": 0.0974
          },
          {
            "token": "Ġersten",
            "token_id": 25321,
            "logprob": -2.7676,
            "prob": 0.0629
          },
          {
            "token": "ĠSub",
            "token_id": 6879,
            "logprob": -3.0254,
            "prob": 0.0486
          },
          {
            "token": "ĠAnf",
            "token_id": 80303,
            "logprob": -3.2207,
            "prob": 0.0399
          },
          {
            "token": "ĠHaupt",
            "token_id": 28510,
            "logprob": -3.5566,
            "prob": 0.0285
          },
          {
            "token": "ĠGrund",
            "token_id": 28207,
            "logprob": -3.6113,
            "prob": 0.027
          },
          {
            "token": "ĠBeginn",
            "token_id": 70650,
            "logprob": -3.6582,
            "prob": 0.0258
          },
          {
            "token": "ĠAnfang",
            "token_id": 59607,
            "logprob": -3.8145,
            "prob": 0.0221
          }
        ]
      },
      "selected_token": {
        "token": "ĠHaupt",
        "token_id": 28510,
        "selection_method": "sampling"
      }
    },
    {
      "step": 70,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass alle \"e\" gerechnet werden) fünf Buchstaben \"e\". \n\nAufzählerisches: \n\n- zwei \"e\" in \"Beschwindi-\", einem zusammengeschrieben Wort, das den Haupt",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass",
        "Ġalle",
        "Ġ\"",
        "e",
        "\"",
        "Ġgere",
        "chnet",
        "Ġwerden",
        ")",
        "Ġfünf",
        "ĠBuch",
        "st",
        "aben",
        "Ġ\"",
        "e",
        "\".",
        "Ġ\n",
        "\n",
        "Auf",
        "z",
        "ähler",
        "isches",
        ":",
        "Ġ\n",
        "\n",
        "-",
        "Ġzwei",
        "Ġ\"",
        "e",
        "\"",
        "Ġin",
        "Ġ\"",
        "Besch",
        "w",
        "indi",
        "-",
        "\",",
        "Ġeinem",
        "Ġzusamm",
        "engesch",
        "rieben",
        "ĠWort",
        ",",
        "Ġdas",
        "Ġden",
        "ĠHaupt"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760, 10528, 1789, 76, 9, 27346, 159065, 9178, 16, 44522,
        19893, 1725, 35346, 1789, 76, 3193, 1973, 206, 51214, 97, 137112, 24804, 33, 1973, 206, 20,
        20293, 1789, 76, 9, 1709, 1789, 83660, 94, 22816, 20, 2040, 12315, 161375, 186367, 44927,
        80704, 19, 4031, 3636, 28510
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "teil",
            "token_id": 23139,
            "logprob": -0.0016,
            "prob": 0.9985
          },
          {
            "token": "bestand",
            "token_id": 191582,
            "logprob": -7.0938,
            "prob": 0.0008
          },
          {
            "token": "anteil",
            "token_id": 222143,
            "logprob": -7.6094,
            "prob": 0.0005
          },
          {
            "token": "kern",
            "token_id": 91666,
            "logprob": -10.4453,
            "prob": 0.0
          },
          {
            "token": "stamm",
            "token_id": 144701,
            "logprob": -10.4766,
            "prob": 0.0
          },
          {
            "token": "ĠBestandteil",
            "token_id": 199868,
            "logprob": -10.4922,
            "prob": 0.0
          },
          {
            "token": "begr",
            "token_id": 163311,
            "logprob": -10.8438,
            "prob": 0.0
          },
          {
            "token": "grund",
            "token_id": 33726,
            "logprob": -10.8984,
            "prob": 0.0
          },
          {
            "token": "Teil",
            "token_id": 212192,
            "logprob": -11.3828,
            "prob": 0.0
          },
          {
            "token": "teils",
            "token_id": 229669,
            "logprob": -11.5391,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "teil",
        "token_id": 23139,
        "selection_method": "sampling"
      }
    },
    {
      "step": 71,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass alle \"e\" gerechnet werden) fünf Buchstaben \"e\". \n\nAufzählerisches: \n\n- zwei \"e\" in \"Beschwindi-\", einem zusammengeschrieben Wort, das den Hauptteil",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass",
        "Ġalle",
        "Ġ\"",
        "e",
        "\"",
        "Ġgere",
        "chnet",
        "Ġwerden",
        ")",
        "Ġfünf",
        "ĠBuch",
        "st",
        "aben",
        "Ġ\"",
        "e",
        "\".",
        "Ġ\n",
        "\n",
        "Auf",
        "z",
        "ähler",
        "isches",
        ":",
        "Ġ\n",
        "\n",
        "-",
        "Ġzwei",
        "Ġ\"",
        "e",
        "\"",
        "Ġin",
        "Ġ\"",
        "Besch",
        "w",
        "indi",
        "-",
        "\",",
        "Ġeinem",
        "Ġzusamm",
        "engesch",
        "rieben",
        "ĠWort",
        ",",
        "Ġdas",
        "Ġden",
        "ĠHaupt",
        "teil"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760, 10528, 1789, 76, 9, 27346, 159065, 9178, 16, 44522,
        19893, 1725, 35346, 1789, 76, 3193, 1973, 206, 51214, 97, 137112, 24804, 33, 1973, 206, 20,
        20293, 1789, 76, 9, 1709, 1789, 83660, 94, 22816, 20, 2040, 12315, 161375, 186367, 44927,
        80704, 19, 4031, 3636, 28510, 23139
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġdes",
            "token_id": 2003,
            "logprob": -0.006,
            "prob": 0.9941
          },
          {
            "token": "Ġder",
            "token_id": 2479,
            "logprob": -5.7734,
            "prob": 0.0031
          },
          {
            "token": "Ġbildet",
            "token_id": 102412,
            "logprob": -5.9609,
            "prob": 0.0026
          },
          {
            "token": "Ġaus",
            "token_id": 6523,
            "logprob": -10.1016,
            "prob": 0.0
          },
          {
            "token": "Ġeines",
            "token_id": 20792,
            "logprob": -10.1484,
            "prob": 0.0
          },
          {
            "token": "Ġvon",
            "token_id": 3615,
            "logprob": -10.5,
            "prob": 0.0
          },
          {
            "token": "Ġ\"",
            "token_id": 1789,
            "logprob": -10.5234,
            "prob": 0.0
          },
          {
            "token": "Ġ",
            "token_id": 228,
            "logprob": -10.6797,
            "prob": 0.0
          },
          {
            "token": "Ġdarstellt",
            "token_id": 254807,
            "logprob": -10.9219,
            "prob": 0.0
          },
          {
            "token": "Ġbilden",
            "token_id": 121345,
            "logprob": -11.2344,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġdes",
        "token_id": 2003,
        "selection_method": "sampling"
      }
    },
    {
      "step": 72,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass alle \"e\" gerechnet werden) fünf Buchstaben \"e\". \n\nAufzählerisches: \n\n- zwei \"e\" in \"Beschwindi-\", einem zusammengeschrieben Wort, das den Hauptteil des",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass",
        "Ġalle",
        "Ġ\"",
        "e",
        "\"",
        "Ġgere",
        "chnet",
        "Ġwerden",
        ")",
        "Ġfünf",
        "ĠBuch",
        "st",
        "aben",
        "Ġ\"",
        "e",
        "\".",
        "Ġ\n",
        "\n",
        "Auf",
        "z",
        "ähler",
        "isches",
        ":",
        "Ġ\n",
        "\n",
        "-",
        "Ġzwei",
        "Ġ\"",
        "e",
        "\"",
        "Ġin",
        "Ġ\"",
        "Besch",
        "w",
        "indi",
        "-",
        "\",",
        "Ġeinem",
        "Ġzusamm",
        "engesch",
        "rieben",
        "ĠWort",
        ",",
        "Ġdas",
        "Ġden",
        "ĠHaupt",
        "teil",
        "Ġdes"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760, 10528, 1789, 76, 9, 27346, 159065, 9178, 16, 44522,
        19893, 1725, 35346, 1789, 76, 3193, 1973, 206, 51214, 97, 137112, 24804, 33, 1973, 206, 20,
        20293, 1789, 76, 9, 1709, 1789, 83660, 94, 22816, 20, 2040, 12315, 161375, 186367, 44927,
        80704, 19, 4031, 3636, 28510, 23139, 2003
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "ĠWort",
            "token_id": 80704,
            "logprob": -0.0065,
            "prob": 0.9937
          },
          {
            "token": "ĠBegr",
            "token_id": 159177,
            "logprob": -5.8672,
            "prob": 0.0028
          },
          {
            "token": "ĠSub",
            "token_id": 6879,
            "logprob": -6.9766,
            "prob": 0.0009
          },
          {
            "token": "ĠWörter",
            "token_id": 187284,
            "logprob": -7.0391,
            "prob": 0.0009
          },
          {
            "token": "ĠWör",
            "token_id": 104945,
            "logprob": -7.3203,
            "prob": 0.0007
          },
          {
            "token": "ĠSatz",
            "token_id": 137432,
            "logprob": -8.1484,
            "prob": 0.0003
          },
          {
            "token": "ĠW",
            "token_id": 1824,
            "logprob": -8.2812,
            "prob": 0.0003
          },
          {
            "token": "ĠAd",
            "token_id": 2894,
            "logprob": -8.7422,
            "prob": 0.0002
          },
          {
            "token": "ĠVer",
            "token_id": 4954,
            "logprob": -9.7812,
            "prob": 0.0001
          },
          {
            "token": "ĠWor",
            "token_id": 47553,
            "logprob": -9.875,
            "prob": 0.0001
          }
        ]
      },
      "selected_token": {
        "token": "ĠWörter",
        "token_id": 187284,
        "selection_method": "sampling"
      }
    },
    {
      "step": 73,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass alle \"e\" gerechnet werden) fünf Buchstaben \"e\". \n\nAufzählerisches: \n\n- zwei \"e\" in \"Beschwindi-\", einem zusammengeschrieben Wort, das den Hauptteil des Wörter",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass",
        "Ġalle",
        "Ġ\"",
        "e",
        "\"",
        "Ġgere",
        "chnet",
        "Ġwerden",
        ")",
        "Ġfünf",
        "ĠBuch",
        "st",
        "aben",
        "Ġ\"",
        "e",
        "\".",
        "Ġ\n",
        "\n",
        "Auf",
        "z",
        "ähler",
        "isches",
        ":",
        "Ġ\n",
        "\n",
        "-",
        "Ġzwei",
        "Ġ\"",
        "e",
        "\"",
        "Ġin",
        "Ġ\"",
        "Besch",
        "w",
        "indi",
        "-",
        "\",",
        "Ġeinem",
        "Ġzusamm",
        "engesch",
        "rieben",
        "ĠWort",
        ",",
        "Ġdas",
        "Ġden",
        "ĠHaupt",
        "teil",
        "Ġdes",
        "ĠWörter"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760, 10528, 1789, 76, 9, 27346, 159065, 9178, 16, 44522,
        19893, 1725, 35346, 1789, 76, 3193, 1973, 206, 51214, 97, 137112, 24804, 33, 1973, 206, 20,
        20293, 1789, 76, 9, 1709, 1789, 83660, 94, 22816, 20, 2040, 12315, 161375, 186367, 44927,
        80704, 19, 4031, 3636, 28510, 23139, 2003, 187284
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġbildet",
            "token_id": 102412,
            "logprob": -0.2268,
            "prob": 0.7969
          },
          {
            "token": "Ġdarstellt",
            "token_id": 254807,
            "logprob": -1.6953,
            "prob": 0.1835
          },
          {
            "token": "Ġaus",
            "token_id": 6523,
            "logprob": -4.7109,
            "prob": 0.009
          },
          {
            "token": "Ġbilden",
            "token_id": 121345,
            "logprob": -5.4531,
            "prob": 0.0043
          },
          {
            "token": "Ġumfasst",
            "token_id": 101309,
            "logprob": -6.9609,
            "prob": 0.0009
          },
          {
            "token": "Ġ\"",
            "token_id": 1789,
            "logprob": -7.1484,
            "prob": 0.0008
          },
          {
            "token": "Ġbesteht",
            "token_id": 57403,
            "logprob": -7.3438,
            "prob": 0.0006
          },
          {
            "token": "Ġgebildet",
            "token_id": 144303,
            "logprob": -7.3516,
            "prob": 0.0006
          },
          {
            "token": "Ġergibt",
            "token_id": 209209,
            "logprob": -7.5312,
            "prob": 0.0005
          },
          {
            "token": "Ġbil",
            "token_id": 10228,
            "logprob": -8.1641,
            "prob": 0.0003
          }
        ]
      },
      "selected_token": {
        "token": "Ġbilden",
        "token_id": 121345,
        "selection_method": "sampling"
      }
    },
    {
      "step": 74,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass alle \"e\" gerechnet werden) fünf Buchstaben \"e\". \n\nAufzählerisches: \n\n- zwei \"e\" in \"Beschwindi-\", einem zusammengeschrieben Wort, das den Hauptteil des Wörter bilden",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass",
        "Ġalle",
        "Ġ\"",
        "e",
        "\"",
        "Ġgere",
        "chnet",
        "Ġwerden",
        ")",
        "Ġfünf",
        "ĠBuch",
        "st",
        "aben",
        "Ġ\"",
        "e",
        "\".",
        "Ġ\n",
        "\n",
        "Auf",
        "z",
        "ähler",
        "isches",
        ":",
        "Ġ\n",
        "\n",
        "-",
        "Ġzwei",
        "Ġ\"",
        "e",
        "\"",
        "Ġin",
        "Ġ\"",
        "Besch",
        "w",
        "indi",
        "-",
        "\",",
        "Ġeinem",
        "Ġzusamm",
        "engesch",
        "rieben",
        "ĠWort",
        ",",
        "Ġdas",
        "Ġden",
        "ĠHaupt",
        "teil",
        "Ġdes",
        "ĠWörter",
        "Ġbilden"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760, 10528, 1789, 76, 9, 27346, 159065, 9178, 16, 44522,
        19893, 1725, 35346, 1789, 76, 3193, 1973, 206, 51214, 97, 137112, 24804, 33, 1973, 206, 20,
        20293, 1789, 76, 9, 1709, 1789, 83660, 94, 22816, 20, 2040, 12315, 161375, 186367, 44927,
        80704, 19, 4031, 3636, 28510, 23139, 2003, 187284, 121345
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "\n",
            "token_id": 206,
            "logprob": -0.0728,
            "prob": 0.9297
          },
          {
            "token": ".",
            "token_id": 21,
            "logprob": -2.9316,
            "prob": 0.0533
          },
          {
            "token": "Ġ",
            "token_id": 228,
            "logprob": -4.4023,
            "prob": 0.0123
          },
          {
            "token": "Ġlässt",
            "token_id": 91540,
            "logprob": -6.6055,
            "prob": 0.0014
          },
          {
            "token": "Ġwird",
            "token_id": 13252,
            "logprob": -7.3398,
            "prob": 0.0006
          },
          {
            "token": "Ġund",
            "token_id": 2280,
            "logprob": -7.9961,
            "prob": 0.0003
          },
          {
            "token": "Ġ\n",
            "token_id": 1973,
            "logprob": -8.0859,
            "prob": 0.0003
          },
          {
            "token": "Ġist",
            "token_id": 5132,
            "logprob": -8.2656,
            "prob": 0.0003
          },
          {
            "token": "Ġwürde",
            "token_id": 114375,
            "logprob": -8.3594,
            "prob": 0.0002
          },
          {
            "token": "Ġ(",
            "token_id": 1737,
            "logprob": -8.4609,
            "prob": 0.0002
          }
        ]
      },
      "selected_token": {
        "token": "Ġ",
        "token_id": 228,
        "selection_method": "sampling"
      }
    },
    {
      "step": 75,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass alle \"e\" gerechnet werden) fünf Buchstaben \"e\". \n\nAufzählerisches: \n\n- zwei \"e\" in \"Beschwindi-\", einem zusammengeschrieben Wort, das den Hauptteil des Wörter bilden ",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass",
        "Ġalle",
        "Ġ\"",
        "e",
        "\"",
        "Ġgere",
        "chnet",
        "Ġwerden",
        ")",
        "Ġfünf",
        "ĠBuch",
        "st",
        "aben",
        "Ġ\"",
        "e",
        "\".",
        "Ġ\n",
        "\n",
        "Auf",
        "z",
        "ähler",
        "isches",
        ":",
        "Ġ\n",
        "\n",
        "-",
        "Ġzwei",
        "Ġ\"",
        "e",
        "\"",
        "Ġin",
        "Ġ\"",
        "Besch",
        "w",
        "indi",
        "-",
        "\",",
        "Ġeinem",
        "Ġzusamm",
        "engesch",
        "rieben",
        "ĠWort",
        ",",
        "Ġdas",
        "Ġden",
        "ĠHaupt",
        "teil",
        "Ġdes",
        "ĠWörter",
        "Ġbilden",
        "Ġ"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760, 10528, 1789, 76, 9, 27346, 159065, 9178, 16, 44522,
        19893, 1725, 35346, 1789, 76, 3193, 1973, 206, 51214, 97, 137112, 24804, 33, 1973, 206, 20,
        20293, 1789, 76, 9, 1709, 1789, 83660, 94, 22816, 20, 2040, 12315, 161375, 186367, 44927,
        80704, 19, 4031, 3636, 28510, 23139, 2003, 187284, 121345, 228
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "\n",
            "token_id": 206,
            "logprob": 0.0,
            "prob": 1.0
          },
          {
            "token": "Ġ(",
            "token_id": 1737,
            "logprob": -17.2344,
            "prob": 0.0
          },
          {
            "token": "<PAD>",
            "token_id": 0,
            "logprob": -36.4688,
            "prob": 0.0
          },
          {
            "token": "<UNK>",
            "token_id": 1,
            "logprob": -32.9688,
            "prob": 0.0
          },
          {
            "token": "<EOS_TOKEN>",
            "token_id": 6,
            "logprob": -21.9062,
            "prob": 0.0
          },
          {
            "token": "<EOP_TOKEN>",
            "token_id": 7,
            "logprob": -36.5312,
            "prob": 0.0
          },
          {
            "token": "<BOS_TOKEN>",
            "token_id": 5,
            "logprob": -34.8438,
            "prob": 0.0
          },
          {
            "token": "<MASK_TOKEN>",
            "token_id": 4,
            "logprob": -36.5,
            "prob": 0.0
          },
          {
            "token": "<CLS>",
            "token_id": 2,
            "logprob": -34.8438,
            "prob": 0.0
          },
          {
            "token": "<SEP>",
            "token_id": 3,
            "logprob": -34.3125,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "\n",
        "token_id": 206,
        "selection_method": "sampling"
      }
    },
    {
      "step": 76,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass alle \"e\" gerechnet werden) fünf Buchstaben \"e\". \n\nAufzählerisches: \n\n- zwei \"e\" in \"Beschwindi-\", einem zusammengeschrieben Wort, das den Hauptteil des Wörter bilden \n",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass",
        "Ġalle",
        "Ġ\"",
        "e",
        "\"",
        "Ġgere",
        "chnet",
        "Ġwerden",
        ")",
        "Ġfünf",
        "ĠBuch",
        "st",
        "aben",
        "Ġ\"",
        "e",
        "\".",
        "Ġ\n",
        "\n",
        "Auf",
        "z",
        "ähler",
        "isches",
        ":",
        "Ġ\n",
        "\n",
        "-",
        "Ġzwei",
        "Ġ\"",
        "e",
        "\"",
        "Ġin",
        "Ġ\"",
        "Besch",
        "w",
        "indi",
        "-",
        "\",",
        "Ġeinem",
        "Ġzusamm",
        "engesch",
        "rieben",
        "ĠWort",
        ",",
        "Ġdas",
        "Ġden",
        "ĠHaupt",
        "teil",
        "Ġdes",
        "ĠWörter",
        "Ġbilden",
        "Ġ",
        "\n"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760, 10528, 1789, 76, 9, 27346, 159065, 9178, 16, 44522,
        19893, 1725, 35346, 1789, 76, 3193, 1973, 206, 51214, 97, 137112, 24804, 33, 1973, 206, 20,
        20293, 1789, 76, 9, 1709, 1789, 83660, 94, 22816, 20, 2040, 12315, 161375, 186367, 44927,
        80704, 19, 4031, 3636, 28510, 23139, 2003, 187284, 121345, 228, 206
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "-",
            "token_id": 20,
            "logprob": 0.0,
            "prob": 1.0
          },
          {
            "token": "<UNK>",
            "token_id": 1,
            "logprob": -40.375,
            "prob": 0.0
          },
          {
            "token": "<SEP>",
            "token_id": 3,
            "logprob": -40.375,
            "prob": 0.0
          },
          {
            "token": "<EOS_TOKEN>",
            "token_id": 6,
            "logprob": -36.625,
            "prob": 0.0
          },
          {
            "token": "<EOP_TOKEN>",
            "token_id": 7,
            "logprob": -40.625,
            "prob": 0.0
          },
          {
            "token": "<PAD>",
            "token_id": 0,
            "logprob": -40.75,
            "prob": 0.0
          },
          {
            "token": "<CLS>",
            "token_id": 2,
            "logprob": -40.9688,
            "prob": 0.0
          },
          {
            "token": "<BOS_TOKEN>",
            "token_id": 5,
            "logprob": -38.4688,
            "prob": 0.0
          },
          {
            "token": "!",
            "token_id": 8,
            "logprob": -32.4062,
            "prob": 0.0
          },
          {
            "token": "<MASK_TOKEN>",
            "token_id": 4,
            "logprob": -40.6562,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "-",
        "token_id": 20,
        "selection_method": "sampling"
      }
    },
    {
      "step": 77,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass alle \"e\" gerechnet werden) fünf Buchstaben \"e\". \n\nAufzählerisches: \n\n- zwei \"e\" in \"Beschwindi-\", einem zusammengeschrieben Wort, das den Hauptteil des Wörter bilden \n-",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass",
        "Ġalle",
        "Ġ\"",
        "e",
        "\"",
        "Ġgere",
        "chnet",
        "Ġwerden",
        ")",
        "Ġfünf",
        "ĠBuch",
        "st",
        "aben",
        "Ġ\"",
        "e",
        "\".",
        "Ġ\n",
        "\n",
        "Auf",
        "z",
        "ähler",
        "isches",
        ":",
        "Ġ\n",
        "\n",
        "-",
        "Ġzwei",
        "Ġ\"",
        "e",
        "\"",
        "Ġin",
        "Ġ\"",
        "Besch",
        "w",
        "indi",
        "-",
        "\",",
        "Ġeinem",
        "Ġzusamm",
        "engesch",
        "rieben",
        "ĠWort",
        ",",
        "Ġdas",
        "Ġden",
        "ĠHaupt",
        "teil",
        "Ġdes",
        "ĠWörter",
        "Ġbilden",
        "Ġ",
        "\n",
        "-"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760, 10528, 1789, 76, 9, 27346, 159065, 9178, 16, 44522,
        19893, 1725, 35346, 1789, 76, 3193, 1973, 206, 51214, 97, 137112, 24804, 33, 1973, 206, 20,
        20293, 1789, 76, 9, 1709, 1789, 83660, 94, 22816, 20, 2040, 12315, 161375, 186367, 44927,
        80704, 19, 4031, 3636, 28510, 23139, 2003, 187284, 121345, 228, 206, 20
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġdrei",
            "token_id": 26089,
            "logprob": -0.2627,
            "prob": 0.769
          },
          {
            "token": "Ġein",
            "token_id": 4001,
            "logprob": -1.6533,
            "prob": 0.1914
          },
          {
            "token": "Ġzwei",
            "token_id": 20293,
            "logprob": -3.2461,
            "prob": 0.0389
          },
          {
            "token": "Ġdre",
            "token_id": 12034,
            "logprob": -8.0625,
            "prob": 0.0003
          },
          {
            "token": "Ġeins",
            "token_id": 114035,
            "logprob": -8.6875,
            "prob": 0.0002
          },
          {
            "token": "Ġeines",
            "token_id": 20792,
            "logprob": -9.2344,
            "prob": 0.0001
          },
          {
            "token": "Ġdrie",
            "token_id": 27875,
            "logprob": -10.6562,
            "prob": 0.0
          },
          {
            "token": "Ġ",
            "token_id": 228,
            "logprob": -10.8125,
            "prob": 0.0
          },
          {
            "token": "Ġeinem",
            "token_id": 12315,
            "logprob": -11.1328,
            "prob": 0.0
          },
          {
            "token": "Ġthree",
            "token_id": 3720,
            "logprob": -11.6016,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġdrei",
        "token_id": 26089,
        "selection_method": "sampling"
      }
    },
    {
      "step": 78,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass alle \"e\" gerechnet werden) fünf Buchstaben \"e\". \n\nAufzählerisches: \n\n- zwei \"e\" in \"Beschwindi-\", einem zusammengeschrieben Wort, das den Hauptteil des Wörter bilden \n- drei",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass",
        "Ġalle",
        "Ġ\"",
        "e",
        "\"",
        "Ġgere",
        "chnet",
        "Ġwerden",
        ")",
        "Ġfünf",
        "ĠBuch",
        "st",
        "aben",
        "Ġ\"",
        "e",
        "\".",
        "Ġ\n",
        "\n",
        "Auf",
        "z",
        "ähler",
        "isches",
        ":",
        "Ġ\n",
        "\n",
        "-",
        "Ġzwei",
        "Ġ\"",
        "e",
        "\"",
        "Ġin",
        "Ġ\"",
        "Besch",
        "w",
        "indi",
        "-",
        "\",",
        "Ġeinem",
        "Ġzusamm",
        "engesch",
        "rieben",
        "ĠWort",
        ",",
        "Ġdas",
        "Ġden",
        "ĠHaupt",
        "teil",
        "Ġdes",
        "ĠWörter",
        "Ġbilden",
        "Ġ",
        "\n",
        "-",
        "Ġdrei"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760, 10528, 1789, 76, 9, 27346, 159065, 9178, 16, 44522,
        19893, 1725, 35346, 1789, 76, 3193, 1973, 206, 51214, 97, 137112, 24804, 33, 1973, 206, 20,
        20293, 1789, 76, 9, 1709, 1789, 83660, 94, 22816, 20, 2040, 12315, 161375, 186367, 44927,
        80704, 19, 4031, 3636, 28510, 23139, 2003, 187284, 121345, 228, 206, 20, 26089
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġ\"",
            "token_id": 1789,
            "logprob": -0.0001,
            "prob": 1.0
          },
          {
            "token": "Ġweitere",
            "token_id": 43392,
            "logprob": -10.1562,
            "prob": 0.0
          },
          {
            "token": "Ġzusätzliche",
            "token_id": 240616,
            "logprob": -10.5469,
            "prob": 0.0
          },
          {
            "token": "Ġ",
            "token_id": 228,
            "logprob": -12.3438,
            "prob": 0.0
          },
          {
            "token": "Ġ„",
            "token_id": 4352,
            "logprob": -14.7344,
            "prob": 0.0
          },
          {
            "token": "Ġzusätz",
            "token_id": 152215,
            "logprob": -15.2422,
            "prob": 0.0
          },
          {
            "token": "Ġzusätzlich",
            "token_id": 133719,
            "logprob": -15.3906,
            "prob": 0.0
          },
          {
            "token": "Ġweiteren",
            "token_id": 62397,
            "logprob": -15.8359,
            "prob": 0.0
          },
          {
            "token": "\"",
            "token_id": 9,
            "logprob": -16.875,
            "prob": 0.0
          },
          {
            "token": "Ġ'",
            "token_id": 2155,
            "logprob": -16.8438,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġ",
        "token_id": 228,
        "selection_method": "sampling"
      }
    },
    {
      "step": 79,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass alle \"e\" gerechnet werden) fünf Buchstaben \"e\". \n\nAufzählerisches: \n\n- zwei \"e\" in \"Beschwindi-\", einem zusammengeschrieben Wort, das den Hauptteil des Wörter bilden \n- drei ",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass",
        "Ġalle",
        "Ġ\"",
        "e",
        "\"",
        "Ġgere",
        "chnet",
        "Ġwerden",
        ")",
        "Ġfünf",
        "ĠBuch",
        "st",
        "aben",
        "Ġ\"",
        "e",
        "\".",
        "Ġ\n",
        "\n",
        "Auf",
        "z",
        "ähler",
        "isches",
        ":",
        "Ġ\n",
        "\n",
        "-",
        "Ġzwei",
        "Ġ\"",
        "e",
        "\"",
        "Ġin",
        "Ġ\"",
        "Besch",
        "w",
        "indi",
        "-",
        "\",",
        "Ġeinem",
        "Ġzusamm",
        "engesch",
        "rieben",
        "ĠWort",
        ",",
        "Ġdas",
        "Ġden",
        "ĠHaupt",
        "teil",
        "Ġdes",
        "ĠWörter",
        "Ġbilden",
        "Ġ",
        "\n",
        "-",
        "Ġdrei",
        "Ġ"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760, 10528, 1789, 76, 9, 27346, 159065, 9178, 16, 44522,
        19893, 1725, 35346, 1789, 76, 3193, 1973, 206, 51214, 97, 137112, 24804, 33, 1973, 206, 20,
        20293, 1789, 76, 9, 1709, 1789, 83660, 94, 22816, 20, 2040, 12315, 161375, 186367, 44927,
        80704, 19, 4031, 3636, 28510, 23139, 2003, 187284, 121345, 228, 206, 20, 26089, 228
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġ\"",
            "token_id": 1789,
            "logprob": -0.0,
            "prob": 1.0
          },
          {
            "token": "Ġzusätzliche",
            "token_id": 240616,
            "logprob": -11.3906,
            "prob": 0.0
          },
          {
            "token": "Ġweitere",
            "token_id": 43392,
            "logprob": -12.125,
            "prob": 0.0
          },
          {
            "token": "Ġseparate",
            "token_id": 12147,
            "logprob": -14.6484,
            "prob": 0.0
          },
          {
            "token": "Ġ„",
            "token_id": 4352,
            "logprob": -14.6875,
            "prob": 0.0
          },
          {
            "token": "\n",
            "token_id": 206,
            "logprob": -14.8203,
            "prob": 0.0
          },
          {
            "token": "Ġzusätzlich",
            "token_id": 133719,
            "logprob": -15.9766,
            "prob": 0.0
          },
          {
            "token": "Ġzusätz",
            "token_id": 152215,
            "logprob": -16.1562,
            "prob": 0.0
          },
          {
            "token": "ĠZus",
            "token_id": 83861,
            "logprob": -15.8906,
            "prob": 0.0
          },
          {
            "token": "Ġ(",
            "token_id": 1737,
            "logprob": -15.7734,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġzusätzliche",
        "token_id": 240616,
        "selection_method": "sampling"
      }
    },
    {
      "step": 80,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass alle \"e\" gerechnet werden) fünf Buchstaben \"e\". \n\nAufzählerisches: \n\n- zwei \"e\" in \"Beschwindi-\", einem zusammengeschrieben Wort, das den Hauptteil des Wörter bilden \n- drei  zusätzliche",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass",
        "Ġalle",
        "Ġ\"",
        "e",
        "\"",
        "Ġgere",
        "chnet",
        "Ġwerden",
        ")",
        "Ġfünf",
        "ĠBuch",
        "st",
        "aben",
        "Ġ\"",
        "e",
        "\".",
        "Ġ\n",
        "\n",
        "Auf",
        "z",
        "ähler",
        "isches",
        ":",
        "Ġ\n",
        "\n",
        "-",
        "Ġzwei",
        "Ġ\"",
        "e",
        "\"",
        "Ġin",
        "Ġ\"",
        "Besch",
        "w",
        "indi",
        "-",
        "\",",
        "Ġeinem",
        "Ġzusamm",
        "engesch",
        "rieben",
        "ĠWort",
        ",",
        "Ġdas",
        "Ġden",
        "ĠHaupt",
        "teil",
        "Ġdes",
        "ĠWörter",
        "Ġbilden",
        "Ġ",
        "\n",
        "-",
        "Ġdrei",
        "Ġ",
        "Ġzusätzliche"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760, 10528, 1789, 76, 9, 27346, 159065, 9178, 16, 44522,
        19893, 1725, 35346, 1789, 76, 3193, 1973, 206, 51214, 97, 137112, 24804, 33, 1973, 206, 20,
        20293, 1789, 76, 9, 1709, 1789, 83660, 94, 22816, 20, 2040, 12315, 161375, 186367, 44927,
        80704, 19, 4031, 3636, 28510, 23139, 2003, 187284, 121345, 228, 206, 20, 26089, 228, 240616
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġ\"",
            "token_id": 1789,
            "logprob": -0.0153,
            "prob": 0.9849
          },
          {
            "token": "Ġ",
            "token_id": 228,
            "logprob": -4.1875,
            "prob": 0.0152
          },
          {
            "token": "Ġ ",
            "token_id": 1667,
            "logprob": -12.2812,
            "prob": 0.0
          },
          {
            "token": "Ġ„",
            "token_id": 4352,
            "logprob": -13.1562,
            "prob": 0.0
          },
          {
            "token": "\"",
            "token_id": 9,
            "logprob": -13.8672,
            "prob": 0.0
          },
          {
            "token": "Ġ'",
            "token_id": 2155,
            "logprob": -14.2656,
            "prob": 0.0
          },
          {
            "token": "ĠBuch",
            "token_id": 19893,
            "logprob": -15.0781,
            "prob": 0.0
          },
          {
            "token": "Ġe",
            "token_id": 1739,
            "logprob": -15.7422,
            "prob": 0.0
          },
          {
            "token": "Ġ  ",
            "token_id": 1742,
            "logprob": -15.8359,
            "prob": 0.0
          },
          {
            "token": "Ġin",
            "token_id": 1709,
            "logprob": -17.1875,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġ\"",
        "token_id": 1789,
        "selection_method": "sampling"
      }
    },
    {
      "step": 81,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass alle \"e\" gerechnet werden) fünf Buchstaben \"e\". \n\nAufzählerisches: \n\n- zwei \"e\" in \"Beschwindi-\", einem zusammengeschrieben Wort, das den Hauptteil des Wörter bilden \n- drei  zusätzliche \"",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass",
        "Ġalle",
        "Ġ\"",
        "e",
        "\"",
        "Ġgere",
        "chnet",
        "Ġwerden",
        ")",
        "Ġfünf",
        "ĠBuch",
        "st",
        "aben",
        "Ġ\"",
        "e",
        "\".",
        "Ġ\n",
        "\n",
        "Auf",
        "z",
        "ähler",
        "isches",
        ":",
        "Ġ\n",
        "\n",
        "-",
        "Ġzwei",
        "Ġ\"",
        "e",
        "\"",
        "Ġin",
        "Ġ\"",
        "Besch",
        "w",
        "indi",
        "-",
        "\",",
        "Ġeinem",
        "Ġzusamm",
        "engesch",
        "rieben",
        "ĠWort",
        ",",
        "Ġdas",
        "Ġden",
        "ĠHaupt",
        "teil",
        "Ġdes",
        "ĠWörter",
        "Ġbilden",
        "Ġ",
        "\n",
        "-",
        "Ġdrei",
        "Ġ",
        "Ġzusätzliche",
        "Ġ\""
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760, 10528, 1789, 76, 9, 27346, 159065, 9178, 16, 44522,
        19893, 1725, 35346, 1789, 76, 3193, 1973, 206, 51214, 97, 137112, 24804, 33, 1973, 206, 20,
        20293, 1789, 76, 9, 1709, 1789, 83660, 94, 22816, 20, 2040, 12315, 161375, 186367, 44927,
        80704, 19, 4031, 3636, 28510, 23139, 2003, 187284, 121345, 228, 206, 20, 26089, 228, 240616,
        1789
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "e",
            "token_id": 76,
            "logprob": -0.0,
            "prob": 1.0
          },
          {
            "token": "E",
            "token_id": 44,
            "logprob": -16.625,
            "prob": 0.0
          },
          {
            "token": "<PAD>",
            "token_id": 0,
            "logprob": -36.5625,
            "prob": 0.0
          },
          {
            "token": "<UNK>",
            "token_id": 1,
            "logprob": -35.5938,
            "prob": 0.0
          },
          {
            "token": "<EOS_TOKEN>",
            "token_id": 6,
            "logprob": -29.0938,
            "prob": 0.0
          },
          {
            "token": "<EOP_TOKEN>",
            "token_id": 7,
            "logprob": -36.6562,
            "prob": 0.0
          },
          {
            "token": "<BOS_TOKEN>",
            "token_id": 5,
            "logprob": -34.375,
            "prob": 0.0
          },
          {
            "token": "<MASK_TOKEN>",
            "token_id": 4,
            "logprob": -36.6562,
            "prob": 0.0
          },
          {
            "token": "<CLS>",
            "token_id": 2,
            "logprob": -36.5,
            "prob": 0.0
          },
          {
            "token": "<SEP>",
            "token_id": 3,
            "logprob": -36.3438,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "e",
        "token_id": 76,
        "selection_method": "sampling"
      }
    },
    {
      "step": 82,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass alle \"e\" gerechnet werden) fünf Buchstaben \"e\". \n\nAufzählerisches: \n\n- zwei \"e\" in \"Beschwindi-\", einem zusammengeschrieben Wort, das den Hauptteil des Wörter bilden \n- drei  zusätzliche \"e",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass",
        "Ġalle",
        "Ġ\"",
        "e",
        "\"",
        "Ġgere",
        "chnet",
        "Ġwerden",
        ")",
        "Ġfünf",
        "ĠBuch",
        "st",
        "aben",
        "Ġ\"",
        "e",
        "\".",
        "Ġ\n",
        "\n",
        "Auf",
        "z",
        "ähler",
        "isches",
        ":",
        "Ġ\n",
        "\n",
        "-",
        "Ġzwei",
        "Ġ\"",
        "e",
        "\"",
        "Ġin",
        "Ġ\"",
        "Besch",
        "w",
        "indi",
        "-",
        "\",",
        "Ġeinem",
        "Ġzusamm",
        "engesch",
        "rieben",
        "ĠWort",
        ",",
        "Ġdas",
        "Ġden",
        "ĠHaupt",
        "teil",
        "Ġdes",
        "ĠWörter",
        "Ġbilden",
        "Ġ",
        "\n",
        "-",
        "Ġdrei",
        "Ġ",
        "Ġzusätzliche",
        "Ġ\"",
        "e"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760, 10528, 1789, 76, 9, 27346, 159065, 9178, 16, 44522,
        19893, 1725, 35346, 1789, 76, 3193, 1973, 206, 51214, 97, 137112, 24804, 33, 1973, 206, 20,
        20293, 1789, 76, 9, 1709, 1789, 83660, 94, 22816, 20, 2040, 12315, 161375, 186367, 44927,
        80704, 19, 4031, 3636, 28510, 23139, 2003, 187284, 121345, 228, 206, 20, 26089, 228, 240616,
        1789, 76
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "\"",
            "token_id": 9,
            "logprob": -0.0,
            "prob": 1.0
          },
          {
            "token": "\",",
            "token_id": 2040,
            "logprob": -10.0938,
            "prob": 0.0
          },
          {
            "token": "Ġ\"",
            "token_id": 1789,
            "logprob": -13.1562,
            "prob": 0.0
          },
          {
            "token": "\"-",
            "token_id": 64403,
            "logprob": -14.4688,
            "prob": 0.0
          },
          {
            "token": "\".",
            "token_id": 3193,
            "logprob": -14.8438,
            "prob": 0.0
          },
          {
            "token": "\":",
            "token_id": 2209,
            "logprob": -15.5312,
            "prob": 0.0
          },
          {
            "token": "\"'",
            "token_id": 59742,
            "logprob": -15.7031,
            "prob": 0.0
          },
          {
            "token": "'s",
            "token_id": 1914,
            "logprob": -16.7656,
            "prob": 0.0
          },
          {
            "token": "Ġin",
            "token_id": 1709,
            "logprob": -16.9688,
            "prob": 0.0
          },
          {
            "token": "'",
            "token_id": 14,
            "logprob": -16.7031,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "\"'",
        "token_id": 59742,
        "selection_method": "sampling"
      }
    },
    {
      "step": 83,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass alle \"e\" gerechnet werden) fünf Buchstaben \"e\". \n\nAufzählerisches: \n\n- zwei \"e\" in \"Beschwindi-\", einem zusammengeschrieben Wort, das den Hauptteil des Wörter bilden \n- drei  zusätzliche \"e\"'",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass",
        "Ġalle",
        "Ġ\"",
        "e",
        "\"",
        "Ġgere",
        "chnet",
        "Ġwerden",
        ")",
        "Ġfünf",
        "ĠBuch",
        "st",
        "aben",
        "Ġ\"",
        "e",
        "\".",
        "Ġ\n",
        "\n",
        "Auf",
        "z",
        "ähler",
        "isches",
        ":",
        "Ġ\n",
        "\n",
        "-",
        "Ġzwei",
        "Ġ\"",
        "e",
        "\"",
        "Ġin",
        "Ġ\"",
        "Besch",
        "w",
        "indi",
        "-",
        "\",",
        "Ġeinem",
        "Ġzusamm",
        "engesch",
        "rieben",
        "ĠWort",
        ",",
        "Ġdas",
        "Ġden",
        "ĠHaupt",
        "teil",
        "Ġdes",
        "ĠWörter",
        "Ġbilden",
        "Ġ",
        "\n",
        "-",
        "Ġdrei",
        "Ġ",
        "Ġzusätzliche",
        "Ġ\"",
        "e",
        "\"'"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760, 10528, 1789, 76, 9, 27346, 159065, 9178, 16, 44522,
        19893, 1725, 35346, 1789, 76, 3193, 1973, 206, 51214, 97, 137112, 24804, 33, 1973, 206, 20,
        20293, 1789, 76, 9, 1709, 1789, 83660, 94, 22816, 20, 2040, 12315, 161375, 186367, 44927,
        80704, 19, 4031, 3636, 28510, 23139, 2003, 187284, 121345, 228, 206, 20, 26089, 228, 240616,
        1789, 76, 59742
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "s",
            "token_id": 90,
            "logprob": -0.0012,
            "prob": 0.9985
          },
          {
            "token": "n",
            "token_id": 85,
            "logprob": -7.0781,
            "prob": 0.0008
          },
          {
            "token": "Ġin",
            "token_id": 1709,
            "logprob": -7.9062,
            "prob": 0.0004
          },
          {
            "token": "Ġ",
            "token_id": 228,
            "logprob": -12.0781,
            "prob": 0.0
          },
          {
            "token": "Ġim",
            "token_id": 2101,
            "logprob": -12.4375,
            "prob": 0.0
          },
          {
            "token": "in",
            "token_id": 1670,
            "logprob": -13.0625,
            "prob": 0.0
          },
          {
            "token": "S",
            "token_id": 58,
            "logprob": -13.625,
            "prob": 0.0
          },
          {
            "token": "en",
            "token_id": 1673,
            "logprob": -14.7656,
            "prob": 0.0
          },
          {
            "token": "Ġs",
            "token_id": 1679,
            "logprob": -15.1016,
            "prob": 0.0
          },
          {
            "token": "m",
            "token_id": 84,
            "logprob": -16.0156,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "s",
        "token_id": 90,
        "selection_method": "sampling"
      }
    },
    {
      "step": 84,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass alle \"e\" gerechnet werden) fünf Buchstaben \"e\". \n\nAufzählerisches: \n\n- zwei \"e\" in \"Beschwindi-\", einem zusammengeschrieben Wort, das den Hauptteil des Wörter bilden \n- drei  zusätzliche \"e\"'s",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass",
        "Ġalle",
        "Ġ\"",
        "e",
        "\"",
        "Ġgere",
        "chnet",
        "Ġwerden",
        ")",
        "Ġfünf",
        "ĠBuch",
        "st",
        "aben",
        "Ġ\"",
        "e",
        "\".",
        "Ġ\n",
        "\n",
        "Auf",
        "z",
        "ähler",
        "isches",
        ":",
        "Ġ\n",
        "\n",
        "-",
        "Ġzwei",
        "Ġ\"",
        "e",
        "\"",
        "Ġin",
        "Ġ\"",
        "Besch",
        "w",
        "indi",
        "-",
        "\",",
        "Ġeinem",
        "Ġzusamm",
        "engesch",
        "rieben",
        "ĠWort",
        ",",
        "Ġdas",
        "Ġden",
        "ĠHaupt",
        "teil",
        "Ġdes",
        "ĠWörter",
        "Ġbilden",
        "Ġ",
        "\n",
        "-",
        "Ġdrei",
        "Ġ",
        "Ġzusätzliche",
        "Ġ\"",
        "e",
        "\"'",
        "s"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760, 10528, 1789, 76, 9, 27346, 159065, 9178, 16, 44522,
        19893, 1725, 35346, 1789, 76, 3193, 1973, 206, 51214, 97, 137112, 24804, 33, 1973, 206, 20,
        20293, 1789, 76, 9, 1709, 1789, 83660, 94, 22816, 20, 2040, 12315, 161375, 186367, 44927,
        80704, 19, 4031, 3636, 28510, 23139, 2003, 187284, 121345, 228, 206, 20, 26089, 228, 240616,
        1789, 76, 59742, 90
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġin",
            "token_id": 1709,
            "logprob": -0.0214,
            "prob": 0.979
          },
          {
            "token": "Ġim",
            "token_id": 2101,
            "logprob": -4.4258,
            "prob": 0.0119
          },
          {
            "token": "Ġ",
            "token_id": 228,
            "logprob": -4.9258,
            "prob": 0.0072
          },
          {
            "token": "Ġnach",
            "token_id": 7920,
            "logprob": -6.7695,
            "prob": 0.0011
          },
          {
            "token": "Ġam",
            "token_id": 2437,
            "logprob": -7.5977,
            "prob": 0.0005
          },
          {
            "token": "Ġan",
            "token_id": 1844,
            "logprob": -8.5781,
            "prob": 0.0002
          },
          {
            "token": ":",
            "token_id": 33,
            "logprob": -9.7656,
            "prob": 0.0001
          },
          {
            "token": "Ġauf",
            "token_id": 5709,
            "logprob": -11.1406,
            "prob": 0.0
          },
          {
            "token": "Ġ(",
            "token_id": 1737,
            "logprob": -11.2891,
            "prob": 0.0
          },
          {
            "token": "Ġals",
            "token_id": 4561,
            "logprob": -11.5156,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġ",
        "token_id": 228,
        "selection_method": "sampling"
      }
    },
    {
      "step": 85,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass alle \"e\" gerechnet werden) fünf Buchstaben \"e\". \n\nAufzählerisches: \n\n- zwei \"e\" in \"Beschwindi-\", einem zusammengeschrieben Wort, das den Hauptteil des Wörter bilden \n- drei  zusätzliche \"e\"'s ",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass",
        "Ġalle",
        "Ġ\"",
        "e",
        "\"",
        "Ġgere",
        "chnet",
        "Ġwerden",
        ")",
        "Ġfünf",
        "ĠBuch",
        "st",
        "aben",
        "Ġ\"",
        "e",
        "\".",
        "Ġ\n",
        "\n",
        "Auf",
        "z",
        "ähler",
        "isches",
        ":",
        "Ġ\n",
        "\n",
        "-",
        "Ġzwei",
        "Ġ\"",
        "e",
        "\"",
        "Ġin",
        "Ġ\"",
        "Besch",
        "w",
        "indi",
        "-",
        "\",",
        "Ġeinem",
        "Ġzusamm",
        "engesch",
        "rieben",
        "ĠWort",
        ",",
        "Ġdas",
        "Ġden",
        "ĠHaupt",
        "teil",
        "Ġdes",
        "ĠWörter",
        "Ġbilden",
        "Ġ",
        "\n",
        "-",
        "Ġdrei",
        "Ġ",
        "Ġzusätzliche",
        "Ġ\"",
        "e",
        "\"'",
        "s",
        "Ġ"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760, 10528, 1789, 76, 9, 27346, 159065, 9178, 16, 44522,
        19893, 1725, 35346, 1789, 76, 3193, 1973, 206, 51214, 97, 137112, 24804, 33, 1973, 206, 20,
        20293, 1789, 76, 9, 1709, 1789, 83660, 94, 22816, 20, 2040, 12315, 161375, 186367, 44927,
        80704, 19, 4031, 3636, 28510, 23139, 2003, 187284, 121345, 228, 206, 20, 26089, 228, 240616,
        1789, 76, 59742, 90, 228
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġin",
            "token_id": 1709,
            "logprob": -0.0038,
            "prob": 0.9961
          },
          {
            "token": "Ġim",
            "token_id": 2101,
            "logprob": -5.8008,
            "prob": 0.003
          },
          {
            "token": "Ġnach",
            "token_id": 7920,
            "logprob": -7.5664,
            "prob": 0.0005
          },
          {
            "token": "Ġan",
            "token_id": 1844,
            "logprob": -8.75,
            "prob": 0.0002
          },
          {
            "token": "Ġam",
            "token_id": 2437,
            "logprob": -11.1641,
            "prob": 0.0
          },
          {
            "token": "\n",
            "token_id": 206,
            "logprob": -11.2031,
            "prob": 0.0
          },
          {
            "token": "Ġ(",
            "token_id": 1737,
            "logprob": -11.2969,
            "prob": 0.0
          },
          {
            "token": "Ġzwischen",
            "token_id": 21459,
            "logprob": -12.9141,
            "prob": 0.0
          },
          {
            "token": "Ġauf",
            "token_id": 5709,
            "logprob": -13.2031,
            "prob": 0.0
          },
          {
            "token": "Ġvor",
            "token_id": 9977,
            "logprob": -13.7656,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġin",
        "token_id": 1709,
        "selection_method": "sampling"
      }
    },
    {
      "step": 86,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass alle \"e\" gerechnet werden) fünf Buchstaben \"e\". \n\nAufzählerisches: \n\n- zwei \"e\" in \"Beschwindi-\", einem zusammengeschrieben Wort, das den Hauptteil des Wörter bilden \n- drei  zusätzliche \"e\"'s  in",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass",
        "Ġalle",
        "Ġ\"",
        "e",
        "\"",
        "Ġgere",
        "chnet",
        "Ġwerden",
        ")",
        "Ġfünf",
        "ĠBuch",
        "st",
        "aben",
        "Ġ\"",
        "e",
        "\".",
        "Ġ\n",
        "\n",
        "Auf",
        "z",
        "ähler",
        "isches",
        ":",
        "Ġ\n",
        "\n",
        "-",
        "Ġzwei",
        "Ġ\"",
        "e",
        "\"",
        "Ġin",
        "Ġ\"",
        "Besch",
        "w",
        "indi",
        "-",
        "\",",
        "Ġeinem",
        "Ġzusamm",
        "engesch",
        "rieben",
        "ĠWort",
        ",",
        "Ġdas",
        "Ġden",
        "ĠHaupt",
        "teil",
        "Ġdes",
        "ĠWörter",
        "Ġbilden",
        "Ġ",
        "\n",
        "-",
        "Ġdrei",
        "Ġ",
        "Ġzusätzliche",
        "Ġ\"",
        "e",
        "\"'",
        "s",
        "Ġ",
        "Ġin"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760, 10528, 1789, 76, 9, 27346, 159065, 9178, 16, 44522,
        19893, 1725, 35346, 1789, 76, 3193, 1973, 206, 51214, 97, 137112, 24804, 33, 1973, 206, 20,
        20293, 1789, 76, 9, 1709, 1789, 83660, 94, 22816, 20, 2040, 12315, 161375, 186367, 44927,
        80704, 19, 4031, 3636, 28510, 23139, 2003, 187284, 121345, 228, 206, 20, 26089, 228, 240616,
        1789, 76, 59742, 90, 228, 1709
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġ\"-",
            "token_id": 20668,
            "logprob": -0.1714,
            "prob": 0.8423
          },
          {
            "token": "Ġ\"",
            "token_id": 1789,
            "logprob": -1.875,
            "prob": 0.1534
          },
          {
            "token": "Ġ",
            "token_id": 228,
            "logprob": -5.6562,
            "prob": 0.0035
          },
          {
            "token": "Ġder",
            "token_id": 2479,
            "logprob": -8.0156,
            "prob": 0.0003
          },
          {
            "token": "\"-",
            "token_id": 64403,
            "logprob": -8.9062,
            "prob": 0.0001
          },
          {
            "token": "Ġdem",
            "token_id": 3297,
            "logprob": -9.8125,
            "prob": 0.0001
          },
          {
            "token": "Ġ'-",
            "token_id": 41468,
            "logprob": -10.125,
            "prob": 0.0
          },
          {
            "token": "(\"-",
            "token_id": 64330,
            "logprob": -11.2656,
            "prob": 0.0
          },
          {
            "token": "Ġden",
            "token_id": 3636,
            "logprob": -11.5156,
            "prob": 0.0
          },
          {
            "token": "Ġ\"--",
            "token_id": 41764,
            "logprob": -12.0312,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġden",
        "token_id": 3636,
        "selection_method": "sampling"
      }
    },
    {
      "step": 87,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass alle \"e\" gerechnet werden) fünf Buchstaben \"e\". \n\nAufzählerisches: \n\n- zwei \"e\" in \"Beschwindi-\", einem zusammengeschrieben Wort, das den Hauptteil des Wörter bilden \n- drei  zusätzliche \"e\"'s  in den",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass",
        "Ġalle",
        "Ġ\"",
        "e",
        "\"",
        "Ġgere",
        "chnet",
        "Ġwerden",
        ")",
        "Ġfünf",
        "ĠBuch",
        "st",
        "aben",
        "Ġ\"",
        "e",
        "\".",
        "Ġ\n",
        "\n",
        "Auf",
        "z",
        "ähler",
        "isches",
        ":",
        "Ġ\n",
        "\n",
        "-",
        "Ġzwei",
        "Ġ\"",
        "e",
        "\"",
        "Ġin",
        "Ġ\"",
        "Besch",
        "w",
        "indi",
        "-",
        "\",",
        "Ġeinem",
        "Ġzusamm",
        "engesch",
        "rieben",
        "ĠWort",
        ",",
        "Ġdas",
        "Ġden",
        "ĠHaupt",
        "teil",
        "Ġdes",
        "ĠWörter",
        "Ġbilden",
        "Ġ",
        "\n",
        "-",
        "Ġdrei",
        "Ġ",
        "Ġzusätzliche",
        "Ġ\"",
        "e",
        "\"'",
        "s",
        "Ġ",
        "Ġin",
        "Ġden"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760, 10528, 1789, 76, 9, 27346, 159065, 9178, 16, 44522,
        19893, 1725, 35346, 1789, 76, 3193, 1973, 206, 51214, 97, 137112, 24804, 33, 1973, 206, 20,
        20293, 1789, 76, 9, 1709, 1789, 83660, 94, 22816, 20, 2040, 12315, 161375, 186367, 44927,
        80704, 19, 4031, 3636, 28510, 23139, 2003, 187284, 121345, 228, 206, 20, 26089, 228, 240616,
        1789, 76, 59742, 90, 228, 1709, 3636
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "ĠS",
            "token_id": 1721,
            "logprob": -0.7681,
            "prob": 0.4639
          },
          {
            "token": "ĠEnd",
            "token_id": 8104,
            "logprob": -1.7607,
            "prob": 0.172
          },
          {
            "token": "Ġbeiden",
            "token_id": 35412,
            "logprob": -2.1426,
            "prob": 0.1172
          },
          {
            "token": "ĠSil",
            "token_id": 6823,
            "logprob": -2.2285,
            "prob": 0.1076
          },
          {
            "token": "ĠNach",
            "token_id": 13475,
            "logprob": -3.6973,
            "prob": 0.0248
          },
          {
            "token": "ĠWör",
            "token_id": 104945,
            "logprob": -4.4961,
            "prob": 0.0112
          },
          {
            "token": "ĠWort",
            "token_id": 80704,
            "logprob": -4.6367,
            "prob": 0.0097
          },
          {
            "token": "Ġzwei",
            "token_id": 20293,
            "logprob": -4.6445,
            "prob": 0.0096
          },
          {
            "token": "Ġeinzelnen",
            "token_id": 119605,
            "logprob": -4.6602,
            "prob": 0.0095
          },
          {
            "token": "Ġ",
            "token_id": 228,
            "logprob": -4.8242,
            "prob": 0.008
          }
        ]
      },
      "selected_token": {
        "token": "Ġ",
        "token_id": 228,
        "selection_method": "sampling"
      }
    },
    {
      "step": 88,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass alle \"e\" gerechnet werden) fünf Buchstaben \"e\". \n\nAufzählerisches: \n\n- zwei \"e\" in \"Beschwindi-\", einem zusammengeschrieben Wort, das den Hauptteil des Wörter bilden \n- drei  zusätzliche \"e\"'s  in den ",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass",
        "Ġalle",
        "Ġ\"",
        "e",
        "\"",
        "Ġgere",
        "chnet",
        "Ġwerden",
        ")",
        "Ġfünf",
        "ĠBuch",
        "st",
        "aben",
        "Ġ\"",
        "e",
        "\".",
        "Ġ\n",
        "\n",
        "Auf",
        "z",
        "ähler",
        "isches",
        ":",
        "Ġ\n",
        "\n",
        "-",
        "Ġzwei",
        "Ġ\"",
        "e",
        "\"",
        "Ġin",
        "Ġ\"",
        "Besch",
        "w",
        "indi",
        "-",
        "\",",
        "Ġeinem",
        "Ġzusamm",
        "engesch",
        "rieben",
        "ĠWort",
        ",",
        "Ġdas",
        "Ġden",
        "ĠHaupt",
        "teil",
        "Ġdes",
        "ĠWörter",
        "Ġbilden",
        "Ġ",
        "\n",
        "-",
        "Ġdrei",
        "Ġ",
        "Ġzusätzliche",
        "Ġ\"",
        "e",
        "\"'",
        "s",
        "Ġ",
        "Ġin",
        "Ġden",
        "Ġ"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760, 10528, 1789, 76, 9, 27346, 159065, 9178, 16, 44522,
        19893, 1725, 35346, 1789, 76, 3193, 1973, 206, 51214, 97, 137112, 24804, 33, 1973, 206, 20,
        20293, 1789, 76, 9, 1709, 1789, 83660, 94, 22816, 20, 2040, 12315, 161375, 186367, 44927,
        80704, 19, 4031, 3636, 28510, 23139, 2003, 187284, 121345, 228, 206, 20, 26089, 228, 240616,
        1789, 76, 59742, 90, 228, 1709, 3636, 228
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "ĠEnd",
            "token_id": 8104,
            "logprob": -0.626,
            "prob": 0.5347
          },
          {
            "token": "ĠS",
            "token_id": 1721,
            "logprob": -1.501,
            "prob": 0.2229
          },
          {
            "token": "ĠSil",
            "token_id": 6823,
            "logprob": -2.1738,
            "prob": 0.1138
          },
          {
            "token": "ĠWort",
            "token_id": 80704,
            "logprob": -2.9629,
            "prob": 0.0517
          },
          {
            "token": "ĠNach",
            "token_id": 13475,
            "logprob": -4.4688,
            "prob": 0.0115
          },
          {
            "token": "ĠWör",
            "token_id": 104945,
            "logprob": -4.7344,
            "prob": 0.0088
          },
          {
            "token": "Ġbeiden",
            "token_id": 35412,
            "logprob": -4.9453,
            "prob": 0.0071
          },
          {
            "token": "ĠZus",
            "token_id": 83861,
            "logprob": -5.4766,
            "prob": 0.0042
          },
          {
            "token": "Ġverschiedenen",
            "token_id": 65541,
            "logprob": -5.9609,
            "prob": 0.0026
          },
          {
            "token": "ĠAble",
            "token_id": 114595,
            "logprob": -6.1172,
            "prob": 0.0022
          }
        ]
      },
      "selected_token": {
        "token": "ĠSil",
        "token_id": 6823,
        "selection_method": "sampling"
      }
    },
    {
      "step": 89,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass alle \"e\" gerechnet werden) fünf Buchstaben \"e\". \n\nAufzählerisches: \n\n- zwei \"e\" in \"Beschwindi-\", einem zusammengeschrieben Wort, das den Hauptteil des Wörter bilden \n- drei  zusätzliche \"e\"'s  in den  Sil",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass",
        "Ġalle",
        "Ġ\"",
        "e",
        "\"",
        "Ġgere",
        "chnet",
        "Ġwerden",
        ")",
        "Ġfünf",
        "ĠBuch",
        "st",
        "aben",
        "Ġ\"",
        "e",
        "\".",
        "Ġ\n",
        "\n",
        "Auf",
        "z",
        "ähler",
        "isches",
        ":",
        "Ġ\n",
        "\n",
        "-",
        "Ġzwei",
        "Ġ\"",
        "e",
        "\"",
        "Ġin",
        "Ġ\"",
        "Besch",
        "w",
        "indi",
        "-",
        "\",",
        "Ġeinem",
        "Ġzusamm",
        "engesch",
        "rieben",
        "ĠWort",
        ",",
        "Ġdas",
        "Ġden",
        "ĠHaupt",
        "teil",
        "Ġdes",
        "ĠWörter",
        "Ġbilden",
        "Ġ",
        "\n",
        "-",
        "Ġdrei",
        "Ġ",
        "Ġzusätzliche",
        "Ġ\"",
        "e",
        "\"'",
        "s",
        "Ġ",
        "Ġin",
        "Ġden",
        "Ġ",
        "ĠSil"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760, 10528, 1789, 76, 9, 27346, 159065, 9178, 16, 44522,
        19893, 1725, 35346, 1789, 76, 3193, 1973, 206, 51214, 97, 137112, 24804, 33, 1973, 206, 20,
        20293, 1789, 76, 9, 1709, 1789, 83660, 94, 22816, 20, 2040, 12315, 161375, 186367, 44927,
        80704, 19, 4031, 3636, 28510, 23139, 2003, 187284, 121345, 228, 206, 20, 26089, 228, 240616,
        1789, 76, 59742, 90, 228, 1709, 3636, 228, 6823
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "ben",
            "token_id": 5881,
            "logprob": -0.0001,
            "prob": 1.0
          },
          {
            "token": "aben",
            "token_id": 35346,
            "logprob": -9.6406,
            "prob": 0.0001
          },
          {
            "token": "b",
            "token_id": 73,
            "logprob": -10.1406,
            "prob": 0.0
          },
          {
            "token": "be",
            "token_id": 2901,
            "logprob": -11.1562,
            "prob": 0.0
          },
          {
            "token": "bent",
            "token_id": 87813,
            "logprob": -12.7969,
            "prob": 0.0
          },
          {
            "token": "big",
            "token_id": 22790,
            "logprob": -13.0781,
            "prob": 0.0
          },
          {
            "token": "bien",
            "token_id": 193651,
            "logprob": -13.125,
            "prob": 0.0
          },
          {
            "token": "eben",
            "token_id": 26384,
            "logprob": -13.1719,
            "prob": 0.0
          },
          {
            "token": "ab",
            "token_id": 1816,
            "logprob": -13.2188,
            "prob": 0.0
          },
          {
            "token": "bend",
            "token_id": 119911,
            "logprob": -13.2344,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "b",
        "token_id": 73,
        "selection_method": "sampling"
      }
    },
    {
      "step": 90,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass alle \"e\" gerechnet werden) fünf Buchstaben \"e\". \n\nAufzählerisches: \n\n- zwei \"e\" in \"Beschwindi-\", einem zusammengeschrieben Wort, das den Hauptteil des Wörter bilden \n- drei  zusätzliche \"e\"'s  in den  Silb",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass",
        "Ġalle",
        "Ġ\"",
        "e",
        "\"",
        "Ġgere",
        "chnet",
        "Ġwerden",
        ")",
        "Ġfünf",
        "ĠBuch",
        "st",
        "aben",
        "Ġ\"",
        "e",
        "\".",
        "Ġ\n",
        "\n",
        "Auf",
        "z",
        "ähler",
        "isches",
        ":",
        "Ġ\n",
        "\n",
        "-",
        "Ġzwei",
        "Ġ\"",
        "e",
        "\"",
        "Ġin",
        "Ġ\"",
        "Besch",
        "w",
        "indi",
        "-",
        "\",",
        "Ġeinem",
        "Ġzusamm",
        "engesch",
        "rieben",
        "ĠWort",
        ",",
        "Ġdas",
        "Ġden",
        "ĠHaupt",
        "teil",
        "Ġdes",
        "ĠWörter",
        "Ġbilden",
        "Ġ",
        "\n",
        "-",
        "Ġdrei",
        "Ġ",
        "Ġzusätzliche",
        "Ġ\"",
        "e",
        "\"'",
        "s",
        "Ġ",
        "Ġin",
        "Ġden",
        "Ġ",
        "ĠSil",
        "b"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760, 10528, 1789, 76, 9, 27346, 159065, 9178, 16, 44522,
        19893, 1725, 35346, 1789, 76, 3193, 1973, 206, 51214, 97, 137112, 24804, 33, 1973, 206, 20,
        20293, 1789, 76, 9, 1709, 1789, 83660, 94, 22816, 20, 2040, 12315, 161375, 186367, 44927,
        80704, 19, 4031, 3636, 28510, 23139, 2003, 187284, 121345, 228, 206, 20, 26089, 228, 240616,
        1789, 76, 59742, 90, 228, 1709, 3636, 228, 6823, 73
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "ente",
            "token_id": 2416,
            "logprob": -0.7651,
            "prob": 0.4653
          },
          {
            "token": "ens",
            "token_id": 2028,
            "logprob": -2.0625,
            "prob": 0.1272
          },
          {
            "token": "enden",
            "token_id": 11404,
            "logprob": -2.4219,
            "prob": 0.0888
          },
          {
            "token": "ena",
            "token_id": 3524,
            "logprob": -2.625,
            "prob": 0.0724
          },
          {
            "token": "enn",
            "token_id": 3259,
            "logprob": -2.8438,
            "prob": 0.0582
          },
          {
            "token": "env",
            "token_id": 12224,
            "logprob": -3.5547,
            "prob": 0.0286
          },
          {
            "token": "ende",
            "token_id": 5087,
            "logprob": -3.7656,
            "prob": 0.0232
          },
          {
            "token": "endes",
            "token_id": 55258,
            "logprob": -4.375,
            "prob": 0.0126
          },
          {
            "token": "ense",
            "token_id": 3465,
            "logprob": -4.4141,
            "prob": 0.0121
          },
          {
            "token": "ensch",
            "token_id": 12527,
            "logprob": -4.5312,
            "prob": 0.0108
          }
        ]
      },
      "selected_token": {
        "token": "enn",
        "token_id": 3259,
        "selection_method": "sampling"
      }
    },
    {
      "step": 91,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass alle \"e\" gerechnet werden) fünf Buchstaben \"e\". \n\nAufzählerisches: \n\n- zwei \"e\" in \"Beschwindi-\", einem zusammengeschrieben Wort, das den Hauptteil des Wörter bilden \n- drei  zusätzliche \"e\"'s  in den  Silbenn",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass",
        "Ġalle",
        "Ġ\"",
        "e",
        "\"",
        "Ġgere",
        "chnet",
        "Ġwerden",
        ")",
        "Ġfünf",
        "ĠBuch",
        "st",
        "aben",
        "Ġ\"",
        "e",
        "\".",
        "Ġ\n",
        "\n",
        "Auf",
        "z",
        "ähler",
        "isches",
        ":",
        "Ġ\n",
        "\n",
        "-",
        "Ġzwei",
        "Ġ\"",
        "e",
        "\"",
        "Ġin",
        "Ġ\"",
        "Besch",
        "w",
        "indi",
        "-",
        "\",",
        "Ġeinem",
        "Ġzusamm",
        "engesch",
        "rieben",
        "ĠWort",
        ",",
        "Ġdas",
        "Ġden",
        "ĠHaupt",
        "teil",
        "Ġdes",
        "ĠWörter",
        "Ġbilden",
        "Ġ",
        "\n",
        "-",
        "Ġdrei",
        "Ġ",
        "Ġzusätzliche",
        "Ġ\"",
        "e",
        "\"'",
        "s",
        "Ġ",
        "Ġin",
        "Ġden",
        "Ġ",
        "ĠSil",
        "b",
        "enn"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760, 10528, 1789, 76, 9, 27346, 159065, 9178, 16, 44522,
        19893, 1725, 35346, 1789, 76, 3193, 1973, 206, 51214, 97, 137112, 24804, 33, 1973, 206, 20,
        20293, 1789, 76, 9, 1709, 1789, 83660, 94, 22816, 20, 2040, 12315, 161375, 186367, 44927,
        80704, 19, 4031, 3636, 28510, 23139, 2003, 187284, 121345, 228, 206, 20, 26089, 228, 240616,
        1789, 76, 59742, 90, 228, 1709, 3636, 228, 6823, 73, 3259
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "ah",
            "token_id": 2058,
            "logprob": -0.8325,
            "prob": 0.4351
          },
          {
            "token": "ach",
            "token_id": 2069,
            "logprob": -0.9419,
            "prob": 0.3899
          },
          {
            "token": "ä",
            "token_id": 2689,
            "logprob": -3.0117,
            "prob": 0.0492
          },
          {
            "token": "amen",
            "token_id": 8460,
            "logprob": -3.6055,
            "prob": 0.0272
          },
          {
            "token": "enden",
            "token_id": 11404,
            "logprob": -4.4961,
            "prob": 0.0111
          },
          {
            "token": "ах",
            "token_id": 4474,
            "logprob": -4.793,
            "prob": 0.0083
          },
          {
            "token": "ationen",
            "token_id": 62823,
            "logprob": -4.9023,
            "prob": 0.0074
          },
          {
            "token": "achen",
            "token_id": 34344,
            "logprob": -5.0664,
            "prob": 0.0063
          },
          {
            "token": "ahr",
            "token_id": 6456,
            "logprob": -5.3242,
            "prob": 0.0049
          },
          {
            "token": "äch",
            "token_id": 19512,
            "logprob": -5.5273,
            "prob": 0.004
          }
        ]
      },
      "selected_token": {
        "token": "ahr",
        "token_id": 6456,
        "selection_method": "sampling"
      }
    },
    {
      "step": 92,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass alle \"e\" gerechnet werden) fünf Buchstaben \"e\". \n\nAufzählerisches: \n\n- zwei \"e\" in \"Beschwindi-\", einem zusammengeschrieben Wort, das den Hauptteil des Wörter bilden \n- drei  zusätzliche \"e\"'s  in den  Silbennahr",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass",
        "Ġalle",
        "Ġ\"",
        "e",
        "\"",
        "Ġgere",
        "chnet",
        "Ġwerden",
        ")",
        "Ġfünf",
        "ĠBuch",
        "st",
        "aben",
        "Ġ\"",
        "e",
        "\".",
        "Ġ\n",
        "\n",
        "Auf",
        "z",
        "ähler",
        "isches",
        ":",
        "Ġ\n",
        "\n",
        "-",
        "Ġzwei",
        "Ġ\"",
        "e",
        "\"",
        "Ġin",
        "Ġ\"",
        "Besch",
        "w",
        "indi",
        "-",
        "\",",
        "Ġeinem",
        "Ġzusamm",
        "engesch",
        "rieben",
        "ĠWort",
        ",",
        "Ġdas",
        "Ġden",
        "ĠHaupt",
        "teil",
        "Ġdes",
        "ĠWörter",
        "Ġbilden",
        "Ġ",
        "\n",
        "-",
        "Ġdrei",
        "Ġ",
        "Ġzusätzliche",
        "Ġ\"",
        "e",
        "\"'",
        "s",
        "Ġ",
        "Ġin",
        "Ġden",
        "Ġ",
        "ĠSil",
        "b",
        "enn",
        "ahr"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760, 10528, 1789, 76, 9, 27346, 159065, 9178, 16, 44522,
        19893, 1725, 35346, 1789, 76, 3193, 1973, 206, 51214, 97, 137112, 24804, 33, 1973, 206, 20,
        20293, 1789, 76, 9, 1709, 1789, 83660, 94, 22816, 20, 2040, 12315, 161375, 186367, 44927,
        80704, 19, 4031, 3636, 28510, 23139, 2003, 187284, 121345, 228, 206, 20, 26089, 228, 240616,
        1789, 76, 59742, 90, 228, 1709, 3636, 228, 6823, 73, 3259, 6456
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "ungen",
            "token_id": 7868,
            "logprob": -0.0694,
            "prob": 0.9331
          },
          {
            "token": "ung",
            "token_id": 2359,
            "logprob": -2.8047,
            "prob": 0.0606
          },
          {
            "token": "ungsk",
            "token_id": 117606,
            "logprob": -6.0703,
            "prob": 0.0023
          },
          {
            "token": "ungs",
            "token_id": 10484,
            "logprob": -6.5859,
            "prob": 0.0014
          },
          {
            "token": "k",
            "token_id": 82,
            "logprob": -6.7109,
            "prob": 0.0012
          },
          {
            "token": "heiten",
            "token_id": 58196,
            "logprob": -7.3672,
            "prob": 0.0006
          },
          {
            "token": "en",
            "token_id": 1673,
            "logprob": -8.2891,
            "prob": 0.0003
          },
          {
            "token": "te",
            "token_id": 1822,
            "logprob": -9.1797,
            "prob": 0.0001
          },
          {
            "token": "ingen",
            "token_id": 8303,
            "logprob": -9.9375,
            "prob": 0.0
          },
          {
            "token": "hen",
            "token_id": 11942,
            "logprob": -9.9375,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "ungen",
        "token_id": 7868,
        "selection_method": "sampling"
      }
    },
    {
      "step": 93,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass alle \"e\" gerechnet werden) fünf Buchstaben \"e\". \n\nAufzählerisches: \n\n- zwei \"e\" in \"Beschwindi-\", einem zusammengeschrieben Wort, das den Hauptteil des Wörter bilden \n- drei  zusätzliche \"e\"'s  in den  Silbennahrungen",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass",
        "Ġalle",
        "Ġ\"",
        "e",
        "\"",
        "Ġgere",
        "chnet",
        "Ġwerden",
        ")",
        "Ġfünf",
        "ĠBuch",
        "st",
        "aben",
        "Ġ\"",
        "e",
        "\".",
        "Ġ\n",
        "\n",
        "Auf",
        "z",
        "ähler",
        "isches",
        ":",
        "Ġ\n",
        "\n",
        "-",
        "Ġzwei",
        "Ġ\"",
        "e",
        "\"",
        "Ġin",
        "Ġ\"",
        "Besch",
        "w",
        "indi",
        "-",
        "\",",
        "Ġeinem",
        "Ġzusamm",
        "engesch",
        "rieben",
        "ĠWort",
        ",",
        "Ġdas",
        "Ġden",
        "ĠHaupt",
        "teil",
        "Ġdes",
        "ĠWörter",
        "Ġbilden",
        "Ġ",
        "\n",
        "-",
        "Ġdrei",
        "Ġ",
        "Ġzusätzliche",
        "Ġ\"",
        "e",
        "\"'",
        "s",
        "Ġ",
        "Ġin",
        "Ġden",
        "Ġ",
        "ĠSil",
        "b",
        "enn",
        "ahr",
        "ungen"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760, 10528, 1789, 76, 9, 27346, 159065, 9178, 16, 44522,
        19893, 1725, 35346, 1789, 76, 3193, 1973, 206, 51214, 97, 137112, 24804, 33, 1973, 206, 20,
        20293, 1789, 76, 9, 1709, 1789, 83660, 94, 22816, 20, 2040, 12315, 161375, 186367, 44927,
        80704, 19, 4031, 3636, 28510, 23139, 2003, 187284, 121345, 228, 206, 20, 26089, 228, 240616,
        1789, 76, 59742, 90, 228, 1709, 3636, 228, 6823, 73, 3259, 6456, 7868
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġ\"",
            "token_id": 1789,
            "logprob": -0.1984,
            "prob": 0.8203
          },
          {
            "token": "Ġ\"-",
            "token_id": 20668,
            "logprob": -2.8867,
            "prob": 0.0558
          },
          {
            "token": "Ġ",
            "token_id": 228,
            "logprob": -3.0117,
            "prob": 0.0493
          },
          {
            "token": "Ġvon",
            "token_id": 3615,
            "logprob": -3.043,
            "prob": 0.0477
          },
          {
            "token": "Ġder",
            "token_id": 2479,
            "logprob": -4.2305,
            "prob": 0.0146
          },
          {
            "token": "Ġ-",
            "token_id": 1945,
            "logprob": -4.8867,
            "prob": 0.0076
          },
          {
            "token": "Ġ(-",
            "token_id": 24468,
            "logprob": -6.1836,
            "prob": 0.0021
          },
          {
            "token": ":",
            "token_id": 33,
            "logprob": -6.8086,
            "prob": 0.0011
          },
          {
            "token": "Ġdes",
            "token_id": 2003,
            "logprob": -7.1523,
            "prob": 0.0008
          },
          {
            "token": "Ġ(\"",
            "token_id": 7176,
            "logprob": -8.7578,
            "prob": 0.0002
          }
        ]
      },
      "selected_token": {
        "token": "Ġ\"",
        "token_id": 1789,
        "selection_method": "sampling"
      }
    },
    {
      "step": 94,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass alle \"e\" gerechnet werden) fünf Buchstaben \"e\". \n\nAufzählerisches: \n\n- zwei \"e\" in \"Beschwindi-\", einem zusammengeschrieben Wort, das den Hauptteil des Wörter bilden \n- drei  zusätzliche \"e\"'s  in den  Silbennahrungen \"",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass",
        "Ġalle",
        "Ġ\"",
        "e",
        "\"",
        "Ġgere",
        "chnet",
        "Ġwerden",
        ")",
        "Ġfünf",
        "ĠBuch",
        "st",
        "aben",
        "Ġ\"",
        "e",
        "\".",
        "Ġ\n",
        "\n",
        "Auf",
        "z",
        "ähler",
        "isches",
        ":",
        "Ġ\n",
        "\n",
        "-",
        "Ġzwei",
        "Ġ\"",
        "e",
        "\"",
        "Ġin",
        "Ġ\"",
        "Besch",
        "w",
        "indi",
        "-",
        "\",",
        "Ġeinem",
        "Ġzusamm",
        "engesch",
        "rieben",
        "ĠWort",
        ",",
        "Ġdas",
        "Ġden",
        "ĠHaupt",
        "teil",
        "Ġdes",
        "ĠWörter",
        "Ġbilden",
        "Ġ",
        "\n",
        "-",
        "Ġdrei",
        "Ġ",
        "Ġzusätzliche",
        "Ġ\"",
        "e",
        "\"'",
        "s",
        "Ġ",
        "Ġin",
        "Ġden",
        "Ġ",
        "ĠSil",
        "b",
        "enn",
        "ahr",
        "ungen",
        "Ġ\""
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760, 10528, 1789, 76, 9, 27346, 159065, 9178, 16, 44522,
        19893, 1725, 35346, 1789, 76, 3193, 1973, 206, 51214, 97, 137112, 24804, 33, 1973, 206, 20,
        20293, 1789, 76, 9, 1709, 1789, 83660, 94, 22816, 20, 2040, 12315, 161375, 186367, 44927,
        80704, 19, 4031, 3636, 28510, 23139, 2003, 187284, 121345, 228, 206, 20, 26089, 228, 240616,
        1789, 76, 59742, 90, 228, 1709, 3636, 228, 6823, 73, 3259, 6456, 7868, 1789
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "G",
            "token_id": 46,
            "logprob": -0.9663,
            "prob": 0.3804
          },
          {
            "token": "Gesch",
            "token_id": 23361,
            "logprob": -2.1543,
            "prob": 0.116
          },
          {
            "token": "ges",
            "token_id": 5325,
            "logprob": -2.2168,
            "prob": 0.109
          },
          {
            "token": "ge",
            "token_id": 2035,
            "logprob": -2.3418,
            "prob": 0.0962
          },
          {
            "token": "g",
            "token_id": 78,
            "logprob": -2.7637,
            "prob": 0.0631
          },
          {
            "token": "گی",
            "token_id": 22023,
            "logprob": -3.3652,
            "prob": 0.0346
          },
          {
            "token": "gesch",
            "token_id": 19238,
            "logprob": -3.373,
            "prob": 0.0343
          },
          {
            "token": "Ges",
            "token_id": 63053,
            "logprob": -3.3809,
            "prob": 0.034
          },
          {
            "token": "gi",
            "token_id": 5348,
            "logprob": -3.4512,
            "prob": 0.0317
          },
          {
            "token": "ギ",
            "token_id": 14984,
            "logprob": -4.457,
            "prob": 0.0116
          }
        ]
      },
      "selected_token": {
        "token": "ges",
        "token_id": 5325,
        "selection_method": "sampling"
      }
    },
    {
      "step": 95,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass alle \"e\" gerechnet werden) fünf Buchstaben \"e\". \n\nAufzählerisches: \n\n- zwei \"e\" in \"Beschwindi-\", einem zusammengeschrieben Wort, das den Hauptteil des Wörter bilden \n- drei  zusätzliche \"e\"'s  in den  Silbennahrungen \"ges",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass",
        "Ġalle",
        "Ġ\"",
        "e",
        "\"",
        "Ġgere",
        "chnet",
        "Ġwerden",
        ")",
        "Ġfünf",
        "ĠBuch",
        "st",
        "aben",
        "Ġ\"",
        "e",
        "\".",
        "Ġ\n",
        "\n",
        "Auf",
        "z",
        "ähler",
        "isches",
        ":",
        "Ġ\n",
        "\n",
        "-",
        "Ġzwei",
        "Ġ\"",
        "e",
        "\"",
        "Ġin",
        "Ġ\"",
        "Besch",
        "w",
        "indi",
        "-",
        "\",",
        "Ġeinem",
        "Ġzusamm",
        "engesch",
        "rieben",
        "ĠWort",
        ",",
        "Ġdas",
        "Ġden",
        "ĠHaupt",
        "teil",
        "Ġdes",
        "ĠWörter",
        "Ġbilden",
        "Ġ",
        "\n",
        "-",
        "Ġdrei",
        "Ġ",
        "Ġzusätzliche",
        "Ġ\"",
        "e",
        "\"'",
        "s",
        "Ġ",
        "Ġin",
        "Ġden",
        "Ġ",
        "ĠSil",
        "b",
        "enn",
        "ahr",
        "ungen",
        "Ġ\"",
        "ges"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760, 10528, 1789, 76, 9, 27346, 159065, 9178, 16, 44522,
        19893, 1725, 35346, 1789, 76, 3193, 1973, 206, 51214, 97, 137112, 24804, 33, 1973, 206, 20,
        20293, 1789, 76, 9, 1709, 1789, 83660, 94, 22816, 20, 2040, 12315, 161375, 186367, 44927,
        80704, 19, 4031, 3636, 28510, 23139, 2003, 187284, 121345, 228, 206, 20, 26089, 228, 240616,
        1789, 76, 59742, 90, 228, 1709, 3636, 228, 6823, 73, 3259, 6456, 7868, 1789, 5325
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "-",
            "token_id": 20,
            "logprob": -0.0601,
            "prob": 0.9419
          },
          {
            "token": "ch",
            "token_id": 1750,
            "logprob": -3.4199,
            "prob": 0.0327
          },
          {
            "token": "p",
            "token_id": 87,
            "logprob": -4.6836,
            "prob": 0.0092
          },
          {
            "token": "-,",
            "token_id": 28668,
            "logprob": -5.0273,
            "prob": 0.0065
          },
          {
            "token": "(",
            "token_id": 15,
            "logprob": -5.2773,
            "prob": 0.0051
          },
          {
            "token": "cht",
            "token_id": 7378,
            "logprob": -7.1445,
            "prob": 0.0008
          },
          {
            "token": "chn",
            "token_id": 3220,
            "logprob": -7.3008,
            "prob": 0.0007
          },
          {
            "token": "chen",
            "token_id": 7788,
            "logprob": -8.1094,
            "prob": 0.0003
          },
          {
            "token": "-(",
            "token_id": 34047,
            "logprob": -8.2188,
            "prob": 0.0003
          },
          {
            "token": "\"-",
            "token_id": 64403,
            "logprob": -8.5234,
            "prob": 0.0002
          }
        ]
      },
      "selected_token": {
        "token": "cht",
        "token_id": 7378,
        "selection_method": "sampling"
      }
    },
    {
      "step": 96,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass alle \"e\" gerechnet werden) fünf Buchstaben \"e\". \n\nAufzählerisches: \n\n- zwei \"e\" in \"Beschwindi-\", einem zusammengeschrieben Wort, das den Hauptteil des Wörter bilden \n- drei  zusätzliche \"e\"'s  in den  Silbennahrungen \"gescht",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass",
        "Ġalle",
        "Ġ\"",
        "e",
        "\"",
        "Ġgere",
        "chnet",
        "Ġwerden",
        ")",
        "Ġfünf",
        "ĠBuch",
        "st",
        "aben",
        "Ġ\"",
        "e",
        "\".",
        "Ġ\n",
        "\n",
        "Auf",
        "z",
        "ähler",
        "isches",
        ":",
        "Ġ\n",
        "\n",
        "-",
        "Ġzwei",
        "Ġ\"",
        "e",
        "\"",
        "Ġin",
        "Ġ\"",
        "Besch",
        "w",
        "indi",
        "-",
        "\",",
        "Ġeinem",
        "Ġzusamm",
        "engesch",
        "rieben",
        "ĠWort",
        ",",
        "Ġdas",
        "Ġden",
        "ĠHaupt",
        "teil",
        "Ġdes",
        "ĠWörter",
        "Ġbilden",
        "Ġ",
        "\n",
        "-",
        "Ġdrei",
        "Ġ",
        "Ġzusätzliche",
        "Ġ\"",
        "e",
        "\"'",
        "s",
        "Ġ",
        "Ġin",
        "Ġden",
        "Ġ",
        "ĠSil",
        "b",
        "enn",
        "ahr",
        "ungen",
        "Ġ\"",
        "ges",
        "cht"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760, 10528, 1789, 76, 9, 27346, 159065, 9178, 16, 44522,
        19893, 1725, 35346, 1789, 76, 3193, 1973, 206, 51214, 97, 137112, 24804, 33, 1973, 206, 20,
        20293, 1789, 76, 9, 1709, 1789, 83660, 94, 22816, 20, 2040, 12315, 161375, 186367, 44927,
        80704, 19, 4031, 3636, 28510, 23139, 2003, 187284, 121345, 228, 206, 20, 26089, 228, 240616,
        1789, 76, 59742, 90, 228, 1709, 3636, 228, 6823, 73, 3259, 6456, 7868, 1789, 5325, 7378
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "-",
            "token_id": 20,
            "logprob": -0.1434,
            "prob": 0.8662
          },
          {
            "token": "\"",
            "token_id": 9,
            "logprob": -2.9941,
            "prob": 0.05
          },
          {
            "token": "(",
            "token_id": 15,
            "logprob": -3.6035,
            "prob": 0.0272
          },
          {
            "token": "*",
            "token_id": 17,
            "logprob": -5.3633,
            "prob": 0.0047
          },
          {
            "token": "\",",
            "token_id": 2040,
            "logprob": -5.5586,
            "prob": 0.0039
          },
          {
            "token": "windigkeit",
            "token_id": 139420,
            "logprob": -5.6914,
            "prob": 0.0034
          },
          {
            "token": "ind",
            "token_id": 2059,
            "logprob": -5.7148,
            "prob": 0.0033
          },
          {
            "token": "wind",
            "token_id": 25965,
            "logprob": -5.793,
            "prob": 0.0031
          },
          {
            "token": "\"-",
            "token_id": 64403,
            "logprob": -5.9258,
            "prob": 0.0027
          },
          {
            "token": "win",
            "token_id": 7904,
            "logprob": -5.9648,
            "prob": 0.0026
          }
        ]
      },
      "selected_token": {
        "token": "-",
        "token_id": 20,
        "selection_method": "sampling"
      }
    },
    {
      "step": 97,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass alle \"e\" gerechnet werden) fünf Buchstaben \"e\". \n\nAufzählerisches: \n\n- zwei \"e\" in \"Beschwindi-\", einem zusammengeschrieben Wort, das den Hauptteil des Wörter bilden \n- drei  zusätzliche \"e\"'s  in den  Silbennahrungen \"gescht-",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass",
        "Ġalle",
        "Ġ\"",
        "e",
        "\"",
        "Ġgere",
        "chnet",
        "Ġwerden",
        ")",
        "Ġfünf",
        "ĠBuch",
        "st",
        "aben",
        "Ġ\"",
        "e",
        "\".",
        "Ġ\n",
        "\n",
        "Auf",
        "z",
        "ähler",
        "isches",
        ":",
        "Ġ\n",
        "\n",
        "-",
        "Ġzwei",
        "Ġ\"",
        "e",
        "\"",
        "Ġin",
        "Ġ\"",
        "Besch",
        "w",
        "indi",
        "-",
        "\",",
        "Ġeinem",
        "Ġzusamm",
        "engesch",
        "rieben",
        "ĠWort",
        ",",
        "Ġdas",
        "Ġden",
        "ĠHaupt",
        "teil",
        "Ġdes",
        "ĠWörter",
        "Ġbilden",
        "Ġ",
        "\n",
        "-",
        "Ġdrei",
        "Ġ",
        "Ġzusätzliche",
        "Ġ\"",
        "e",
        "\"'",
        "s",
        "Ġ",
        "Ġin",
        "Ġden",
        "Ġ",
        "ĠSil",
        "b",
        "enn",
        "ahr",
        "ungen",
        "Ġ\"",
        "ges",
        "cht",
        "-"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760, 10528, 1789, 76, 9, 27346, 159065, 9178, 16, 44522,
        19893, 1725, 35346, 1789, 76, 3193, 1973, 206, 51214, 97, 137112, 24804, 33, 1973, 206, 20,
        20293, 1789, 76, 9, 1709, 1789, 83660, 94, 22816, 20, 2040, 12315, 161375, 186367, 44927,
        80704, 19, 4031, 3636, 28510, 23139, 2003, 187284, 121345, 228, 206, 20, 26089, 228, 240616,
        1789, 76, 59742, 90, 228, 1709, 3636, 228, 6823, 73, 3259, 6456, 7868, 1789, 5325, 7378, 20
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġ(",
            "token_id": 1737,
            "logprob": -0.0187,
            "prob": 0.9814
          },
          {
            "token": "wind",
            "token_id": 25965,
            "logprob": -4.5352,
            "prob": 0.0107
          },
          {
            "token": "wi",
            "token_id": 6088,
            "logprob": -5.832,
            "prob": 0.0029
          },
          {
            "token": "win",
            "token_id": 7904,
            "logprob": -6.3008,
            "prob": 0.0018
          },
          {
            "token": "\n",
            "token_id": 206,
            "logprob": -7.8086,
            "prob": 0.0004
          },
          {
            "token": "\",",
            "token_id": 2040,
            "logprob": -7.957,
            "prob": 0.0004
          },
          {
            "token": "w",
            "token_id": 94,
            "logprob": -8.2266,
            "prob": 0.0003
          },
          {
            "token": "in",
            "token_id": 1670,
            "logprob": -8.3672,
            "prob": 0.0002
          },
          {
            "token": "di",
            "token_id": 9467,
            "logprob": -9.1953,
            "prob": 0.0001
          },
          {
            "token": "d",
            "token_id": 75,
            "logprob": -9.2969,
            "prob": 0.0001
          }
        ]
      },
      "selected_token": {
        "token": "in",
        "token_id": 1670,
        "selection_method": "sampling"
      }
    },
    {
      "step": 98,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass alle \"e\" gerechnet werden) fünf Buchstaben \"e\". \n\nAufzählerisches: \n\n- zwei \"e\" in \"Beschwindi-\", einem zusammengeschrieben Wort, das den Hauptteil des Wörter bilden \n- drei  zusätzliche \"e\"'s  in den  Silbennahrungen \"gescht-in",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass",
        "Ġalle",
        "Ġ\"",
        "e",
        "\"",
        "Ġgere",
        "chnet",
        "Ġwerden",
        ")",
        "Ġfünf",
        "ĠBuch",
        "st",
        "aben",
        "Ġ\"",
        "e",
        "\".",
        "Ġ\n",
        "\n",
        "Auf",
        "z",
        "ähler",
        "isches",
        ":",
        "Ġ\n",
        "\n",
        "-",
        "Ġzwei",
        "Ġ\"",
        "e",
        "\"",
        "Ġin",
        "Ġ\"",
        "Besch",
        "w",
        "indi",
        "-",
        "\",",
        "Ġeinem",
        "Ġzusamm",
        "engesch",
        "rieben",
        "ĠWort",
        ",",
        "Ġdas",
        "Ġden",
        "ĠHaupt",
        "teil",
        "Ġdes",
        "ĠWörter",
        "Ġbilden",
        "Ġ",
        "\n",
        "-",
        "Ġdrei",
        "Ġ",
        "Ġzusätzliche",
        "Ġ\"",
        "e",
        "\"'",
        "s",
        "Ġ",
        "Ġin",
        "Ġden",
        "Ġ",
        "ĠSil",
        "b",
        "enn",
        "ahr",
        "ungen",
        "Ġ\"",
        "ges",
        "cht",
        "-",
        "in"
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760, 10528, 1789, 76, 9, 27346, 159065, 9178, 16, 44522,
        19893, 1725, 35346, 1789, 76, 3193, 1973, 206, 51214, 97, 137112, 24804, 33, 1973, 206, 20,
        20293, 1789, 76, 9, 1709, 1789, 83660, 94, 22816, 20, 2040, 12315, 161375, 186367, 44927,
        80704, 19, 4031, 3636, 28510, 23139, 2003, 187284, 121345, 228, 206, 20, 26089, 228, 240616,
        1789, 76, 59742, 90, 228, 1709, 3636, 228, 6823, 73, 3259, 6456, 7868, 1789, 5325, 7378, 20,
        1670
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "-",
            "token_id": 20,
            "logprob": -0.0007,
            "prob": 0.9995
          },
          {
            "token": "-(",
            "token_id": 34047,
            "logprob": -7.5312,
            "prob": 0.0005
          },
          {
            "token": "(",
            "token_id": 15,
            "logprob": -8.7188,
            "prob": 0.0002
          },
          {
            "token": "-\\",
            "token_id": 83653,
            "logprob": -12.4531,
            "prob": 0.0
          },
          {
            "token": "-[",
            "token_id": 63476,
            "logprob": -13.3516,
            "prob": 0.0
          },
          {
            "token": "(-",
            "token_id": 22348,
            "logprob": -13.4219,
            "prob": 0.0
          },
          {
            "token": "-/",
            "token_id": 130398,
            "logprob": -13.9297,
            "prob": 0.0
          },
          {
            "token": "-|",
            "token_id": 160802,
            "logprob": -14.1875,
            "prob": 0.0
          },
          {
            "token": "-\"",
            "token_id": 62562,
            "logprob": -14.1094,
            "prob": 0.0
          },
          {
            "token": "-*",
            "token_id": 232105,
            "logprob": -14.2656,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "-[",
        "token_id": 63476,
        "selection_method": "sampling"
      }
    },
    {
      "step": 99,
      "input_text": "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>Im Wort \"Geschwindigkeitsbegrenung\" gab es inklusive Nachdehnen (um sicherzugehmen, dass alle \"e\" gerechnet werden) fünf Buchstaben \"e\". \n\nAufzählerisches: \n\n- zwei \"e\" in \"Beschwindi-\", einem zusammengeschrieben Wort, das den Hauptteil des Wörter bilden \n- drei  zusätzliche \"e\"'s  in den  Silbennahrungen \"gescht-in-[",
      "tokens": [
        "<BOS_TOKEN>",
        "<|START_OF_TURN_TOKEN|>",
        "<|USER_TOKEN|>",
        "Wie",
        "Ġviele",
        "Ġ\"",
        "e",
        "\"",
        "Ġgibt",
        "Ġes",
        "Ġim",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "zung",
        "\"?",
        "<|END_OF_TURN_TOKEN|>",
        "<|START_OF_TURN_TOKEN|>",
        "<|CHATBOT_TOKEN|>",
        "Im",
        "ĠWort",
        "Ġ\"",
        "Gesch",
        "wind",
        "igkeits",
        "beg",
        "ren",
        "ung",
        "\"",
        "Ġgab",
        "Ġes",
        "Ġin",
        "kl",
        "usive",
        "ĠNach",
        "de",
        "hnen",
        "Ġ(",
        "um",
        "Ġsicher",
        "zuge",
        "hmen",
        ",",
        "Ġdass",
        "Ġalle",
        "Ġ\"",
        "e",
        "\"",
        "Ġgere",
        "chnet",
        "Ġwerden",
        ")",
        "Ġfünf",
        "ĠBuch",
        "st",
        "aben",
        "Ġ\"",
        "e",
        "\".",
        "Ġ\n",
        "\n",
        "Auf",
        "z",
        "ähler",
        "isches",
        ":",
        "Ġ\n",
        "\n",
        "-",
        "Ġzwei",
        "Ġ\"",
        "e",
        "\"",
        "Ġin",
        "Ġ\"",
        "Besch",
        "w",
        "indi",
        "-",
        "\",",
        "Ġeinem",
        "Ġzusamm",
        "engesch",
        "rieben",
        "ĠWort",
        ",",
        "Ġdas",
        "Ġden",
        "ĠHaupt",
        "teil",
        "Ġdes",
        "ĠWörter",
        "Ġbilden",
        "Ġ",
        "\n",
        "-",
        "Ġdrei",
        "Ġ",
        "Ġzusätzliche",
        "Ġ\"",
        "e",
        "\"'",
        "s",
        "Ġ",
        "Ġin",
        "Ġden",
        "Ġ",
        "ĠSil",
        "b",
        "enn",
        "ahr",
        "ungen",
        "Ġ\"",
        "ges",
        "cht",
        "-",
        "in",
        "-["
      ],
      "token_ids": [
        5, 255000, 255006, 93488, 62909, 1789, 76, 9, 38561, 2278, 2101, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 60525, 89492, 255001, 255000, 255007, 4928, 80704, 1789, 23361, 25965,
        173625, 43882, 2764, 2359, 9, 25163, 2278, 1709, 9817, 44269, 13475, 1966, 87881, 1737,
        1783, 171491, 208867, 28089, 19, 15760, 10528, 1789, 76, 9, 27346, 159065, 9178, 16, 44522,
        19893, 1725, 35346, 1789, 76, 3193, 1973, 206, 51214, 97, 137112, 24804, 33, 1973, 206, 20,
        20293, 1789, 76, 9, 1709, 1789, 83660, 94, 22816, 20, 2040, 12315, 161375, 186367, 44927,
        80704, 19, 4031, 3636, 28510, 23139, 2003, 187284, 121345, 228, 206, 20, 26089, 228, 240616,
        1789, 76, 59742, 90, 228, 1709, 3636, 228, 6823, 73, 3259, 6456, 7868, 1789, 5325, 7378, 20,
        1670, 63476
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "d",
            "token_id": 75,
            "logprob": -0.4165,
            "prob": 0.6592
          },
          {
            "token": "e",
            "token_id": 76,
            "logprob": -1.167,
            "prob": 0.3115
          },
          {
            "token": "n",
            "token_id": 85,
            "logprob": -5.3711,
            "prob": 0.0047
          },
          {
            "token": "a",
            "token_id": 72,
            "logprob": -5.3711,
            "prob": 0.0047
          },
          {
            "token": "i",
            "token_id": 80,
            "logprob": -5.6992,
            "prob": 0.0034
          },
          {
            "token": "s",
            "token_id": 90,
            "logprob": -5.7695,
            "prob": 0.0031
          },
          {
            "token": "b",
            "token_id": 73,
            "logprob": -6.0352,
            "prob": 0.0024
          },
          {
            "token": "be",
            "token_id": 2901,
            "logprob": -6.2148,
            "prob": 0.002
          },
          {
            "token": "beg",
            "token_id": 43882,
            "logprob": -6.4102,
            "prob": 0.0016
          },
          {
            "token": "t",
            "token_id": 91,
            "logprob": -6.6367,
            "prob": 0.0013
          }
        ]
      },
      "selected_token": {
        "token": "t",
        "token_id": 91,
        "selection_method": "sampling"
      }
    }
  ]
}
