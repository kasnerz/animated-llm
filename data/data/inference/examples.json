{
  "examples": [
    {
      "id": "ar-001-greedy-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "مرحبا بالعالم!",
      "language": "ar",
      "description": "مرحبا بالعالم!",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "ar/ar-001-greedy-CohereLabs-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "ar-001-random-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "مرحبا بالعالم!",
      "language": "ar",
      "description": "مرحبا بالعالم!",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "ar/ar-001-random-CohereLabs-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "ar-001-sampling-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "مرحبا بالعالم!",
      "language": "ar",
      "description": "مرحبا بالعالم!",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "ar/ar-001-sampling-CohereLabs-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "ar-002-greedy-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "من أنت؟",
      "language": "ar",
      "description": "من أنت؟",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "ar/ar-002-greedy-CohereLabs-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "ar-002-random-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "من أنت؟",
      "language": "ar",
      "description": "من أنت؟",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "ar/ar-002-random-CohereLabs-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "ar-002-sampling-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "من أنت؟",
      "language": "ar",
      "description": "من أنت؟",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "ar/ar-002-sampling-CohereLabs-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "ar-003-greedy-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "أخبرني نكتة",
      "language": "ar",
      "description": "أخبرني نكتة",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "ar/ar-003-greedy-CohereLabs-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "ar-003-random-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "أخبرني نكتة",
      "language": "ar",
      "description": "أخبرني نكتة",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "ar/ar-003-random-CohereLabs-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "ar-003-sampling-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "أخبرني نكتة",
      "language": "ar",
      "description": "أخبرني نكتة",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "ar/ar-003-sampling-CohereLabs-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "ar-004-greedy-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "ما هي عاصمة مصر؟",
      "language": "ar",
      "description": "ما هي عاصمة مصر؟",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "ar/ar-004-greedy-CohereLabs-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "ar-004-random-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "ما هي عاصمة مصر؟",
      "language": "ar",
      "description": "ما هي عاصمة مصر؟",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "ar/ar-004-random-CohereLabs-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "ar-004-sampling-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "ما هي عاصمة مصر؟",
      "language": "ar",
      "description": "ما هي عاصمة مصر؟",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "ar/ar-004-sampling-CohereLabs-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "ar-005-greedy-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "ترجم إلى الإنجليزية: العلم نور والجهل ظلام",
      "language": "ar",
      "description": "ترجم إلى الإنجليزية: العلم نور والجهل ظلام",
      "num_tokens": 29,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "ar/ar-005-greedy-CohereLabs-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "ar-005-random-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "ترجم إلى الإنجليزية: العلم نور والجهل ظلام",
      "language": "ar",
      "description": "ترجم إلى الإنجليزية: العلم نور والجهل ظلام",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "ar/ar-005-random-CohereLabs-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "ar-005-sampling-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "ترجم إلى الإنجليزية: العلم نور والجهل ظلام",
      "language": "ar",
      "description": "ترجم إلى الإنجليزية: العلم نور والجهل ظلام",
      "num_tokens": 31,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "ar/ar-005-sampling-CohereLabs-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "ar-006-greedy-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "كم عدد حرف \"م\" في كلمة \"متحمسون\"؟",
      "language": "ar",
      "description": "كم عدد حرف \"م\" في كلمة \"متحمسون\"؟",
      "num_tokens": 16,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "ar/ar-006-greedy-CohereLabs-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "ar-006-random-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "كم عدد حرف \"م\" في كلمة \"متحمسون\"؟",
      "language": "ar",
      "description": "كم عدد حرف \"م\" في كلمة \"متحمسون\"؟",
      "num_tokens": 19,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "ar/ar-006-random-CohereLabs-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "ar-006-sampling-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "كم عدد حرف \"م\" في كلمة \"متحمسون\"؟",
      "language": "ar",
      "description": "كم عدد حرف \"م\" في كلمة \"متحمسون\"؟",
      "num_tokens": 16,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "ar/ar-006-sampling-CohereLabs-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "cs-001-greedy-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "Ahoj světe!",
      "language": "cs",
      "description": "Ahoj světe!",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "cs/cs-001-greedy-CohereLabs-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "cs-001-random-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "Ahoj světe!",
      "language": "cs",
      "description": "Ahoj světe!",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "cs/cs-001-random-CohereLabs-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "cs-001-sampling-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "Ahoj světe!",
      "language": "cs",
      "description": "Ahoj světe!",
      "num_tokens": 57,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "cs/cs-001-sampling-CohereLabs-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "cs-002-greedy-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "Kdo jsi?",
      "language": "cs",
      "description": "Kdo jsi?",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "cs/cs-002-greedy-CohereLabs-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "cs-002-random-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "Kdo jsi?",
      "language": "cs",
      "description": "Kdo jsi?",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "cs/cs-002-random-CohereLabs-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "cs-002-sampling-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "Kdo jsi?",
      "language": "cs",
      "description": "Kdo jsi?",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "cs/cs-002-sampling-CohereLabs-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "cs-003-greedy-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "Řekni mi vtip",
      "language": "cs",
      "description": "Řekni mi vtip",
      "num_tokens": 74,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "cs/cs-003-greedy-CohereLabs-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "cs-003-random-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "Řekni mi vtip",
      "language": "cs",
      "description": "Řekni mi vtip",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "cs/cs-003-random-CohereLabs-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "cs-003-sampling-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "Řekni mi vtip",
      "language": "cs",
      "description": "Řekni mi vtip",
      "num_tokens": 64,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "cs/cs-003-sampling-CohereLabs-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "cs-004-greedy-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "Jaké je hlavní město Česka?",
      "language": "cs",
      "description": "Jaké je hlavní město Česka?",
      "num_tokens": 45,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "cs/cs-004-greedy-CohereLabs-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "cs-004-random-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "Jaké je hlavní město Česka?",
      "language": "cs",
      "description": "Jaké je hlavní město Česka?",
      "num_tokens": 63,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "cs/cs-004-random-CohereLabs-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "cs-004-sampling-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "Jaké je hlavní město Česka?",
      "language": "cs",
      "description": "Jaké je hlavní město Česka?",
      "num_tokens": 75,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "cs/cs-004-sampling-CohereLabs-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "cs-005-greedy-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "Přelož do angličtiny: Žluťoučký kůň úpěl ďábelské ódy",
      "language": "cs",
      "description": "Přelož do angličtiny: Žluťoučký kůň úpěl ďábelské ódy",
      "num_tokens": 41,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "cs/cs-005-greedy-CohereLabs-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "cs-005-random-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "Přelož do angličtiny: Žluťoučký kůň úpěl ďábelské ódy",
      "language": "cs",
      "description": "Přelož do angličtiny: Žluťoučký kůň úpěl ďábelské ódy",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "cs/cs-005-random-CohereLabs-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "cs-005-sampling-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "Přelož do angličtiny: Žluťoučký kůň úpěl ďábelské ódy",
      "language": "cs",
      "description": "Přelož do angličtiny: Žluťoučký kůň úpěl ďábelské ódy",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "cs/cs-005-sampling-CohereLabs-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "cs-006-greedy-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "Kolik \"k\" je ve slově \"nejkulinkaťoulinkatější\"?",
      "language": "cs",
      "description": "Kolik \"k\" je ve slově \"nejkulinkaťoulinkatější\"?",
      "num_tokens": 81,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "cs/cs-006-greedy-CohereLabs-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "cs-006-random-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "Kolik \"k\" je ve slově \"nejkulinkaťoulinkatější\"?",
      "language": "cs",
      "description": "Kolik \"k\" je ve slově \"nejkulinkaťoulinkatější\"?",
      "num_tokens": 43,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "cs/cs-006-random-CohereLabs-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "cs-006-sampling-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "Kolik \"k\" je ve slově \"nejkulinkaťoulinkatější\"?",
      "language": "cs",
      "description": "Kolik \"k\" je ve slově \"nejkulinkaťoulinkatější\"?",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "cs/cs-006-sampling-CohereLabs-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "de-001-greedy-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "Hallo Welt!",
      "language": "de",
      "description": "Hallo Welt!",
      "num_tokens": 70,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "de/de-001-greedy-CohereLabs-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "de-001-random-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "Hallo Welt!",
      "language": "de",
      "description": "Hallo Welt!",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "de/de-001-random-CohereLabs-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "de-001-sampling-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "Hallo Welt!",
      "language": "de",
      "description": "Hallo Welt!",
      "num_tokens": 32,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "de/de-001-sampling-CohereLabs-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "de-002-greedy-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "Wer bist du?",
      "language": "de",
      "description": "Wer bist du?",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "de/de-002-greedy-CohereLabs-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "de-002-random-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "Wer bist du?",
      "language": "de",
      "description": "Wer bist du?",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "de/de-002-random-CohereLabs-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "de-002-sampling-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "Wer bist du?",
      "language": "de",
      "description": "Wer bist du?",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "de/de-002-sampling-CohereLabs-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "de-003-greedy-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "Erzähl mir einen Witz",
      "language": "de",
      "description": "Erzähl mir einen Witz",
      "num_tokens": 60,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "de/de-003-greedy-CohereLabs-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "de-003-random-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "Erzähl mir einen Witz",
      "language": "de",
      "description": "Erzähl mir einen Witz",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "de/de-003-random-CohereLabs-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "de-003-sampling-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "Erzähl mir einen Witz",
      "language": "de",
      "description": "Erzähl mir einen Witz",
      "num_tokens": 52,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "de/de-003-sampling-CohereLabs-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "de-004-greedy-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "Was ist die Hauptstadt von Deutschland?",
      "language": "de",
      "description": "Was ist die Hauptstadt von Deutschland?",
      "num_tokens": 72,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "de/de-004-greedy-CohereLabs-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "de-004-random-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "Was ist die Hauptstadt von Deutschland?",
      "language": "de",
      "description": "Was ist die Hauptstadt von Deutschland?",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "de/de-004-random-CohereLabs-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "de-004-sampling-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "Was ist die Hauptstadt von Deutschland?",
      "language": "de",
      "description": "Was ist die Hauptstadt von Deutschland?",
      "num_tokens": 72,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "de/de-004-sampling-CohereLabs-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "de-005-greedy-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "Übersetze ins Englische: Der frühe Vogel fängt den Wurm",
      "language": "de",
      "description": "Übersetze ins Englische: Der frühe Vogel fängt den Wurm",
      "num_tokens": 66,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "de/de-005-greedy-CohereLabs-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "de-005-random-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "Übersetze ins Englische: Der frühe Vogel fängt den Wurm",
      "language": "de",
      "description": "Übersetze ins Englische: Der frühe Vogel fängt den Wurm",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "de/de-005-random-CohereLabs-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "de-005-sampling-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "Übersetze ins Englische: Der frühe Vogel fängt den Wurm",
      "language": "de",
      "description": "Übersetze ins Englische: Der frühe Vogel fängt den Wurm",
      "num_tokens": 59,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "de/de-005-sampling-CohereLabs-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "de-006-greedy-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?",
      "language": "de",
      "description": "Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?",
      "num_tokens": 18,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "de/de-006-greedy-CohereLabs-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "de-006-random-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?",
      "language": "de",
      "description": "Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?",
      "num_tokens": 84,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "de/de-006-random-CohereLabs-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "de-006-sampling-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?",
      "language": "de",
      "description": "Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?",
      "num_tokens": 17,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "de/de-006-sampling-CohereLabs-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "en-001-greedy-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "Hello world!",
      "language": "en",
      "description": "Hello world!",
      "num_tokens": 61,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "en/en-001-greedy-CohereLabs-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "en-001-random-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "Hello world!",
      "language": "en",
      "description": "Hello world!",
      "num_tokens": 68,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "en/en-001-random-CohereLabs-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "en-001-sampling-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "Hello world!",
      "language": "en",
      "description": "Hello world!",
      "num_tokens": 26,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "en/en-001-sampling-CohereLabs-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "en-002-greedy-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "Who are you?",
      "language": "en",
      "description": "Who are you?",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "en/en-002-greedy-CohereLabs-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "en-002-random-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "Who are you?",
      "language": "en",
      "description": "Who are you?",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "en/en-002-random-CohereLabs-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "en-002-sampling-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "Who are you?",
      "language": "en",
      "description": "Who are you?",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "en/en-002-sampling-CohereLabs-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "en-003-greedy-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "Tell me a joke",
      "language": "en",
      "description": "Tell me a joke",
      "num_tokens": 50,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "en/en-003-greedy-CohereLabs-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "en-003-random-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "Tell me a joke",
      "language": "en",
      "description": "Tell me a joke",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "en/en-003-random-CohereLabs-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "en-003-sampling-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "Tell me a joke",
      "language": "en",
      "description": "Tell me a joke",
      "num_tokens": 53,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "en/en-003-sampling-CohereLabs-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "en-004-greedy-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "What is the capital of France?",
      "language": "en",
      "description": "What is the capital of France?",
      "num_tokens": 81,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "en/en-004-greedy-CohereLabs-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "en-004-random-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "What is the capital of France?",
      "language": "en",
      "description": "What is the capital of France?",
      "num_tokens": 53,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "en/en-004-random-CohereLabs-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "en-004-sampling-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "What is the capital of France?",
      "language": "en",
      "description": "What is the capital of France?",
      "num_tokens": 52,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "en/en-004-sampling-CohereLabs-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "en-005-greedy-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "Translate to French: The quick brown fox jumps over the lazy dog",
      "language": "en",
      "description": "Translate to French: The quick brown fox jumps over the lazy dog",
      "num_tokens": 51,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "en/en-005-greedy-CohereLabs-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "en-005-random-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "Translate to French: The quick brown fox jumps over the lazy dog",
      "language": "en",
      "description": "Translate to French: The quick brown fox jumps over the lazy dog",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "en/en-005-random-CohereLabs-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "en-005-sampling-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "Translate to French: The quick brown fox jumps over the lazy dog",
      "language": "en",
      "description": "Translate to French: The quick brown fox jumps over the lazy dog",
      "num_tokens": 66,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "en/en-005-sampling-CohereLabs-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "en-006-greedy-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "How many r's are in \"strawberry\"?",
      "language": "en",
      "description": "How many r's are in \"strawberry\"?",
      "num_tokens": 12,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "en/en-006-greedy-CohereLabs-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "en-006-random-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "How many r's are in \"strawberry\"?",
      "language": "en",
      "description": "How many r's are in \"strawberry\"?",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "en/en-006-random-CohereLabs-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "en-006-sampling-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "How many r's are in \"strawberry\"?",
      "language": "en",
      "description": "How many r's are in \"strawberry\"?",
      "num_tokens": 12,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "en/en-006-sampling-CohereLabs-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "es-001-greedy-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "¡Hola mundo!",
      "language": "es",
      "description": "¡Hola mundo!",
      "num_tokens": 14,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "es/es-001-greedy-CohereLabs-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "es-001-random-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "¡Hola mundo!",
      "language": "es",
      "description": "¡Hola mundo!",
      "num_tokens": 34,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "es/es-001-random-CohereLabs-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "es-001-sampling-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "¡Hola mundo!",
      "language": "es",
      "description": "¡Hola mundo!",
      "num_tokens": 14,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "es/es-001-sampling-CohereLabs-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "es-002-greedy-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "¿Quién eres?",
      "language": "es",
      "description": "¿Quién eres?",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "es/es-002-greedy-CohereLabs-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "es-002-random-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "¿Quién eres?",
      "language": "es",
      "description": "¿Quién eres?",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "es/es-002-random-CohereLabs-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "es-002-sampling-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "¿Quién eres?",
      "language": "es",
      "description": "¿Quién eres?",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "es/es-002-sampling-CohereLabs-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "es-003-greedy-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "Cuéntame un chiste",
      "language": "es",
      "description": "Cuéntame un chiste",
      "num_tokens": 45,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "es/es-003-greedy-CohereLabs-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "es-003-random-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "Cuéntame un chiste",
      "language": "es",
      "description": "Cuéntame un chiste",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "es/es-003-random-CohereLabs-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "es-003-sampling-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "Cuéntame un chiste",
      "language": "es",
      "description": "Cuéntame un chiste",
      "num_tokens": 45,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "es/es-003-sampling-CohereLabs-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "es-004-greedy-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "¿Cuál es la capital de España?",
      "language": "es",
      "description": "¿Cuál es la capital de España?",
      "num_tokens": 58,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "es/es-004-greedy-CohereLabs-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "es-004-random-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "¿Cuál es la capital de España?",
      "language": "es",
      "description": "¿Cuál es la capital de España?",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "es/es-004-random-CohereLabs-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "es-004-sampling-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "¿Cuál es la capital de España?",
      "language": "es",
      "description": "¿Cuál es la capital de España?",
      "num_tokens": 62,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "es/es-004-sampling-CohereLabs-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "es-005-greedy-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "Traduce al inglés: No hay mal que por bien no venga",
      "language": "es",
      "description": "Traduce al inglés: No hay mal que por bien no venga",
      "num_tokens": 63,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "es/es-005-greedy-CohereLabs-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "es-005-random-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "Traduce al inglés: No hay mal que por bien no venga",
      "language": "es",
      "description": "Traduce al inglés: No hay mal que por bien no venga",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "es/es-005-random-CohereLabs-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "es-005-sampling-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "Traduce al inglés: No hay mal que por bien no venga",
      "language": "es",
      "description": "Traduce al inglés: No hay mal que por bien no venga",
      "num_tokens": 55,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "es/es-005-sampling-CohereLabs-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "es-006-greedy-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "¿Cuántas \"r\" hay en la palabra \"ferrocarril\"?",
      "language": "es",
      "description": "¿Cuántas \"r\" hay en la palabra \"ferrocarril\"?",
      "num_tokens": 15,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "es/es-006-greedy-CohereLabs-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "es-006-random-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "¿Cuántas \"r\" hay en la palabra \"ferrocarril\"?",
      "language": "es",
      "description": "¿Cuántas \"r\" hay en la palabra \"ferrocarril\"?",
      "num_tokens": 15,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "es/es-006-random-CohereLabs-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "es-006-sampling-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "¿Cuántas \"r\" hay en la palabra \"ferrocarril\"?",
      "language": "es",
      "description": "¿Cuántas \"r\" hay en la palabra \"ferrocarril\"?",
      "num_tokens": 15,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "es/es-006-sampling-CohereLabs-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "fr-001-greedy-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "Bonjour le monde!",
      "language": "fr",
      "description": "Bonjour le monde!",
      "num_tokens": 74,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "fr/fr-001-greedy-CohereLabs-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "fr-001-random-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "Bonjour le monde!",
      "language": "fr",
      "description": "Bonjour le monde!",
      "num_tokens": 18,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "fr/fr-001-random-CohereLabs-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "fr-001-sampling-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "Bonjour le monde!",
      "language": "fr",
      "description": "Bonjour le monde!",
      "num_tokens": 85,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "fr/fr-001-sampling-CohereLabs-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "fr-002-greedy-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "Qui es-tu?",
      "language": "fr",
      "description": "Qui es-tu?",
      "num_tokens": 95,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "fr/fr-002-greedy-CohereLabs-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "fr-002-random-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "Qui es-tu?",
      "language": "fr",
      "description": "Qui es-tu?",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "fr/fr-002-random-CohereLabs-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "fr-002-sampling-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "Qui es-tu?",
      "language": "fr",
      "description": "Qui es-tu?",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "fr/fr-002-sampling-CohereLabs-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "fr-003-greedy-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "Raconte-moi une blague",
      "language": "fr",
      "description": "Raconte-moi une blague",
      "num_tokens": 61,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "fr/fr-003-greedy-CohereLabs-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "fr-003-random-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "Raconte-moi une blague",
      "language": "fr",
      "description": "Raconte-moi une blague",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "fr/fr-003-random-CohereLabs-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "fr-003-sampling-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "Raconte-moi une blague",
      "language": "fr",
      "description": "Raconte-moi une blague",
      "num_tokens": 92,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "fr/fr-003-sampling-CohereLabs-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "fr-004-greedy-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "Quelle est la capitale de la France?",
      "language": "fr",
      "description": "Quelle est la capitale de la France?",
      "num_tokens": 83,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "fr/fr-004-greedy-CohereLabs-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "fr-004-random-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "Quelle est la capitale de la France?",
      "language": "fr",
      "description": "Quelle est la capitale de la France?",
      "num_tokens": 51,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "fr/fr-004-random-CohereLabs-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "fr-004-sampling-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "Quelle est la capitale de la France?",
      "language": "fr",
      "description": "Quelle est la capitale de la France?",
      "num_tokens": 91,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "fr/fr-004-sampling-CohereLabs-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "fr-005-greedy-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "Traduis en anglais: La mariée était en blanc",
      "language": "fr",
      "description": "Traduis en anglais: La mariée était en blanc",
      "num_tokens": 24,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "fr/fr-005-greedy-CohereLabs-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "fr-005-random-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "Traduis en anglais: La mariée était en blanc",
      "language": "fr",
      "description": "Traduis en anglais: La mariée était en blanc",
      "num_tokens": 13,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "fr/fr-005-random-CohereLabs-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "fr-005-sampling-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "Traduis en anglais: La mariée était en blanc",
      "language": "fr",
      "description": "Traduis en anglais: La mariée était en blanc",
      "num_tokens": 24,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "fr/fr-005-sampling-CohereLabs-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "fr-006-greedy-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "Combien de \"e\" y a-t-il dans le mot \"merveilleusement\"?",
      "language": "fr",
      "description": "Combien de \"e\" y a-t-il dans le mot \"merveilleusement\"?",
      "num_tokens": 19,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "fr/fr-006-greedy-CohereLabs-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "fr-006-random-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "Combien de \"e\" y a-t-il dans le mot \"merveilleusement\"?",
      "language": "fr",
      "description": "Combien de \"e\" y a-t-il dans le mot \"merveilleusement\"?",
      "num_tokens": 16,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "fr/fr-006-random-CohereLabs-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "fr-006-sampling-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "Combien de \"e\" y a-t-il dans le mot \"merveilleusement\"?",
      "language": "fr",
      "description": "Combien de \"e\" y a-t-il dans le mot \"merveilleusement\"?",
      "num_tokens": 19,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "fr/fr-006-sampling-CohereLabs-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "hi-001-greedy-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "नमस्ते दुनिया!",
      "language": "hi",
      "description": "नमस्ते दुनिया!",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "hi/hi-001-greedy-CohereLabs-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "hi-001-random-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "नमस्ते दुनिया!",
      "language": "hi",
      "description": "नमस्ते दुनिया!",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "hi/hi-001-random-CohereLabs-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "hi-001-sampling-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "नमस्ते दुनिया!",
      "language": "hi",
      "description": "नमस्ते दुनिया!",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "hi/hi-001-sampling-CohereLabs-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "hi-002-greedy-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "तुम कौन हो?",
      "language": "hi",
      "description": "तुम कौन हो?",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "hi/hi-002-greedy-CohereLabs-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "hi-002-random-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "तुम कौन हो?",
      "language": "hi",
      "description": "तुम कौन हो?",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "hi/hi-002-random-CohereLabs-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "hi-002-sampling-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "तुम कौन हो?",
      "language": "hi",
      "description": "तुम कौन हो?",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "hi/hi-002-sampling-CohereLabs-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "hi-003-greedy-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "मुझे एक चुटकुला सुनाओ",
      "language": "hi",
      "description": "मुझे एक चुटकुला सुनाओ",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "hi/hi-003-greedy-CohereLabs-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "hi-003-random-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "मुझे एक चुटकुला सुनाओ",
      "language": "hi",
      "description": "मुझे एक चुटकुला सुनाओ",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "hi/hi-003-random-CohereLabs-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "hi-003-sampling-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "मुझे एक चुटकुला सुनाओ",
      "language": "hi",
      "description": "मुझे एक चुटकुला सुनाओ",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "hi/hi-003-sampling-CohereLabs-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "hi-004-greedy-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "भारत की राजधानी क्या है?",
      "language": "hi",
      "description": "भारत की राजधानी क्या है?",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "hi/hi-004-greedy-CohereLabs-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "hi-004-random-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "भारत की राजधानी क्या है?",
      "language": "hi",
      "description": "भारत की राजधानी क्या है?",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "hi/hi-004-random-CohereLabs-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "hi-004-sampling-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "भारत की राजधानी क्या है?",
      "language": "hi",
      "description": "भारत की राजधानी क्या है?",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "hi/hi-004-sampling-CohereLabs-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "hi-005-greedy-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "अंग्रेजी में अनुवाद करें: अंधों में काना राजा",
      "language": "hi",
      "description": "अंग्रेजी में अनुवाद करें: अंधों में काना राजा",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "hi/hi-005-greedy-CohereLabs-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "hi-005-random-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "अंग्रेजी में अनुवाद करें: अंधों में काना राजा",
      "language": "hi",
      "description": "अंग्रेजी में अनुवाद करें: अंधों में काना राजा",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "hi/hi-005-random-CohereLabs-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "hi-005-sampling-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "अंग्रेजी में अनुवाद करें: अंधों में काना राजा",
      "language": "hi",
      "description": "अंग्रेजी में अनुवाद करें: अंधों में काना राजा",
      "num_tokens": 86,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "hi/hi-005-sampling-CohereLabs-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "hi-006-greedy-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "\"अच्छा\" शब्द में कितने \"च\" हैं?",
      "language": "hi",
      "description": "\"अच्छा\" शब्द में कितने \"च\" हैं?",
      "num_tokens": 87,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "hi/hi-006-greedy-CohereLabs-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "hi-006-random-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "\"अच्छा\" शब्द में कितने \"च\" हैं?",
      "language": "hi",
      "description": "\"अच्छा\" शब्द में कितने \"च\" हैं?",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "hi/hi-006-random-CohereLabs-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "hi-006-sampling-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "\"अच्छा\" शब्द में कितने \"च\" हैं?",
      "language": "hi",
      "description": "\"अच्छा\" शब्द में कितने \"च\" हैं?",
      "num_tokens": 83,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "hi/hi-006-sampling-CohereLabs-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "ja-001-greedy-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "こんにちは世界！",
      "language": "ja",
      "description": "こんにちは世界！",
      "num_tokens": 95,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "ja/ja-001-greedy-CohereLabs-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "ja-001-random-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "こんにちは世界！",
      "language": "ja",
      "description": "こんにちは世界！",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "ja/ja-001-random-CohereLabs-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "ja-001-sampling-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "こんにちは世界！",
      "language": "ja",
      "description": "こんにちは世界！",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "ja/ja-001-sampling-CohereLabs-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "ja-002-greedy-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "あなたは誰ですか？",
      "language": "ja",
      "description": "あなたは誰ですか？",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "ja/ja-002-greedy-CohereLabs-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "ja-002-random-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "あなたは誰ですか？",
      "language": "ja",
      "description": "あなたは誰ですか？",
      "num_tokens": 85,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "ja/ja-002-random-CohereLabs-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "ja-002-sampling-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "あなたは誰ですか？",
      "language": "ja",
      "description": "あなたは誰ですか？",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "ja/ja-002-sampling-CohereLabs-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "ja-003-greedy-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "冗談を言ってください",
      "language": "ja",
      "description": "冗談を言ってください",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "ja/ja-003-greedy-CohereLabs-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "ja-003-random-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "冗談を言ってください",
      "language": "ja",
      "description": "冗談を言ってください",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "ja/ja-003-random-CohereLabs-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "ja-003-sampling-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "冗談を言ってください",
      "language": "ja",
      "description": "冗談を言ってください",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "ja/ja-003-sampling-CohereLabs-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "ja-004-greedy-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "日本の首都はどこですか？",
      "language": "ja",
      "description": "日本の首都はどこですか？",
      "num_tokens": 26,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "ja/ja-004-greedy-CohereLabs-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "ja-004-random-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "日本の首都はどこですか？",
      "language": "ja",
      "description": "日本の首都はどこですか？",
      "num_tokens": 33,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "ja/ja-004-random-CohereLabs-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "ja-004-sampling-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "日本の首都はどこですか？",
      "language": "ja",
      "description": "日本の首都はどこですか？",
      "num_tokens": 27,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "ja/ja-004-sampling-CohereLabs-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "ja-005-greedy-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "英語に翻訳してください：急がば回れ",
      "language": "ja",
      "description": "英語に翻訳してください：急がば回れ",
      "num_tokens": 82,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "ja/ja-005-greedy-CohereLabs-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "ja-005-random-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "英語に翻訳してください：急がば回れ",
      "language": "ja",
      "description": "英語に翻訳してください：急がば回れ",
      "num_tokens": 86,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "ja/ja-005-random-CohereLabs-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "ja-005-sampling-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "英語に翻訳してください：急がば回れ",
      "language": "ja",
      "description": "英語に翻訳してください：急がば回れ",
      "num_tokens": 85,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "ja/ja-005-sampling-CohereLabs-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "ja-006-greedy-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "「ありがとうございます」という言葉には「あ」がいくつありますか？",
      "language": "ja",
      "description": "「ありがとうございます」という言葉には「あ」がいくつありますか？",
      "num_tokens": 21,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "ja/ja-006-greedy-CohereLabs-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "ja-006-random-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "「ありがとうございます」という言葉には「あ」がいくつありますか？",
      "language": "ja",
      "description": "「ありがとうございます」という言葉には「あ」がいくつありますか？",
      "num_tokens": 21,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "ja/ja-006-random-CohereLabs-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "ja-006-sampling-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "「ありがとうございます」という言葉には「あ」がいくつありますか？",
      "language": "ja",
      "description": "「ありがとうございます」という言葉には「あ」がいくつありますか？",
      "num_tokens": 21,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "ja/ja-006-sampling-CohereLabs-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "zh-001-greedy-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "你好世界！",
      "language": "zh",
      "description": "你好世界！",
      "num_tokens": 93,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "zh/zh-001-greedy-CohereLabs-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "zh-001-random-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "你好世界！",
      "language": "zh",
      "description": "你好世界！",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "zh/zh-001-random-CohereLabs-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "zh-001-sampling-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "你好世界！",
      "language": "zh",
      "description": "你好世界！",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "zh/zh-001-sampling-CohereLabs-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "zh-002-greedy-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "你是谁？",
      "language": "zh",
      "description": "你是谁？",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "zh/zh-002-greedy-CohereLabs-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "zh-002-random-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "你是谁？",
      "language": "zh",
      "description": "你是谁？",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "zh/zh-002-random-CohereLabs-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "zh-002-sampling-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "你是谁？",
      "language": "zh",
      "description": "你是谁？",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "zh/zh-002-sampling-CohereLabs-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "zh-003-greedy-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "给我讲个笑话",
      "language": "zh",
      "description": "给我讲个笑话",
      "num_tokens": 59,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "zh/zh-003-greedy-CohereLabs-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "zh-003-random-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "给我讲个笑话",
      "language": "zh",
      "description": "给我讲个笑话",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "zh/zh-003-random-CohereLabs-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "zh-003-sampling-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "给我讲个笑话",
      "language": "zh",
      "description": "给我讲个笑话",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "zh/zh-003-sampling-CohereLabs-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "zh-004-greedy-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "中国的首都是哪里？",
      "language": "zh",
      "description": "中国的首都是哪里？",
      "num_tokens": 31,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "zh/zh-004-greedy-CohereLabs-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "zh-004-random-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "中国的首都是哪里？",
      "language": "zh",
      "description": "中国的首都是哪里？",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "zh/zh-004-random-CohereLabs-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "zh-004-sampling-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "中国的首都是哪里？",
      "language": "zh",
      "description": "中国的首都是哪里？",
      "num_tokens": 31,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "zh/zh-004-sampling-CohereLabs-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "zh-005-greedy-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "翻译成英文：早起的鸟儿有虫吃",
      "language": "zh",
      "description": "翻译成英文：早起的鸟儿有虫吃",
      "num_tokens": 48,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "zh/zh-005-greedy-CohereLabs-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "zh-005-random-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "翻译成英文：早起的鸟儿有虫吃",
      "language": "zh",
      "description": "翻译成英文：早起的鸟儿有虫吃",
      "num_tokens": 71,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "zh/zh-005-random-CohereLabs-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "zh-005-sampling-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "翻译成英文：早起的鸟儿有虫吃",
      "language": "zh",
      "description": "翻译成英文：早起的鸟儿有虫吃",
      "num_tokens": 55,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "zh/zh-005-sampling-CohereLabs-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "zh-006-greedy-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "\"美丽\"这个词里有几个\"丽\"？",
      "language": "zh",
      "description": "\"美丽\"这个词里有几个\"丽\"？",
      "num_tokens": 48,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "zh/zh-006-greedy-CohereLabs-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "zh-006-random-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "\"美丽\"这个词里有几个\"丽\"？",
      "language": "zh",
      "description": "\"美丽\"这个词里有几个\"丽\"？",
      "num_tokens": 100,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "zh/zh-006-random-CohereLabs-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "zh-006-sampling-CohereLabs-aya-expanse-8b",
      "type": "inference",
      "prompt": "\"美丽\"这个词里有几个\"丽\"？",
      "language": "zh",
      "description": "\"美丽\"这个词里有几个\"丽\"？",
      "num_tokens": 78,
      "model_id": "CohereLabs/aya-expanse-8b",
      "file": "zh/zh-006-sampling-CohereLabs-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "ar-001-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "مرحبا بالعالم!",
      "language": "ar",
      "description": "مرحبا بالعالم!",
      "num_tokens": 18,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "ar/ar-001-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "ar-001-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "مرحبا بالعالم!",
      "language": "ar",
      "description": "مرحبا بالعالم!",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "ar/ar-001-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "ar-001-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "مرحبا بالعالم!",
      "language": "ar",
      "description": "مرحبا بالعالم!",
      "num_tokens": 21,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "ar/ar-001-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "ar-002-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "من أنت؟",
      "language": "ar",
      "description": "من أنت؟",
      "num_tokens": 28,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "ar/ar-002-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "ar-002-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "من أنت؟",
      "language": "ar",
      "description": "من أنت؟",
      "num_tokens": 41,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "ar/ar-002-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "ar-002-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "من أنت؟",
      "language": "ar",
      "description": "من أنت؟",
      "num_tokens": 38,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "ar/ar-002-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "ar-003-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "أخبرني نكتة",
      "language": "ar",
      "description": "أخبرني نكتة",
      "num_tokens": 21,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "ar/ar-003-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "ar-003-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "أخبرني نكتة",
      "language": "ar",
      "description": "أخبرني نكتة",
      "num_tokens": 9,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "ar/ar-003-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "ar-003-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "أخبرني نكتة",
      "language": "ar",
      "description": "أخبرني نكتة",
      "num_tokens": 25,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "ar/ar-003-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "ar-004-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "ما هي عاصمة مصر؟",
      "language": "ar",
      "description": "ما هي عاصمة مصر؟",
      "num_tokens": 9,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "ar/ar-004-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "ar-004-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "ما هي عاصمة مصر؟",
      "language": "ar",
      "description": "ما هي عاصمة مصر؟",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "ar/ar-004-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "ar-004-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "ما هي عاصمة مصر؟",
      "language": "ar",
      "description": "ما هي عاصمة مصر؟",
      "num_tokens": 16,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "ar/ar-004-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "ar-005-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "ترجم إلى الإنجليزية: العلم نور والجهل ظلام",
      "language": "ar",
      "description": "ترجم إلى الإنجليزية: العلم نور والجهل ظلام",
      "num_tokens": 9,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "ar/ar-005-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "ar-005-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "ترجم إلى الإنجليزية: العلم نور والجهل ظلام",
      "language": "ar",
      "description": "ترجم إلى الإنجليزية: العلم نور والجهل ظلام",
      "num_tokens": 15,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "ar/ar-005-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "ar-005-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "ترجم إلى الإنجليزية: العلم نور والجهل ظلام",
      "language": "ar",
      "description": "ترجم إلى الإنجليزية: العلم نور والجهل ظلام",
      "num_tokens": 8,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "ar/ar-005-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "ar-006-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "كم عدد حرف \"م\" في كلمة \"متحمسون\"؟",
      "language": "ar",
      "description": "كم عدد حرف \"م\" في كلمة \"متحمسون\"؟",
      "num_tokens": 8,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "ar/ar-006-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "ar-006-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "كم عدد حرف \"م\" في كلمة \"متحمسون\"؟",
      "language": "ar",
      "description": "كم عدد حرف \"م\" في كلمة \"متحمسون\"؟",
      "num_tokens": 84,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "ar/ar-006-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "ar-006-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "كم عدد حرف \"م\" في كلمة \"متحمسون\"؟",
      "language": "ar",
      "description": "كم عدد حرف \"م\" في كلمة \"متحمسون\"؟",
      "num_tokens": 8,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "ar/ar-006-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "cs-001-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Ahoj světe!",
      "language": "cs",
      "description": "Ahoj světe!",
      "num_tokens": 9,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "cs/cs-001-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "cs-001-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Ahoj světe!",
      "language": "cs",
      "description": "Ahoj světe!",
      "num_tokens": 49,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "cs/cs-001-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "cs-001-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Ahoj světe!",
      "language": "cs",
      "description": "Ahoj světe!",
      "num_tokens": 12,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "cs/cs-001-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "cs-002-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Kdo jsi?",
      "language": "cs",
      "description": "Kdo jsi?",
      "num_tokens": 19,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "cs/cs-002-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "cs-002-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Kdo jsi?",
      "language": "cs",
      "description": "Kdo jsi?",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "cs/cs-002-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "cs-002-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Kdo jsi?",
      "language": "cs",
      "description": "Kdo jsi?",
      "num_tokens": 21,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "cs/cs-002-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "cs-003-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Řekni mi vtip",
      "language": "cs",
      "description": "Řekni mi vtip",
      "num_tokens": 42,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "cs/cs-003-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "cs-003-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Řekni mi vtip",
      "language": "cs",
      "description": "Řekni mi vtip",
      "num_tokens": 9,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "cs/cs-003-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "cs-003-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Řekni mi vtip",
      "language": "cs",
      "description": "Řekni mi vtip",
      "num_tokens": 36,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "cs/cs-003-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "cs-004-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Jaké je hlavní město Česka?",
      "language": "cs",
      "description": "Jaké je hlavní město Česka?",
      "num_tokens": 11,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "cs/cs-004-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "cs-004-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Jaké je hlavní město Česka?",
      "language": "cs",
      "description": "Jaké je hlavní město Česka?",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "cs/cs-004-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "cs-004-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Jaké je hlavní město Česka?",
      "language": "cs",
      "description": "Jaké je hlavní město Česka?",
      "num_tokens": 32,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "cs/cs-004-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "cs-005-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Přelož do angličtiny: Žluťoučký kůň úpěl ďábelské ódy",
      "language": "cs",
      "description": "Přelož do angličtiny: Žluťoučký kůň úpěl ďábelské ódy",
      "num_tokens": 8,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "cs/cs-005-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "cs-005-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Přelož do angličtiny: Žluťoučký kůň úpěl ďábelské ódy",
      "language": "cs",
      "description": "Přelož do angličtiny: Žluťoučký kůň úpěl ďábelské ódy",
      "num_tokens": 28,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "cs/cs-005-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "cs-005-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Přelož do angličtiny: Žluťoučký kůň úpěl ďábelské ódy",
      "language": "cs",
      "description": "Přelož do angličtiny: Žluťoučký kůň úpěl ďábelské ódy",
      "num_tokens": 15,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "cs/cs-005-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "cs-006-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Kolik \"k\" je ve slově \"nejkulinkaťoulinkatější\"?",
      "language": "cs",
      "description": "Kolik \"k\" je ve slově \"nejkulinkaťoulinkatější\"?",
      "num_tokens": 69,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "cs/cs-006-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "cs-006-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Kolik \"k\" je ve slově \"nejkulinkaťoulinkatější\"?",
      "language": "cs",
      "description": "Kolik \"k\" je ve slově \"nejkulinkaťoulinkatější\"?",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "cs/cs-006-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "cs-006-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Kolik \"k\" je ve slově \"nejkulinkaťoulinkatější\"?",
      "language": "cs",
      "description": "Kolik \"k\" je ve slově \"nejkulinkaťoulinkatější\"?",
      "num_tokens": 58,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "cs/cs-006-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "de-001-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Hallo Welt!",
      "language": "de",
      "description": "Hallo Welt!",
      "num_tokens": 18,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "de/de-001-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "de-001-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Hallo Welt!",
      "language": "de",
      "description": "Hallo Welt!",
      "num_tokens": 75,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "de/de-001-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "de-001-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Hallo Welt!",
      "language": "de",
      "description": "Hallo Welt!",
      "num_tokens": 9,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "de/de-001-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "de-002-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Wer bist du?",
      "language": "de",
      "description": "Wer bist du?",
      "num_tokens": 29,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "de/de-002-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "de-002-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Wer bist du?",
      "language": "de",
      "description": "Wer bist du?",
      "num_tokens": 39,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "de/de-002-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "de-002-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Wer bist du?",
      "language": "de",
      "description": "Wer bist du?",
      "num_tokens": 29,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "de/de-002-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "de-003-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Erzähl mir einen Witz",
      "language": "de",
      "description": "Erzähl mir einen Witz",
      "num_tokens": 17,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "de/de-003-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "de-003-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Erzähl mir einen Witz",
      "language": "de",
      "description": "Erzähl mir einen Witz",
      "num_tokens": 71,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "de/de-003-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "de-003-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Erzähl mir einen Witz",
      "language": "de",
      "description": "Erzähl mir einen Witz",
      "num_tokens": 33,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "de/de-003-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "de-004-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Was ist die Hauptstadt von Deutschland?",
      "language": "de",
      "description": "Was ist die Hauptstadt von Deutschland?",
      "num_tokens": 9,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "de/de-004-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "de-004-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Was ist die Hauptstadt von Deutschland?",
      "language": "de",
      "description": "Was ist die Hauptstadt von Deutschland?",
      "num_tokens": 32,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "de/de-004-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "de-004-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Was ist die Hauptstadt von Deutschland?",
      "language": "de",
      "description": "Was ist die Hauptstadt von Deutschland?",
      "num_tokens": 11,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "de/de-004-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "de-005-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Übersetze ins Englische: Der frühe Vogel fängt den Wurm",
      "language": "de",
      "description": "Übersetze ins Englische: Der frühe Vogel fängt den Wurm",
      "num_tokens": 12,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "de/de-005-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "de-005-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Übersetze ins Englische: Der frühe Vogel fängt den Wurm",
      "language": "de",
      "description": "Übersetze ins Englische: Der frühe Vogel fängt den Wurm",
      "num_tokens": 55,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "de/de-005-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "de-005-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Übersetze ins Englische: Der frühe Vogel fängt den Wurm",
      "language": "de",
      "description": "Übersetze ins Englische: Der frühe Vogel fängt den Wurm",
      "num_tokens": 12,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "de/de-005-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "de-006-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?",
      "language": "de",
      "description": "Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?",
      "num_tokens": 22,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "de/de-006-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "de-006-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?",
      "language": "de",
      "description": "Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "de/de-006-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "de-006-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?",
      "language": "de",
      "description": "Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?",
      "num_tokens": 22,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "de/de-006-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "en-001-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Hello world!",
      "language": "en",
      "description": "Hello world!",
      "num_tokens": 27,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "en/en-001-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "en-001-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Hello world!",
      "language": "en",
      "description": "Hello world!",
      "num_tokens": 4,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "en/en-001-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "en-001-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Hello world!",
      "language": "en",
      "description": "Hello world!",
      "num_tokens": 25,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "en/en-001-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "en-002-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Who are you?",
      "language": "en",
      "description": "Who are you?",
      "num_tokens": 23,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "en/en-002-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "en-002-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Who are you?",
      "language": "en",
      "description": "Who are you?",
      "num_tokens": 93,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "en/en-002-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "en-002-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Who are you?",
      "language": "en",
      "description": "Who are you?",
      "num_tokens": 23,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "en/en-002-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "en-003-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Tell me a joke",
      "language": "en",
      "description": "Tell me a joke",
      "num_tokens": 54,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "en/en-003-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "en-003-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Tell me a joke",
      "language": "en",
      "description": "Tell me a joke",
      "num_tokens": 49,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "en/en-003-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "en-003-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Tell me a joke",
      "language": "en",
      "description": "Tell me a joke",
      "num_tokens": 55,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "en/en-003-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "en-004-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "What is the capital of France?",
      "language": "en",
      "description": "What is the capital of France?",
      "num_tokens": 8,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "en/en-004-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "en-004-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "What is the capital of France?",
      "language": "en",
      "description": "What is the capital of France?",
      "num_tokens": 16,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "en/en-004-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "en-004-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "What is the capital of France?",
      "language": "en",
      "description": "What is the capital of France?",
      "num_tokens": 8,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "en/en-004-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "en-005-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Translate to French: The quick brown fox jumps over the lazy dog",
      "language": "en",
      "description": "Translate to French: The quick brown fox jumps over the lazy dog",
      "num_tokens": 39,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "en/en-005-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "en-005-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Translate to French: The quick brown fox jumps over the lazy dog",
      "language": "en",
      "description": "Translate to French: The quick brown fox jumps over the lazy dog",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "en/en-005-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "en-005-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Translate to French: The quick brown fox jumps over the lazy dog",
      "language": "en",
      "description": "Translate to French: The quick brown fox jumps over the lazy dog",
      "num_tokens": 19,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "en/en-005-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "en-006-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "How many r's are in \"strawberry\"?",
      "language": "en",
      "description": "How many r's are in \"strawberry\"?",
      "num_tokens": 13,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "en/en-006-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "en-006-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "How many r's are in \"strawberry\"?",
      "language": "en",
      "description": "How many r's are in \"strawberry\"?",
      "num_tokens": 29,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "en/en-006-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "en-006-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "How many r's are in \"strawberry\"?",
      "language": "en",
      "description": "How many r's are in \"strawberry\"?",
      "num_tokens": 13,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "en/en-006-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "es-001-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "¡Hola mundo!",
      "language": "es",
      "description": "¡Hola mundo!",
      "num_tokens": 12,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "es/es-001-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "es-001-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "¡Hola mundo!",
      "language": "es",
      "description": "¡Hola mundo!",
      "num_tokens": 27,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "es/es-001-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "es-001-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "¡Hola mundo!",
      "language": "es",
      "description": "¡Hola mundo!",
      "num_tokens": 13,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "es/es-001-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "es-002-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "¿Quién eres?",
      "language": "es",
      "description": "¿Quién eres?",
      "num_tokens": 29,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "es/es-002-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "es-002-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "¿Quién eres?",
      "language": "es",
      "description": "¿Quién eres?",
      "num_tokens": 44,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "es/es-002-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "es-002-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "¿Quién eres?",
      "language": "es",
      "description": "¿Quién eres?",
      "num_tokens": 76,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "es/es-002-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "es-003-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Cuéntame un chiste",
      "language": "es",
      "description": "Cuéntame un chiste",
      "num_tokens": 65,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "es/es-003-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "es-003-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Cuéntame un chiste",
      "language": "es",
      "description": "Cuéntame un chiste",
      "num_tokens": 8,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "es/es-003-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "es-003-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Cuéntame un chiste",
      "language": "es",
      "description": "Cuéntame un chiste",
      "num_tokens": 31,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "es/es-003-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "es-004-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "¿Cuál es la capital de España?",
      "language": "es",
      "description": "¿Cuál es la capital de España?",
      "num_tokens": 8,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "es/es-004-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "es-004-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "¿Cuál es la capital de España?",
      "language": "es",
      "description": "¿Cuál es la capital de España?",
      "num_tokens": 3,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "es/es-004-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "es-004-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "¿Cuál es la capital de España?",
      "language": "es",
      "description": "¿Cuál es la capital de España?",
      "num_tokens": 8,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "es/es-004-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "es-005-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Traduce al inglés: No hay mal que por bien no venga",
      "language": "es",
      "description": "Traduce al inglés: No hay mal que por bien no venga",
      "num_tokens": 10,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "es/es-005-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "es-005-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Traduce al inglés: No hay mal que por bien no venga",
      "language": "es",
      "description": "Traduce al inglés: No hay mal que por bien no venga",
      "num_tokens": 16,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "es/es-005-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "es-005-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Traduce al inglés: No hay mal que por bien no venga",
      "language": "es",
      "description": "Traduce al inglés: No hay mal que por bien no venga",
      "num_tokens": 11,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "es/es-005-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "es-006-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "¿Cuántas \"r\" hay en la palabra \"ferrocarril\"?",
      "language": "es",
      "description": "¿Cuántas \"r\" hay en la palabra \"ferrocarril\"?",
      "num_tokens": 16,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "es/es-006-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "es-006-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "¿Cuántas \"r\" hay en la palabra \"ferrocarril\"?",
      "language": "es",
      "description": "¿Cuántas \"r\" hay en la palabra \"ferrocarril\"?",
      "num_tokens": 23,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "es/es-006-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "es-006-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "¿Cuántas \"r\" hay en la palabra \"ferrocarril\"?",
      "language": "es",
      "description": "¿Cuántas \"r\" hay en la palabra \"ferrocarril\"?",
      "num_tokens": 20,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "es/es-006-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "fr-001-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Bonjour le monde!",
      "language": "fr",
      "description": "Bonjour le monde!",
      "num_tokens": 16,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "fr/fr-001-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "fr-001-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Bonjour le monde!",
      "language": "fr",
      "description": "Bonjour le monde!",
      "num_tokens": 40,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "fr/fr-001-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "fr-001-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Bonjour le monde!",
      "language": "fr",
      "description": "Bonjour le monde!",
      "num_tokens": 10,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "fr/fr-001-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "fr-002-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Qui es-tu?",
      "language": "fr",
      "description": "Qui es-tu?",
      "num_tokens": 38,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "fr/fr-002-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "fr-002-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Qui es-tu?",
      "language": "fr",
      "description": "Qui es-tu?",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "fr/fr-002-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "fr-002-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Qui es-tu?",
      "language": "fr",
      "description": "Qui es-tu?",
      "num_tokens": 38,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "fr/fr-002-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "fr-003-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Raconte-moi une blague",
      "language": "fr",
      "description": "Raconte-moi une blague",
      "num_tokens": 39,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "fr/fr-003-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "fr-003-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Raconte-moi une blague",
      "language": "fr",
      "description": "Raconte-moi une blague",
      "num_tokens": 34,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "fr/fr-003-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "fr-003-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Raconte-moi une blague",
      "language": "fr",
      "description": "Raconte-moi une blague",
      "num_tokens": 40,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "fr/fr-003-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "fr-004-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Quelle est la capitale de la France?",
      "language": "fr",
      "description": "Quelle est la capitale de la France?",
      "num_tokens": 10,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "fr/fr-004-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "fr-004-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Quelle est la capitale de la France?",
      "language": "fr",
      "description": "Quelle est la capitale de la France?",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "fr/fr-004-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "fr-004-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Quelle est la capitale de la France?",
      "language": "fr",
      "description": "Quelle est la capitale de la France?",
      "num_tokens": 86,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "fr/fr-004-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "fr-005-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Traduis en anglais: La mariée était en blanc",
      "language": "fr",
      "description": "Traduis en anglais: La mariée était en blanc",
      "num_tokens": 22,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "fr/fr-005-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "fr-005-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Traduis en anglais: La mariée était en blanc",
      "language": "fr",
      "description": "Traduis en anglais: La mariée était en blanc",
      "num_tokens": 7,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "fr/fr-005-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "fr-005-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Traduis en anglais: La mariée était en blanc",
      "language": "fr",
      "description": "Traduis en anglais: La mariée était en blanc",
      "num_tokens": 26,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "fr/fr-005-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "fr-006-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Combien de \"e\" y a-t-il dans le mot \"merveilleusement\"?",
      "language": "fr",
      "description": "Combien de \"e\" y a-t-il dans le mot \"merveilleusement\"?",
      "num_tokens": 19,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "fr/fr-006-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "fr-006-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Combien de \"e\" y a-t-il dans le mot \"merveilleusement\"?",
      "language": "fr",
      "description": "Combien de \"e\" y a-t-il dans le mot \"merveilleusement\"?",
      "num_tokens": 75,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "fr/fr-006-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "fr-006-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Combien de \"e\" y a-t-il dans le mot \"merveilleusement\"?",
      "language": "fr",
      "description": "Combien de \"e\" y a-t-il dans le mot \"merveilleusement\"?",
      "num_tokens": 22,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "fr/fr-006-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "hi-001-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "नमस्ते दुनिया!",
      "language": "hi",
      "description": "नमस्ते दुनिया!",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "hi/hi-001-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "hi-001-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "नमस्ते दुनिया!",
      "language": "hi",
      "description": "नमस्ते दुनिया!",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "hi/hi-001-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "hi-001-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "नमस्ते दुनिया!",
      "language": "hi",
      "description": "नमस्ते दुनिया!",
      "num_tokens": 81,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "hi/hi-001-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "hi-002-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "तुम कौन हो?",
      "language": "hi",
      "description": "तुम कौन हो?",
      "num_tokens": 96,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "hi/hi-002-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "hi-002-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "तुम कौन हो?",
      "language": "hi",
      "description": "तुम कौन हो?",
      "num_tokens": 97,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "hi/hi-002-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "hi-002-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "तुम कौन हो?",
      "language": "hi",
      "description": "तुम कौन हो?",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "hi/hi-002-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "hi-003-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "मुझे एक चुटकुला सुनाओ",
      "language": "hi",
      "description": "मुझे एक चुटकुला सुनाओ",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "hi/hi-003-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "hi-003-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "मुझे एक चुटकुला सुनाओ",
      "language": "hi",
      "description": "मुझे एक चुटकुला सुनाओ",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "hi/hi-003-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "hi-003-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "मुझे एक चुटकुला सुनाओ",
      "language": "hi",
      "description": "मुझे एक चुटकुला सुनाओ",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "hi/hi-003-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "hi-004-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "भारत की राजधानी क्या है?",
      "language": "hi",
      "description": "भारत की राजधानी क्या है?",
      "num_tokens": 17,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "hi/hi-004-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "hi-004-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "भारत की राजधानी क्या है?",
      "language": "hi",
      "description": "भारत की राजधानी क्या है?",
      "num_tokens": 46,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "hi/hi-004-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "hi-004-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "भारत की राजधानी क्या है?",
      "language": "hi",
      "description": "भारत की राजधानी क्या है?",
      "num_tokens": 17,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "hi/hi-004-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "hi-005-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "अंग्रेजी में अनुवाद करें: अंधों में काना राजा",
      "language": "hi",
      "description": "अंग्रेजी में अनुवाद करें: अंधों में काना राजा",
      "num_tokens": 9,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "hi/hi-005-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "hi-005-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "अंग्रेजी में अनुवाद करें: अंधों में काना राजा",
      "language": "hi",
      "description": "अंग्रेजी में अनुवाद करें: अंधों में काना राजा",
      "num_tokens": 31,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "hi/hi-005-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "hi-005-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "अंग्रेजी में अनुवाद करें: अंधों में काना राजा",
      "language": "hi",
      "description": "अंग्रेजी में अनुवाद करें: अंधों में काना राजा",
      "num_tokens": 13,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "hi/hi-005-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "hi-006-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "\"अच्छा\" शब्द में कितने \"च\" हैं?",
      "language": "hi",
      "description": "\"अच्छा\" शब्द में कितने \"च\" हैं?",
      "num_tokens": 16,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "hi/hi-006-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "hi-006-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "\"अच्छा\" शब्द में कितने \"च\" हैं?",
      "language": "hi",
      "description": "\"अच्छा\" शब्द में कितने \"च\" हैं?",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "hi/hi-006-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "hi-006-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "\"अच्छा\" शब्द में कितने \"च\" हैं?",
      "language": "hi",
      "description": "\"अच्छा\" शब्द में कितने \"च\" हैं?",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "hi/hi-006-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "ja-001-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "こんにちは世界！",
      "language": "ja",
      "description": "こんにちは世界！",
      "num_tokens": 8,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "ja/ja-001-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "ja-001-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "こんにちは世界！",
      "language": "ja",
      "description": "こんにちは世界！",
      "num_tokens": 31,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "ja/ja-001-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "ja-001-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "こんにちは世界！",
      "language": "ja",
      "description": "こんにちは世界！",
      "num_tokens": 6,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "ja/ja-001-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "ja-002-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "あなたは誰ですか？",
      "language": "ja",
      "description": "あなたは誰ですか？",
      "num_tokens": 13,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "ja/ja-002-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "ja-002-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "あなたは誰ですか？",
      "language": "ja",
      "description": "あなたは誰ですか？",
      "num_tokens": 39,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "ja/ja-002-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "ja-002-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "あなたは誰ですか？",
      "language": "ja",
      "description": "あなたは誰ですか？",
      "num_tokens": 17,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "ja/ja-002-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "ja-003-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "冗談を言ってください",
      "language": "ja",
      "description": "冗談を言ってください",
      "num_tokens": 73,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "ja/ja-003-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "ja-003-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "冗談を言ってください",
      "language": "ja",
      "description": "冗談を言ってください",
      "num_tokens": 95,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "ja/ja-003-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "ja-003-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "冗談を言ってください",
      "language": "ja",
      "description": "冗談を言ってください",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "ja/ja-003-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "ja-004-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "日本の首都はどこですか？",
      "language": "ja",
      "description": "日本の首都はどこですか？",
      "num_tokens": 10,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "ja/ja-004-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "ja-004-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "日本の首都はどこですか？",
      "language": "ja",
      "description": "日本の首都はどこですか？",
      "num_tokens": 2,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "ja/ja-004-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "ja-004-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "日本の首都はどこですか？",
      "language": "ja",
      "description": "日本の首都はどこですか？",
      "num_tokens": 34,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "ja/ja-004-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "ja-005-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "英語に翻訳してください：急がば回れ",
      "language": "ja",
      "description": "英語に翻訳してください：急がば回れ",
      "num_tokens": 6,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "ja/ja-005-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "ja-005-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "英語に翻訳してください：急がば回れ",
      "language": "ja",
      "description": "英語に翻訳してください：急がば回れ",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "ja/ja-005-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "ja-005-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "英語に翻訳してください：急がば回れ",
      "language": "ja",
      "description": "英語に翻訳してください：急がば回れ",
      "num_tokens": 16,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "ja/ja-005-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "ja-006-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "「ありがとうございます」という言葉には「あ」がいくつありますか？",
      "language": "ja",
      "description": "「ありがとうございます」という言葉には「あ」がいくつありますか？",
      "num_tokens": 17,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "ja/ja-006-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "ja-006-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "「ありがとうございます」という言葉には「あ」がいくつありますか？",
      "language": "ja",
      "description": "「ありがとうございます」という言葉には「あ」がいくつありますか？",
      "num_tokens": 15,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "ja/ja-006-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "ja-006-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "「ありがとうございます」という言葉には「あ」がいくつありますか？",
      "language": "ja",
      "description": "「ありがとうございます」という言葉には「あ」がいくつありますか？",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "ja/ja-006-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "zh-001-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "你好世界！",
      "language": "zh",
      "description": "你好世界！",
      "num_tokens": 17,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "zh/zh-001-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "zh-001-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "你好世界！",
      "language": "zh",
      "description": "你好世界！",
      "num_tokens": 52,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "zh/zh-001-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "zh-001-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "你好世界！",
      "language": "zh",
      "description": "你好世界！",
      "num_tokens": 6,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "zh/zh-001-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "zh-002-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "你是谁？",
      "language": "zh",
      "description": "你是谁？",
      "num_tokens": 15,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "zh/zh-002-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "zh-002-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "你是谁？",
      "language": "zh",
      "description": "你是谁？",
      "num_tokens": 93,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "zh/zh-002-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "zh-002-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "你是谁？",
      "language": "zh",
      "description": "你是谁？",
      "num_tokens": 23,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "zh/zh-002-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "zh-003-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "给我讲个笑话",
      "language": "zh",
      "description": "给我讲个笑话",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "zh/zh-003-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "zh-003-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "给我讲个笑话",
      "language": "zh",
      "description": "给我讲个笑话",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "zh/zh-003-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "zh-003-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "给我讲个笑话",
      "language": "zh",
      "description": "给我讲个笑话",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "zh/zh-003-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "zh-004-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "中国的首都是哪里？",
      "language": "zh",
      "description": "中国的首都是哪里？",
      "num_tokens": 7,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "zh/zh-004-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "zh-004-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "中国的首都是哪里？",
      "language": "zh",
      "description": "中国的首都是哪里？",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "zh/zh-004-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "zh-004-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "中国的首都是哪里？",
      "language": "zh",
      "description": "中国的首都是哪里？",
      "num_tokens": 6,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "zh/zh-004-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "zh-005-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "翻译成英文：早起的鸟儿有虫吃",
      "language": "zh",
      "description": "翻译成英文：早起的鸟儿有虫吃",
      "num_tokens": 15,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "zh/zh-005-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "zh-005-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "翻译成英文：早起的鸟儿有虫吃",
      "language": "zh",
      "description": "翻译成英文：早起的鸟儿有虫吃",
      "num_tokens": 4,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "zh/zh-005-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "zh-005-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "翻译成英文：早起的鸟儿有虫吃",
      "language": "zh",
      "description": "翻译成英文：早起的鸟儿有虫吃",
      "num_tokens": 11,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "zh/zh-005-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "zh-006-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "\"美丽\"这个词里有几个\"丽\"？",
      "language": "zh",
      "description": "\"美丽\"这个词里有几个\"丽\"？",
      "num_tokens": 16,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "zh/zh-006-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "zh-006-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "\"美丽\"这个词里有几个\"丽\"？",
      "language": "zh",
      "description": "\"美丽\"这个词里有几个\"丽\"？",
      "num_tokens": 43,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "zh/zh-006-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "zh-006-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "\"美丽\"这个词里有几个\"丽\"？",
      "language": "zh",
      "description": "\"美丽\"这个词里有几个\"丽\"？",
      "num_tokens": 52,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "zh/zh-006-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "ar-001-greedy-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "مرحبا بالعالم!",
      "language": "ar",
      "description": "مرحبا بالعالم!",
      "num_tokens": 97,
      "model_id": "openai/gpt-oss-20b",
      "file": "ar/ar-001-greedy-openai-gpt-oss-20b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "ar-001-random-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "مرحبا بالعالم!",
      "language": "ar",
      "description": "مرحبا بالعالم!",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "ar/ar-001-random-openai-gpt-oss-20b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "ar-001-sampling-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "مرحبا بالعالم!",
      "language": "ar",
      "description": "مرحبا بالعالم!",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "ar/ar-001-sampling-openai-gpt-oss-20b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "ar-002-greedy-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "من أنت؟",
      "language": "ar",
      "description": "من أنت؟",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "ar/ar-002-greedy-openai-gpt-oss-20b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "ar-002-random-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "من أنت؟",
      "language": "ar",
      "description": "من أنت؟",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "ar/ar-002-random-openai-gpt-oss-20b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "ar-002-sampling-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "من أنت؟",
      "language": "ar",
      "description": "من أنت؟",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "ar/ar-002-sampling-openai-gpt-oss-20b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "ar-003-greedy-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "أخبرني نكتة",
      "language": "ar",
      "description": "أخبرني نكتة",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "ar/ar-003-greedy-openai-gpt-oss-20b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "ar-003-random-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "أخبرني نكتة",
      "language": "ar",
      "description": "أخبرني نكتة",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "ar/ar-003-random-openai-gpt-oss-20b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "ar-003-sampling-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "أخبرني نكتة",
      "language": "ar",
      "description": "أخبرني نكتة",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "ar/ar-003-sampling-openai-gpt-oss-20b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "ar-004-greedy-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "ما هي عاصمة مصر؟",
      "language": "ar",
      "description": "ما هي عاصمة مصر؟",
      "num_tokens": 52,
      "model_id": "openai/gpt-oss-20b",
      "file": "ar/ar-004-greedy-openai-gpt-oss-20b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "ar-004-random-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "ما هي عاصمة مصر؟",
      "language": "ar",
      "description": "ما هي عاصمة مصر؟",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "ar/ar-004-random-openai-gpt-oss-20b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "ar-004-sampling-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "ما هي عاصمة مصر؟",
      "language": "ar",
      "description": "ما هي عاصمة مصر؟",
      "num_tokens": 47,
      "model_id": "openai/gpt-oss-20b",
      "file": "ar/ar-004-sampling-openai-gpt-oss-20b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "ar-005-greedy-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "ترجم إلى الإنجليزية: العلم نور والجهل ظلام",
      "language": "ar",
      "description": "ترجم إلى الإنجليزية: العلم نور والجهل ظلام",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "ar/ar-005-greedy-openai-gpt-oss-20b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "ar-005-random-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "ترجم إلى الإنجليزية: العلم نور والجهل ظلام",
      "language": "ar",
      "description": "ترجم إلى الإنجليزية: العلم نور والجهل ظلام",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "ar/ar-005-random-openai-gpt-oss-20b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "ar-005-sampling-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "ترجم إلى الإنجليزية: العلم نور والجهل ظلام",
      "language": "ar",
      "description": "ترجم إلى الإنجليزية: العلم نور والجهل ظلام",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "ar/ar-005-sampling-openai-gpt-oss-20b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "ar-006-greedy-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "كم عدد حرف \"م\" في كلمة \"متحمسون\"؟",
      "language": "ar",
      "description": "كم عدد حرف \"م\" في كلمة \"متحمسون\"؟",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "ar/ar-006-greedy-openai-gpt-oss-20b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "ar-006-random-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "كم عدد حرف \"م\" في كلمة \"متحمسون\"؟",
      "language": "ar",
      "description": "كم عدد حرف \"م\" في كلمة \"متحمسون\"؟",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "ar/ar-006-random-openai-gpt-oss-20b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "ar-006-sampling-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "كم عدد حرف \"م\" في كلمة \"متحمسون\"؟",
      "language": "ar",
      "description": "كم عدد حرف \"م\" في كلمة \"متحمسون\"؟",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "ar/ar-006-sampling-openai-gpt-oss-20b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "cs-001-greedy-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "Ahoj světe!",
      "language": "cs",
      "description": "Ahoj světe!",
      "num_tokens": 83,
      "model_id": "openai/gpt-oss-20b",
      "file": "cs/cs-001-greedy-openai-gpt-oss-20b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "cs-001-random-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "Ahoj světe!",
      "language": "cs",
      "description": "Ahoj světe!",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "cs/cs-001-random-openai-gpt-oss-20b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "cs-001-sampling-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "Ahoj světe!",
      "language": "cs",
      "description": "Ahoj světe!",
      "num_tokens": 74,
      "model_id": "openai/gpt-oss-20b",
      "file": "cs/cs-001-sampling-openai-gpt-oss-20b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "cs-002-greedy-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "Kdo jsi?",
      "language": "cs",
      "description": "Kdo jsi?",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "cs/cs-002-greedy-openai-gpt-oss-20b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "cs-002-random-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "Kdo jsi?",
      "language": "cs",
      "description": "Kdo jsi?",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "cs/cs-002-random-openai-gpt-oss-20b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "cs-002-sampling-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "Kdo jsi?",
      "language": "cs",
      "description": "Kdo jsi?",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "cs/cs-002-sampling-openai-gpt-oss-20b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "cs-003-greedy-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "Řekni mi vtip",
      "language": "cs",
      "description": "Řekni mi vtip",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "cs/cs-003-greedy-openai-gpt-oss-20b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "cs-003-random-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "Řekni mi vtip",
      "language": "cs",
      "description": "Řekni mi vtip",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "cs/cs-003-random-openai-gpt-oss-20b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "cs-003-sampling-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "Řekni mi vtip",
      "language": "cs",
      "description": "Řekni mi vtip",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "cs/cs-003-sampling-openai-gpt-oss-20b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "cs-004-greedy-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "Jaké je hlavní město Česka?",
      "language": "cs",
      "description": "Jaké je hlavní město Česka?",
      "num_tokens": 59,
      "model_id": "openai/gpt-oss-20b",
      "file": "cs/cs-004-greedy-openai-gpt-oss-20b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "cs-004-random-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "Jaké je hlavní město Česka?",
      "language": "cs",
      "description": "Jaké je hlavní město Česka?",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "cs/cs-004-random-openai-gpt-oss-20b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "cs-004-sampling-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "Jaké je hlavní město Česka?",
      "language": "cs",
      "description": "Jaké je hlavní město Česka?",
      "num_tokens": 69,
      "model_id": "openai/gpt-oss-20b",
      "file": "cs/cs-004-sampling-openai-gpt-oss-20b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "cs-005-greedy-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "Přelož do angličtiny: Žluťoučký kůň úpěl ďábelské ódy",
      "language": "cs",
      "description": "Přelož do angličtiny: Žluťoučký kůň úpěl ďábelské ódy",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "cs/cs-005-greedy-openai-gpt-oss-20b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "cs-005-random-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "Přelož do angličtiny: Žluťoučký kůň úpěl ďábelské ódy",
      "language": "cs",
      "description": "Přelož do angličtiny: Žluťoučký kůň úpěl ďábelské ódy",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "cs/cs-005-random-openai-gpt-oss-20b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "cs-005-sampling-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "Přelož do angličtiny: Žluťoučký kůň úpěl ďábelské ódy",
      "language": "cs",
      "description": "Přelož do angličtiny: Žluťoučký kůň úpěl ďábelské ódy",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "cs/cs-005-sampling-openai-gpt-oss-20b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "cs-006-greedy-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "Kolik \"k\" je ve slově \"nejkulinkaťoulinkatější\"?",
      "language": "cs",
      "description": "Kolik \"k\" je ve slově \"nejkulinkaťoulinkatější\"?",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "cs/cs-006-greedy-openai-gpt-oss-20b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "cs-006-random-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "Kolik \"k\" je ve slově \"nejkulinkaťoulinkatější\"?",
      "language": "cs",
      "description": "Kolik \"k\" je ve slově \"nejkulinkaťoulinkatější\"?",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "cs/cs-006-random-openai-gpt-oss-20b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "cs-006-sampling-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "Kolik \"k\" je ve slově \"nejkulinkaťoulinkatější\"?",
      "language": "cs",
      "description": "Kolik \"k\" je ve slově \"nejkulinkaťoulinkatější\"?",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "cs/cs-006-sampling-openai-gpt-oss-20b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "de-001-greedy-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "Hallo Welt!",
      "language": "de",
      "description": "Hallo Welt!",
      "num_tokens": 64,
      "model_id": "openai/gpt-oss-20b",
      "file": "de/de-001-greedy-openai-gpt-oss-20b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "de-001-random-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "Hallo Welt!",
      "language": "de",
      "description": "Hallo Welt!",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "de/de-001-random-openai-gpt-oss-20b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "de-001-sampling-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "Hallo Welt!",
      "language": "de",
      "description": "Hallo Welt!",
      "num_tokens": 72,
      "model_id": "openai/gpt-oss-20b",
      "file": "de/de-001-sampling-openai-gpt-oss-20b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "de-002-greedy-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "Wer bist du?",
      "language": "de",
      "description": "Wer bist du?",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "de/de-002-greedy-openai-gpt-oss-20b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "de-002-random-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "Wer bist du?",
      "language": "de",
      "description": "Wer bist du?",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "de/de-002-random-openai-gpt-oss-20b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "de-002-sampling-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "Wer bist du?",
      "language": "de",
      "description": "Wer bist du?",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "de/de-002-sampling-openai-gpt-oss-20b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "de-003-greedy-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "Erzähl mir einen Witz",
      "language": "de",
      "description": "Erzähl mir einen Witz",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "de/de-003-greedy-openai-gpt-oss-20b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "de-003-random-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "Erzähl mir einen Witz",
      "language": "de",
      "description": "Erzähl mir einen Witz",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "de/de-003-random-openai-gpt-oss-20b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "de-003-sampling-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "Erzähl mir einen Witz",
      "language": "de",
      "description": "Erzähl mir einen Witz",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "de/de-003-sampling-openai-gpt-oss-20b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "de-004-greedy-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "Was ist die Hauptstadt von Deutschland?",
      "language": "de",
      "description": "Was ist die Hauptstadt von Deutschland?",
      "num_tokens": 47,
      "model_id": "openai/gpt-oss-20b",
      "file": "de/de-004-greedy-openai-gpt-oss-20b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "de-004-random-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "Was ist die Hauptstadt von Deutschland?",
      "language": "de",
      "description": "Was ist die Hauptstadt von Deutschland?",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "de/de-004-random-openai-gpt-oss-20b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "de-004-sampling-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "Was ist die Hauptstadt von Deutschland?",
      "language": "de",
      "description": "Was ist die Hauptstadt von Deutschland?",
      "num_tokens": 50,
      "model_id": "openai/gpt-oss-20b",
      "file": "de/de-004-sampling-openai-gpt-oss-20b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "de-005-greedy-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "Übersetze ins Englische: Der frühe Vogel fängt den Wurm",
      "language": "de",
      "description": "Übersetze ins Englische: Der frühe Vogel fängt den Wurm",
      "num_tokens": 87,
      "model_id": "openai/gpt-oss-20b",
      "file": "de/de-005-greedy-openai-gpt-oss-20b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "de-005-random-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "Übersetze ins Englische: Der frühe Vogel fängt den Wurm",
      "language": "de",
      "description": "Übersetze ins Englische: Der frühe Vogel fängt den Wurm",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "de/de-005-random-openai-gpt-oss-20b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "de-005-sampling-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "Übersetze ins Englische: Der frühe Vogel fängt den Wurm",
      "language": "de",
      "description": "Übersetze ins Englische: Der frühe Vogel fängt den Wurm",
      "num_tokens": 73,
      "model_id": "openai/gpt-oss-20b",
      "file": "de/de-005-sampling-openai-gpt-oss-20b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "de-006-greedy-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?",
      "language": "de",
      "description": "Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "de/de-006-greedy-openai-gpt-oss-20b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "de-006-random-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?",
      "language": "de",
      "description": "Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "de/de-006-random-openai-gpt-oss-20b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "de-006-sampling-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?",
      "language": "de",
      "description": "Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "de/de-006-sampling-openai-gpt-oss-20b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "en-001-greedy-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "Hello world!",
      "language": "en",
      "description": "Hello world!",
      "num_tokens": 41,
      "model_id": "openai/gpt-oss-20b",
      "file": "en/en-001-greedy-openai-gpt-oss-20b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "en-001-random-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "Hello world!",
      "language": "en",
      "description": "Hello world!",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "en/en-001-random-openai-gpt-oss-20b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "en-001-sampling-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "Hello world!",
      "language": "en",
      "description": "Hello world!",
      "num_tokens": 53,
      "model_id": "openai/gpt-oss-20b",
      "file": "en/en-001-sampling-openai-gpt-oss-20b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "en-002-greedy-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "Who are you?",
      "language": "en",
      "description": "Who are you?",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "en/en-002-greedy-openai-gpt-oss-20b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "en-002-random-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "Who are you?",
      "language": "en",
      "description": "Who are you?",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "en/en-002-random-openai-gpt-oss-20b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "en-002-sampling-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "Who are you?",
      "language": "en",
      "description": "Who are you?",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "en/en-002-sampling-openai-gpt-oss-20b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "en-003-greedy-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "Tell me a joke",
      "language": "en",
      "description": "Tell me a joke",
      "num_tokens": 40,
      "model_id": "openai/gpt-oss-20b",
      "file": "en/en-003-greedy-openai-gpt-oss-20b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "en-003-random-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "Tell me a joke",
      "language": "en",
      "description": "Tell me a joke",
      "num_tokens": 91,
      "model_id": "openai/gpt-oss-20b",
      "file": "en/en-003-random-openai-gpt-oss-20b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "en-003-sampling-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "Tell me a joke",
      "language": "en",
      "description": "Tell me a joke",
      "num_tokens": 75,
      "model_id": "openai/gpt-oss-20b",
      "file": "en/en-003-sampling-openai-gpt-oss-20b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "en-004-greedy-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "What is the capital of France?",
      "language": "en",
      "description": "What is the capital of France?",
      "num_tokens": 40,
      "model_id": "openai/gpt-oss-20b",
      "file": "en/en-004-greedy-openai-gpt-oss-20b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "en-004-random-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "What is the capital of France?",
      "language": "en",
      "description": "What is the capital of France?",
      "num_tokens": 92,
      "model_id": "openai/gpt-oss-20b",
      "file": "en/en-004-random-openai-gpt-oss-20b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "en-004-sampling-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "What is the capital of France?",
      "language": "en",
      "description": "What is the capital of France?",
      "num_tokens": 31,
      "model_id": "openai/gpt-oss-20b",
      "file": "en/en-004-sampling-openai-gpt-oss-20b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "en-005-greedy-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "Translate to French: The quick brown fox jumps over the lazy dog",
      "language": "en",
      "description": "Translate to French: The quick brown fox jumps over the lazy dog",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "en/en-005-greedy-openai-gpt-oss-20b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "en-005-random-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "Translate to French: The quick brown fox jumps over the lazy dog",
      "language": "en",
      "description": "Translate to French: The quick brown fox jumps over the lazy dog",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "en/en-005-random-openai-gpt-oss-20b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "en-005-sampling-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "Translate to French: The quick brown fox jumps over the lazy dog",
      "language": "en",
      "description": "Translate to French: The quick brown fox jumps over the lazy dog",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "en/en-005-sampling-openai-gpt-oss-20b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "en-006-greedy-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "How many r's are in \"strawberry\"?",
      "language": "en",
      "description": "How many r's are in \"strawberry\"?",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "en/en-006-greedy-openai-gpt-oss-20b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "en-006-random-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "How many r's are in \"strawberry\"?",
      "language": "en",
      "description": "How many r's are in \"strawberry\"?",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "en/en-006-random-openai-gpt-oss-20b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "en-006-sampling-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "How many r's are in \"strawberry\"?",
      "language": "en",
      "description": "How many r's are in \"strawberry\"?",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "en/en-006-sampling-openai-gpt-oss-20b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "es-001-greedy-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "¡Hola mundo!",
      "language": "es",
      "description": "¡Hola mundo!",
      "num_tokens": 53,
      "model_id": "openai/gpt-oss-20b",
      "file": "es/es-001-greedy-openai-gpt-oss-20b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "es-001-random-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "¡Hola mundo!",
      "language": "es",
      "description": "¡Hola mundo!",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "es/es-001-random-openai-gpt-oss-20b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "es-001-sampling-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "¡Hola mundo!",
      "language": "es",
      "description": "¡Hola mundo!",
      "num_tokens": 68,
      "model_id": "openai/gpt-oss-20b",
      "file": "es/es-001-sampling-openai-gpt-oss-20b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "es-002-greedy-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "¿Quién eres?",
      "language": "es",
      "description": "¿Quién eres?",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "es/es-002-greedy-openai-gpt-oss-20b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "es-002-random-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "¿Quién eres?",
      "language": "es",
      "description": "¿Quién eres?",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "es/es-002-random-openai-gpt-oss-20b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "es-002-sampling-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "¿Quién eres?",
      "language": "es",
      "description": "¿Quién eres?",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "es/es-002-sampling-openai-gpt-oss-20b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "es-003-greedy-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "Cuéntame un chiste",
      "language": "es",
      "description": "Cuéntame un chiste",
      "num_tokens": 91,
      "model_id": "openai/gpt-oss-20b",
      "file": "es/es-003-greedy-openai-gpt-oss-20b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "es-003-random-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "Cuéntame un chiste",
      "language": "es",
      "description": "Cuéntame un chiste",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "es/es-003-random-openai-gpt-oss-20b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "es-003-sampling-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "Cuéntame un chiste",
      "language": "es",
      "description": "Cuéntame un chiste",
      "num_tokens": 88,
      "model_id": "openai/gpt-oss-20b",
      "file": "es/es-003-sampling-openai-gpt-oss-20b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "es-004-greedy-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "¿Cuál es la capital de España?",
      "language": "es",
      "description": "¿Cuál es la capital de España?",
      "num_tokens": 43,
      "model_id": "openai/gpt-oss-20b",
      "file": "es/es-004-greedy-openai-gpt-oss-20b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "es-004-random-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "¿Cuál es la capital de España?",
      "language": "es",
      "description": "¿Cuál es la capital de España?",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "es/es-004-random-openai-gpt-oss-20b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "es-004-sampling-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "¿Cuál es la capital de España?",
      "language": "es",
      "description": "¿Cuál es la capital de España?",
      "num_tokens": 51,
      "model_id": "openai/gpt-oss-20b",
      "file": "es/es-004-sampling-openai-gpt-oss-20b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "es-005-greedy-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "Traduce al inglés: No hay mal que por bien no venga",
      "language": "es",
      "description": "Traduce al inglés: No hay mal que por bien no venga",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "es/es-005-greedy-openai-gpt-oss-20b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "es-005-random-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "Traduce al inglés: No hay mal que por bien no venga",
      "language": "es",
      "description": "Traduce al inglés: No hay mal que por bien no venga",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "es/es-005-random-openai-gpt-oss-20b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "es-005-sampling-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "Traduce al inglés: No hay mal que por bien no venga",
      "language": "es",
      "description": "Traduce al inglés: No hay mal que por bien no venga",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "es/es-005-sampling-openai-gpt-oss-20b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "es-006-greedy-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "¿Cuántas \"r\" hay en la palabra \"ferrocarril\"?",
      "language": "es",
      "description": "¿Cuántas \"r\" hay en la palabra \"ferrocarril\"?",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "es/es-006-greedy-openai-gpt-oss-20b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "es-006-random-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "¿Cuántas \"r\" hay en la palabra \"ferrocarril\"?",
      "language": "es",
      "description": "¿Cuántas \"r\" hay en la palabra \"ferrocarril\"?",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "es/es-006-random-openai-gpt-oss-20b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "es-006-sampling-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "¿Cuántas \"r\" hay en la palabra \"ferrocarril\"?",
      "language": "es",
      "description": "¿Cuántas \"r\" hay en la palabra \"ferrocarril\"?",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "es/es-006-sampling-openai-gpt-oss-20b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "fr-001-greedy-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "Bonjour le monde!",
      "language": "fr",
      "description": "Bonjour le monde!",
      "num_tokens": 86,
      "model_id": "openai/gpt-oss-20b",
      "file": "fr/fr-001-greedy-openai-gpt-oss-20b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "fr-001-random-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "Bonjour le monde!",
      "language": "fr",
      "description": "Bonjour le monde!",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "fr/fr-001-random-openai-gpt-oss-20b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "fr-001-sampling-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "Bonjour le monde!",
      "language": "fr",
      "description": "Bonjour le monde!",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "fr/fr-001-sampling-openai-gpt-oss-20b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "fr-002-greedy-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "Qui es-tu?",
      "language": "fr",
      "description": "Qui es-tu?",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "fr/fr-002-greedy-openai-gpt-oss-20b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "fr-002-random-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "Qui es-tu?",
      "language": "fr",
      "description": "Qui es-tu?",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "fr/fr-002-random-openai-gpt-oss-20b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "fr-002-sampling-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "Qui es-tu?",
      "language": "fr",
      "description": "Qui es-tu?",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "fr/fr-002-sampling-openai-gpt-oss-20b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "fr-003-greedy-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "Raconte-moi une blague",
      "language": "fr",
      "description": "Raconte-moi une blague",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "fr/fr-003-greedy-openai-gpt-oss-20b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "fr-003-random-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "Raconte-moi une blague",
      "language": "fr",
      "description": "Raconte-moi une blague",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "fr/fr-003-random-openai-gpt-oss-20b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "fr-003-sampling-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "Raconte-moi une blague",
      "language": "fr",
      "description": "Raconte-moi une blague",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "fr/fr-003-sampling-openai-gpt-oss-20b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "fr-004-greedy-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "Quelle est la capitale de la France?",
      "language": "fr",
      "description": "Quelle est la capitale de la France?",
      "num_tokens": 47,
      "model_id": "openai/gpt-oss-20b",
      "file": "fr/fr-004-greedy-openai-gpt-oss-20b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "fr-004-random-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "Quelle est la capitale de la France?",
      "language": "fr",
      "description": "Quelle est la capitale de la France?",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "fr/fr-004-random-openai-gpt-oss-20b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "fr-004-sampling-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "Quelle est la capitale de la France?",
      "language": "fr",
      "description": "Quelle est la capitale de la France?",
      "num_tokens": 47,
      "model_id": "openai/gpt-oss-20b",
      "file": "fr/fr-004-sampling-openai-gpt-oss-20b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "fr-005-greedy-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "Traduis en anglais: La mariée était en blanc",
      "language": "fr",
      "description": "Traduis en anglais: La mariée était en blanc",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "fr/fr-005-greedy-openai-gpt-oss-20b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "fr-005-random-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "Traduis en anglais: La mariée était en blanc",
      "language": "fr",
      "description": "Traduis en anglais: La mariée était en blanc",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "fr/fr-005-random-openai-gpt-oss-20b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "fr-005-sampling-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "Traduis en anglais: La mariée était en blanc",
      "language": "fr",
      "description": "Traduis en anglais: La mariée était en blanc",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "fr/fr-005-sampling-openai-gpt-oss-20b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "fr-006-greedy-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "Combien de \"e\" y a-t-il dans le mot \"merveilleusement\"?",
      "language": "fr",
      "description": "Combien de \"e\" y a-t-il dans le mot \"merveilleusement\"?",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "fr/fr-006-greedy-openai-gpt-oss-20b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "fr-006-random-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "Combien de \"e\" y a-t-il dans le mot \"merveilleusement\"?",
      "language": "fr",
      "description": "Combien de \"e\" y a-t-il dans le mot \"merveilleusement\"?",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "fr/fr-006-random-openai-gpt-oss-20b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "fr-006-sampling-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "Combien de \"e\" y a-t-il dans le mot \"merveilleusement\"?",
      "language": "fr",
      "description": "Combien de \"e\" y a-t-il dans le mot \"merveilleusement\"?",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "fr/fr-006-sampling-openai-gpt-oss-20b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "hi-001-greedy-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "नमस्ते दुनिया!",
      "language": "hi",
      "description": "नमस्ते दुनिया!",
      "num_tokens": 74,
      "model_id": "openai/gpt-oss-20b",
      "file": "hi/hi-001-greedy-openai-gpt-oss-20b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "hi-001-random-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "नमस्ते दुनिया!",
      "language": "hi",
      "description": "नमस्ते दुनिया!",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "hi/hi-001-random-openai-gpt-oss-20b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "hi-001-sampling-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "नमस्ते दुनिया!",
      "language": "hi",
      "description": "नमस्ते दुनिया!",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "hi/hi-001-sampling-openai-gpt-oss-20b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "hi-002-greedy-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "तुम कौन हो?",
      "language": "hi",
      "description": "तुम कौन हो?",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "hi/hi-002-greedy-openai-gpt-oss-20b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "hi-002-random-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "तुम कौन हो?",
      "language": "hi",
      "description": "तुम कौन हो?",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "hi/hi-002-random-openai-gpt-oss-20b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "hi-002-sampling-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "तुम कौन हो?",
      "language": "hi",
      "description": "तुम कौन हो?",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "hi/hi-002-sampling-openai-gpt-oss-20b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "hi-003-greedy-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "मुझे एक चुटकुला सुनाओ",
      "language": "hi",
      "description": "मुझे एक चुटकुला सुनाओ",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "hi/hi-003-greedy-openai-gpt-oss-20b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "hi-003-random-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "मुझे एक चुटकुला सुनाओ",
      "language": "hi",
      "description": "मुझे एक चुटकुला सुनाओ",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "hi/hi-003-random-openai-gpt-oss-20b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "hi-003-sampling-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "मुझे एक चुटकुला सुनाओ",
      "language": "hi",
      "description": "मुझे एक चुटकुला सुनाओ",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "hi/hi-003-sampling-openai-gpt-oss-20b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "hi-004-greedy-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "भारत की राजधानी क्या है?",
      "language": "hi",
      "description": "भारत की राजधानी क्या है?",
      "num_tokens": 51,
      "model_id": "openai/gpt-oss-20b",
      "file": "hi/hi-004-greedy-openai-gpt-oss-20b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "hi-004-random-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "भारत की राजधानी क्या है?",
      "language": "hi",
      "description": "भारत की राजधानी क्या है?",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "hi/hi-004-random-openai-gpt-oss-20b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "hi-004-sampling-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "भारत की राजधानी क्या है?",
      "language": "hi",
      "description": "भारत की राजधानी क्या है?",
      "num_tokens": 62,
      "model_id": "openai/gpt-oss-20b",
      "file": "hi/hi-004-sampling-openai-gpt-oss-20b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "hi-005-greedy-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "अंग्रेजी में अनुवाद करें: अंधों में काना राजा",
      "language": "hi",
      "description": "अंग्रेजी में अनुवाद करें: अंधों में काना राजा",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "hi/hi-005-greedy-openai-gpt-oss-20b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "hi-005-random-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "अंग्रेजी में अनुवाद करें: अंधों में काना राजा",
      "language": "hi",
      "description": "अंग्रेजी में अनुवाद करें: अंधों में काना राजा",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "hi/hi-005-random-openai-gpt-oss-20b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "hi-005-sampling-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "अंग्रेजी में अनुवाद करें: अंधों में काना राजा",
      "language": "hi",
      "description": "अंग्रेजी में अनुवाद करें: अंधों में काना राजा",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "hi/hi-005-sampling-openai-gpt-oss-20b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "hi-006-greedy-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "\"अच्छा\" शब्द में कितने \"च\" हैं?",
      "language": "hi",
      "description": "\"अच्छा\" शब्द में कितने \"च\" हैं?",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "hi/hi-006-greedy-openai-gpt-oss-20b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "hi-006-random-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "\"अच्छा\" शब्द में कितने \"च\" हैं?",
      "language": "hi",
      "description": "\"अच्छा\" शब्द में कितने \"च\" हैं?",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "hi/hi-006-random-openai-gpt-oss-20b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "hi-006-sampling-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "\"अच्छा\" शब्द में कितने \"च\" हैं?",
      "language": "hi",
      "description": "\"अच्छा\" शब्द में कितने \"च\" हैं?",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "hi/hi-006-sampling-openai-gpt-oss-20b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "ja-001-greedy-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "こんにちは世界！",
      "language": "ja",
      "description": "こんにちは世界！",
      "num_tokens": 94,
      "model_id": "openai/gpt-oss-20b",
      "file": "ja/ja-001-greedy-openai-gpt-oss-20b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "ja-001-random-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "こんにちは世界！",
      "language": "ja",
      "description": "こんにちは世界！",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "ja/ja-001-random-openai-gpt-oss-20b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "ja-001-sampling-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "こんにちは世界！",
      "language": "ja",
      "description": "こんにちは世界！",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "ja/ja-001-sampling-openai-gpt-oss-20b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "ja-002-greedy-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "あなたは誰ですか？",
      "language": "ja",
      "description": "あなたは誰ですか？",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "ja/ja-002-greedy-openai-gpt-oss-20b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "ja-002-random-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "あなたは誰ですか？",
      "language": "ja",
      "description": "あなたは誰ですか？",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "ja/ja-002-random-openai-gpt-oss-20b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "ja-002-sampling-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "あなたは誰ですか？",
      "language": "ja",
      "description": "あなたは誰ですか？",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "ja/ja-002-sampling-openai-gpt-oss-20b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "ja-003-greedy-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "冗談を言ってください",
      "language": "ja",
      "description": "冗談を言ってください",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "ja/ja-003-greedy-openai-gpt-oss-20b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "ja-003-random-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "冗談を言ってください",
      "language": "ja",
      "description": "冗談を言ってください",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "ja/ja-003-random-openai-gpt-oss-20b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "ja-003-sampling-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "冗談を言ってください",
      "language": "ja",
      "description": "冗談を言ってください",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "ja/ja-003-sampling-openai-gpt-oss-20b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "ja-004-greedy-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "日本の首都はどこですか？",
      "language": "ja",
      "description": "日本の首都はどこですか？",
      "num_tokens": 62,
      "model_id": "openai/gpt-oss-20b",
      "file": "ja/ja-004-greedy-openai-gpt-oss-20b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "ja-004-random-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "日本の首都はどこですか？",
      "language": "ja",
      "description": "日本の首都はどこですか？",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "ja/ja-004-random-openai-gpt-oss-20b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "ja-004-sampling-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "日本の首都はどこですか？",
      "language": "ja",
      "description": "日本の首都はどこですか？",
      "num_tokens": 87,
      "model_id": "openai/gpt-oss-20b",
      "file": "ja/ja-004-sampling-openai-gpt-oss-20b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "ja-005-greedy-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "英語に翻訳してください：急がば回れ",
      "language": "ja",
      "description": "英語に翻訳してください：急がば回れ",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "ja/ja-005-greedy-openai-gpt-oss-20b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "ja-005-random-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "英語に翻訳してください：急がば回れ",
      "language": "ja",
      "description": "英語に翻訳してください：急がば回れ",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "ja/ja-005-random-openai-gpt-oss-20b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "ja-005-sampling-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "英語に翻訳してください：急がば回れ",
      "language": "ja",
      "description": "英語に翻訳してください：急がば回れ",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "ja/ja-005-sampling-openai-gpt-oss-20b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "ja-006-greedy-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "「ありがとうございます」という言葉には「あ」がいくつありますか？",
      "language": "ja",
      "description": "「ありがとうございます」という言葉には「あ」がいくつありますか？",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "ja/ja-006-greedy-openai-gpt-oss-20b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "ja-006-random-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "「ありがとうございます」という言葉には「あ」がいくつありますか？",
      "language": "ja",
      "description": "「ありがとうございます」という言葉には「あ」がいくつありますか？",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "ja/ja-006-random-openai-gpt-oss-20b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "ja-006-sampling-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "「ありがとうございます」という言葉には「あ」がいくつありますか？",
      "language": "ja",
      "description": "「ありがとうございます」という言葉には「あ」がいくつありますか？",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "ja/ja-006-sampling-openai-gpt-oss-20b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "zh-001-greedy-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "你好世界！",
      "language": "zh",
      "description": "你好世界！",
      "num_tokens": 86,
      "model_id": "openai/gpt-oss-20b",
      "file": "zh/zh-001-greedy-openai-gpt-oss-20b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "zh-001-random-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "你好世界！",
      "language": "zh",
      "description": "你好世界！",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "zh/zh-001-random-openai-gpt-oss-20b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "zh-001-sampling-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "你好世界！",
      "language": "zh",
      "description": "你好世界！",
      "num_tokens": 93,
      "model_id": "openai/gpt-oss-20b",
      "file": "zh/zh-001-sampling-openai-gpt-oss-20b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "zh-002-greedy-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "你是谁？",
      "language": "zh",
      "description": "你是谁？",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "zh/zh-002-greedy-openai-gpt-oss-20b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "zh-002-random-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "你是谁？",
      "language": "zh",
      "description": "你是谁？",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "zh/zh-002-random-openai-gpt-oss-20b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "zh-002-sampling-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "你是谁？",
      "language": "zh",
      "description": "你是谁？",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "zh/zh-002-sampling-openai-gpt-oss-20b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "zh-003-greedy-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "给我讲个笑话",
      "language": "zh",
      "description": "给我讲个笑话",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "zh/zh-003-greedy-openai-gpt-oss-20b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "zh-003-random-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "给我讲个笑话",
      "language": "zh",
      "description": "给我讲个笑话",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "zh/zh-003-random-openai-gpt-oss-20b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "zh-003-sampling-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "给我讲个笑话",
      "language": "zh",
      "description": "给我讲个笑话",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "zh/zh-003-sampling-openai-gpt-oss-20b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "zh-004-greedy-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "中国的首都是哪里？",
      "language": "zh",
      "description": "中国的首都是哪里？",
      "num_tokens": 50,
      "model_id": "openai/gpt-oss-20b",
      "file": "zh/zh-004-greedy-openai-gpt-oss-20b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "zh-004-random-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "中国的首都是哪里？",
      "language": "zh",
      "description": "中国的首都是哪里？",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "zh/zh-004-random-openai-gpt-oss-20b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "zh-004-sampling-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "中国的首都是哪里？",
      "language": "zh",
      "description": "中国的首都是哪里？",
      "num_tokens": 59,
      "model_id": "openai/gpt-oss-20b",
      "file": "zh/zh-004-sampling-openai-gpt-oss-20b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "zh-005-greedy-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "翻译成英文：早起的鸟儿有虫吃",
      "language": "zh",
      "description": "翻译成英文：早起的鸟儿有虫吃",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "zh/zh-005-greedy-openai-gpt-oss-20b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "zh-005-random-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "翻译成英文：早起的鸟儿有虫吃",
      "language": "zh",
      "description": "翻译成英文：早起的鸟儿有虫吃",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "zh/zh-005-random-openai-gpt-oss-20b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "zh-005-sampling-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "翻译成英文：早起的鸟儿有虫吃",
      "language": "zh",
      "description": "翻译成英文：早起的鸟儿有虫吃",
      "num_tokens": 69,
      "model_id": "openai/gpt-oss-20b",
      "file": "zh/zh-005-sampling-openai-gpt-oss-20b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "zh-006-greedy-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "\"美丽\"这个词里有几个\"丽\"？",
      "language": "zh",
      "description": "\"美丽\"这个词里有几个\"丽\"？",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "zh/zh-006-greedy-openai-gpt-oss-20b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "zh-006-random-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "\"美丽\"这个词里有几个\"丽\"？",
      "language": "zh",
      "description": "\"美丽\"这个词里有几个\"丽\"？",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "zh/zh-006-random-openai-gpt-oss-20b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "zh-006-sampling-openai-gpt-oss-20b",
      "type": "inference",
      "prompt": "\"美丽\"这个词里有几个\"丽\"？",
      "language": "zh",
      "description": "\"美丽\"这个词里有几个\"丽\"？",
      "num_tokens": 100,
      "model_id": "openai/gpt-oss-20b",
      "file": "zh/zh-006-sampling-openai-gpt-oss-20b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "ar-001-greedy-openai-community-gpt2",
      "type": "inference",
      "prompt": "مرحبا بالعالم!",
      "language": "ar",
      "description": "مرحبا بالعالم!",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "ar/ar-001-greedy-openai-community-gpt2.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "ar-001-random-openai-community-gpt2",
      "type": "inference",
      "prompt": "مرحبا بالعالم!",
      "language": "ar",
      "description": "مرحبا بالعالم!",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "ar/ar-001-random-openai-community-gpt2.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "ar-001-sampling-openai-community-gpt2",
      "type": "inference",
      "prompt": "مرحبا بالعالم!",
      "language": "ar",
      "description": "مرحبا بالعالم!",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "ar/ar-001-sampling-openai-community-gpt2.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "ar-002-greedy-openai-community-gpt2",
      "type": "inference",
      "prompt": "من أنت؟",
      "language": "ar",
      "description": "من أنت؟",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "ar/ar-002-greedy-openai-community-gpt2.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "ar-002-random-openai-community-gpt2",
      "type": "inference",
      "prompt": "من أنت؟",
      "language": "ar",
      "description": "من أنت؟",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "ar/ar-002-random-openai-community-gpt2.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "ar-002-sampling-openai-community-gpt2",
      "type": "inference",
      "prompt": "من أنت؟",
      "language": "ar",
      "description": "من أنت؟",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "ar/ar-002-sampling-openai-community-gpt2.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "ar-003-greedy-openai-community-gpt2",
      "type": "inference",
      "prompt": "أخبرني نكتة",
      "language": "ar",
      "description": "أخبرني نكتة",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "ar/ar-003-greedy-openai-community-gpt2.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "ar-003-random-openai-community-gpt2",
      "type": "inference",
      "prompt": "أخبرني نكتة",
      "language": "ar",
      "description": "أخبرني نكتة",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "ar/ar-003-random-openai-community-gpt2.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "ar-003-sampling-openai-community-gpt2",
      "type": "inference",
      "prompt": "أخبرني نكتة",
      "language": "ar",
      "description": "أخبرني نكتة",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "ar/ar-003-sampling-openai-community-gpt2.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "ar-004-greedy-openai-community-gpt2",
      "type": "inference",
      "prompt": "ما هي عاصمة مصر؟",
      "language": "ar",
      "description": "ما هي عاصمة مصر؟",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "ar/ar-004-greedy-openai-community-gpt2.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "ar-004-random-openai-community-gpt2",
      "type": "inference",
      "prompt": "ما هي عاصمة مصر؟",
      "language": "ar",
      "description": "ما هي عاصمة مصر؟",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "ar/ar-004-random-openai-community-gpt2.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "ar-004-sampling-openai-community-gpt2",
      "type": "inference",
      "prompt": "ما هي عاصمة مصر؟",
      "language": "ar",
      "description": "ما هي عاصمة مصر؟",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "ar/ar-004-sampling-openai-community-gpt2.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "ar-005-greedy-openai-community-gpt2",
      "type": "inference",
      "prompt": "ترجم إلى الإنجليزية: العلم نور والجهل ظلام",
      "language": "ar",
      "description": "ترجم إلى الإنجليزية: العلم نور والجهل ظلام",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "ar/ar-005-greedy-openai-community-gpt2.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "ar-005-random-openai-community-gpt2",
      "type": "inference",
      "prompt": "ترجم إلى الإنجليزية: العلم نور والجهل ظلام",
      "language": "ar",
      "description": "ترجم إلى الإنجليزية: العلم نور والجهل ظلام",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "ar/ar-005-random-openai-community-gpt2.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "ar-005-sampling-openai-community-gpt2",
      "type": "inference",
      "prompt": "ترجم إلى الإنجليزية: العلم نور والجهل ظلام",
      "language": "ar",
      "description": "ترجم إلى الإنجليزية: العلم نور والجهل ظلام",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "ar/ar-005-sampling-openai-community-gpt2.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "ar-006-greedy-openai-community-gpt2",
      "type": "inference",
      "prompt": "كم عدد حرف \"م\" في كلمة \"متحمسون\"؟",
      "language": "ar",
      "description": "كم عدد حرف \"م\" في كلمة \"متحمسون\"؟",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "ar/ar-006-greedy-openai-community-gpt2.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "ar-006-random-openai-community-gpt2",
      "type": "inference",
      "prompt": "كم عدد حرف \"م\" في كلمة \"متحمسون\"؟",
      "language": "ar",
      "description": "كم عدد حرف \"م\" في كلمة \"متحمسون\"؟",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "ar/ar-006-random-openai-community-gpt2.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "ar-006-sampling-openai-community-gpt2",
      "type": "inference",
      "prompt": "كم عدد حرف \"م\" في كلمة \"متحمسون\"؟",
      "language": "ar",
      "description": "كم عدد حرف \"م\" في كلمة \"متحمسون\"؟",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "ar/ar-006-sampling-openai-community-gpt2.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "cs-001-greedy-openai-community-gpt2",
      "type": "inference",
      "prompt": "Ahoj světe!",
      "language": "cs",
      "description": "Ahoj světe!",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "cs/cs-001-greedy-openai-community-gpt2.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "cs-001-random-openai-community-gpt2",
      "type": "inference",
      "prompt": "Ahoj světe!",
      "language": "cs",
      "description": "Ahoj světe!",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "cs/cs-001-random-openai-community-gpt2.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "cs-001-sampling-openai-community-gpt2",
      "type": "inference",
      "prompt": "Ahoj světe!",
      "language": "cs",
      "description": "Ahoj světe!",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "cs/cs-001-sampling-openai-community-gpt2.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "cs-002-greedy-openai-community-gpt2",
      "type": "inference",
      "prompt": "Kdo jsi?",
      "language": "cs",
      "description": "Kdo jsi?",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "cs/cs-002-greedy-openai-community-gpt2.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "cs-002-random-openai-community-gpt2",
      "type": "inference",
      "prompt": "Kdo jsi?",
      "language": "cs",
      "description": "Kdo jsi?",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "cs/cs-002-random-openai-community-gpt2.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "cs-002-sampling-openai-community-gpt2",
      "type": "inference",
      "prompt": "Kdo jsi?",
      "language": "cs",
      "description": "Kdo jsi?",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "cs/cs-002-sampling-openai-community-gpt2.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "cs-003-greedy-openai-community-gpt2",
      "type": "inference",
      "prompt": "Řekni mi vtip",
      "language": "cs",
      "description": "Řekni mi vtip",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "cs/cs-003-greedy-openai-community-gpt2.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "cs-003-random-openai-community-gpt2",
      "type": "inference",
      "prompt": "Řekni mi vtip",
      "language": "cs",
      "description": "Řekni mi vtip",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "cs/cs-003-random-openai-community-gpt2.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "cs-003-sampling-openai-community-gpt2",
      "type": "inference",
      "prompt": "Řekni mi vtip",
      "language": "cs",
      "description": "Řekni mi vtip",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "cs/cs-003-sampling-openai-community-gpt2.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "cs-004-greedy-openai-community-gpt2",
      "type": "inference",
      "prompt": "Jaké je hlavní město Česka?",
      "language": "cs",
      "description": "Jaké je hlavní město Česka?",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "cs/cs-004-greedy-openai-community-gpt2.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "cs-004-random-openai-community-gpt2",
      "type": "inference",
      "prompt": "Jaké je hlavní město Česka?",
      "language": "cs",
      "description": "Jaké je hlavní město Česka?",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "cs/cs-004-random-openai-community-gpt2.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "cs-004-sampling-openai-community-gpt2",
      "type": "inference",
      "prompt": "Jaké je hlavní město Česka?",
      "language": "cs",
      "description": "Jaké je hlavní město Česka?",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "cs/cs-004-sampling-openai-community-gpt2.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "cs-005-greedy-openai-community-gpt2",
      "type": "inference",
      "prompt": "Přelož do angličtiny: Žluťoučký kůň úpěl ďábelské ódy",
      "language": "cs",
      "description": "Přelož do angličtiny: Žluťoučký kůň úpěl ďábelské ódy",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "cs/cs-005-greedy-openai-community-gpt2.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "cs-005-random-openai-community-gpt2",
      "type": "inference",
      "prompt": "Přelož do angličtiny: Žluťoučký kůň úpěl ďábelské ódy",
      "language": "cs",
      "description": "Přelož do angličtiny: Žluťoučký kůň úpěl ďábelské ódy",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "cs/cs-005-random-openai-community-gpt2.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "cs-005-sampling-openai-community-gpt2",
      "type": "inference",
      "prompt": "Přelož do angličtiny: Žluťoučký kůň úpěl ďábelské ódy",
      "language": "cs",
      "description": "Přelož do angličtiny: Žluťoučký kůň úpěl ďábelské ódy",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "cs/cs-005-sampling-openai-community-gpt2.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "cs-006-greedy-openai-community-gpt2",
      "type": "inference",
      "prompt": "Kolik \"k\" je ve slově \"nejkulinkaťoulinkatější\"?",
      "language": "cs",
      "description": "Kolik \"k\" je ve slově \"nejkulinkaťoulinkatější\"?",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "cs/cs-006-greedy-openai-community-gpt2.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "cs-006-random-openai-community-gpt2",
      "type": "inference",
      "prompt": "Kolik \"k\" je ve slově \"nejkulinkaťoulinkatější\"?",
      "language": "cs",
      "description": "Kolik \"k\" je ve slově \"nejkulinkaťoulinkatější\"?",
      "num_tokens": 25,
      "model_id": "openai-community/gpt2",
      "file": "cs/cs-006-random-openai-community-gpt2.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "cs-006-sampling-openai-community-gpt2",
      "type": "inference",
      "prompt": "Kolik \"k\" je ve slově \"nejkulinkaťoulinkatější\"?",
      "language": "cs",
      "description": "Kolik \"k\" je ve slově \"nejkulinkaťoulinkatější\"?",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "cs/cs-006-sampling-openai-community-gpt2.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "de-001-greedy-openai-community-gpt2",
      "type": "inference",
      "prompt": "Hallo Welt!",
      "language": "de",
      "description": "Hallo Welt!",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "de/de-001-greedy-openai-community-gpt2.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "de-001-random-openai-community-gpt2",
      "type": "inference",
      "prompt": "Hallo Welt!",
      "language": "de",
      "description": "Hallo Welt!",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "de/de-001-random-openai-community-gpt2.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "de-001-sampling-openai-community-gpt2",
      "type": "inference",
      "prompt": "Hallo Welt!",
      "language": "de",
      "description": "Hallo Welt!",
      "num_tokens": 1,
      "model_id": "openai-community/gpt2",
      "file": "de/de-001-sampling-openai-community-gpt2.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "de-002-greedy-openai-community-gpt2",
      "type": "inference",
      "prompt": "Wer bist du?",
      "language": "de",
      "description": "Wer bist du?",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "de/de-002-greedy-openai-community-gpt2.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "de-002-random-openai-community-gpt2",
      "type": "inference",
      "prompt": "Wer bist du?",
      "language": "de",
      "description": "Wer bist du?",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "de/de-002-random-openai-community-gpt2.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "de-002-sampling-openai-community-gpt2",
      "type": "inference",
      "prompt": "Wer bist du?",
      "language": "de",
      "description": "Wer bist du?",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "de/de-002-sampling-openai-community-gpt2.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "de-003-greedy-openai-community-gpt2",
      "type": "inference",
      "prompt": "Erzähl mir einen Witz",
      "language": "de",
      "description": "Erzähl mir einen Witz",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "de/de-003-greedy-openai-community-gpt2.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "de-003-random-openai-community-gpt2",
      "type": "inference",
      "prompt": "Erzähl mir einen Witz",
      "language": "de",
      "description": "Erzähl mir einen Witz",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "de/de-003-random-openai-community-gpt2.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "de-003-sampling-openai-community-gpt2",
      "type": "inference",
      "prompt": "Erzähl mir einen Witz",
      "language": "de",
      "description": "Erzähl mir einen Witz",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "de/de-003-sampling-openai-community-gpt2.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "de-004-greedy-openai-community-gpt2",
      "type": "inference",
      "prompt": "Was ist die Hauptstadt von Deutschland?",
      "language": "de",
      "description": "Was ist die Hauptstadt von Deutschland?",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "de/de-004-greedy-openai-community-gpt2.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "de-004-random-openai-community-gpt2",
      "type": "inference",
      "prompt": "Was ist die Hauptstadt von Deutschland?",
      "language": "de",
      "description": "Was ist die Hauptstadt von Deutschland?",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "de/de-004-random-openai-community-gpt2.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "de-004-sampling-openai-community-gpt2",
      "type": "inference",
      "prompt": "Was ist die Hauptstadt von Deutschland?",
      "language": "de",
      "description": "Was ist die Hauptstadt von Deutschland?",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "de/de-004-sampling-openai-community-gpt2.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "de-005-greedy-openai-community-gpt2",
      "type": "inference",
      "prompt": "Übersetze ins Englische: Der frühe Vogel fängt den Wurm",
      "language": "de",
      "description": "Übersetze ins Englische: Der frühe Vogel fängt den Wurm",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "de/de-005-greedy-openai-community-gpt2.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "de-005-random-openai-community-gpt2",
      "type": "inference",
      "prompt": "Übersetze ins Englische: Der frühe Vogel fängt den Wurm",
      "language": "de",
      "description": "Übersetze ins Englische: Der frühe Vogel fängt den Wurm",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "de/de-005-random-openai-community-gpt2.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "de-005-sampling-openai-community-gpt2",
      "type": "inference",
      "prompt": "Übersetze ins Englische: Der frühe Vogel fängt den Wurm",
      "language": "de",
      "description": "Übersetze ins Englische: Der frühe Vogel fängt den Wurm",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "de/de-005-sampling-openai-community-gpt2.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "de-006-greedy-openai-community-gpt2",
      "type": "inference",
      "prompt": "Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?",
      "language": "de",
      "description": "Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "de/de-006-greedy-openai-community-gpt2.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "de-006-random-openai-community-gpt2",
      "type": "inference",
      "prompt": "Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?",
      "language": "de",
      "description": "Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "de/de-006-random-openai-community-gpt2.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "de-006-sampling-openai-community-gpt2",
      "type": "inference",
      "prompt": "Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?",
      "language": "de",
      "description": "Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "de/de-006-sampling-openai-community-gpt2.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "en-001-greedy-openai-community-gpt2",
      "type": "inference",
      "prompt": "Hello world!",
      "language": "en",
      "description": "Hello world!",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "en/en-001-greedy-openai-community-gpt2.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "en-001-random-openai-community-gpt2",
      "type": "inference",
      "prompt": "Hello world!",
      "language": "en",
      "description": "Hello world!",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "en/en-001-random-openai-community-gpt2.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "en-001-sampling-openai-community-gpt2",
      "type": "inference",
      "prompt": "Hello world!",
      "language": "en",
      "description": "Hello world!",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "en/en-001-sampling-openai-community-gpt2.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "en-002-greedy-openai-community-gpt2",
      "type": "inference",
      "prompt": "Who are you?",
      "language": "en",
      "description": "Who are you?",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "en/en-002-greedy-openai-community-gpt2.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "en-002-random-openai-community-gpt2",
      "type": "inference",
      "prompt": "Who are you?",
      "language": "en",
      "description": "Who are you?",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "en/en-002-random-openai-community-gpt2.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "en-002-sampling-openai-community-gpt2",
      "type": "inference",
      "prompt": "Who are you?",
      "language": "en",
      "description": "Who are you?",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "en/en-002-sampling-openai-community-gpt2.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "en-003-greedy-openai-community-gpt2",
      "type": "inference",
      "prompt": "Tell me a joke",
      "language": "en",
      "description": "Tell me a joke",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "en/en-003-greedy-openai-community-gpt2.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "en-003-random-openai-community-gpt2",
      "type": "inference",
      "prompt": "Tell me a joke",
      "language": "en",
      "description": "Tell me a joke",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "en/en-003-random-openai-community-gpt2.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "en-003-sampling-openai-community-gpt2",
      "type": "inference",
      "prompt": "Tell me a joke",
      "language": "en",
      "description": "Tell me a joke",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "en/en-003-sampling-openai-community-gpt2.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "en-004-greedy-openai-community-gpt2",
      "type": "inference",
      "prompt": "What is the capital of France?",
      "language": "en",
      "description": "What is the capital of France?",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "en/en-004-greedy-openai-community-gpt2.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "en-004-random-openai-community-gpt2",
      "type": "inference",
      "prompt": "What is the capital of France?",
      "language": "en",
      "description": "What is the capital of France?",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "en/en-004-random-openai-community-gpt2.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "en-004-sampling-openai-community-gpt2",
      "type": "inference",
      "prompt": "What is the capital of France?",
      "language": "en",
      "description": "What is the capital of France?",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "en/en-004-sampling-openai-community-gpt2.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "en-005-greedy-openai-community-gpt2",
      "type": "inference",
      "prompt": "Translate to French: The quick brown fox jumps over the lazy dog",
      "language": "en",
      "description": "Translate to French: The quick brown fox jumps over the lazy dog",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "en/en-005-greedy-openai-community-gpt2.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "en-005-random-openai-community-gpt2",
      "type": "inference",
      "prompt": "Translate to French: The quick brown fox jumps over the lazy dog",
      "language": "en",
      "description": "Translate to French: The quick brown fox jumps over the lazy dog",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "en/en-005-random-openai-community-gpt2.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "en-005-sampling-openai-community-gpt2",
      "type": "inference",
      "prompt": "Translate to French: The quick brown fox jumps over the lazy dog",
      "language": "en",
      "description": "Translate to French: The quick brown fox jumps over the lazy dog",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "en/en-005-sampling-openai-community-gpt2.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "en-006-greedy-openai-community-gpt2",
      "type": "inference",
      "prompt": "How many r's are in \"strawberry\"?",
      "language": "en",
      "description": "How many r's are in \"strawberry\"?",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "en/en-006-greedy-openai-community-gpt2.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "en-006-random-openai-community-gpt2",
      "type": "inference",
      "prompt": "How many r's are in \"strawberry\"?",
      "language": "en",
      "description": "How many r's are in \"strawberry\"?",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "en/en-006-random-openai-community-gpt2.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "en-006-sampling-openai-community-gpt2",
      "type": "inference",
      "prompt": "How many r's are in \"strawberry\"?",
      "language": "en",
      "description": "How many r's are in \"strawberry\"?",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "en/en-006-sampling-openai-community-gpt2.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "es-001-greedy-openai-community-gpt2",
      "type": "inference",
      "prompt": "¡Hola mundo!",
      "language": "es",
      "description": "¡Hola mundo!",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "es/es-001-greedy-openai-community-gpt2.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "es-001-random-openai-community-gpt2",
      "type": "inference",
      "prompt": "¡Hola mundo!",
      "language": "es",
      "description": "¡Hola mundo!",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "es/es-001-random-openai-community-gpt2.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "es-001-sampling-openai-community-gpt2",
      "type": "inference",
      "prompt": "¡Hola mundo!",
      "language": "es",
      "description": "¡Hola mundo!",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "es/es-001-sampling-openai-community-gpt2.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "es-002-greedy-openai-community-gpt2",
      "type": "inference",
      "prompt": "¿Quién eres?",
      "language": "es",
      "description": "¿Quién eres?",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "es/es-002-greedy-openai-community-gpt2.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "es-002-random-openai-community-gpt2",
      "type": "inference",
      "prompt": "¿Quién eres?",
      "language": "es",
      "description": "¿Quién eres?",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "es/es-002-random-openai-community-gpt2.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "es-002-sampling-openai-community-gpt2",
      "type": "inference",
      "prompt": "¿Quién eres?",
      "language": "es",
      "description": "¿Quién eres?",
      "num_tokens": 1,
      "model_id": "openai-community/gpt2",
      "file": "es/es-002-sampling-openai-community-gpt2.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "es-003-greedy-openai-community-gpt2",
      "type": "inference",
      "prompt": "Cuéntame un chiste",
      "language": "es",
      "description": "Cuéntame un chiste",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "es/es-003-greedy-openai-community-gpt2.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "es-003-random-openai-community-gpt2",
      "type": "inference",
      "prompt": "Cuéntame un chiste",
      "language": "es",
      "description": "Cuéntame un chiste",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "es/es-003-random-openai-community-gpt2.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "es-003-sampling-openai-community-gpt2",
      "type": "inference",
      "prompt": "Cuéntame un chiste",
      "language": "es",
      "description": "Cuéntame un chiste",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "es/es-003-sampling-openai-community-gpt2.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "es-004-greedy-openai-community-gpt2",
      "type": "inference",
      "prompt": "¿Cuál es la capital de España?",
      "language": "es",
      "description": "¿Cuál es la capital de España?",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "es/es-004-greedy-openai-community-gpt2.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "es-004-random-openai-community-gpt2",
      "type": "inference",
      "prompt": "¿Cuál es la capital de España?",
      "language": "es",
      "description": "¿Cuál es la capital de España?",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "es/es-004-random-openai-community-gpt2.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "es-004-sampling-openai-community-gpt2",
      "type": "inference",
      "prompt": "¿Cuál es la capital de España?",
      "language": "es",
      "description": "¿Cuál es la capital de España?",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "es/es-004-sampling-openai-community-gpt2.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "es-005-greedy-openai-community-gpt2",
      "type": "inference",
      "prompt": "Traduce al inglés: No hay mal que por bien no venga",
      "language": "es",
      "description": "Traduce al inglés: No hay mal que por bien no venga",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "es/es-005-greedy-openai-community-gpt2.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "es-005-random-openai-community-gpt2",
      "type": "inference",
      "prompt": "Traduce al inglés: No hay mal que por bien no venga",
      "language": "es",
      "description": "Traduce al inglés: No hay mal que por bien no venga",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "es/es-005-random-openai-community-gpt2.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "es-005-sampling-openai-community-gpt2",
      "type": "inference",
      "prompt": "Traduce al inglés: No hay mal que por bien no venga",
      "language": "es",
      "description": "Traduce al inglés: No hay mal que por bien no venga",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "es/es-005-sampling-openai-community-gpt2.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "es-006-greedy-openai-community-gpt2",
      "type": "inference",
      "prompt": "¿Cuántas \"r\" hay en la palabra \"ferrocarril\"?",
      "language": "es",
      "description": "¿Cuántas \"r\" hay en la palabra \"ferrocarril\"?",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "es/es-006-greedy-openai-community-gpt2.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "es-006-random-openai-community-gpt2",
      "type": "inference",
      "prompt": "¿Cuántas \"r\" hay en la palabra \"ferrocarril\"?",
      "language": "es",
      "description": "¿Cuántas \"r\" hay en la palabra \"ferrocarril\"?",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "es/es-006-random-openai-community-gpt2.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "es-006-sampling-openai-community-gpt2",
      "type": "inference",
      "prompt": "¿Cuántas \"r\" hay en la palabra \"ferrocarril\"?",
      "language": "es",
      "description": "¿Cuántas \"r\" hay en la palabra \"ferrocarril\"?",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "es/es-006-sampling-openai-community-gpt2.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "fr-001-greedy-openai-community-gpt2",
      "type": "inference",
      "prompt": "Bonjour le monde!",
      "language": "fr",
      "description": "Bonjour le monde!",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "fr/fr-001-greedy-openai-community-gpt2.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "fr-001-random-openai-community-gpt2",
      "type": "inference",
      "prompt": "Bonjour le monde!",
      "language": "fr",
      "description": "Bonjour le monde!",
      "num_tokens": 92,
      "model_id": "openai-community/gpt2",
      "file": "fr/fr-001-random-openai-community-gpt2.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "fr-001-sampling-openai-community-gpt2",
      "type": "inference",
      "prompt": "Bonjour le monde!",
      "language": "fr",
      "description": "Bonjour le monde!",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "fr/fr-001-sampling-openai-community-gpt2.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "fr-002-greedy-openai-community-gpt2",
      "type": "inference",
      "prompt": "Qui es-tu?",
      "language": "fr",
      "description": "Qui es-tu?",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "fr/fr-002-greedy-openai-community-gpt2.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "fr-002-random-openai-community-gpt2",
      "type": "inference",
      "prompt": "Qui es-tu?",
      "language": "fr",
      "description": "Qui es-tu?",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "fr/fr-002-random-openai-community-gpt2.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "fr-002-sampling-openai-community-gpt2",
      "type": "inference",
      "prompt": "Qui es-tu?",
      "language": "fr",
      "description": "Qui es-tu?",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "fr/fr-002-sampling-openai-community-gpt2.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "fr-003-greedy-openai-community-gpt2",
      "type": "inference",
      "prompt": "Raconte-moi une blague",
      "language": "fr",
      "description": "Raconte-moi une blague",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "fr/fr-003-greedy-openai-community-gpt2.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "fr-003-random-openai-community-gpt2",
      "type": "inference",
      "prompt": "Raconte-moi une blague",
      "language": "fr",
      "description": "Raconte-moi une blague",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "fr/fr-003-random-openai-community-gpt2.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "fr-003-sampling-openai-community-gpt2",
      "type": "inference",
      "prompt": "Raconte-moi une blague",
      "language": "fr",
      "description": "Raconte-moi une blague",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "fr/fr-003-sampling-openai-community-gpt2.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "fr-004-greedy-openai-community-gpt2",
      "type": "inference",
      "prompt": "Quelle est la capitale de la France?",
      "language": "fr",
      "description": "Quelle est la capitale de la France?",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "fr/fr-004-greedy-openai-community-gpt2.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "fr-004-random-openai-community-gpt2",
      "type": "inference",
      "prompt": "Quelle est la capitale de la France?",
      "language": "fr",
      "description": "Quelle est la capitale de la France?",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "fr/fr-004-random-openai-community-gpt2.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "fr-004-sampling-openai-community-gpt2",
      "type": "inference",
      "prompt": "Quelle est la capitale de la France?",
      "language": "fr",
      "description": "Quelle est la capitale de la France?",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "fr/fr-004-sampling-openai-community-gpt2.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "fr-005-greedy-openai-community-gpt2",
      "type": "inference",
      "prompt": "Traduis en anglais: La mariée était en blanc",
      "language": "fr",
      "description": "Traduis en anglais: La mariée était en blanc",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "fr/fr-005-greedy-openai-community-gpt2.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "fr-005-random-openai-community-gpt2",
      "type": "inference",
      "prompt": "Traduis en anglais: La mariée était en blanc",
      "language": "fr",
      "description": "Traduis en anglais: La mariée était en blanc",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "fr/fr-005-random-openai-community-gpt2.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "fr-005-sampling-openai-community-gpt2",
      "type": "inference",
      "prompt": "Traduis en anglais: La mariée était en blanc",
      "language": "fr",
      "description": "Traduis en anglais: La mariée était en blanc",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "fr/fr-005-sampling-openai-community-gpt2.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "fr-006-greedy-openai-community-gpt2",
      "type": "inference",
      "prompt": "Combien de \"e\" y a-t-il dans le mot \"merveilleusement\"?",
      "language": "fr",
      "description": "Combien de \"e\" y a-t-il dans le mot \"merveilleusement\"?",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "fr/fr-006-greedy-openai-community-gpt2.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "fr-006-random-openai-community-gpt2",
      "type": "inference",
      "prompt": "Combien de \"e\" y a-t-il dans le mot \"merveilleusement\"?",
      "language": "fr",
      "description": "Combien de \"e\" y a-t-il dans le mot \"merveilleusement\"?",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "fr/fr-006-random-openai-community-gpt2.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "fr-006-sampling-openai-community-gpt2",
      "type": "inference",
      "prompt": "Combien de \"e\" y a-t-il dans le mot \"merveilleusement\"?",
      "language": "fr",
      "description": "Combien de \"e\" y a-t-il dans le mot \"merveilleusement\"?",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "fr/fr-006-sampling-openai-community-gpt2.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "hi-001-greedy-openai-community-gpt2",
      "type": "inference",
      "prompt": "नमस्ते दुनिया!",
      "language": "hi",
      "description": "नमस्ते दुनिया!",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "hi/hi-001-greedy-openai-community-gpt2.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "hi-001-random-openai-community-gpt2",
      "type": "inference",
      "prompt": "नमस्ते दुनिया!",
      "language": "hi",
      "description": "नमस्ते दुनिया!",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "hi/hi-001-random-openai-community-gpt2.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "hi-001-sampling-openai-community-gpt2",
      "type": "inference",
      "prompt": "नमस्ते दुनिया!",
      "language": "hi",
      "description": "नमस्ते दुनिया!",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "hi/hi-001-sampling-openai-community-gpt2.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "hi-002-greedy-openai-community-gpt2",
      "type": "inference",
      "prompt": "तुम कौन हो?",
      "language": "hi",
      "description": "तुम कौन हो?",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "hi/hi-002-greedy-openai-community-gpt2.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "hi-002-random-openai-community-gpt2",
      "type": "inference",
      "prompt": "तुम कौन हो?",
      "language": "hi",
      "description": "तुम कौन हो?",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "hi/hi-002-random-openai-community-gpt2.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "hi-002-sampling-openai-community-gpt2",
      "type": "inference",
      "prompt": "तुम कौन हो?",
      "language": "hi",
      "description": "तुम कौन हो?",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "hi/hi-002-sampling-openai-community-gpt2.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "hi-003-greedy-openai-community-gpt2",
      "type": "inference",
      "prompt": "मुझे एक चुटकुला सुनाओ",
      "language": "hi",
      "description": "मुझे एक चुटकुला सुनाओ",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "hi/hi-003-greedy-openai-community-gpt2.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "hi-003-random-openai-community-gpt2",
      "type": "inference",
      "prompt": "मुझे एक चुटकुला सुनाओ",
      "language": "hi",
      "description": "मुझे एक चुटकुला सुनाओ",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "hi/hi-003-random-openai-community-gpt2.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "hi-003-sampling-openai-community-gpt2",
      "type": "inference",
      "prompt": "मुझे एक चुटकुला सुनाओ",
      "language": "hi",
      "description": "मुझे एक चुटकुला सुनाओ",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "hi/hi-003-sampling-openai-community-gpt2.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "hi-004-greedy-openai-community-gpt2",
      "type": "inference",
      "prompt": "भारत की राजधानी क्या है?",
      "language": "hi",
      "description": "भारत की राजधानी क्या है?",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "hi/hi-004-greedy-openai-community-gpt2.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "hi-004-random-openai-community-gpt2",
      "type": "inference",
      "prompt": "भारत की राजधानी क्या है?",
      "language": "hi",
      "description": "भारत की राजधानी क्या है?",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "hi/hi-004-random-openai-community-gpt2.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "hi-004-sampling-openai-community-gpt2",
      "type": "inference",
      "prompt": "भारत की राजधानी क्या है?",
      "language": "hi",
      "description": "भारत की राजधानी क्या है?",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "hi/hi-004-sampling-openai-community-gpt2.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "hi-005-greedy-openai-community-gpt2",
      "type": "inference",
      "prompt": "अंग्रेजी में अनुवाद करें: अंधों में काना राजा",
      "language": "hi",
      "description": "अंग्रेजी में अनुवाद करें: अंधों में काना राजा",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "hi/hi-005-greedy-openai-community-gpt2.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "hi-005-random-openai-community-gpt2",
      "type": "inference",
      "prompt": "अंग्रेजी में अनुवाद करें: अंधों में काना राजा",
      "language": "hi",
      "description": "अंग्रेजी में अनुवाद करें: अंधों में काना राजा",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "hi/hi-005-random-openai-community-gpt2.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "hi-005-sampling-openai-community-gpt2",
      "type": "inference",
      "prompt": "अंग्रेजी में अनुवाद करें: अंधों में काना राजा",
      "language": "hi",
      "description": "अंग्रेजी में अनुवाद करें: अंधों में काना राजा",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "hi/hi-005-sampling-openai-community-gpt2.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "hi-006-greedy-openai-community-gpt2",
      "type": "inference",
      "prompt": "\"अच्छा\" शब्द में कितने \"च\" हैं?",
      "language": "hi",
      "description": "\"अच्छा\" शब्द में कितने \"च\" हैं?",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "hi/hi-006-greedy-openai-community-gpt2.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "hi-006-random-openai-community-gpt2",
      "type": "inference",
      "prompt": "\"अच्छा\" शब्द में कितने \"च\" हैं?",
      "language": "hi",
      "description": "\"अच्छा\" शब्द में कितने \"च\" हैं?",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "hi/hi-006-random-openai-community-gpt2.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "hi-006-sampling-openai-community-gpt2",
      "type": "inference",
      "prompt": "\"अच्छा\" शब्द में कितने \"च\" हैं?",
      "language": "hi",
      "description": "\"अच्छा\" शब्द में कितने \"च\" हैं?",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "hi/hi-006-sampling-openai-community-gpt2.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "ja-001-greedy-openai-community-gpt2",
      "type": "inference",
      "prompt": "こんにちは世界！",
      "language": "ja",
      "description": "こんにちは世界！",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "ja/ja-001-greedy-openai-community-gpt2.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "ja-001-random-openai-community-gpt2",
      "type": "inference",
      "prompt": "こんにちは世界！",
      "language": "ja",
      "description": "こんにちは世界！",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "ja/ja-001-random-openai-community-gpt2.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "ja-001-sampling-openai-community-gpt2",
      "type": "inference",
      "prompt": "こんにちは世界！",
      "language": "ja",
      "description": "こんにちは世界！",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "ja/ja-001-sampling-openai-community-gpt2.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "ja-002-greedy-openai-community-gpt2",
      "type": "inference",
      "prompt": "あなたは誰ですか？",
      "language": "ja",
      "description": "あなたは誰ですか？",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "ja/ja-002-greedy-openai-community-gpt2.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "ja-002-random-openai-community-gpt2",
      "type": "inference",
      "prompt": "あなたは誰ですか？",
      "language": "ja",
      "description": "あなたは誰ですか？",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "ja/ja-002-random-openai-community-gpt2.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "ja-002-sampling-openai-community-gpt2",
      "type": "inference",
      "prompt": "あなたは誰ですか？",
      "language": "ja",
      "description": "あなたは誰ですか？",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "ja/ja-002-sampling-openai-community-gpt2.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "ja-003-greedy-openai-community-gpt2",
      "type": "inference",
      "prompt": "冗談を言ってください",
      "language": "ja",
      "description": "冗談を言ってください",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "ja/ja-003-greedy-openai-community-gpt2.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "ja-003-random-openai-community-gpt2",
      "type": "inference",
      "prompt": "冗談を言ってください",
      "language": "ja",
      "description": "冗談を言ってください",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "ja/ja-003-random-openai-community-gpt2.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "ja-003-sampling-openai-community-gpt2",
      "type": "inference",
      "prompt": "冗談を言ってください",
      "language": "ja",
      "description": "冗談を言ってください",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "ja/ja-003-sampling-openai-community-gpt2.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "ja-004-greedy-openai-community-gpt2",
      "type": "inference",
      "prompt": "日本の首都はどこですか？",
      "language": "ja",
      "description": "日本の首都はどこですか？",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "ja/ja-004-greedy-openai-community-gpt2.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "ja-004-random-openai-community-gpt2",
      "type": "inference",
      "prompt": "日本の首都はどこですか？",
      "language": "ja",
      "description": "日本の首都はどこですか？",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "ja/ja-004-random-openai-community-gpt2.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "ja-004-sampling-openai-community-gpt2",
      "type": "inference",
      "prompt": "日本の首都はどこですか？",
      "language": "ja",
      "description": "日本の首都はどこですか？",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "ja/ja-004-sampling-openai-community-gpt2.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "ja-005-greedy-openai-community-gpt2",
      "type": "inference",
      "prompt": "英語に翻訳してください：急がば回れ",
      "language": "ja",
      "description": "英語に翻訳してください：急がば回れ",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "ja/ja-005-greedy-openai-community-gpt2.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "ja-005-random-openai-community-gpt2",
      "type": "inference",
      "prompt": "英語に翻訳してください：急がば回れ",
      "language": "ja",
      "description": "英語に翻訳してください：急がば回れ",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "ja/ja-005-random-openai-community-gpt2.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "ja-005-sampling-openai-community-gpt2",
      "type": "inference",
      "prompt": "英語に翻訳してください：急がば回れ",
      "language": "ja",
      "description": "英語に翻訳してください：急がば回れ",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "ja/ja-005-sampling-openai-community-gpt2.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "ja-006-greedy-openai-community-gpt2",
      "type": "inference",
      "prompt": "「ありがとうございます」という言葉には「あ」がいくつありますか？",
      "language": "ja",
      "description": "「ありがとうございます」という言葉には「あ」がいくつありますか？",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "ja/ja-006-greedy-openai-community-gpt2.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "ja-006-random-openai-community-gpt2",
      "type": "inference",
      "prompt": "「ありがとうございます」という言葉には「あ」がいくつありますか？",
      "language": "ja",
      "description": "「ありがとうございます」という言葉には「あ」がいくつありますか？",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "ja/ja-006-random-openai-community-gpt2.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "ja-006-sampling-openai-community-gpt2",
      "type": "inference",
      "prompt": "「ありがとうございます」という言葉には「あ」がいくつありますか？",
      "language": "ja",
      "description": "「ありがとうございます」という言葉には「あ」がいくつありますか？",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "ja/ja-006-sampling-openai-community-gpt2.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "zh-001-greedy-openai-community-gpt2",
      "type": "inference",
      "prompt": "你好世界！",
      "language": "zh",
      "description": "你好世界！",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "zh/zh-001-greedy-openai-community-gpt2.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "zh-001-random-openai-community-gpt2",
      "type": "inference",
      "prompt": "你好世界！",
      "language": "zh",
      "description": "你好世界！",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "zh/zh-001-random-openai-community-gpt2.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "zh-001-sampling-openai-community-gpt2",
      "type": "inference",
      "prompt": "你好世界！",
      "language": "zh",
      "description": "你好世界！",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "zh/zh-001-sampling-openai-community-gpt2.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "zh-002-greedy-openai-community-gpt2",
      "type": "inference",
      "prompt": "你是谁？",
      "language": "zh",
      "description": "你是谁？",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "zh/zh-002-greedy-openai-community-gpt2.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "zh-002-random-openai-community-gpt2",
      "type": "inference",
      "prompt": "你是谁？",
      "language": "zh",
      "description": "你是谁？",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "zh/zh-002-random-openai-community-gpt2.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "zh-002-sampling-openai-community-gpt2",
      "type": "inference",
      "prompt": "你是谁？",
      "language": "zh",
      "description": "你是谁？",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "zh/zh-002-sampling-openai-community-gpt2.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "zh-003-greedy-openai-community-gpt2",
      "type": "inference",
      "prompt": "给我讲个笑话",
      "language": "zh",
      "description": "给我讲个笑话",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "zh/zh-003-greedy-openai-community-gpt2.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "zh-003-random-openai-community-gpt2",
      "type": "inference",
      "prompt": "给我讲个笑话",
      "language": "zh",
      "description": "给我讲个笑话",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "zh/zh-003-random-openai-community-gpt2.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "zh-003-sampling-openai-community-gpt2",
      "type": "inference",
      "prompt": "给我讲个笑话",
      "language": "zh",
      "description": "给我讲个笑话",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "zh/zh-003-sampling-openai-community-gpt2.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "zh-004-greedy-openai-community-gpt2",
      "type": "inference",
      "prompt": "中国的首都是哪里？",
      "language": "zh",
      "description": "中国的首都是哪里？",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "zh/zh-004-greedy-openai-community-gpt2.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "zh-004-random-openai-community-gpt2",
      "type": "inference",
      "prompt": "中国的首都是哪里？",
      "language": "zh",
      "description": "中国的首都是哪里？",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "zh/zh-004-random-openai-community-gpt2.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "zh-004-sampling-openai-community-gpt2",
      "type": "inference",
      "prompt": "中国的首都是哪里？",
      "language": "zh",
      "description": "中国的首都是哪里？",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "zh/zh-004-sampling-openai-community-gpt2.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "zh-005-greedy-openai-community-gpt2",
      "type": "inference",
      "prompt": "翻译成英文：早起的鸟儿有虫吃",
      "language": "zh",
      "description": "翻译成英文：早起的鸟儿有虫吃",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "zh/zh-005-greedy-openai-community-gpt2.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "zh-005-random-openai-community-gpt2",
      "type": "inference",
      "prompt": "翻译成英文：早起的鸟儿有虫吃",
      "language": "zh",
      "description": "翻译成英文：早起的鸟儿有虫吃",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "zh/zh-005-random-openai-community-gpt2.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "zh-005-sampling-openai-community-gpt2",
      "type": "inference",
      "prompt": "翻译成英文：早起的鸟儿有虫吃",
      "language": "zh",
      "description": "翻译成英文：早起的鸟儿有虫吃",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "zh/zh-005-sampling-openai-community-gpt2.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "zh-006-greedy-openai-community-gpt2",
      "type": "inference",
      "prompt": "\"美丽\"这个词里有几个\"丽\"？",
      "language": "zh",
      "description": "\"美丽\"这个词里有几个\"丽\"？",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "zh/zh-006-greedy-openai-community-gpt2.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "zh-006-random-openai-community-gpt2",
      "type": "inference",
      "prompt": "\"美丽\"这个词里有几个\"丽\"？",
      "language": "zh",
      "description": "\"美丽\"这个词里有几个\"丽\"？",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "zh/zh-006-random-openai-community-gpt2.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "zh-006-sampling-openai-community-gpt2",
      "type": "inference",
      "prompt": "\"美丽\"这个词里有几个\"丽\"？",
      "language": "zh",
      "description": "\"美丽\"这个词里有几个\"丽\"？",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2",
      "file": "zh/zh-006-sampling-openai-community-gpt2.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "ar-001-greedy-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "مرحبا بالعالم!",
      "language": "ar",
      "description": "مرحبا بالعالم!",
      "num_tokens": 13,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "ar/ar-001-greedy-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "ar-001-random-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "مرحبا بالعالم!",
      "language": "ar",
      "description": "مرحبا بالعالم!",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "ar/ar-001-random-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "ar-001-sampling-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "مرحبا بالعالم!",
      "language": "ar",
      "description": "مرحبا بالعالم!",
      "num_tokens": 14,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "ar/ar-001-sampling-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "ar-002-greedy-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "من أنت؟",
      "language": "ar",
      "description": "من أنت؟",
      "num_tokens": 68,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "ar/ar-002-greedy-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "ar-002-random-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "من أنت؟",
      "language": "ar",
      "description": "من أنت؟",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "ar/ar-002-random-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "ar-002-sampling-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "من أنت؟",
      "language": "ar",
      "description": "من أنت؟",
      "num_tokens": 30,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "ar/ar-002-sampling-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "ar-003-greedy-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "أخبرني نكتة",
      "language": "ar",
      "description": "أخبرني نكتة",
      "num_tokens": 29,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "ar/ar-003-greedy-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "ar-003-random-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "أخبرني نكتة",
      "language": "ar",
      "description": "أخبرني نكتة",
      "num_tokens": 61,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "ar/ar-003-random-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "ar-003-sampling-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "أخبرني نكتة",
      "language": "ar",
      "description": "أخبرني نكتة",
      "num_tokens": 47,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "ar/ar-003-sampling-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "ar-004-greedy-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "ما هي عاصمة مصر؟",
      "language": "ar",
      "description": "ما هي عاصمة مصر؟",
      "num_tokens": 9,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "ar/ar-004-greedy-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "ar-004-random-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "ما هي عاصمة مصر؟",
      "language": "ar",
      "description": "ما هي عاصمة مصر؟",
      "num_tokens": 89,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "ar/ar-004-random-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "ar-004-sampling-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "ما هي عاصمة مصر؟",
      "language": "ar",
      "description": "ما هي عاصمة مصر؟",
      "num_tokens": 9,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "ar/ar-004-sampling-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "ar-005-greedy-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "ترجم إلى الإنجليزية: العلم نور والجهل ظلام",
      "language": "ar",
      "description": "ترجم إلى الإنجليزية: العلم نور والجهل ظلام",
      "num_tokens": 8,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "ar/ar-005-greedy-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "ar-005-random-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "ترجم إلى الإنجليزية: العلم نور والجهل ظلام",
      "language": "ar",
      "description": "ترجم إلى الإنجليزية: العلم نور والجهل ظلام",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "ar/ar-005-random-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "ar-005-sampling-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "ترجم إلى الإنجليزية: العلم نور والجهل ظلام",
      "language": "ar",
      "description": "ترجم إلى الإنجليزية: العلم نور والجهل ظلام",
      "num_tokens": 9,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "ar/ar-005-sampling-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "ar-006-greedy-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "كم عدد حرف \"م\" في كلمة \"متحمسون\"؟",
      "language": "ar",
      "description": "كم عدد حرف \"م\" في كلمة \"متحمسون\"؟",
      "num_tokens": 19,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "ar/ar-006-greedy-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "ar-006-random-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "كم عدد حرف \"م\" في كلمة \"متحمسون\"؟",
      "language": "ar",
      "description": "كم عدد حرف \"م\" في كلمة \"متحمسون\"؟",
      "num_tokens": 30,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "ar/ar-006-random-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "ar-006-sampling-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "كم عدد حرف \"م\" في كلمة \"متحمسون\"؟",
      "language": "ar",
      "description": "كم عدد حرف \"م\" في كلمة \"متحمسون\"؟",
      "num_tokens": 19,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "ar/ar-006-sampling-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "cs-001-greedy-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "Ahoj světe!",
      "language": "cs",
      "description": "Ahoj světe!",
      "num_tokens": 11,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "cs/cs-001-greedy-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "cs-001-random-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "Ahoj světe!",
      "language": "cs",
      "description": "Ahoj světe!",
      "num_tokens": 27,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "cs/cs-001-random-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "cs-001-sampling-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "Ahoj světe!",
      "language": "cs",
      "description": "Ahoj světe!",
      "num_tokens": 18,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "cs/cs-001-sampling-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "cs-002-greedy-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "Kdo jsi?",
      "language": "cs",
      "description": "Kdo jsi?",
      "num_tokens": 28,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "cs/cs-002-greedy-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "cs-002-random-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "Kdo jsi?",
      "language": "cs",
      "description": "Kdo jsi?",
      "num_tokens": 14,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "cs/cs-002-random-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "cs-002-sampling-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "Kdo jsi?",
      "language": "cs",
      "description": "Kdo jsi?",
      "num_tokens": 28,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "cs/cs-002-sampling-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "cs-003-greedy-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "Řekni mi vtip",
      "language": "cs",
      "description": "Řekni mi vtip",
      "num_tokens": 17,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "cs/cs-003-greedy-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "cs-003-random-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "Řekni mi vtip",
      "language": "cs",
      "description": "Řekni mi vtip",
      "num_tokens": 23,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "cs/cs-003-random-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "cs-003-sampling-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "Řekni mi vtip",
      "language": "cs",
      "description": "Řekni mi vtip",
      "num_tokens": 38,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "cs/cs-003-sampling-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "cs-004-greedy-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "Jaké je hlavní město Česka?",
      "language": "cs",
      "description": "Jaké je hlavní město Česka?",
      "num_tokens": 12,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "cs/cs-004-greedy-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "cs-004-random-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "Jaké je hlavní město Česka?",
      "language": "cs",
      "description": "Jaké je hlavní město Česka?",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "cs/cs-004-random-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "cs-004-sampling-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "Jaké je hlavní město Česka?",
      "language": "cs",
      "description": "Jaké je hlavní město Česka?",
      "num_tokens": 12,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "cs/cs-004-sampling-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "cs-005-greedy-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "Přelož do angličtiny: Žluťoučký kůň úpěl ďábelské ódy",
      "language": "cs",
      "description": "Přelož do angličtiny: Žluťoučký kůň úpěl ďábelské ódy",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "cs/cs-005-greedy-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "cs-005-random-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "Přelož do angličtiny: Žluťoučký kůň úpěl ďábelské ódy",
      "language": "cs",
      "description": "Přelož do angličtiny: Žluťoučký kůň úpěl ďábelské ódy",
      "num_tokens": 63,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "cs/cs-005-random-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "cs-005-sampling-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "Přelož do angličtiny: Žluťoučký kůň úpěl ďábelské ódy",
      "language": "cs",
      "description": "Přelož do angličtiny: Žluťoučký kůň úpěl ďábelské ódy",
      "num_tokens": 9,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "cs/cs-005-sampling-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "cs-006-greedy-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "Kolik \"k\" je ve slově \"nejkulinkaťoulinkatější\"?",
      "language": "cs",
      "description": "Kolik \"k\" je ve slově \"nejkulinkaťoulinkatější\"?",
      "num_tokens": 24,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "cs/cs-006-greedy-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "cs-006-random-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "Kolik \"k\" je ve slově \"nejkulinkaťoulinkatější\"?",
      "language": "cs",
      "description": "Kolik \"k\" je ve slově \"nejkulinkaťoulinkatější\"?",
      "num_tokens": 20,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "cs/cs-006-random-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "cs-006-sampling-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "Kolik \"k\" je ve slově \"nejkulinkaťoulinkatější\"?",
      "language": "cs",
      "description": "Kolik \"k\" je ve slově \"nejkulinkaťoulinkatější\"?",
      "num_tokens": 24,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "cs/cs-006-sampling-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "de-001-greedy-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "Hallo Welt!",
      "language": "de",
      "description": "Hallo Welt!",
      "num_tokens": 22,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "de/de-001-greedy-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "de-001-random-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "Hallo Welt!",
      "language": "de",
      "description": "Hallo Welt!",
      "num_tokens": 76,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "de/de-001-random-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "de-001-sampling-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "Hallo Welt!",
      "language": "de",
      "description": "Hallo Welt!",
      "num_tokens": 31,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "de/de-001-sampling-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "de-002-greedy-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "Wer bist du?",
      "language": "de",
      "description": "Wer bist du?",
      "num_tokens": 29,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "de/de-002-greedy-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "de-002-random-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "Wer bist du?",
      "language": "de",
      "description": "Wer bist du?",
      "num_tokens": 35,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "de/de-002-random-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "de-002-sampling-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "Wer bist du?",
      "language": "de",
      "description": "Wer bist du?",
      "num_tokens": 29,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "de/de-002-sampling-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "de-003-greedy-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "Erzähl mir einen Witz",
      "language": "de",
      "description": "Erzähl mir einen Witz",
      "num_tokens": 17,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "de/de-003-greedy-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "de-003-random-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "Erzähl mir einen Witz",
      "language": "de",
      "description": "Erzähl mir einen Witz",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "de/de-003-random-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "de-003-sampling-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "Erzähl mir einen Witz",
      "language": "de",
      "description": "Erzähl mir einen Witz",
      "num_tokens": 17,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "de/de-003-sampling-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "de-004-greedy-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "Was ist die Hauptstadt von Deutschland?",
      "language": "de",
      "description": "Was ist die Hauptstadt von Deutschland?",
      "num_tokens": 9,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "de/de-004-greedy-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "de-004-random-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "Was ist die Hauptstadt von Deutschland?",
      "language": "de",
      "description": "Was ist die Hauptstadt von Deutschland?",
      "num_tokens": 63,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "de/de-004-random-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "de-004-sampling-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "Was ist die Hauptstadt von Deutschland?",
      "language": "de",
      "description": "Was ist die Hauptstadt von Deutschland?",
      "num_tokens": 9,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "de/de-004-sampling-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "de-005-greedy-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "Übersetze ins Englische: Der frühe Vogel fängt den Wurm",
      "language": "de",
      "description": "Übersetze ins Englische: Der frühe Vogel fängt den Wurm",
      "num_tokens": 81,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "de/de-005-greedy-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "de-005-random-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "Übersetze ins Englische: Der frühe Vogel fängt den Wurm",
      "language": "de",
      "description": "Übersetze ins Englische: Der frühe Vogel fängt den Wurm",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "de/de-005-random-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "de-005-sampling-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "Übersetze ins Englische: Der frühe Vogel fängt den Wurm",
      "language": "de",
      "description": "Übersetze ins Englische: Der frühe Vogel fängt den Wurm",
      "num_tokens": 62,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "de/de-005-sampling-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "de-006-greedy-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?",
      "language": "de",
      "description": "Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?",
      "num_tokens": 24,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "de/de-006-greedy-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "de-006-random-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?",
      "language": "de",
      "description": "Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?",
      "num_tokens": 85,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "de/de-006-random-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "de-006-sampling-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?",
      "language": "de",
      "description": "Wie viele \"e\" gibt es im Wort \"Geschwindigkeitsbegrenzung\"?",
      "num_tokens": 25,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "de/de-006-sampling-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "en-001-greedy-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "Hello world!",
      "language": "en",
      "description": "Hello world!",
      "num_tokens": 25,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "en/en-001-greedy-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "en-001-random-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "Hello world!",
      "language": "en",
      "description": "Hello world!",
      "num_tokens": 92,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "en/en-001-random-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "en-001-sampling-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "Hello world!",
      "language": "en",
      "description": "Hello world!",
      "num_tokens": 25,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "en/en-001-sampling-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "en-002-greedy-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "Who are you?",
      "language": "en",
      "description": "Who are you?",
      "num_tokens": 23,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "en/en-002-greedy-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "en-002-random-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "Who are you?",
      "language": "en",
      "description": "Who are you?",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "en/en-002-random-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "en-002-sampling-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "Who are you?",
      "language": "en",
      "description": "Who are you?",
      "num_tokens": 23,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "en/en-002-sampling-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "en-003-greedy-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "Tell me a joke",
      "language": "en",
      "description": "Tell me a joke",
      "num_tokens": 32,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "en/en-003-greedy-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "en-003-random-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "Tell me a joke",
      "language": "en",
      "description": "Tell me a joke",
      "num_tokens": 68,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "en/en-003-random-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "en-003-sampling-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "Tell me a joke",
      "language": "en",
      "description": "Tell me a joke",
      "num_tokens": 33,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "en/en-003-sampling-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "en-004-greedy-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "What is the capital of France?",
      "language": "en",
      "description": "What is the capital of France?",
      "num_tokens": 8,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "en/en-004-greedy-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "en-004-random-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "What is the capital of France?",
      "language": "en",
      "description": "What is the capital of France?",
      "num_tokens": 9,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "en/en-004-random-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "en-004-sampling-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "What is the capital of France?",
      "language": "en",
      "description": "What is the capital of France?",
      "num_tokens": 8,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "en/en-004-sampling-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "en-005-greedy-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "Translate to French: The quick brown fox jumps over the lazy dog",
      "language": "en",
      "description": "Translate to French: The quick brown fox jumps over the lazy dog",
      "num_tokens": 60,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "en/en-005-greedy-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "en-005-random-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "Translate to French: The quick brown fox jumps over the lazy dog",
      "language": "en",
      "description": "Translate to French: The quick brown fox jumps over the lazy dog",
      "num_tokens": 93,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "en/en-005-random-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "en-005-sampling-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "Translate to French: The quick brown fox jumps over the lazy dog",
      "language": "en",
      "description": "Translate to French: The quick brown fox jumps over the lazy dog",
      "num_tokens": 57,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "en/en-005-sampling-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "en-006-greedy-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "How many r's are in \"strawberry\"?",
      "language": "en",
      "description": "How many r's are in \"strawberry\"?",
      "num_tokens": 59,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "en/en-006-greedy-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "en-006-random-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "How many r's are in \"strawberry\"?",
      "language": "en",
      "description": "How many r's are in \"strawberry\"?",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "en/en-006-random-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "en-006-sampling-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "How many r's are in \"strawberry\"?",
      "language": "en",
      "description": "How many r's are in \"strawberry\"?",
      "num_tokens": 31,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "en/en-006-sampling-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "es-001-greedy-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "¡Hola mundo!",
      "language": "es",
      "description": "¡Hola mundo!",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "es/es-001-greedy-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "es-001-random-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "¡Hola mundo!",
      "language": "es",
      "description": "¡Hola mundo!",
      "num_tokens": 51,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "es/es-001-random-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "es-001-sampling-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "¡Hola mundo!",
      "language": "es",
      "description": "¡Hola mundo!",
      "num_tokens": 93,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "es/es-001-sampling-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "es-002-greedy-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "¿Quién eres?",
      "language": "es",
      "description": "¿Quién eres?",
      "num_tokens": 29,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "es/es-002-greedy-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "es-002-random-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "¿Quién eres?",
      "language": "es",
      "description": "¿Quién eres?",
      "num_tokens": 48,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "es/es-002-random-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "es-002-sampling-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "¿Quién eres?",
      "language": "es",
      "description": "¿Quién eres?",
      "num_tokens": 29,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "es/es-002-sampling-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "es-003-greedy-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "Cuéntame un chiste",
      "language": "es",
      "description": "Cuéntame un chiste",
      "num_tokens": 17,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "es/es-003-greedy-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "es-003-random-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "Cuéntame un chiste",
      "language": "es",
      "description": "Cuéntame un chiste",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "es/es-003-random-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "es-003-sampling-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "Cuéntame un chiste",
      "language": "es",
      "description": "Cuéntame un chiste",
      "num_tokens": 17,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "es/es-003-sampling-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "es-004-greedy-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "¿Cuál es la capital de España?",
      "language": "es",
      "description": "¿Cuál es la capital de España?",
      "num_tokens": 8,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "es/es-004-greedy-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "es-004-random-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "¿Cuál es la capital de España?",
      "language": "es",
      "description": "¿Cuál es la capital de España?",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "es/es-004-random-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "es-004-sampling-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "¿Cuál es la capital de España?",
      "language": "es",
      "description": "¿Cuál es la capital de España?",
      "num_tokens": 8,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "es/es-004-sampling-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "es-005-greedy-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "Traduce al inglés: No hay mal que por bien no venga",
      "language": "es",
      "description": "Traduce al inglés: No hay mal que por bien no venga",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "es/es-005-greedy-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "es-005-random-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "Traduce al inglés: No hay mal que por bien no venga",
      "language": "es",
      "description": "Traduce al inglés: No hay mal que por bien no venga",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "es/es-005-random-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "es-005-sampling-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "Traduce al inglés: No hay mal que por bien no venga",
      "language": "es",
      "description": "Traduce al inglés: No hay mal que por bien no venga",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "es/es-005-sampling-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "es-006-greedy-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "¿Cuántas \"r\" hay en la palabra \"ferrocarril\"?",
      "language": "es",
      "description": "¿Cuántas \"r\" hay en la palabra \"ferrocarril\"?",
      "num_tokens": 15,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "es/es-006-greedy-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "es-006-random-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "¿Cuántas \"r\" hay en la palabra \"ferrocarril\"?",
      "language": "es",
      "description": "¿Cuántas \"r\" hay en la palabra \"ferrocarril\"?",
      "num_tokens": 5,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "es/es-006-random-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "es-006-sampling-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "¿Cuántas \"r\" hay en la palabra \"ferrocarril\"?",
      "language": "es",
      "description": "¿Cuántas \"r\" hay en la palabra \"ferrocarril\"?",
      "num_tokens": 16,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "es/es-006-sampling-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "fr-001-greedy-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "Bonjour le monde!",
      "language": "fr",
      "description": "Bonjour le monde!",
      "num_tokens": 34,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "fr/fr-001-greedy-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "fr-001-random-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "Bonjour le monde!",
      "language": "fr",
      "description": "Bonjour le monde!",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "fr/fr-001-random-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "fr-001-sampling-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "Bonjour le monde!",
      "language": "fr",
      "description": "Bonjour le monde!",
      "num_tokens": 35,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "fr/fr-001-sampling-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "fr-002-greedy-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "Qui es-tu?",
      "language": "fr",
      "description": "Qui es-tu?",
      "num_tokens": 38,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "fr/fr-002-greedy-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "fr-002-random-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "Qui es-tu?",
      "language": "fr",
      "description": "Qui es-tu?",
      "num_tokens": 75,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "fr/fr-002-random-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "fr-002-sampling-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "Qui es-tu?",
      "language": "fr",
      "description": "Qui es-tu?",
      "num_tokens": 38,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "fr/fr-002-sampling-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "fr-003-greedy-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "Raconte-moi une blague",
      "language": "fr",
      "description": "Raconte-moi une blague",
      "num_tokens": 38,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "fr/fr-003-greedy-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "fr-003-random-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "Raconte-moi une blague",
      "language": "fr",
      "description": "Raconte-moi une blague",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "fr/fr-003-random-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "fr-003-sampling-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "Raconte-moi une blague",
      "language": "fr",
      "description": "Raconte-moi une blague",
      "num_tokens": 49,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "fr/fr-003-sampling-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "fr-004-greedy-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "Quelle est la capitale de la France?",
      "language": "fr",
      "description": "Quelle est la capitale de la France?",
      "num_tokens": 10,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "fr/fr-004-greedy-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "fr-004-random-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "Quelle est la capitale de la France?",
      "language": "fr",
      "description": "Quelle est la capitale de la France?",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "fr/fr-004-random-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "fr-004-sampling-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "Quelle est la capitale de la France?",
      "language": "fr",
      "description": "Quelle est la capitale de la France?",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "fr/fr-004-sampling-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "fr-005-greedy-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "Traduis en anglais: La mariée était en blanc",
      "language": "fr",
      "description": "Traduis en anglais: La mariée était en blanc",
      "num_tokens": 24,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "fr/fr-005-greedy-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "fr-005-random-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "Traduis en anglais: La mariée était en blanc",
      "language": "fr",
      "description": "Traduis en anglais: La mariée était en blanc",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "fr/fr-005-random-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "fr-005-sampling-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "Traduis en anglais: La mariée était en blanc",
      "language": "fr",
      "description": "Traduis en anglais: La mariée était en blanc",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "fr/fr-005-sampling-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "fr-006-greedy-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "Combien de \"e\" y a-t-il dans le mot \"merveilleusement\"?",
      "language": "fr",
      "description": "Combien de \"e\" y a-t-il dans le mot \"merveilleusement\"?",
      "num_tokens": 18,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "fr/fr-006-greedy-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "fr-006-random-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "Combien de \"e\" y a-t-il dans le mot \"merveilleusement\"?",
      "language": "fr",
      "description": "Combien de \"e\" y a-t-il dans le mot \"merveilleusement\"?",
      "num_tokens": 50,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "fr/fr-006-random-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "fr-006-sampling-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "Combien de \"e\" y a-t-il dans le mot \"merveilleusement\"?",
      "language": "fr",
      "description": "Combien de \"e\" y a-t-il dans le mot \"merveilleusement\"?",
      "num_tokens": 29,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "fr/fr-006-sampling-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "hi-001-greedy-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "नमस्ते दुनिया!",
      "language": "hi",
      "description": "नमस्ते दुनिया!",
      "num_tokens": 29,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "hi/hi-001-greedy-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "hi-001-random-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "नमस्ते दुनिया!",
      "language": "hi",
      "description": "नमस्ते दुनिया!",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "hi/hi-001-random-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "hi-001-sampling-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "नमस्ते दुनिया!",
      "language": "hi",
      "description": "नमस्ते दुनिया!",
      "num_tokens": 29,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "hi/hi-001-sampling-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "hi-002-greedy-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "तुम कौन हो?",
      "language": "hi",
      "description": "तुम कौन हो?",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "hi/hi-002-greedy-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "hi-002-random-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "तुम कौन हो?",
      "language": "hi",
      "description": "तुम कौन हो?",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "hi/hi-002-random-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "hi-002-sampling-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "तुम कौन हो?",
      "language": "hi",
      "description": "तुम कौन हो?",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "hi/hi-002-sampling-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "hi-003-greedy-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "मुझे एक चुटकुला सुनाओ",
      "language": "hi",
      "description": "मुझे एक चुटकुला सुनाओ",
      "num_tokens": 78,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "hi/hi-003-greedy-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "hi-003-random-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "मुझे एक चुटकुला सुनाओ",
      "language": "hi",
      "description": "मुझे एक चुटकुला सुनाओ",
      "num_tokens": 88,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "hi/hi-003-random-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "hi-003-sampling-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "मुझे एक चुटकुला सुनाओ",
      "language": "hi",
      "description": "मुझे एक चुटकुला सुनाओ",
      "num_tokens": 82,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "hi/hi-003-sampling-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "hi-004-greedy-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "भारत की राजधानी क्या है?",
      "language": "hi",
      "description": "भारत की राजधानी क्या है?",
      "num_tokens": 17,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "hi/hi-004-greedy-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "hi-004-random-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "भारत की राजधानी क्या है?",
      "language": "hi",
      "description": "भारत की राजधानी क्या है?",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "hi/hi-004-random-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "hi-004-sampling-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "भारत की राजधानी क्या है?",
      "language": "hi",
      "description": "भारत की राजधानी क्या है?",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "hi/hi-004-sampling-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "hi-005-greedy-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "अंग्रेजी में अनुवाद करें: अंधों में काना राजा",
      "language": "hi",
      "description": "अंग्रेजी में अनुवाद करें: अंधों में काना राजा",
      "num_tokens": 15,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "hi/hi-005-greedy-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "hi-005-random-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "अंग्रेजी में अनुवाद करें: अंधों में काना राजा",
      "language": "hi",
      "description": "अंग्रेजी में अनुवाद करें: अंधों में काना राजा",
      "num_tokens": 29,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "hi/hi-005-random-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "hi-005-sampling-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "अंग्रेजी में अनुवाद करें: अंधों में काना राजा",
      "language": "hi",
      "description": "अंग्रेजी में अनुवाद करें: अंधों में काना राजा",
      "num_tokens": 14,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "hi/hi-005-sampling-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "hi-006-greedy-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "\"अच्छा\" शब्द में कितने \"च\" हैं?",
      "language": "hi",
      "description": "\"अच्छा\" शब्द में कितने \"च\" हैं?",
      "num_tokens": 18,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "hi/hi-006-greedy-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "hi-006-random-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "\"अच्छा\" शब्द में कितने \"च\" हैं?",
      "language": "hi",
      "description": "\"अच्छा\" शब्द में कितने \"च\" हैं?",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "hi/hi-006-random-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "hi-006-sampling-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "\"अच्छा\" शब्द में कितने \"च\" हैं?",
      "language": "hi",
      "description": "\"अच्छा\" शब्द में कितने \"च\" हैं?",
      "num_tokens": 80,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "hi/hi-006-sampling-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "ja-001-greedy-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "こんにちは世界！",
      "language": "ja",
      "description": "こんにちは世界！",
      "num_tokens": 11,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "ja/ja-001-greedy-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "ja-001-random-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "こんにちは世界！",
      "language": "ja",
      "description": "こんにちは世界！",
      "num_tokens": 77,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "ja/ja-001-random-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "ja-001-sampling-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "こんにちは世界！",
      "language": "ja",
      "description": "こんにちは世界！",
      "num_tokens": 7,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "ja/ja-001-sampling-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "ja-002-greedy-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "あなたは誰ですか？",
      "language": "ja",
      "description": "あなたは誰ですか？",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "ja/ja-002-greedy-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "ja-002-random-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "あなたは誰ですか？",
      "language": "ja",
      "description": "あなたは誰ですか？",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "ja/ja-002-random-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "ja-002-sampling-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "あなたは誰ですか？",
      "language": "ja",
      "description": "あなたは誰ですか？",
      "num_tokens": 44,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "ja/ja-002-sampling-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "ja-003-greedy-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "冗談を言ってください",
      "language": "ja",
      "description": "冗談を言ってください",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "ja/ja-003-greedy-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "ja-003-random-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "冗談を言ってください",
      "language": "ja",
      "description": "冗談を言ってください",
      "num_tokens": 60,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "ja/ja-003-random-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "ja-003-sampling-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "冗談を言ってください",
      "language": "ja",
      "description": "冗談を言ってください",
      "num_tokens": 35,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "ja/ja-003-sampling-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "ja-004-greedy-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "日本の首都はどこですか？",
      "language": "ja",
      "description": "日本の首都はどこですか？",
      "num_tokens": 9,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "ja/ja-004-greedy-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "ja-004-random-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "日本の首都はどこですか？",
      "language": "ja",
      "description": "日本の首都はどこですか？",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "ja/ja-004-random-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "ja-004-sampling-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "日本の首都はどこですか？",
      "language": "ja",
      "description": "日本の首都はどこですか？",
      "num_tokens": 8,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "ja/ja-004-sampling-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "ja-005-greedy-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "英語に翻訳してください：急がば回れ",
      "language": "ja",
      "description": "英語に翻訳してください：急がば回れ",
      "num_tokens": 77,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "ja/ja-005-greedy-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "ja-005-random-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "英語に翻訳してください：急がば回れ",
      "language": "ja",
      "description": "英語に翻訳してください：急がば回れ",
      "num_tokens": 42,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "ja/ja-005-random-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "ja-005-sampling-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "英語に翻訳してください：急がば回れ",
      "language": "ja",
      "description": "英語に翻訳してください：急がば回れ",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "ja/ja-005-sampling-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "ja-006-greedy-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "「ありがとうございます」という言葉には「あ」がいくつありますか？",
      "language": "ja",
      "description": "「ありがとうございます」という言葉には「あ」がいくつありますか？",
      "num_tokens": 9,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "ja/ja-006-greedy-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "ja-006-random-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "「ありがとうございます」という言葉には「あ」がいくつありますか？",
      "language": "ja",
      "description": "「ありがとうございます」という言葉には「あ」がいくつありますか？",
      "num_tokens": 32,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "ja/ja-006-random-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "ja-006-sampling-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "「ありがとうございます」という言葉には「あ」がいくつありますか？",
      "language": "ja",
      "description": "「ありがとうございます」という言葉には「あ」がいくつありますか？",
      "num_tokens": 9,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "ja/ja-006-sampling-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "zh-001-greedy-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "你好世界！",
      "language": "zh",
      "description": "你好世界！",
      "num_tokens": 25,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "zh/zh-001-greedy-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "zh-001-random-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "你好世界！",
      "language": "zh",
      "description": "你好世界！",
      "num_tokens": 45,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "zh/zh-001-random-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "zh-001-sampling-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "你好世界！",
      "language": "zh",
      "description": "你好世界！",
      "num_tokens": 20,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "zh/zh-001-sampling-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "zh-002-greedy-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "你是谁？",
      "language": "zh",
      "description": "你是谁？",
      "num_tokens": 56,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "zh/zh-002-greedy-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "zh-002-random-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "你是谁？",
      "language": "zh",
      "description": "你是谁？",
      "num_tokens": 95,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "zh/zh-002-random-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "zh-002-sampling-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "你是谁？",
      "language": "zh",
      "description": "你是谁？",
      "num_tokens": 40,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "zh/zh-002-sampling-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "zh-003-greedy-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "给我讲个笑话",
      "language": "zh",
      "description": "给我讲个笑话",
      "num_tokens": 27,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "zh/zh-003-greedy-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "zh-003-random-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "给我讲个笑话",
      "language": "zh",
      "description": "给我讲个笑话",
      "num_tokens": 25,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "zh/zh-003-random-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "zh-003-sampling-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "给我讲个笑话",
      "language": "zh",
      "description": "给我讲个笑话",
      "num_tokens": 38,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "zh/zh-003-sampling-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "zh-004-greedy-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "中国的首都是哪里？",
      "language": "zh",
      "description": "中国的首都是哪里？",
      "num_tokens": 7,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "zh/zh-004-greedy-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "zh-004-random-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "中国的首都是哪里？",
      "language": "zh",
      "description": "中国的首都是哪里？",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "zh/zh-004-random-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "zh-004-sampling-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "中国的首都是哪里？",
      "language": "zh",
      "description": "中国的首都是哪里？",
      "num_tokens": 7,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "zh/zh-004-sampling-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "zh-005-greedy-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "翻译成英文：早起的鸟儿有虫吃",
      "language": "zh",
      "description": "翻译成英文：早起的鸟儿有虫吃",
      "num_tokens": 49,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "zh/zh-005-greedy-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "zh-005-random-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "翻译成英文：早起的鸟儿有虫吃",
      "language": "zh",
      "description": "翻译成英文：早起的鸟儿有虫吃",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "zh/zh-005-random-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "zh-005-sampling-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "翻译成英文：早起的鸟儿有虫吃",
      "language": "zh",
      "description": "翻译成英文：早起的鸟儿有虫吃",
      "num_tokens": 47,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "zh/zh-005-sampling-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "zh-006-greedy-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "\"美丽\"这个词里有几个\"丽\"？",
      "language": "zh",
      "description": "\"美丽\"这个词里有几个\"丽\"？",
      "num_tokens": 14,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "zh/zh-006-greedy-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "zh-006-random-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "\"美丽\"这个词里有几个\"丽\"？",
      "language": "zh",
      "description": "\"美丽\"这个词里有几个\"丽\"？",
      "num_tokens": 74,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "zh/zh-006-random-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "zh-006-sampling-meta-llama-Llama-3.3-70B-Instruct",
      "type": "inference",
      "prompt": "\"美丽\"这个词里有几个\"丽\"？",
      "language": "zh",
      "description": "\"美丽\"这个词里有几个\"丽\"？",
      "num_tokens": 13,
      "model_id": "meta-llama/Llama-3.3-70B-Instruct",
      "file": "zh/zh-006-sampling-meta-llama-Llama-3.3-70B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    }
  ]
}