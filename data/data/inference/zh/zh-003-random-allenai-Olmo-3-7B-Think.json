{
  "id": "generated_20251203_121318",
  "prompt": "中国的首都是",
  "language": "zh",
  "temperature": 5.0,
  "top_k": 10,
  "model_info": {
    "name": "allenai/Olmo-3-7B-Think",
    "num_layers": 32,
    "hidden_size": 4096,
    "num_attention_heads": 32,
    "vocab_size": 100278
  },
  "generation_steps": [
    {
      "step": 0,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Okay",
            "token_id": 33413,
            "logprob": -0.0121,
            "prob": 0.9883
          },
          {
            "token": "Alright",
            "token_id": 72586,
            "logprob": -4.75,
            "prob": 0.0085
          },
          {
            "token": "�",
            "token_id": 161,
            "logprob": -6.0,
            "prob": 0.0024
          },
          {
            "token": "ĠOkay",
            "token_id": 36539,
            "logprob": -8.5,
            "prob": 0.0002
          },
          {
            "token": "ĠAlright",
            "token_id": 98693,
            "logprob": -9.25,
            "prob": 0.0001
          },
          {
            "token": "中",
            "token_id": 16325,
            "logprob": -9.4375,
            "prob": 0.0001
          },
          {
            "token": "好",
            "token_id": 53901,
            "logprob": -9.4375,
            "prob": 0.0001
          },
          {
            "token": "okay",
            "token_id": 94317,
            "logprob": -10.125,
            "prob": 0.0
          },
          {
            "token": "Hmm",
            "token_id": 81122,
            "logprob": -10.375,
            "prob": 0.0
          },
          {
            "token": "OK",
            "token_id": 4012,
            "logprob": -10.5625,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Alright",
        "token_id": 72586,
        "selection_method": "sampling"
      }
    },
    {
      "step": 1,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": ",",
            "token_id": 11,
            "logprob": -0.0,
            "prob": 1.0
          },
          {
            "token": "，",
            "token_id": 3922,
            "logprob": -11.6875,
            "prob": 0.0
          },
          {
            "token": "Ġlet",
            "token_id": 1095,
            "logprob": -14.0625,
            "prob": 0.0
          },
          {
            "token": ":",
            "token_id": 25,
            "logprob": -14.125,
            "prob": 0.0
          },
          {
            "token": "Ġlets",
            "token_id": 15714,
            "logprob": -15.0,
            "prob": 0.0
          },
          {
            "token": "Ġdar",
            "token_id": 15627,
            "logprob": -15.0,
            "prob": 0.0
          },
          {
            "token": "!",
            "token_id": 0,
            "logprob": -15.0625,
            "prob": 0.0
          },
          {
            "token": "?",
            "token_id": 30,
            "logprob": -15.1875,
            "prob": 0.0
          },
          {
            "token": ",...",
            "token_id": 29775,
            "logprob": -15.3125,
            "prob": 0.0
          },
          {
            "token": ".",
            "token_id": 13,
            "logprob": -15.4375,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "?",
        "token_id": 30,
        "selection_method": "sampling"
      }
    },
    {
      "step": 2,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright?",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "ĠThe",
            "token_id": 578,
            "logprob": -0.0359,
            "prob": 0.9648
          },
          {
            "token": "ĠUser",
            "token_id": 2724,
            "logprob": -4.0312,
            "prob": 0.0177
          },
          {
            "token": "ĠLet",
            "token_id": 6914,
            "logprob": -4.4062,
            "prob": 0.0121
          },
          {
            "token": "用户",
            "token_id": 20600,
            "logprob": -6.5312,
            "prob": 0.0014
          },
          {
            "token": "ĠWait",
            "token_id": 14144,
            "logprob": -6.5312,
            "prob": 0.0014
          },
          {
            "token": "ĠSo",
            "token_id": 2100,
            "logprob": -7.7188,
            "prob": 0.0004
          },
          {
            "token": "ĠSomeone",
            "token_id": 35272,
            "logprob": -8.5,
            "prob": 0.0002
          },
          {
            "token": "ĠI",
            "token_id": 358,
            "logprob": -8.6875,
            "prob": 0.0002
          },
          {
            "token": "ĠQuestion",
            "token_id": 16225,
            "logprob": -8.6875,
            "prob": 0.0002
          },
          {
            "token": "ĠWhat",
            "token_id": 3639,
            "logprob": -8.8125,
            "prob": 0.0002
          }
        ]
      },
      "selected_token": {
        "token": "ĠI",
        "token_id": 358,
        "selection_method": "sampling"
      }
    },
    {
      "step": 3,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġneed",
            "token_id": 1205,
            "logprob": -0.1885,
            "prob": 0.8281
          },
          {
            "token": "Ġjust",
            "token_id": 1120,
            "logprob": -2.5625,
            "prob": 0.0771
          },
          {
            "token": "Ġwas",
            "token_id": 574,
            "logprob": -3.3125,
            "prob": 0.0364
          },
          {
            "token": "Ġgot",
            "token_id": 2751,
            "logprob": -3.9375,
            "prob": 0.0195
          },
          {
            "token": "'m",
            "token_id": 2846,
            "logprob": -4.3125,
            "prob": 0.0134
          },
          {
            "token": "'ve",
            "token_id": 3077,
            "logprob": -4.5625,
            "prob": 0.0104
          },
          {
            "token": "Ġget",
            "token_id": 636,
            "logprob": -5.8125,
            "prob": 0.003
          },
          {
            "token": "Ġhave",
            "token_id": 617,
            "logprob": -6.1875,
            "prob": 0.0021
          },
          {
            "token": "Ġthink",
            "token_id": 1781,
            "logprob": -6.1875,
            "prob": 0.0021
          },
          {
            "token": "Ġsee",
            "token_id": 1518,
            "logprob": -6.5625,
            "prob": 0.0014
          }
        ]
      },
      "selected_token": {
        "token": "Ġhave",
        "token_id": 617,
        "selection_method": "sampling"
      }
    },
    {
      "step": 4,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġto",
            "token_id": 311,
            "logprob": -0.5625,
            "prob": 0.5703
          },
          {
            "token": "Ġthis",
            "token_id": 420,
            "logprob": -1.0625,
            "prob": 0.3457
          },
          {
            "token": "Ġa",
            "token_id": 264,
            "logprob": -2.8125,
            "prob": 0.0601
          },
          {
            "token": "Ġbeen",
            "token_id": 1027,
            "logprob": -4.0625,
            "prob": 0.0172
          },
          {
            "token": "Ġan",
            "token_id": 459,
            "logprob": -5.9375,
            "prob": 0.0026
          },
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -6.6875,
            "prob": 0.0013
          },
          {
            "token": "Ġthat",
            "token_id": 430,
            "logprob": -7.3125,
            "prob": 0.0007
          },
          {
            "token": "Ġreceived",
            "token_id": 4036,
            "logprob": -7.6875,
            "prob": 0.0005
          },
          {
            "token": "Ġgot",
            "token_id": 2751,
            "logprob": -8.5625,
            "prob": 0.0002
          },
          {
            "token": "Ġsome",
            "token_id": 1063,
            "logprob": -8.6875,
            "prob": 0.0002
          }
        ]
      },
      "selected_token": {
        "token": "Ġthat",
        "token_id": 430,
        "selection_method": "sampling"
      }
    },
    {
      "step": 5,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġquestion",
            "token_id": 3488,
            "logprob": -0.0801,
            "prob": 0.9219
          },
          {
            "token": "Ġquery",
            "token_id": 3319,
            "logprob": -3.8281,
            "prob": 0.0217
          },
          {
            "token": "Ġuser",
            "token_id": 1217,
            "logprob": -4.3438,
            "prob": 0.0132
          },
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -4.6562,
            "prob": 0.0096
          },
          {
            "token": "Ġone",
            "token_id": 832,
            "logprob": -5.4062,
            "prob": 0.0045
          },
          {
            "token": "ĠChina",
            "token_id": 5734,
            "logprob": -5.5312,
            "prob": 0.004
          },
          {
            "token": "ĠChinese",
            "token_id": 8620,
            "logprob": -6.0938,
            "prob": 0.0023
          },
          {
            "token": "Ġ\"",
            "token_id": 330,
            "logprob": -6.2188,
            "prob": 0.002
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -6.3438,
            "prob": 0.0018
          },
          {
            "token": "Ġa",
            "token_id": 264,
            "logprob": -6.6562,
            "prob": 0.0013
          }
        ]
      },
      "selected_token": {
        "token": "Ġquestion",
        "token_id": 3488,
        "selection_method": "sampling"
      }
    },
    {
      "step": 6,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġhere",
            "token_id": 1618,
            "logprob": -0.5742,
            "prob": 0.5625
          },
          {
            "token": ":",
            "token_id": 25,
            "logprob": -1.0703,
            "prob": 0.3418
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -3.2031,
            "prob": 0.0408
          },
          {
            "token": "Ġfrom",
            "token_id": 505,
            "logprob": -3.5781,
            "prob": 0.0281
          },
          {
            "token": ",",
            "token_id": 11,
            "logprob": -5.1875,
            "prob": 0.0055
          },
          {
            "token": "Ġabout",
            "token_id": 922,
            "logprob": -5.3125,
            "prob": 0.0049
          },
          {
            "token": ".",
            "token_id": 13,
            "logprob": -5.5625,
            "prob": 0.0038
          },
          {
            "token": "Ġright",
            "token_id": 1314,
            "logprob": -6.1875,
            "prob": 0.002
          },
          {
            "token": "Ġ\"",
            "token_id": 330,
            "logprob": -6.3125,
            "prob": 0.0018
          },
          {
            "token": "Ġcoming",
            "token_id": 5108,
            "logprob": -6.3125,
            "prob": 0.0018
          }
        ]
      },
      "selected_token": {
        "token": ".",
        "token_id": 13,
        "selection_method": "sampling"
      }
    },
    {
      "step": 7,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question.",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        "."
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "ĠThe",
            "token_id": 578,
            "logprob": -0.8398,
            "prob": 0.4316
          },
          {
            "token": "Ġ\"",
            "token_id": 330,
            "logprob": -0.8398,
            "prob": 0.4316
          },
          {
            "token": "ĠLet",
            "token_id": 6914,
            "logprob": -3.0938,
            "prob": 0.0454
          },
          {
            "token": "ĠWhat",
            "token_id": 3639,
            "logprob": -3.0938,
            "prob": 0.0454
          },
          {
            "token": "ĠIt",
            "token_id": 1102,
            "logprob": -3.9688,
            "prob": 0.0189
          },
          {
            "token": "ĠUser",
            "token_id": 2724,
            "logprob": -4.0938,
            "prob": 0.0167
          },
          {
            "token": "ĠWait",
            "token_id": 14144,
            "logprob": -6.0312,
            "prob": 0.0024
          },
          {
            "token": "ĠChina",
            "token_id": 5734,
            "logprob": -6.7188,
            "prob": 0.0012
          },
          {
            "token": "Ġ用户",
            "token_id": 51142,
            "logprob": -6.7812,
            "prob": 0.0011
          },
          {
            "token": "用户",
            "token_id": 20600,
            "logprob": -7.4062,
            "prob": 0.0006
          }
        ]
      },
      "selected_token": {
        "token": "ĠLet",
        "token_id": 6914,
        "selection_method": "sampling"
      }
    },
    {
      "step": 8,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġme",
            "token_id": 757,
            "logprob": -0.1006,
            "prob": 0.9062
          },
          {
            "token": "'s",
            "token_id": 596,
            "logprob": -2.3438,
            "prob": 0.0952
          },
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -9.75,
            "prob": 0.0001
          },
          {
            "token": "Ġsee",
            "token_id": 1518,
            "logprob": -10.5625,
            "prob": 0.0
          },
          {
            "token": "Ġmet",
            "token_id": 2322,
            "logprob": -10.625,
            "prob": 0.0
          },
          {
            "token": "Ġmy",
            "token_id": 856,
            "logprob": -11.1875,
            "prob": 0.0
          },
          {
            "token": "me",
            "token_id": 2727,
            "logprob": -11.875,
            "prob": 0.0
          },
          {
            "token": "Ġit",
            "token_id": 433,
            "logprob": -12.5,
            "prob": 0.0
          },
          {
            "token": "'m",
            "token_id": 2846,
            "logprob": -12.875,
            "prob": 0.0
          },
          {
            "token": "Ġthem",
            "token_id": 1124,
            "logprob": -13.125,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġsee",
        "token_id": 1518,
        "selection_method": "sampling"
      }
    },
    {
      "step": 9,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": ",",
            "token_id": 11,
            "logprob": -0.75,
            "prob": 0.4727
          },
          {
            "token": "...",
            "token_id": 1131,
            "logprob": -0.875,
            "prob": 0.416
          },
          {
            "token": ".",
            "token_id": 13,
            "logprob": -2.5,
            "prob": 0.082
          },
          {
            "token": ":",
            "token_id": 25,
            "logprob": -4.5,
            "prob": 0.0111
          },
          {
            "token": "Ġwhat",
            "token_id": 1148,
            "logprob": -5.375,
            "prob": 0.0046
          },
          {
            "token": "Ġhere",
            "token_id": 1618,
            "logprob": -5.5,
            "prob": 0.0041
          },
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -5.625,
            "prob": 0.0036
          },
          {
            "token": "…",
            "token_id": 1981,
            "logprob": -6.625,
            "prob": 0.0013
          },
          {
            "token": ",...",
            "token_id": 29775,
            "logprob": -7.25,
            "prob": 0.0007
          },
          {
            "token": "Ġit",
            "token_id": 433,
            "logprob": -7.625,
            "prob": 0.0005
          }
        ]
      },
      "selected_token": {
        "token": ",",
        "token_id": 11,
        "selection_method": "sampling"
      }
    },
    {
      "step": 10,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see,",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ","
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -0.2432,
            "prob": 0.7852
          },
          {
            "token": "Ġ\"",
            "token_id": 330,
            "logprob": -2.125,
            "prob": 0.1201
          },
          {
            "token": "Ġwhat",
            "token_id": 1148,
            "logprob": -2.625,
            "prob": 0.0728
          },
          {
            "token": "Ġuser",
            "token_id": 1217,
            "logprob": -4.875,
            "prob": 0.0077
          },
          {
            "token": "Ġwho",
            "token_id": 889,
            "logprob": -5.625,
            "prob": 0.0036
          },
          {
            "token": "Ġit",
            "token_id": 433,
            "logprob": -5.625,
            "prob": 0.0036
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -5.625,
            "prob": 0.0036
          },
          {
            "token": "Ġsomeone",
            "token_id": 4423,
            "logprob": -6.125,
            "prob": 0.0022
          },
          {
            "token": "ĠChina",
            "token_id": 5734,
            "logprob": -8.0,
            "prob": 0.0003
          },
          {
            "token": "Ġquestion",
            "token_id": 3488,
            "logprob": -8.625,
            "prob": 0.0002
          }
        ]
      },
      "selected_token": {
        "token": "Ġthe",
        "token_id": 279,
        "selection_method": "sampling"
      }
    },
    {
      "step": 11,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġuser",
            "token_id": 1217,
            "logprob": -0.0264,
            "prob": 0.9727
          },
          {
            "token": "Ġquestion",
            "token_id": 3488,
            "logprob": -3.6562,
            "prob": 0.026
          },
          {
            "token": "用户",
            "token_id": 20600,
            "logprob": -11.875,
            "prob": 0.0
          },
          {
            "token": "Ġfirst",
            "token_id": 1176,
            "logprob": -12.25,
            "prob": 0.0
          },
          {
            "token": "ĠChinese",
            "token_id": 8620,
            "logprob": -12.625,
            "prob": 0.0
          },
          {
            "token": "Ġoriginal",
            "token_id": 4113,
            "logprob": -12.75,
            "prob": 0.0
          },
          {
            "token": "Ġperson",
            "token_id": 1732,
            "logprob": -13.1875,
            "prob": 0.0
          },
          {
            "token": "Ġusers",
            "token_id": 3932,
            "logprob": -13.625,
            "prob": 0.0
          },
          {
            "token": "Ġtitle",
            "token_id": 2316,
            "logprob": -13.75,
            "prob": 0.0
          },
          {
            "token": "Ġcapital",
            "token_id": 6864,
            "logprob": -13.8125,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġuser",
        "token_id": 1217,
        "selection_method": "sampling"
      }
    },
    {
      "step": 12,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġis",
            "token_id": 374,
            "logprob": -0.0811,
            "prob": 0.9219
          },
          {
            "token": "Ġasked",
            "token_id": 4691,
            "logprob": -2.5781,
            "prob": 0.0757
          },
          {
            "token": "Ġwrote",
            "token_id": 6267,
            "logprob": -6.7188,
            "prob": 0.0012
          },
          {
            "token": "Ġasks",
            "token_id": 17501,
            "logprob": -7.0938,
            "prob": 0.0008
          },
          {
            "token": "Ġasking",
            "token_id": 10371,
            "logprob": -9.3125,
            "prob": 0.0001
          },
          {
            "token": "Ġsaid",
            "token_id": 1071,
            "logprob": -9.6875,
            "prob": 0.0001
          },
          {
            "token": "Ġjust",
            "token_id": 1120,
            "logprob": -10.0625,
            "prob": 0.0
          },
          {
            "token": "Ġsays",
            "token_id": 2795,
            "logprob": -10.1875,
            "prob": 0.0
          },
          {
            "token": "Ġwants",
            "token_id": 6944,
            "logprob": -10.375,
            "prob": 0.0
          },
          {
            "token": "Ġask",
            "token_id": 2610,
            "logprob": -11.625,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġasked",
        "token_id": 4691,
        "selection_method": "sampling"
      }
    },
    {
      "step": 13,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġ\"",
            "token_id": 330,
            "logprob": -0.5586,
            "prob": 0.5703
          },
          {
            "token": ",",
            "token_id": 11,
            "logprob": -0.9336,
            "prob": 0.3926
          },
          {
            "token": ":",
            "token_id": 25,
            "logprob": -3.6875,
            "prob": 0.0251
          },
          {
            "token": "Ġabout",
            "token_id": 922,
            "logprob": -5.0625,
            "prob": 0.0063
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -6.0625,
            "prob": 0.0023
          },
          {
            "token": "Ġwhat",
            "token_id": 1148,
            "logprob": -6.9375,
            "prob": 0.001
          },
          {
            "token": "Ġsomething",
            "token_id": 2555,
            "logprob": -8.0625,
            "prob": 0.0003
          },
          {
            "token": "Ġ“",
            "token_id": 1054,
            "logprob": -8.125,
            "prob": 0.0003
          },
          {
            "token": ",\"",
            "token_id": 1359,
            "logprob": -9.3125,
            "prob": 0.0001
          },
          {
            "token": "Ġfor",
            "token_id": 369,
            "logprob": -11.0,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġsomething",
        "token_id": 2555,
        "selection_method": "sampling"
      }
    },
    {
      "step": 14,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -0.0255,
            "prob": 0.9766
          },
          {
            "token": "Ġabout",
            "token_id": 922,
            "logprob": -3.7812,
            "prob": 0.0229
          },
          {
            "token": "Ġlike",
            "token_id": 1093,
            "logprob": -6.6562,
            "prob": 0.0013
          },
          {
            "token": ":",
            "token_id": 25,
            "logprob": -7.9062,
            "prob": 0.0004
          },
          {
            "token": ".",
            "token_id": 13,
            "logprob": -9.125,
            "prob": 0.0001
          },
          {
            "token": ",",
            "token_id": 11,
            "logprob": -9.375,
            "prob": 0.0001
          },
          {
            "token": "ĠChinese",
            "token_id": 8620,
            "logprob": -9.75,
            "prob": 0.0001
          },
          {
            "token": "Ġthat",
            "token_id": 430,
            "logprob": -9.8125,
            "prob": 0.0001
          },
          {
            "token": "Ġis",
            "token_id": 374,
            "logprob": -10.25,
            "prob": 0.0
          },
          {
            "token": "Ġsimple",
            "token_id": 4382,
            "logprob": -10.4375,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġthat",
        "token_id": 430,
        "selection_method": "sampling"
      }
    },
    {
      "step": 15,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġlooks",
            "token_id": 5992,
            "logprob": -0.4941,
            "prob": 0.6094
          },
          {
            "token": "Ġsounds",
            "token_id": 10578,
            "logprob": -2.125,
            "prob": 0.1201
          },
          {
            "token": "Ġseems",
            "token_id": 5084,
            "logprob": -2.25,
            "prob": 0.106
          },
          {
            "token": "'s",
            "token_id": 596,
            "logprob": -2.75,
            "prob": 0.0645
          },
          {
            "token": "Ġsays",
            "token_id": 2795,
            "logprob": -3.0,
            "prob": 0.05
          },
          {
            "token": "ĠI",
            "token_id": 358,
            "logprob": -3.875,
            "prob": 0.0209
          },
          {
            "token": "Ġtranslates",
            "token_id": 48018,
            "logprob": -4.625,
            "prob": 0.0099
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -5.625,
            "prob": 0.0036
          },
          {
            "token": "Ġjust",
            "token_id": 1120,
            "logprob": -6.0,
            "prob": 0.0025
          },
          {
            "token": "Ġwas",
            "token_id": 574,
            "logprob": -6.3125,
            "prob": 0.0018
          }
        ]
      },
      "selected_token": {
        "token": "Ġsounds",
        "token_id": 10578,
        "selection_method": "sampling"
      }
    },
    {
      "step": 16,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġlike",
            "token_id": 1093,
            "logprob": -0.2285,
            "prob": 0.7969
          },
          {
            "token": "ĠChinese",
            "token_id": 8620,
            "logprob": -2.6094,
            "prob": 0.0742
          },
          {
            "token": "Ġsimple",
            "token_id": 4382,
            "logprob": -2.7344,
            "prob": 0.0654
          },
          {
            "token": "Ġstraightforward",
            "token_id": 31439,
            "logprob": -3.9844,
            "prob": 0.0187
          },
          {
            "token": "Ġfamiliar",
            "token_id": 11537,
            "logprob": -4.3438,
            "prob": 0.0129
          },
          {
            "token": "Ġa",
            "token_id": 264,
            "logprob": -5.0938,
            "prob": 0.0061
          },
          {
            "token": "Ġpretty",
            "token_id": 5128,
            "logprob": -5.1562,
            "prob": 0.0057
          },
          {
            "token": "Ġalmost",
            "token_id": 4661,
            "logprob": -5.3438,
            "prob": 0.0047
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -5.4688,
            "prob": 0.0042
          },
          {
            "token": "Ġvery",
            "token_id": 1633,
            "logprob": -6.2188,
            "prob": 0.002
          }
        ]
      },
      "selected_token": {
        "token": "Ġfamiliar",
        "token_id": 11537,
        "selection_method": "sampling"
      }
    },
    {
      "step": 17,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": ".",
            "token_id": 13,
            "logprob": -0.1943,
            "prob": 0.8242
          },
          {
            "token": "Ġbut",
            "token_id": 719,
            "logprob": -2.4375,
            "prob": 0.0869
          },
          {
            "token": ",",
            "token_id": 11,
            "logprob": -2.9375,
            "prob": 0.0527
          },
          {
            "token": ":",
            "token_id": 25,
            "logprob": -4.1875,
            "prob": 0.0151
          },
          {
            "token": "...",
            "token_id": 1131,
            "logprob": -4.6875,
            "prob": 0.0092
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -4.8125,
            "prob": 0.0081
          },
          {
            "token": "Ġfrom",
            "token_id": 505,
            "logprob": -6.4375,
            "prob": 0.0016
          },
          {
            "token": "Ġto",
            "token_id": 311,
            "logprob": -6.5625,
            "prob": 0.0014
          },
          {
            "token": ".\n\n",
            "token_id": 382,
            "logprob": -7.4375,
            "prob": 0.0006
          },
          {
            "token": "?",
            "token_id": 30,
            "logprob": -8.4375,
            "prob": 0.0002
          }
        ]
      },
      "selected_token": {
        "token": ":",
        "token_id": 25,
        "selection_method": "sampling"
      }
    },
    {
      "step": 18,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar:",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġ\"",
            "token_id": 330,
            "logprob": -0.008,
            "prob": 0.9922
          },
          {
            "token": "Ġ“",
            "token_id": 1054,
            "logprob": -5.0,
            "prob": 0.0067
          },
          {
            "token": "ĠChina",
            "token_id": 5734,
            "logprob": -8.25,
            "prob": 0.0003
          },
          {
            "token": "Ġ",
            "token_id": 220,
            "logprob": -8.5,
            "prob": 0.0002
          },
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -8.625,
            "prob": 0.0002
          },
          {
            "token": "Ġ�",
            "token_id": 10447,
            "logprob": -9.375,
            "prob": 0.0001
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -10.125,
            "prob": 0.0
          },
          {
            "token": "中国",
            "token_id": 59795,
            "logprob": -10.375,
            "prob": 0.0
          },
          {
            "token": "Ġwhat",
            "token_id": 1148,
            "logprob": -10.5,
            "prob": 0.0
          },
          {
            "token": "Ġ'",
            "token_id": 364,
            "logprob": -10.6875,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġ\"",
        "token_id": 330,
        "selection_method": "sampling"
      }
    },
    {
      "step": 19,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\""
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "中国",
            "token_id": 59795,
            "logprob": -0.1865,
            "prob": 0.8281
          },
          {
            "token": "China",
            "token_id": 23078,
            "logprob": -1.8125,
            "prob": 0.1631
          },
          {
            "token": "ĠChina",
            "token_id": 5734,
            "logprob": -6.0,
            "prob": 0.0025
          },
          {
            "token": "What",
            "token_id": 3923,
            "logprob": -6.875,
            "prob": 0.001
          },
          {
            "token": "Chinese",
            "token_id": 46023,
            "logprob": -6.9375,
            "prob": 0.001
          },
          {
            "token": "中",
            "token_id": 16325,
            "logprob": -7.0625,
            "prob": 0.0009
          },
          {
            "token": "ĠChinese",
            "token_id": 8620,
            "logprob": -8.625,
            "prob": 0.0002
          },
          {
            "token": "Ġchina",
            "token_id": 33625,
            "logprob": -9.3125,
            "prob": 0.0001
          },
          {
            "token": "china",
            "token_id": 81236,
            "logprob": -9.875,
            "prob": 0.0001
          },
          {
            "token": "国",
            "token_id": 29504,
            "logprob": -10.3125,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "China",
        "token_id": 23078,
        "selection_method": "sampling"
      }
    },
    {
      "step": 20,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "'s",
            "token_id": 596,
            "logprob": -0.0383,
            "prob": 0.9609
          },
          {
            "token": "的",
            "token_id": 9554,
            "logprob": -3.5312,
            "prob": 0.0291
          },
          {
            "token": "’s",
            "token_id": 753,
            "logprob": -4.9062,
            "prob": 0.0074
          },
          {
            "token": "Ġ的",
            "token_id": 44689,
            "logprob": -7.6562,
            "prob": 0.0005
          },
          {
            "token": "の",
            "token_id": 16144,
            "logprob": -8.0625,
            "prob": 0.0003
          },
          {
            "token": "의",
            "token_id": 21028,
            "logprob": -10.0,
            "prob": 0.0
          },
          {
            "token": "\"s",
            "token_id": 41887,
            "logprob": -10.625,
            "prob": 0.0
          },
          {
            "token": "'",
            "token_id": 6,
            "logprob": -10.875,
            "prob": 0.0
          },
          {
            "token": "\"",
            "token_id": 1,
            "logprob": -11.5,
            "prob": 0.0
          },
          {
            "token": "Ġ'",
            "token_id": 364,
            "logprob": -11.8125,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "'s",
        "token_id": 596,
        "selection_method": "sampling"
      }
    },
    {
      "step": 21,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġcapital",
            "token_id": 6864,
            "logprob": -0.0004,
            "prob": 1.0
          },
          {
            "token": "capital",
            "token_id": 66163,
            "logprob": -9.0625,
            "prob": 0.0001
          },
          {
            "token": "Ġcapitals",
            "token_id": 94099,
            "logprob": -9.25,
            "prob": 0.0001
          },
          {
            "token": "首",
            "token_id": 61075,
            "logprob": -9.4375,
            "prob": 0.0001
          },
          {
            "token": "ĠCapital",
            "token_id": 18880,
            "logprob": -10.1875,
            "prob": 0.0
          },
          {
            "token": "Ġnational",
            "token_id": 5426,
            "logprob": -10.8125,
            "prob": 0.0
          },
          {
            "token": "-capital",
            "token_id": 98231,
            "logprob": -11.0,
            "prob": 0.0
          },
          {
            "token": "Ġfirst",
            "token_id": 1176,
            "logprob": -11.1875,
            "prob": 0.0
          },
          {
            "token": "ĠCAPITAL",
            "token_id": 78387,
            "logprob": -11.375,
            "prob": 0.0
          },
          {
            "token": "Capital",
            "token_id": 64693,
            "logprob": -11.5,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "ĠCAPITAL",
        "token_id": 78387,
        "selection_method": "sampling"
      }
    },
    {
      "step": 22,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġis",
            "token_id": 374,
            "logprob": -0.0327,
            "prob": 0.9688
          },
          {
            "token": "ĠIS",
            "token_id": 3507,
            "logprob": -3.6562,
            "prob": 0.0258
          },
          {
            "token": "Ġcity",
            "token_id": 3363,
            "logprob": -6.0938,
            "prob": 0.0023
          },
          {
            "token": "?\"",
            "token_id": 7673,
            "logprob": -6.4062,
            "prob": 0.0016
          },
          {
            "token": ".\"",
            "token_id": 1210,
            "logprob": -7.7812,
            "prob": 0.0004
          },
          {
            "token": "?",
            "token_id": 30,
            "logprob": -8.0625,
            "prob": 0.0003
          },
          {
            "token": "是",
            "token_id": 21043,
            "logprob": -8.125,
            "prob": 0.0003
          },
          {
            "token": "ĠIs",
            "token_id": 2209,
            "logprob": -8.25,
            "prob": 0.0003
          },
          {
            "token": "\".",
            "token_id": 3343,
            "logprob": -8.9375,
            "prob": 0.0001
          },
          {
            "token": "Ġ是",
            "token_id": 55951,
            "logprob": -9.1875,
            "prob": 0.0001
          }
        ]
      },
      "selected_token": {
        "token": "ĠIS",
        "token_id": 3507,
        "selection_method": "sampling"
      }
    },
    {
      "step": 23,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "...\"",
            "token_id": 21908,
            "logprob": -0.5234,
            "prob": 0.5938
          },
          {
            "token": "?\"",
            "token_id": 7673,
            "logprob": -1.7734,
            "prob": 0.1699
          },
          {
            "token": "\".",
            "token_id": 3343,
            "logprob": -2.4062,
            "prob": 0.0908
          },
          {
            "token": "\"",
            "token_id": 1,
            "logprob": -3.2188,
            "prob": 0.0403
          },
          {
            "token": "...",
            "token_id": 1131,
            "logprob": -3.5938,
            "prob": 0.0277
          },
          {
            "token": ".\"",
            "token_id": 1210,
            "logprob": -3.7812,
            "prob": 0.0229
          },
          {
            "token": "___",
            "token_id": 6101,
            "logprob": -4.4688,
            "prob": 0.0115
          },
          {
            "token": "\"?",
            "token_id": 44969,
            "logprob": -4.9062,
            "prob": 0.0074
          },
          {
            "token": "Ġ___",
            "token_id": 7588,
            "logprob": -5.0312,
            "prob": 0.0066
          },
          {
            "token": "____",
            "token_id": 2179,
            "logprob": -5.2188,
            "prob": 0.0055
          }
        ]
      },
      "selected_token": {
        "token": "?\"",
        "token_id": 7673,
        "selection_method": "sampling"
      }
    },
    {
      "step": 24,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\"",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\""
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "ĠWait",
            "token_id": 14144,
            "logprob": -0.7305,
            "prob": 0.4824
          },
          {
            "token": "ĠHmm",
            "token_id": 89290,
            "logprob": -2.3594,
            "prob": 0.0947
          },
          {
            "token": "ĠOr",
            "token_id": 2582,
            "logprob": -2.4844,
            "prob": 0.084
          },
          {
            "token": "ĠBut",
            "token_id": 2030,
            "logprob": -2.6094,
            "prob": 0.0737
          },
          {
            "token": "Ġor",
            "token_id": 477,
            "logprob": -2.7344,
            "prob": 0.0654
          },
          {
            "token": "ĠWell",
            "token_id": 8489,
            "logprob": -2.7344,
            "prob": 0.0654
          },
          {
            "token": "ĠThe",
            "token_id": 578,
            "logprob": -3.9844,
            "prob": 0.0187
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -3.9844,
            "prob": 0.0187
          },
          {
            "token": "ĠIn",
            "token_id": 763,
            "logprob": -4.2188,
            "prob": 0.0145
          },
          {
            "token": "ĠMaybe",
            "token_id": 10926,
            "logprob": -4.2188,
            "prob": 0.0145
          }
        ]
      },
      "selected_token": {
        "token": "ĠHmm",
        "token_id": 89290,
        "selection_method": "sampling"
      }
    },
    {
      "step": 25,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": ",",
            "token_id": 11,
            "logprob": -0.104,
            "prob": 0.9023
          },
          {
            "token": ".",
            "token_id": 13,
            "logprob": -2.3594,
            "prob": 0.0952
          },
          {
            "token": "...",
            "token_id": 1131,
            "logprob": -6.5938,
            "prob": 0.0014
          },
          {
            "token": "Ġokay",
            "token_id": 17339,
            "logprob": -7.0938,
            "prob": 0.0008
          },
          {
            "token": ".\n\n",
            "token_id": 382,
            "logprob": -7.5938,
            "prob": 0.0005
          },
          {
            "token": "Ġwait",
            "token_id": 3868,
            "logprob": -8.125,
            "prob": 0.0003
          },
          {
            "token": "Ġyeah",
            "token_id": 22371,
            "logprob": -9.5,
            "prob": 0.0001
          },
          {
            "token": "Ġmaybe",
            "token_id": 7344,
            "logprob": -9.625,
            "prob": 0.0001
          },
          {
            "token": "?",
            "token_id": 30,
            "logprob": -10.0,
            "prob": 0.0
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -10.0,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġokay",
        "token_id": 17339,
        "selection_method": "sampling"
      }
    },
    {
      "step": 26,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm okay",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm",
        "Ġokay"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290,
        17339
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": ",",
            "token_id": 11,
            "logprob": -0.4805,
            "prob": 0.6172
          },
          {
            "token": ".",
            "token_id": 13,
            "logprob": -0.9805,
            "prob": 0.375
          },
          {
            "token": ".\n\n",
            "token_id": 382,
            "logprob": -5.5938,
            "prob": 0.0037
          },
          {
            "token": "Ġso",
            "token_id": 779,
            "logprob": -7.2188,
            "prob": 0.0007
          },
          {
            "token": "Ġfirst",
            "token_id": 1176,
            "logprob": -7.5938,
            "prob": 0.0005
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -7.9688,
            "prob": 0.0003
          },
          {
            "token": "Ġmaybe",
            "token_id": 7344,
            "logprob": -9.25,
            "prob": 0.0001
          },
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -9.5,
            "prob": 0.0001
          },
          {
            "token": "...",
            "token_id": 1131,
            "logprob": -10.0,
            "prob": 0.0
          },
          {
            "token": "Ġwell",
            "token_id": 1664,
            "logprob": -10.125,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "...",
        "token_id": 1131,
        "selection_method": "sampling"
      }
    },
    {
      "step": 27,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm okay...",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm",
        "Ġokay",
        "..."
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290,
        17339,
        1131
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "ĠFirst",
            "token_id": 5629,
            "logprob": -1.4375,
            "prob": 0.2383
          },
          {
            "token": "ĠWell",
            "token_id": 8489,
            "logprob": -1.5625,
            "prob": 0.21
          },
          {
            "token": "ĠI",
            "token_id": 358,
            "logprob": -2.1875,
            "prob": 0.1128
          },
          {
            "token": "ĠWait",
            "token_id": 14144,
            "logprob": -2.1875,
            "prob": 0.1128
          },
          {
            "token": "ĠLet",
            "token_id": 6914,
            "logprob": -2.3125,
            "prob": 0.0991
          },
          {
            "token": "ĠChina",
            "token_id": 5734,
            "logprob": -2.4375,
            "prob": 0.0879
          },
          {
            "token": "ĠSo",
            "token_id": 2100,
            "logprob": -3.3125,
            "prob": 0.0366
          },
          {
            "token": "Ġfirst",
            "token_id": 1176,
            "logprob": -3.6875,
            "prob": 0.0251
          },
          {
            "token": "ĠThe",
            "token_id": 578,
            "logprob": -4.0625,
            "prob": 0.0172
          },
          {
            "token": "Ġso",
            "token_id": 779,
            "logprob": -4.5,
            "prob": 0.0112
          }
        ]
      },
      "selected_token": {
        "token": "Ġso",
        "token_id": 779,
        "selection_method": "sampling"
      }
    },
    {
      "step": 28,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm okay... so",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm",
        "Ġokay",
        "...",
        "Ġso"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290,
        17339,
        1131,
        779
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġfirst",
            "token_id": 1176,
            "logprob": -1.0938,
            "prob": 0.334
          },
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -1.5938,
            "prob": 0.2021
          },
          {
            "token": "ĠI",
            "token_id": 358,
            "logprob": -1.7188,
            "prob": 0.1787
          },
          {
            "token": "Ġthey",
            "token_id": 814,
            "logprob": -1.8438,
            "prob": 0.1582
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -3.0938,
            "prob": 0.0452
          },
          {
            "token": ",",
            "token_id": 11,
            "logprob": -3.2188,
            "prob": 0.0398
          },
          {
            "token": "Ġwhat",
            "token_id": 1148,
            "logprob": -3.7188,
            "prob": 0.0242
          },
          {
            "token": "Ġmaybe",
            "token_id": 7344,
            "logprob": -5.9688,
            "prob": 0.0025
          },
          {
            "token": "Ġbasically",
            "token_id": 13524,
            "logprob": -6.3438,
            "prob": 0.0018
          },
          {
            "token": "Ġwhich",
            "token_id": 902,
            "logprob": -6.4688,
            "prob": 0.0015
          }
        ]
      },
      "selected_token": {
        "token": "Ġfirst",
        "token_id": 1176,
        "selection_method": "sampling"
      }
    },
    {
      "step": 29,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm okay... so first",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm",
        "Ġokay",
        "...",
        "Ġso",
        "Ġfirst"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290,
        17339,
        1131,
        779,
        1176
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": ",",
            "token_id": 11,
            "logprob": -0.25,
            "prob": 0.7773
          },
          {
            "token": "ĠI",
            "token_id": 358,
            "logprob": -2.25,
            "prob": 0.1055
          },
          {
            "token": "Ġoff",
            "token_id": 1022,
            "logprob": -2.625,
            "prob": 0.0723
          },
          {
            "token": "Ġstep",
            "token_id": 3094,
            "logprob": -4.0,
            "prob": 0.0183
          },
          {
            "token": "Ġthing",
            "token_id": 3245,
            "logprob": -4.375,
            "prob": 0.0126
          },
          {
            "token": "Ġthought",
            "token_id": 3463,
            "logprob": -5.125,
            "prob": 0.006
          },
          {
            "token": "Ġof",
            "token_id": 315,
            "logprob": -5.875,
            "prob": 0.0028
          },
          {
            "token": "Ġthings",
            "token_id": 2574,
            "logprob": -6.125,
            "prob": 0.0022
          },
          {
            "token": "Ġlanguage",
            "token_id": 4221,
            "logprob": -8.25,
            "prob": 0.0003
          },
          {
            "token": "Ġthoughts",
            "token_id": 11555,
            "logprob": -8.875,
            "prob": 0.0001
          }
        ]
      },
      "selected_token": {
        "token": "Ġof",
        "token_id": 315,
        "selection_method": "sampling"
      }
    },
    {
      "step": 30,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm okay... so first of",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm",
        "Ġokay",
        "...",
        "Ġso",
        "Ġfirst",
        "Ġof"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290,
        17339,
        1131,
        779,
        1176,
        315
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġall",
            "token_id": 682,
            "logprob": -0.0002,
            "prob": 1.0
          },
          {
            "token": "Ġcourse",
            "token_id": 3388,
            "logprob": -9.0625,
            "prob": 0.0001
          },
          {
            "token": "ĠI",
            "token_id": 358,
            "logprob": -9.625,
            "prob": 0.0001
          },
          {
            "token": ",",
            "token_id": 11,
            "logprob": -11.6875,
            "prob": 0.0
          },
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -12.375,
            "prob": 0.0
          },
          {
            "token": "Ġmy",
            "token_id": 856,
            "logprob": -12.5,
            "prob": 0.0
          },
          {
            "token": "Ġme",
            "token_id": 757,
            "logprob": -12.75,
            "prob": 0.0
          },
          {
            "token": "Ġfirst",
            "token_id": 1176,
            "logprob": -13.25,
            "prob": 0.0
          },
          {
            "token": "Ġforemost",
            "token_id": 43780,
            "logprob": -13.3125,
            "prob": 0.0
          },
          {
            "token": "all",
            "token_id": 543,
            "logprob": -13.375,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġcourse",
        "token_id": 3388,
        "selection_method": "sampling"
      }
    },
    {
      "step": 31,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm okay... so first of course",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm",
        "Ġokay",
        "...",
        "Ġso",
        "Ġfirst",
        "Ġof",
        "Ġcourse"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290,
        17339,
        1131,
        779,
        1176,
        315,
        3388
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": ",",
            "token_id": 11,
            "logprob": -0.6875,
            "prob": 0.5039
          },
          {
            "token": "ĠI",
            "token_id": 358,
            "logprob": -0.8125,
            "prob": 0.4453
          },
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -3.9375,
            "prob": 0.0195
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -4.0625,
            "prob": 0.0172
          },
          {
            "token": "Ġit",
            "token_id": 433,
            "logprob": -5.6875,
            "prob": 0.0034
          },
          {
            "token": "ĠChina",
            "token_id": 5734,
            "logprob": -5.9375,
            "prob": 0.0026
          },
          {
            "token": "Ġmy",
            "token_id": 856,
            "logprob": -6.1875,
            "prob": 0.0021
          },
          {
            "token": "Ġthey",
            "token_id": 814,
            "logprob": -6.3125,
            "prob": 0.0018
          },
          {
            "token": ".",
            "token_id": 13,
            "logprob": -6.9375,
            "prob": 0.001
          },
          {
            "token": "Ġ\"",
            "token_id": 330,
            "logprob": -7.1875,
            "prob": 0.0008
          }
        ]
      },
      "selected_token": {
        "token": "ĠI",
        "token_id": 358,
        "selection_method": "sampling"
      }
    },
    {
      "step": 32,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm okay... so first of course I",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm",
        "Ġokay",
        "...",
        "Ġso",
        "Ġfirst",
        "Ġof",
        "Ġcourse",
        "ĠI"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290,
        17339,
        1131,
        779,
        1176,
        315,
        3388,
        358
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġneed",
            "token_id": 1205,
            "logprob": -0.3828,
            "prob": 0.6836
          },
          {
            "token": "Ġknow",
            "token_id": 1440,
            "logprob": -1.5078,
            "prob": 0.2217
          },
          {
            "token": "Ġshould",
            "token_id": 1288,
            "logprob": -2.5156,
            "prob": 0.0815
          },
          {
            "token": "Ġremember",
            "token_id": 6227,
            "logprob": -5.125,
            "prob": 0.0059
          },
          {
            "token": "Ġhave",
            "token_id": 617,
            "logprob": -5.75,
            "prob": 0.0032
          },
          {
            "token": "'m",
            "token_id": 2846,
            "logprob": -6.125,
            "prob": 0.0022
          },
          {
            "token": "Ġrecognize",
            "token_id": 15641,
            "logprob": -6.5,
            "prob": 0.0015
          },
          {
            "token": "Ġcan",
            "token_id": 649,
            "logprob": -7.0,
            "prob": 0.0009
          },
          {
            "token": "Ġjust",
            "token_id": 1120,
            "logprob": -8.125,
            "prob": 0.0003
          },
          {
            "token": "Ġthink",
            "token_id": 1781,
            "logprob": -8.25,
            "prob": 0.0003
          }
        ]
      },
      "selected_token": {
        "token": "Ġneed",
        "token_id": 1205,
        "selection_method": "sampling"
      }
    },
    {
      "step": 33,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm okay... so first of course I need",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm",
        "Ġokay",
        "...",
        "Ġso",
        "Ġfirst",
        "Ġof",
        "Ġcourse",
        "ĠI",
        "Ġneed"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290,
        17339,
        1131,
        779,
        1176,
        315,
        3388,
        358,
        1205
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġto",
            "token_id": 311,
            "logprob": -0.0005,
            "prob": 1.0
          },
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -9.25,
            "prob": 0.0001
          },
          {
            "token": "Ġremember",
            "token_id": 6227,
            "logprob": -9.375,
            "prob": 0.0001
          },
          {
            "token": "Ġfigure",
            "token_id": 7216,
            "logprob": -9.375,
            "prob": 0.0001
          },
          {
            "token": "Ġrecall",
            "token_id": 19635,
            "logprob": -9.5,
            "prob": 0.0001
          },
          {
            "token": "Ġconfirm",
            "token_id": 7838,
            "logprob": -10.375,
            "prob": 0.0
          },
          {
            "token": "Ġthink",
            "token_id": 1781,
            "logprob": -10.5,
            "prob": 0.0
          },
          {
            "token": "Ġtranslate",
            "token_id": 15025,
            "logprob": -10.625,
            "prob": 0.0
          },
          {
            "token": "Ġmake",
            "token_id": 1304,
            "logprob": -10.75,
            "prob": 0.0
          },
          {
            "token": "Ġknow",
            "token_id": 1440,
            "logprob": -12.625,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġfigure",
        "token_id": 7216,
        "selection_method": "sampling"
      }
    },
    {
      "step": 34,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm okay... so first of course I need figure",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm",
        "Ġokay",
        "...",
        "Ġso",
        "Ġfirst",
        "Ġof",
        "Ġcourse",
        "ĠI",
        "Ġneed",
        "Ġfigure"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290,
        17339,
        1131,
        779,
        1176,
        315,
        3388,
        358,
        1205,
        7216
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġout",
            "token_id": 704,
            "logprob": -0.0034,
            "prob": 0.9961
          },
          {
            "token": "Ġit",
            "token_id": 433,
            "logprob": -6.375,
            "prob": 0.0017
          },
          {
            "token": "Ġthis",
            "token_id": 420,
            "logprob": -6.625,
            "prob": 0.0013
          },
          {
            "token": "Ġwhat",
            "token_id": 1148,
            "logprob": -8.875,
            "prob": 0.0001
          },
          {
            "token": "Ġthat",
            "token_id": 430,
            "logprob": -9.375,
            "prob": 0.0001
          },
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -9.875,
            "prob": 0.0001
          },
          {
            "token": "Ġif",
            "token_id": 422,
            "logprob": -10.75,
            "prob": 0.0
          },
          {
            "token": "Ġexactly",
            "token_id": 7041,
            "logprob": -11.875,
            "prob": 0.0
          },
          {
            "token": "Ġwhich",
            "token_id": 902,
            "logprob": -12.0625,
            "prob": 0.0
          },
          {
            "token": "out",
            "token_id": 412,
            "logprob": -12.125,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġout",
        "token_id": 704,
        "selection_method": "sampling"
      }
    },
    {
      "step": 35,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm okay... so first of course I need figure out",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm",
        "Ġokay",
        "...",
        "Ġso",
        "Ġfirst",
        "Ġof",
        "Ġcourse",
        "ĠI",
        "Ġneed",
        "Ġfigure",
        "Ġout"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290,
        17339,
        1131,
        779,
        1176,
        315,
        3388,
        358,
        1205,
        7216,
        704
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġwhat",
            "token_id": 1148,
            "logprob": -0.2539,
            "prob": 0.7773
          },
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -1.8828,
            "prob": 0.1523
          },
          {
            "token": "Ġwhich",
            "token_id": 902,
            "logprob": -3.25,
            "prob": 0.0386
          },
          {
            "token": "Ġexactly",
            "token_id": 7041,
            "logprob": -4.0,
            "prob": 0.0182
          },
          {
            "token": "Ġif",
            "token_id": 422,
            "logprob": -4.75,
            "prob": 0.0086
          },
          {
            "token": "Ġwhether",
            "token_id": 3508,
            "logprob": -5.75,
            "prob": 0.0032
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -7.25,
            "prob": 0.0007
          },
          {
            "token": "Ġwhere",
            "token_id": 1405,
            "logprob": -7.375,
            "prob": 0.0006
          },
          {
            "token": "Ġhow",
            "token_id": 1268,
            "logprob": -7.5,
            "prob": 0.0005
          },
          {
            "token": "Ġwho",
            "token_id": 889,
            "logprob": -8.0,
            "prob": 0.0003
          }
        ]
      },
      "selected_token": {
        "token": "Ġwhich",
        "token_id": 902,
        "selection_method": "sampling"
      }
    },
    {
      "step": 36,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm okay... so first of course I need figure out which",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm",
        "Ġokay",
        "...",
        "Ġso",
        "Ġfirst",
        "Ġof",
        "Ġcourse",
        "ĠI",
        "Ġneed",
        "Ġfigure",
        "Ġout",
        "Ġwhich"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290,
        17339,
        1131,
        779,
        1176,
        315,
        3388,
        358,
        1205,
        7216,
        704,
        902
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġcity",
            "token_id": 3363,
            "logprob": -0.6836,
            "prob": 0.5039
          },
          {
            "token": "Ġcountry",
            "token_id": 3224,
            "logprob": -1.1875,
            "prob": 0.3066
          },
          {
            "token": "Ġlanguage",
            "token_id": 4221,
            "logprob": -2.3125,
            "prob": 0.0991
          },
          {
            "token": "Ġcapital",
            "token_id": 6864,
            "logprob": -3.0625,
            "prob": 0.0469
          },
          {
            "token": "ĠChinese",
            "token_id": 8620,
            "logprob": -4.0625,
            "prob": 0.0172
          },
          {
            "token": "Ġquestion",
            "token_id": 3488,
            "logprob": -4.4375,
            "prob": 0.0118
          },
          {
            "token": "Ġcities",
            "token_id": 9919,
            "logprob": -5.3125,
            "prob": 0.0049
          },
          {
            "token": "Ġone",
            "token_id": 832,
            "logprob": -5.6875,
            "prob": 0.0034
          },
          {
            "token": "Ġcountries",
            "token_id": 5961,
            "logprob": -7.0,
            "prob": 0.0009
          },
          {
            "token": "Ġis",
            "token_id": 374,
            "logprob": -7.125,
            "prob": 0.0008
          }
        ]
      },
      "selected_token": {
        "token": "Ġone",
        "token_id": 832,
        "selection_method": "sampling"
      }
    },
    {
      "step": 37,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm okay... so first of course I need figure out which one",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm",
        "Ġokay",
        "...",
        "Ġso",
        "Ġfirst",
        "Ġof",
        "Ġcourse",
        "ĠI",
        "Ġneed",
        "Ġfigure",
        "Ġout",
        "Ġwhich",
        "Ġone"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290,
        17339,
        1131,
        779,
        1176,
        315,
        3388,
        358,
        1205,
        7216,
        704,
        902,
        832
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġis",
            "token_id": 374,
            "logprob": -0.5508,
            "prob": 0.5781
          },
          {
            "token": "Ġthey",
            "token_id": 814,
            "logprob": -1.8047,
            "prob": 0.165
          },
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -2.4219,
            "prob": 0.0884
          },
          {
            "token": "Ġit",
            "token_id": 433,
            "logprob": -2.6719,
            "prob": 0.0688
          },
          {
            "token": ".",
            "token_id": 13,
            "logprob": -2.9219,
            "prob": 0.0537
          },
          {
            "token": "Ġof",
            "token_id": 315,
            "logprob": -3.5469,
            "prob": 0.0287
          },
          {
            "token": "'s",
            "token_id": 596,
            "logprob": -5.4375,
            "prob": 0.0044
          },
          {
            "token": "Ġcity",
            "token_id": 3363,
            "logprob": -5.4375,
            "prob": 0.0044
          },
          {
            "token": "ĠChina",
            "token_id": 5734,
            "logprob": -6.0625,
            "prob": 0.0023
          },
          {
            "token": "Ġthat",
            "token_id": 430,
            "logprob": -6.1875,
            "prob": 0.0021
          }
        ]
      },
      "selected_token": {
        "token": "Ġof",
        "token_id": 315,
        "selection_method": "sampling"
      }
    },
    {
      "step": 38,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm okay... so first of course I need figure out which one of",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm",
        "Ġokay",
        "...",
        "Ġso",
        "Ġfirst",
        "Ġof",
        "Ġcourse",
        "ĠI",
        "Ġneed",
        "Ġfigure",
        "Ġout",
        "Ġwhich",
        "Ġone",
        "Ġof"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290,
        17339,
        1131,
        779,
        1176,
        315,
        3388,
        358,
        1205,
        7216,
        704,
        902,
        832,
        315
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -0.1196,
            "prob": 0.8867
          },
          {
            "token": "ĠChina",
            "token_id": 5734,
            "logprob": -3.125,
            "prob": 0.0442
          },
          {
            "token": "Ġthem",
            "token_id": 1124,
            "logprob": -3.625,
            "prob": 0.0267
          },
          {
            "token": "Ġthose",
            "token_id": 1884,
            "logprob": -4.375,
            "prob": 0.0126
          },
          {
            "token": "Ġthese",
            "token_id": 1521,
            "logprob": -4.875,
            "prob": 0.0077
          },
          {
            "token": "Ġseveral",
            "token_id": 3892,
            "logprob": -5.375,
            "prob": 0.0047
          },
          {
            "token": "Ġtheir",
            "token_id": 872,
            "logprob": -5.375,
            "prob": 0.0047
          },
          {
            "token": "Ġtwo",
            "token_id": 1403,
            "logprob": -5.625,
            "prob": 0.0036
          },
          {
            "token": "Ġits",
            "token_id": 1202,
            "logprob": -6.0625,
            "prob": 0.0023
          },
          {
            "token": "Ġcities",
            "token_id": 9919,
            "logprob": -6.75,
            "prob": 0.0012
          }
        ]
      },
      "selected_token": {
        "token": "Ġthese",
        "token_id": 1521,
        "selection_method": "sampling"
      }
    },
    {
      "step": 39,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm okay... so first of course I need figure out which one of these",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm",
        "Ġokay",
        "...",
        "Ġso",
        "Ġfirst",
        "Ġof",
        "Ġcourse",
        "ĠI",
        "Ġneed",
        "Ġfigure",
        "Ġout",
        "Ġwhich",
        "Ġone",
        "Ġof",
        "Ġthese"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290,
        17339,
        1131,
        779,
        1176,
        315,
        3388,
        358,
        1205,
        7216,
        704,
        902,
        832,
        315,
        1521
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġis",
            "token_id": 374,
            "logprob": -1.4609,
            "prob": 0.2314
          },
          {
            "token": "Ġcities",
            "token_id": 9919,
            "logprob": -1.7109,
            "prob": 0.1797
          },
          {
            "token": "Ġtwo",
            "token_id": 1403,
            "logprob": -2.2188,
            "prob": 0.1094
          },
          {
            "token": "Ġcountries",
            "token_id": 5961,
            "logprob": -2.4688,
            "prob": 0.085
          },
          {
            "token": "Ġquestions",
            "token_id": 4860,
            "logprob": -2.7188,
            "prob": 0.0664
          },
          {
            "token": "ĠI",
            "token_id": 358,
            "logprob": -2.8438,
            "prob": 0.0586
          },
          {
            "token": "Ġcapitals",
            "token_id": 94099,
            "logprob": -3.2188,
            "prob": 0.0403
          },
          {
            "token": "Ġcapital",
            "token_id": 6864,
            "logprob": -3.4062,
            "prob": 0.0334
          },
          {
            "token": "Ġthey",
            "token_id": 814,
            "logprob": -3.7812,
            "prob": 0.0229
          },
          {
            "token": "Ġthings",
            "token_id": 2574,
            "logprob": -4.3438,
            "prob": 0.0131
          }
        ]
      },
      "selected_token": {
        "token": "Ġcountries",
        "token_id": 5961,
        "selection_method": "sampling"
      }
    },
    {
      "step": 40,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm okay... so first of course I need figure out which one of these countries",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm",
        "Ġokay",
        "...",
        "Ġso",
        "Ġfirst",
        "Ġof",
        "Ġcourse",
        "ĠI",
        "Ġneed",
        "Ġfigure",
        "Ġout",
        "Ġwhich",
        "Ġone",
        "Ġof",
        "Ġthese",
        "Ġcountries"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290,
        17339,
        1131,
        779,
        1176,
        315,
        3388,
        358,
        1205,
        7216,
        704,
        902,
        832,
        315,
        1521,
        5961
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġcapitals",
            "token_id": 94099,
            "logprob": -0.8633,
            "prob": 0.4219
          },
          {
            "token": "Ġcapital",
            "token_id": 6864,
            "logprob": -1.8672,
            "prob": 0.1553
          },
          {
            "token": "'",
            "token_id": 6,
            "logprob": -2.1094,
            "prob": 0.1206
          },
          {
            "token": "Ġhas",
            "token_id": 706,
            "logprob": -2.1094,
            "prob": 0.1206
          },
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -2.6094,
            "prob": 0.0732
          },
          {
            "token": "Ġthey",
            "token_id": 814,
            "logprob": -3.3594,
            "prob": 0.0347
          },
          {
            "token": "Ġis",
            "token_id": 374,
            "logprob": -3.8594,
            "prob": 0.021
          },
          {
            "token": ",",
            "token_id": 11,
            "logprob": -4.875,
            "prob": 0.0077
          },
          {
            "token": "Ġcities",
            "token_id": 9919,
            "logprob": -5.3125,
            "prob": 0.005
          },
          {
            "token": "Ġmain",
            "token_id": 1925,
            "logprob": -5.625,
            "prob": 0.0036
          }
        ]
      },
      "selected_token": {
        "token": ",",
        "token_id": 11,
        "selection_method": "sampling"
      }
    },
    {
      "step": 41,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm okay... so first of course I need figure out which one of these countries,",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm",
        "Ġokay",
        "...",
        "Ġso",
        "Ġfirst",
        "Ġof",
        "Ġcourse",
        "ĠI",
        "Ġneed",
        "Ġfigure",
        "Ġout",
        "Ġwhich",
        "Ġone",
        "Ġof",
        "Ġthese",
        "Ġcountries",
        ","
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290,
        17339,
        1131,
        779,
        1176,
        315,
        3388,
        358,
        1205,
        7216,
        704,
        902,
        832,
        315,
        1521,
        5961,
        11
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "ĠChina",
            "token_id": 5734,
            "logprob": -0.4453,
            "prob": 0.6406
          },
          {
            "token": "Ġbut",
            "token_id": 719,
            "logprob": -2.5625,
            "prob": 0.0767
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -2.6875,
            "prob": 0.0674
          },
          {
            "token": "Ġwell",
            "token_id": 1664,
            "logprob": -2.8125,
            "prob": 0.0596
          },
          {
            "token": "Ġor",
            "token_id": 477,
            "logprob": -3.1875,
            "prob": 0.041
          },
          {
            "token": "Ġspecifically",
            "token_id": 11951,
            "logprob": -3.4375,
            "prob": 0.032
          },
          {
            "token": "Ġwhich",
            "token_id": 902,
            "logprob": -3.9375,
            "prob": 0.0194
          },
          {
            "token": "Ġprobably",
            "token_id": 4762,
            "logprob": -4.4375,
            "prob": 0.0117
          },
          {
            "token": "Ġif",
            "token_id": 422,
            "logprob": -4.6875,
            "prob": 0.0092
          },
          {
            "token": "Ġright",
            "token_id": 1314,
            "logprob": -4.8125,
            "prob": 0.0081
          }
        ]
      },
      "selected_token": {
        "token": "Ġif",
        "token_id": 422,
        "selection_method": "sampling"
      }
    },
    {
      "step": 42,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm okay... so first of course I need figure out which one of these countries, if",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm",
        "Ġokay",
        "...",
        "Ġso",
        "Ġfirst",
        "Ġof",
        "Ġcourse",
        "ĠI",
        "Ġneed",
        "Ġfigure",
        "Ġout",
        "Ġwhich",
        "Ġone",
        "Ġof",
        "Ġthese",
        "Ġcountries",
        ",",
        "Ġif"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290,
        17339,
        1131,
        779,
        1176,
        315,
        3388,
        358,
        1205,
        7216,
        704,
        902,
        832,
        315,
        1521,
        5961,
        11,
        422
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġany",
            "token_id": 904,
            "logprob": -0.166,
            "prob": 0.8477
          },
          {
            "token": "Ġit",
            "token_id": 433,
            "logprob": -2.5469,
            "prob": 0.0786
          },
          {
            "token": "ĠChina",
            "token_id": 5734,
            "logprob": -3.2969,
            "prob": 0.0371
          },
          {
            "token": "Ġnot",
            "token_id": 539,
            "logprob": -3.9219,
            "prob": 0.0199
          },
          {
            "token": "Ġthis",
            "token_id": 420,
            "logprob": -5.5312,
            "prob": 0.0039
          },
          {
            "token": "Ġthey",
            "token_id": 814,
            "logprob": -5.6562,
            "prob": 0.0035
          },
          {
            "token": "ĠI",
            "token_id": 358,
            "logprob": -5.6562,
            "prob": 0.0035
          },
          {
            "token": "Ġyou",
            "token_id": 499,
            "logprob": -6.6562,
            "prob": 0.0013
          },
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -6.7812,
            "prob": 0.0011
          },
          {
            "token": "Ġthere",
            "token_id": 1070,
            "logprob": -7.5312,
            "prob": 0.0005
          }
        ]
      },
      "selected_token": {
        "token": "ĠChina",
        "token_id": 5734,
        "selection_method": "sampling"
      }
    },
    {
      "step": 43,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm okay... so first of course I need figure out which one of these countries, if China",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm",
        "Ġokay",
        "...",
        "Ġso",
        "Ġfirst",
        "Ġof",
        "Ġcourse",
        "ĠI",
        "Ġneed",
        "Ġfigure",
        "Ġout",
        "Ġwhich",
        "Ġone",
        "Ġof",
        "Ġthese",
        "Ġcountries",
        ",",
        "Ġif",
        "ĠChina"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290,
        17339,
        1131,
        779,
        1176,
        315,
        3388,
        358,
        1205,
        7216,
        704,
        902,
        832,
        315,
        1521,
        5961,
        11,
        422,
        5734
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": ",",
            "token_id": 11,
            "logprob": -0.4824,
            "prob": 0.6172
          },
          {
            "token": "Ġis",
            "token_id": 374,
            "logprob": -1.3594,
            "prob": 0.2578
          },
          {
            "token": "'s",
            "token_id": 596,
            "logprob": -3.3594,
            "prob": 0.0349
          },
          {
            "token": "Ġhas",
            "token_id": 706,
            "logprob": -3.4844,
            "prob": 0.0308
          },
          {
            "token": "Ġeven",
            "token_id": 1524,
            "logprob": -3.8594,
            "prob": 0.0211
          },
          {
            "token": "Ġexists",
            "token_id": 6866,
            "logprob": -5.2188,
            "prob": 0.0053
          },
          {
            "token": "...",
            "token_id": 1131,
            "logprob": -5.3438,
            "prob": 0.0047
          },
          {
            "token": "Ġwas",
            "token_id": 574,
            "logprob": -5.7188,
            "prob": 0.0032
          },
          {
            "token": "Ġthen",
            "token_id": 1243,
            "logprob": -5.7188,
            "prob": 0.0032
          },
          {
            "token": "Ġisn",
            "token_id": 4536,
            "logprob": -5.8438,
            "prob": 0.0029
          }
        ]
      },
      "selected_token": {
        "token": "Ġexists",
        "token_id": 6866,
        "selection_method": "sampling"
      }
    },
    {
      "step": 44,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm okay... so first of course I need figure out which one of these countries, if China exists",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm",
        "Ġokay",
        "...",
        "Ġso",
        "Ġfirst",
        "Ġof",
        "Ġcourse",
        "ĠI",
        "Ġneed",
        "Ġfigure",
        "Ġout",
        "Ġwhich",
        "Ġone",
        "Ġof",
        "Ġthese",
        "Ġcountries",
        ",",
        "Ġif",
        "ĠChina",
        "Ġexists"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290,
        17339,
        1131,
        779,
        1176,
        315,
        3388,
        358,
        1205,
        7216,
        704,
        902,
        832,
        315,
        1521,
        5961,
        11,
        422,
        5734,
        6866
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": ",",
            "token_id": 11,
            "logprob": -0.3965,
            "prob": 0.6719
          },
          {
            "token": "Ġas",
            "token_id": 439,
            "logprob": -2.2656,
            "prob": 0.103
          },
          {
            "token": "?",
            "token_id": 30,
            "logprob": -2.8906,
            "prob": 0.0552
          },
          {
            "token": "...",
            "token_id": 1131,
            "logprob": -3.2656,
            "prob": 0.0378
          },
          {
            "token": ".",
            "token_id": 13,
            "logprob": -3.6406,
            "prob": 0.0261
          },
          {
            "token": "Ġand",
            "token_id": 323,
            "logprob": -3.6406,
            "prob": 0.0261
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -3.7656,
            "prob": 0.0229
          },
          {
            "token": "Ġ(",
            "token_id": 320,
            "logprob": -3.8906,
            "prob": 0.0203
          },
          {
            "token": "Ġor",
            "token_id": 477,
            "logprob": -5.0312,
            "prob": 0.0066
          },
          {
            "token": "Ġat",
            "token_id": 520,
            "logprob": -5.4062,
            "prob": 0.0045
          }
        ]
      },
      "selected_token": {
        "token": ",",
        "token_id": 11,
        "selection_method": "sampling"
      }
    },
    {
      "step": 45,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm okay... so first of course I need figure out which one of these countries, if China exists,",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm",
        "Ġokay",
        "...",
        "Ġso",
        "Ġfirst",
        "Ġof",
        "Ġcourse",
        "ĠI",
        "Ġneed",
        "Ġfigure",
        "Ġout",
        "Ġwhich",
        "Ġone",
        "Ġof",
        "Ġthese",
        "Ġcountries",
        ",",
        "Ġif",
        "ĠChina",
        "Ġexists",
        ","
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290,
        17339,
        1131,
        779,
        1176,
        315,
        3388,
        358,
        1205,
        7216,
        704,
        902,
        832,
        315,
        1521,
        5961,
        11,
        422,
        5734,
        6866,
        11
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġwhat",
            "token_id": 1148,
            "logprob": -0.7148,
            "prob": 0.4902
          },
          {
            "token": "Ġhas",
            "token_id": 706,
            "logprob": -1.4609,
            "prob": 0.2314
          },
          {
            "token": "Ġwhich",
            "token_id": 902,
            "logprob": -2.3438,
            "prob": 0.0967
          },
          {
            "token": "Ġand",
            "token_id": 323,
            "logprob": -3.2188,
            "prob": 0.0403
          },
          {
            "token": "Ġits",
            "token_id": 1202,
            "logprob": -3.3438,
            "prob": 0.0354
          },
          {
            "token": "Ġbut",
            "token_id": 719,
            "logprob": -3.3438,
            "prob": 0.0354
          },
          {
            "token": "Ġthen",
            "token_id": 1243,
            "logprob": -3.5938,
            "prob": 0.0277
          },
          {
            "token": "Ġwhere",
            "token_id": 1405,
            "logprob": -4.3438,
            "prob": 0.0131
          },
          {
            "token": "Ġor",
            "token_id": 477,
            "logprob": -4.7188,
            "prob": 0.009
          },
          {
            "token": "Ġtheir",
            "token_id": 872,
            "logprob": -5.0938,
            "prob": 0.0062
          }
        ]
      },
      "selected_token": {
        "token": "Ġor",
        "token_id": 477,
        "selection_method": "sampling"
      }
    },
    {
      "step": 46,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm okay... so first of course I need figure out which one of these countries, if China exists, or",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm",
        "Ġokay",
        "...",
        "Ġso",
        "Ġfirst",
        "Ġof",
        "Ġcourse",
        "ĠI",
        "Ġneed",
        "Ġfigure",
        "Ġout",
        "Ġwhich",
        "Ġone",
        "Ġof",
        "Ġthese",
        "Ġcountries",
        ",",
        "Ġif",
        "ĠChina",
        "Ġexists",
        ",",
        "Ġor"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290,
        17339,
        1131,
        779,
        1176,
        315,
        3388,
        358,
        1205,
        7216,
        704,
        902,
        832,
        315,
        1521,
        5961,
        11,
        422,
        5734,
        6866,
        11,
        477
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġmaybe",
            "token_id": 7344,
            "logprob": -1.5078,
            "prob": 0.2207
          },
          {
            "token": "Ġif",
            "token_id": 422,
            "logprob": -1.7578,
            "prob": 0.1719
          },
          {
            "token": "Ġis",
            "token_id": 374,
            "logprob": -1.8828,
            "prob": 0.1514
          },
          {
            "token": "Ġwhatever",
            "token_id": 8996,
            "logprob": -2.7656,
            "prob": 0.0635
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -3.0156,
            "prob": 0.0493
          },
          {
            "token": "Ġdoes",
            "token_id": 1587,
            "logprob": -3.0156,
            "prob": 0.0493
          },
          {
            "token": "Ġhas",
            "token_id": 706,
            "logprob": -3.3906,
            "prob": 0.0339
          },
          {
            "token": "Ġwhat",
            "token_id": 1148,
            "logprob": -3.6406,
            "prob": 0.0264
          },
          {
            "token": "Ġeven",
            "token_id": 1524,
            "logprob": -3.6406,
            "prob": 0.0264
          },
          {
            "token": "Ġwas",
            "token_id": 574,
            "logprob": -3.7656,
            "prob": 0.0233
          }
        ]
      },
      "selected_token": {
        "token": "Ġdoes",
        "token_id": 1587,
        "selection_method": "sampling"
      }
    },
    {
      "step": 47,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm okay... so first of course I need figure out which one of these countries, if China exists, or does",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm",
        "Ġokay",
        "...",
        "Ġso",
        "Ġfirst",
        "Ġof",
        "Ġcourse",
        "ĠI",
        "Ġneed",
        "Ġfigure",
        "Ġout",
        "Ġwhich",
        "Ġone",
        "Ġof",
        "Ġthese",
        "Ġcountries",
        ",",
        "Ġif",
        "ĠChina",
        "Ġexists",
        ",",
        "Ġor",
        "Ġdoes"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290,
        17339,
        1131,
        779,
        1176,
        315,
        3388,
        358,
        1205,
        7216,
        704,
        902,
        832,
        315,
        1521,
        5961,
        11,
        422,
        5734,
        6866,
        11,
        477,
        1587
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġit",
            "token_id": 433,
            "logprob": -0.1855,
            "prob": 0.832
          },
          {
            "token": "ĠChina",
            "token_id": 5734,
            "logprob": -1.8125,
            "prob": 0.1631
          },
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -5.9375,
            "prob": 0.0026
          },
          {
            "token": "Ġthat",
            "token_id": 430,
            "logprob": -6.5,
            "prob": 0.0015
          },
          {
            "token": "Ġits",
            "token_id": 1202,
            "logprob": -8.0,
            "prob": 0.0003
          },
          {
            "token": "?",
            "token_id": 30,
            "logprob": -8.375,
            "prob": 0.0002
          },
          {
            "token": "Ġ\"",
            "token_id": 330,
            "logprob": -8.4375,
            "prob": 0.0002
          },
          {
            "token": "Ġthis",
            "token_id": 420,
            "logprob": -8.4375,
            "prob": 0.0002
          },
          {
            "token": "ĠChinese",
            "token_id": 8620,
            "logprob": -9.3125,
            "prob": 0.0001
          },
          {
            "token": "Ġanyone",
            "token_id": 5606,
            "logprob": -9.4375,
            "prob": 0.0001
          }
        ]
      },
      "selected_token": {
        "token": "Ġits",
        "token_id": 1202,
        "selection_method": "sampling"
      }
    },
    {
      "step": 48,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm okay... so first of course I need figure out which one of these countries, if China exists, or does its",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm",
        "Ġokay",
        "...",
        "Ġso",
        "Ġfirst",
        "Ġof",
        "Ġcourse",
        "ĠI",
        "Ġneed",
        "Ġfigure",
        "Ġout",
        "Ġwhich",
        "Ġone",
        "Ġof",
        "Ġthese",
        "Ġcountries",
        ",",
        "Ġif",
        "ĠChina",
        "Ġexists",
        ",",
        "Ġor",
        "Ġdoes",
        "Ġits"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290,
        17339,
        1131,
        779,
        1176,
        315,
        3388,
        358,
        1205,
        7216,
        704,
        902,
        832,
        315,
        1521,
        5961,
        11,
        422,
        5734,
        6866,
        11,
        477,
        1587,
        1202
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġcapital",
            "token_id": 6864,
            "logprob": -0.0801,
            "prob": 0.9219
          },
          {
            "token": "Ġexistence",
            "token_id": 14209,
            "logprob": -4.2812,
            "prob": 0.014
          },
          {
            "token": "Ġname",
            "token_id": 836,
            "logprob": -4.4062,
            "prob": 0.0124
          },
          {
            "token": "Ġexist",
            "token_id": 3073,
            "logprob": -4.8438,
            "prob": 0.008
          },
          {
            "token": "?",
            "token_id": 30,
            "logprob": -5.0312,
            "prob": 0.0066
          },
          {
            "token": "Ġgovernment",
            "token_id": 3109,
            "logprob": -5.4688,
            "prob": 0.0043
          },
          {
            "token": "Ġown",
            "token_id": 1866,
            "logprob": -5.7812,
            "prob": 0.0031
          },
          {
            "token": "Ġcountry",
            "token_id": 3224,
            "logprob": -5.9062,
            "prob": 0.0028
          },
          {
            "token": "Ġstatus",
            "token_id": 2704,
            "logprob": -6.1562,
            "prob": 0.0022
          },
          {
            "token": "Ġfirst",
            "token_id": 1176,
            "logprob": -6.2812,
            "prob": 0.0019
          }
        ]
      },
      "selected_token": {
        "token": "Ġown",
        "token_id": 1866,
        "selection_method": "sampling"
      }
    },
    {
      "step": 49,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm okay... so first of course I need figure out which one of these countries, if China exists, or does its own",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm",
        "Ġokay",
        "...",
        "Ġso",
        "Ġfirst",
        "Ġof",
        "Ġcourse",
        "ĠI",
        "Ġneed",
        "Ġfigure",
        "Ġout",
        "Ġwhich",
        "Ġone",
        "Ġof",
        "Ġthese",
        "Ġcountries",
        ",",
        "Ġif",
        "ĠChina",
        "Ġexists",
        ",",
        "Ġor",
        "Ġdoes",
        "Ġits",
        "Ġown"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290,
        17339,
        1131,
        779,
        1176,
        315,
        3388,
        358,
        1205,
        7216,
        704,
        902,
        832,
        315,
        1521,
        5961,
        11,
        422,
        5734,
        6866,
        11,
        477,
        1587,
        1202,
        1866
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġcapital",
            "token_id": 6864,
            "logprob": -0.0625,
            "prob": 0.9375
          },
          {
            "token": "Ġcountry",
            "token_id": 3224,
            "logprob": -4.4375,
            "prob": 0.0118
          },
          {
            "token": "Ġexistence",
            "token_id": 14209,
            "logprob": -4.8125,
            "prob": 0.0081
          },
          {
            "token": "Ġgovernment",
            "token_id": 3109,
            "logprob": -4.875,
            "prob": 0.0076
          },
          {
            "token": "Ġname",
            "token_id": 836,
            "logprob": -5.125,
            "prob": 0.006
          },
          {
            "token": "?",
            "token_id": 30,
            "logprob": -5.5,
            "prob": 0.0041
          },
          {
            "token": ",",
            "token_id": 11,
            "logprob": -5.6875,
            "prob": 0.0034
          },
          {
            "token": ".",
            "token_id": 13,
            "logprob": -5.75,
            "prob": 0.0032
          },
          {
            "token": "Ġexist",
            "token_id": 3073,
            "logprob": -6.3125,
            "prob": 0.0018
          },
          {
            "token": "Ġquestion",
            "token_id": 3488,
            "logprob": -6.875,
            "prob": 0.001
          }
        ]
      },
      "selected_token": {
        "token": "Ġquestion",
        "token_id": 3488,
        "selection_method": "sampling"
      }
    },
    {
      "step": 50,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm okay... so first of course I need figure out which one of these countries, if China exists, or does its own question",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm",
        "Ġokay",
        "...",
        "Ġso",
        "Ġfirst",
        "Ġof",
        "Ġcourse",
        "ĠI",
        "Ġneed",
        "Ġfigure",
        "Ġout",
        "Ġwhich",
        "Ġone",
        "Ġof",
        "Ġthese",
        "Ġcountries",
        ",",
        "Ġif",
        "ĠChina",
        "Ġexists",
        ",",
        "Ġor",
        "Ġdoes",
        "Ġits",
        "Ġown",
        "Ġquestion"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290,
        17339,
        1131,
        779,
        1176,
        315,
        3388,
        358,
        1205,
        7216,
        704,
        902,
        832,
        315,
        1521,
        5961,
        11,
        422,
        5734,
        6866,
        11,
        477,
        1587,
        1202,
        1866,
        3488
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": ".",
            "token_id": 13,
            "logprob": -1.8906,
            "prob": 0.1514
          },
          {
            "token": "?",
            "token_id": 30,
            "logprob": -1.8906,
            "prob": 0.1514
          },
          {
            "token": ",",
            "token_id": 11,
            "logprob": -2.1406,
            "prob": 0.1177
          },
          {
            "token": "...",
            "token_id": 1131,
            "logprob": -2.6406,
            "prob": 0.0713
          },
          {
            "token": "Ġabout",
            "token_id": 922,
            "logprob": -2.7656,
            "prob": 0.063
          },
          {
            "token": "Ġis",
            "token_id": 374,
            "logprob": -2.8906,
            "prob": 0.0557
          },
          {
            "token": "Ġeven",
            "token_id": 1524,
            "logprob": -3.2656,
            "prob": 0.0381
          },
          {
            "token": "Ġexist",
            "token_id": 3073,
            "logprob": -3.2656,
            "prob": 0.0381
          },
          {
            "token": "Ġmake",
            "token_id": 1304,
            "logprob": -3.3906,
            "prob": 0.0337
          },
          {
            "token": ".\n\n",
            "token_id": 382,
            "logprob": -3.6406,
            "prob": 0.0262
          }
        ]
      },
      "selected_token": {
        "token": "Ġis",
        "token_id": 374,
        "selection_method": "sampling"
      }
    },
    {
      "step": 51,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm okay... so first of course I need figure out which one of these countries, if China exists, or does its own question is",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm",
        "Ġokay",
        "...",
        "Ġso",
        "Ġfirst",
        "Ġof",
        "Ġcourse",
        "ĠI",
        "Ġneed",
        "Ġfigure",
        "Ġout",
        "Ġwhich",
        "Ġone",
        "Ġof",
        "Ġthese",
        "Ġcountries",
        ",",
        "Ġif",
        "ĠChina",
        "Ġexists",
        ",",
        "Ġor",
        "Ġdoes",
        "Ġits",
        "Ġown",
        "Ġquestion",
        "Ġis"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290,
        17339,
        1131,
        779,
        1176,
        315,
        3388,
        358,
        1205,
        7216,
        704,
        902,
        832,
        315,
        1521,
        5961,
        11,
        422,
        5734,
        6866,
        11,
        477,
        1587,
        1202,
        1866,
        3488,
        374
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġabout",
            "token_id": 922,
            "logprob": -1.3203,
            "prob": 0.2676
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -1.4453,
            "prob": 0.2363
          },
          {
            "token": "Ġcorrect",
            "token_id": 4495,
            "logprob": -1.9453,
            "prob": 0.1436
          },
          {
            "token": "Ġa",
            "token_id": 264,
            "logprob": -3.0625,
            "prob": 0.0466
          },
          {
            "token": "Ġph",
            "token_id": 1343,
            "logprob": -3.8125,
            "prob": 0.022
          },
          {
            "token": "Ġeven",
            "token_id": 1524,
            "logprob": -3.8125,
            "prob": 0.022
          },
          {
            "token": "Ġright",
            "token_id": 1314,
            "logprob": -3.9375,
            "prob": 0.0194
          },
          {
            "token": "Ġwrong",
            "token_id": 5076,
            "logprob": -3.9375,
            "prob": 0.0194
          },
          {
            "token": "Ġbeing",
            "token_id": 1694,
            "logprob": -3.9375,
            "prob": 0.0194
          },
          {
            "token": "Ġwhether",
            "token_id": 3508,
            "logprob": -4.3125,
            "prob": 0.0133
          }
        ]
      },
      "selected_token": {
        "token": "Ġin",
        "token_id": 304,
        "selection_method": "sampling"
      }
    },
    {
      "step": 52,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm okay... so first of course I need figure out which one of these countries, if China exists, or does its own question is in",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm",
        "Ġokay",
        "...",
        "Ġso",
        "Ġfirst",
        "Ġof",
        "Ġcourse",
        "ĠI",
        "Ġneed",
        "Ġfigure",
        "Ġout",
        "Ġwhich",
        "Ġone",
        "Ġof",
        "Ġthese",
        "Ġcountries",
        ",",
        "Ġif",
        "ĠChina",
        "Ġexists",
        ",",
        "Ġor",
        "Ġdoes",
        "Ġits",
        "Ġown",
        "Ġquestion",
        "Ġis",
        "Ġin"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290,
        17339,
        1131,
        779,
        1176,
        315,
        3388,
        358,
        1205,
        7216,
        704,
        902,
        832,
        315,
        1521,
        5961,
        11,
        422,
        5734,
        6866,
        11,
        477,
        1587,
        1202,
        1866,
        3488,
        374,
        304
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "ĠChinese",
            "token_id": 8620,
            "logprob": -0.168,
            "prob": 0.8438
          },
          {
            "token": "ĠEnglish",
            "token_id": 6498,
            "logprob": -3.0469,
            "prob": 0.0476
          },
          {
            "token": "Ġanother",
            "token_id": 2500,
            "logprob": -3.0469,
            "prob": 0.0476
          },
          {
            "token": "Ġa",
            "token_id": 264,
            "logprob": -4.0312,
            "prob": 0.0176
          },
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -4.1562,
            "prob": 0.0155
          },
          {
            "token": "Ġquestion",
            "token_id": 3488,
            "logprob": -5.0312,
            "prob": 0.0064
          },
          {
            "token": "Ġdoubt",
            "token_id": 10712,
            "logprob": -5.5938,
            "prob": 0.0037
          },
          {
            "token": "ĠMandarin",
            "token_id": 83871,
            "logprob": -5.9688,
            "prob": 0.0025
          },
          {
            "token": "Ġits",
            "token_id": 1202,
            "logprob": -6.0938,
            "prob": 0.0022
          },
          {
            "token": "Ġerror",
            "token_id": 1493,
            "logprob": -6.1562,
            "prob": 0.0021
          }
        ]
      },
      "selected_token": {
        "token": "Ġquestion",
        "token_id": 3488,
        "selection_method": "sampling"
      }
    },
    {
      "step": 53,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm okay... so first of course I need figure out which one of these countries, if China exists, or does its own question is in question",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm",
        "Ġokay",
        "...",
        "Ġso",
        "Ġfirst",
        "Ġof",
        "Ġcourse",
        "ĠI",
        "Ġneed",
        "Ġfigure",
        "Ġout",
        "Ġwhich",
        "Ġone",
        "Ġof",
        "Ġthese",
        "Ġcountries",
        ",",
        "Ġif",
        "ĠChina",
        "Ġexists",
        ",",
        "Ġor",
        "Ġdoes",
        "Ġits",
        "Ġown",
        "Ġquestion",
        "Ġis",
        "Ġin",
        "Ġquestion"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290,
        17339,
        1131,
        779,
        1176,
        315,
        3388,
        358,
        1205,
        7216,
        704,
        902,
        832,
        315,
        1521,
        5961,
        11,
        422,
        5734,
        6866,
        11,
        477,
        1587,
        1202,
        1866,
        3488,
        374,
        304,
        3488
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "?",
            "token_id": 30,
            "logprob": -0.6172,
            "prob": 0.5391
          },
          {
            "token": ".",
            "token_id": 13,
            "logprob": -2.1094,
            "prob": 0.1206
          },
          {
            "token": "?\n\n",
            "token_id": 1980,
            "logprob": -2.2344,
            "prob": 0.1064
          },
          {
            "token": ",",
            "token_id": 11,
            "logprob": -2.2344,
            "prob": 0.1064
          },
          {
            "token": ".\n\n",
            "token_id": 382,
            "logprob": -3.1094,
            "prob": 0.0442
          },
          {
            "token": "Ġhere",
            "token_id": 1618,
            "logprob": -3.1094,
            "prob": 0.0442
          },
          {
            "token": "...",
            "token_id": 1131,
            "logprob": -4.125,
            "prob": 0.0164
          },
          {
            "token": "Ġwhether",
            "token_id": 3508,
            "logprob": -4.875,
            "prob": 0.0077
          },
          {
            "token": "Ġitself",
            "token_id": 5196,
            "logprob": -6.25,
            "prob": 0.0019
          },
          {
            "token": "Ġmaybe",
            "token_id": 7344,
            "logprob": -6.25,
            "prob": 0.0019
          }
        ]
      },
      "selected_token": {
        "token": "Ġhere",
        "token_id": 1618,
        "selection_method": "sampling"
      }
    },
    {
      "step": 54,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm okay... so first of course I need figure out which one of these countries, if China exists, or does its own question is in question here",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm",
        "Ġokay",
        "...",
        "Ġso",
        "Ġfirst",
        "Ġof",
        "Ġcourse",
        "ĠI",
        "Ġneed",
        "Ġfigure",
        "Ġout",
        "Ġwhich",
        "Ġone",
        "Ġof",
        "Ġthese",
        "Ġcountries",
        ",",
        "Ġif",
        "ĠChina",
        "Ġexists",
        ",",
        "Ġor",
        "Ġdoes",
        "Ġits",
        "Ġown",
        "Ġquestion",
        "Ġis",
        "Ġin",
        "Ġquestion",
        "Ġhere"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290,
        17339,
        1131,
        779,
        1176,
        315,
        3388,
        358,
        1205,
        7216,
        704,
        902,
        832,
        315,
        1521,
        5961,
        11,
        422,
        5734,
        6866,
        11,
        477,
        1587,
        1202,
        1866,
        3488,
        374,
        304,
        3488,
        1618
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "?",
            "token_id": 30,
            "logprob": -0.7891,
            "prob": 0.4551
          },
          {
            "token": ".",
            "token_id": 13,
            "logprob": -1.4141,
            "prob": 0.2432
          },
          {
            "token": "?\n\n",
            "token_id": 1980,
            "logprob": -1.9141,
            "prob": 0.1475
          },
          {
            "token": ".\n\n",
            "token_id": 382,
            "logprob": -2.5469,
            "prob": 0.0791
          },
          {
            "token": ",",
            "token_id": 11,
            "logprob": -2.9219,
            "prob": 0.0542
          },
          {
            "token": "...",
            "token_id": 1131,
            "logprob": -4.0312,
            "prob": 0.0176
          },
          {
            "token": "...\n\n",
            "token_id": 2195,
            "logprob": -6.2812,
            "prob": 0.0019
          },
          {
            "token": "Ġbut",
            "token_id": 719,
            "logprob": -7.2812,
            "prob": 0.0007
          },
          {
            "token": "Ġwhether",
            "token_id": 3508,
            "logprob": -8.5625,
            "prob": 0.0002
          },
          {
            "token": "Ġmaybe",
            "token_id": 7344,
            "logprob": -8.6875,
            "prob": 0.0002
          }
        ]
      },
      "selected_token": {
        "token": "?\n\n",
        "token_id": 1980,
        "selection_method": "sampling"
      }
    },
    {
      "step": 55,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm okay... so first of course I need figure out which one of these countries, if China exists, or does its own question is in question here?\n\n",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm",
        "Ġokay",
        "...",
        "Ġso",
        "Ġfirst",
        "Ġof",
        "Ġcourse",
        "ĠI",
        "Ġneed",
        "Ġfigure",
        "Ġout",
        "Ġwhich",
        "Ġone",
        "Ġof",
        "Ġthese",
        "Ġcountries",
        ",",
        "Ġif",
        "ĠChina",
        "Ġexists",
        ",",
        "Ġor",
        "Ġdoes",
        "Ġits",
        "Ġown",
        "Ġquestion",
        "Ġis",
        "Ġin",
        "Ġquestion",
        "Ġhere",
        "?\n\n"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290,
        17339,
        1131,
        779,
        1176,
        315,
        3388,
        358,
        1205,
        7216,
        704,
        902,
        832,
        315,
        1521,
        5961,
        11,
        422,
        5734,
        6866,
        11,
        477,
        1587,
        1202,
        1866,
        3488,
        374,
        304,
        3488,
        1618,
        1980
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Wait",
            "token_id": 14524,
            "logprob": -0.0781,
            "prob": 0.9258
          },
          {
            "token": "Well",
            "token_id": 11649,
            "logprob": -2.9531,
            "prob": 0.0522
          },
          {
            "token": "First",
            "token_id": 5451,
            "logprob": -4.4688,
            "prob": 0.0117
          },
          {
            "token": "Okay",
            "token_id": 33413,
            "logprob": -5.5938,
            "prob": 0.0038
          },
          {
            "token": "Alright",
            "token_id": 72586,
            "logprob": -5.8438,
            "prob": 0.0029
          },
          {
            "token": "But",
            "token_id": 4071,
            "logprob": -6.5938,
            "prob": 0.0014
          },
          {
            "token": "Let",
            "token_id": 10267,
            "logprob": -6.7188,
            "prob": 0.0012
          },
          {
            "token": "China",
            "token_id": 23078,
            "logprob": -7.8438,
            "prob": 0.0004
          },
          {
            "token": "Ass",
            "token_id": 5733,
            "logprob": -8.1875,
            "prob": 0.0003
          },
          {
            "token": "Hold",
            "token_id": 48527,
            "logprob": -8.25,
            "prob": 0.0003
          }
        ]
      },
      "selected_token": {
        "token": "Wait",
        "token_id": 14524,
        "selection_method": "sampling"
      }
    },
    {
      "step": 56,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm okay... so first of course I need figure out which one of these countries, if China exists, or does its own question is in question here?\n\nWait",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm",
        "Ġokay",
        "...",
        "Ġso",
        "Ġfirst",
        "Ġof",
        "Ġcourse",
        "ĠI",
        "Ġneed",
        "Ġfigure",
        "Ġout",
        "Ġwhich",
        "Ġone",
        "Ġof",
        "Ġthese",
        "Ġcountries",
        ",",
        "Ġif",
        "ĠChina",
        "Ġexists",
        ",",
        "Ġor",
        "Ġdoes",
        "Ġits",
        "Ġown",
        "Ġquestion",
        "Ġis",
        "Ġin",
        "Ġquestion",
        "Ġhere",
        "?\n\n",
        "Wait"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290,
        17339,
        1131,
        779,
        1176,
        315,
        3388,
        358,
        1205,
        7216,
        704,
        902,
        832,
        315,
        1521,
        5961,
        11,
        422,
        5734,
        6866,
        11,
        477,
        1587,
        1202,
        1866,
        3488,
        374,
        304,
        3488,
        1618,
        1980,
        14524
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": ",",
            "token_id": 11,
            "logprob": -0.0349,
            "prob": 0.9648
          },
          {
            "token": "Ġa",
            "token_id": 264,
            "logprob": -4.1562,
            "prob": 0.0156
          },
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -5.2812,
            "prob": 0.0051
          },
          {
            "token": "ĠChina",
            "token_id": 5734,
            "logprob": -5.4062,
            "prob": 0.0045
          },
          {
            "token": "Ġno",
            "token_id": 912,
            "logprob": -5.7812,
            "prob": 0.0031
          },
          {
            "token": "Ġhold",
            "token_id": 3412,
            "logprob": -6.4062,
            "prob": 0.0016
          },
          {
            "token": "Ġfirst",
            "token_id": 1176,
            "logprob": -6.7812,
            "prob": 0.0011
          },
          {
            "token": "Ġbut",
            "token_id": 719,
            "logprob": -7.1562,
            "prob": 0.0008
          },
          {
            "token": "Ġwait",
            "token_id": 3868,
            "logprob": -7.1562,
            "prob": 0.0008
          },
          {
            "token": "Ġactually",
            "token_id": 3604,
            "logprob": -7.2812,
            "prob": 0.0007
          }
        ]
      },
      "selected_token": {
        "token": ",",
        "token_id": 11,
        "selection_method": "sampling"
      }
    },
    {
      "step": 57,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm okay... so first of course I need figure out which one of these countries, if China exists, or does its own question is in question here?\n\nWait,",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm",
        "Ġokay",
        "...",
        "Ġso",
        "Ġfirst",
        "Ġof",
        "Ġcourse",
        "ĠI",
        "Ġneed",
        "Ġfigure",
        "Ġout",
        "Ġwhich",
        "Ġone",
        "Ġof",
        "Ġthese",
        "Ġcountries",
        ",",
        "Ġif",
        "ĠChina",
        "Ġexists",
        ",",
        "Ġor",
        "Ġdoes",
        "Ġits",
        "Ġown",
        "Ġquestion",
        "Ġis",
        "Ġin",
        "Ġquestion",
        "Ġhere",
        "?\n\n",
        "Wait",
        ","
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290,
        17339,
        1131,
        779,
        1176,
        315,
        3388,
        358,
        1205,
        7216,
        704,
        902,
        832,
        315,
        1521,
        5961,
        11,
        422,
        5734,
        6866,
        11,
        477,
        1587,
        1202,
        1866,
        3488,
        374,
        304,
        3488,
        1618,
        1980,
        14524,
        11
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "ĠChina",
            "token_id": 5734,
            "logprob": -0.8125,
            "prob": 0.4434
          },
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -1.8125,
            "prob": 0.1631
          },
          {
            "token": "Ġwell",
            "token_id": 1664,
            "logprob": -2.6875,
            "prob": 0.0684
          },
          {
            "token": "Ġbut",
            "token_id": 719,
            "logprob": -2.6875,
            "prob": 0.0684
          },
          {
            "token": "Ġhold",
            "token_id": 3412,
            "logprob": -3.0625,
            "prob": 0.0469
          },
          {
            "token": "Ġfirst",
            "token_id": 1176,
            "logprob": -3.0625,
            "prob": 0.0469
          },
          {
            "token": "Ġactually",
            "token_id": 3604,
            "logprob": -3.0625,
            "prob": 0.0469
          },
          {
            "token": "Ġmaybe",
            "token_id": 7344,
            "logprob": -3.6875,
            "prob": 0.025
          },
          {
            "token": "Ġassuming",
            "token_id": 26619,
            "logprob": -4.0625,
            "prob": 0.0172
          },
          {
            "token": "Ġno",
            "token_id": 912,
            "logprob": -4.1875,
            "prob": 0.0152
          }
        ]
      },
      "selected_token": {
        "token": "Ġfirst",
        "token_id": 1176,
        "selection_method": "sampling"
      }
    },
    {
      "step": 58,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm okay... so first of course I need figure out which one of these countries, if China exists, or does its own question is in question here?\n\nWait, first",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm",
        "Ġokay",
        "...",
        "Ġso",
        "Ġfirst",
        "Ġof",
        "Ġcourse",
        "ĠI",
        "Ġneed",
        "Ġfigure",
        "Ġout",
        "Ġwhich",
        "Ġone",
        "Ġof",
        "Ġthese",
        "Ġcountries",
        ",",
        "Ġif",
        "ĠChina",
        "Ġexists",
        ",",
        "Ġor",
        "Ġdoes",
        "Ġits",
        "Ġown",
        "Ġquestion",
        "Ġis",
        "Ġin",
        "Ġquestion",
        "Ġhere",
        "?\n\n",
        "Wait",
        ",",
        "Ġfirst"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290,
        17339,
        1131,
        779,
        1176,
        315,
        3388,
        358,
        1205,
        7216,
        704,
        902,
        832,
        315,
        1521,
        5961,
        11,
        422,
        5734,
        6866,
        11,
        477,
        1587,
        1202,
        1866,
        3488,
        374,
        304,
        3488,
        1618,
        1980,
        14524,
        11,
        1176
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": ",",
            "token_id": 11,
            "logprob": -0.8359,
            "prob": 0.4336
          },
          {
            "token": "Ġthing",
            "token_id": 3245,
            "logprob": -1.7109,
            "prob": 0.1807
          },
          {
            "token": "Ġoff",
            "token_id": 1022,
            "logprob": -2.4531,
            "prob": 0.0854
          },
          {
            "token": "Ġstep",
            "token_id": 3094,
            "logprob": -2.5781,
            "prob": 0.0752
          },
          {
            "token": "Ġthings",
            "token_id": 2574,
            "logprob": -2.7031,
            "prob": 0.0664
          },
          {
            "token": "Ġof",
            "token_id": 315,
            "logprob": -2.7031,
            "prob": 0.0664
          },
          {
            "token": "ĠI",
            "token_id": 358,
            "logprob": -3.9531,
            "prob": 0.019
          },
          {
            "token": "Ġassumption",
            "token_id": 25329,
            "logprob": -4.2188,
            "prob": 0.0148
          },
          {
            "token": "Ġconfirm",
            "token_id": 7838,
            "logprob": -4.5938,
            "prob": 0.0102
          },
          {
            "token": "Ġlet",
            "token_id": 1095,
            "logprob": -5.0938,
            "prob": 0.0062
          }
        ]
      },
      "selected_token": {
        "token": "Ġconfirm",
        "token_id": 7838,
        "selection_method": "sampling"
      }
    },
    {
      "step": 59,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm okay... so first of course I need figure out which one of these countries, if China exists, or does its own question is in question here?\n\nWait, first confirm",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm",
        "Ġokay",
        "...",
        "Ġso",
        "Ġfirst",
        "Ġof",
        "Ġcourse",
        "ĠI",
        "Ġneed",
        "Ġfigure",
        "Ġout",
        "Ġwhich",
        "Ġone",
        "Ġof",
        "Ġthese",
        "Ġcountries",
        ",",
        "Ġif",
        "ĠChina",
        "Ġexists",
        ",",
        "Ġor",
        "Ġdoes",
        "Ġits",
        "Ġown",
        "Ġquestion",
        "Ġis",
        "Ġin",
        "Ġquestion",
        "Ġhere",
        "?\n\n",
        "Wait",
        ",",
        "Ġfirst",
        "Ġconfirm"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290,
        17339,
        1131,
        779,
        1176,
        315,
        3388,
        358,
        1205,
        7216,
        704,
        902,
        832,
        315,
        1521,
        5961,
        11,
        422,
        5734,
        6866,
        11,
        477,
        1587,
        1202,
        1866,
        3488,
        374,
        304,
        3488,
        1618,
        1980,
        14524,
        11,
        1176,
        7838
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġthat",
            "token_id": 430,
            "logprob": -0.7773,
            "prob": 0.459
          },
          {
            "token": "Ġif",
            "token_id": 422,
            "logprob": -1.5234,
            "prob": 0.2168
          },
          {
            "token": ":",
            "token_id": 25,
            "logprob": -2.1562,
            "prob": 0.1162
          },
          {
            "token": "Ġwhether",
            "token_id": 3508,
            "logprob": -2.4062,
            "prob": 0.0903
          },
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -3.1562,
            "prob": 0.0427
          },
          {
            "token": "Ġbasic",
            "token_id": 6913,
            "logprob": -3.5312,
            "prob": 0.0294
          },
          {
            "token": "Ġexistence",
            "token_id": 14209,
            "logprob": -4.4062,
            "prob": 0.0123
          },
          {
            "token": ",",
            "token_id": 11,
            "logprob": -4.9062,
            "prob": 0.0074
          },
          {
            "token": "ĠChina",
            "token_id": 5734,
            "logprob": -5.4062,
            "prob": 0.0045
          },
          {
            "token": "Ġwhat",
            "token_id": 1148,
            "logprob": -5.5312,
            "prob": 0.004
          }
        ]
      },
      "selected_token": {
        "token": "Ġwhat",
        "token_id": 1148,
        "selection_method": "sampling"
      }
    },
    {
      "step": 60,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm okay... so first of course I need figure out which one of these countries, if China exists, or does its own question is in question here?\n\nWait, first confirm what",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm",
        "Ġokay",
        "...",
        "Ġso",
        "Ġfirst",
        "Ġof",
        "Ġcourse",
        "ĠI",
        "Ġneed",
        "Ġfigure",
        "Ġout",
        "Ġwhich",
        "Ġone",
        "Ġof",
        "Ġthese",
        "Ġcountries",
        ",",
        "Ġif",
        "ĠChina",
        "Ġexists",
        ",",
        "Ġor",
        "Ġdoes",
        "Ġits",
        "Ġown",
        "Ġquestion",
        "Ġis",
        "Ġin",
        "Ġquestion",
        "Ġhere",
        "?\n\n",
        "Wait",
        ",",
        "Ġfirst",
        "Ġconfirm",
        "Ġwhat"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290,
        17339,
        1131,
        779,
        1176,
        315,
        3388,
        358,
        1205,
        7216,
        704,
        902,
        832,
        315,
        1521,
        5961,
        11,
        422,
        5734,
        6866,
        11,
        477,
        1587,
        1202,
        1866,
        3488,
        374,
        304,
        3488,
        1618,
        1980,
        14524,
        11,
        1176,
        7838,
        1148
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -0.4902,
            "prob": 0.6133
          },
          {
            "token": "Ġcountry",
            "token_id": 3224,
            "logprob": -1.9922,
            "prob": 0.1367
          },
          {
            "token": "'s",
            "token_id": 596,
            "logprob": -2.3594,
            "prob": 0.0938
          },
          {
            "token": "Ġexactly",
            "token_id": 7041,
            "logprob": -2.4844,
            "prob": 0.083
          },
          {
            "token": "Ġis",
            "token_id": 374,
            "logprob": -3.4844,
            "prob": 0.0305
          },
          {
            "token": "ĠChina",
            "token_id": 5734,
            "logprob": -3.8594,
            "prob": 0.021
          },
          {
            "token": "Ġcountries",
            "token_id": 5961,
            "logprob": -5.0,
            "prob": 0.0068
          },
          {
            "token": "ĠI",
            "token_id": 358,
            "logprob": -5.375,
            "prob": 0.0047
          },
          {
            "token": "Ġ\"",
            "token_id": 330,
            "logprob": -6.0,
            "prob": 0.0025
          },
          {
            "token": "Ġwe",
            "token_id": 584,
            "logprob": -6.375,
            "prob": 0.0017
          }
        ]
      },
      "selected_token": {
        "token": "Ġthe",
        "token_id": 279,
        "selection_method": "sampling"
      }
    },
    {
      "step": 61,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm okay... so first of course I need figure out which one of these countries, if China exists, or does its own question is in question here?\n\nWait, first confirm what the",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm",
        "Ġokay",
        "...",
        "Ġso",
        "Ġfirst",
        "Ġof",
        "Ġcourse",
        "ĠI",
        "Ġneed",
        "Ġfigure",
        "Ġout",
        "Ġwhich",
        "Ġone",
        "Ġof",
        "Ġthese",
        "Ġcountries",
        ",",
        "Ġif",
        "ĠChina",
        "Ġexists",
        ",",
        "Ġor",
        "Ġdoes",
        "Ġits",
        "Ġown",
        "Ġquestion",
        "Ġis",
        "Ġin",
        "Ġquestion",
        "Ġhere",
        "?\n\n",
        "Wait",
        ",",
        "Ġfirst",
        "Ġconfirm",
        "Ġwhat",
        "Ġthe"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290,
        17339,
        1131,
        779,
        1176,
        315,
        3388,
        358,
        1205,
        7216,
        704,
        902,
        832,
        315,
        1521,
        5961,
        11,
        422,
        5734,
        6866,
        11,
        477,
        1587,
        1202,
        1866,
        3488,
        374,
        304,
        3488,
        1618,
        1980,
        14524,
        11,
        1176,
        7838,
        1148,
        279
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġquestion",
            "token_id": 3488,
            "logprob": -0.2656,
            "prob": 0.7656
          },
          {
            "token": "Ġuser",
            "token_id": 1217,
            "logprob": -1.8906,
            "prob": 0.1514
          },
          {
            "token": "Ġcapital",
            "token_id": 6864,
            "logprob": -3.2656,
            "prob": 0.0381
          },
          {
            "token": "Ġactual",
            "token_id": 5150,
            "logprob": -4.0312,
            "prob": 0.0181
          },
          {
            "token": "Ġcurrent",
            "token_id": 1510,
            "logprob": -4.9062,
            "prob": 0.0075
          },
          {
            "token": "Ġofficial",
            "token_id": 4033,
            "logprob": -5.1562,
            "prob": 0.0059
          },
          {
            "token": "Ġcountry",
            "token_id": 3224,
            "logprob": -5.4062,
            "prob": 0.0045
          },
          {
            "token": "Ġstandard",
            "token_id": 5410,
            "logprob": -6.5312,
            "prob": 0.0015
          },
          {
            "token": "Ġbasic",
            "token_id": 6913,
            "logprob": -6.7812,
            "prob": 0.0012
          },
          {
            "token": "Ġmain",
            "token_id": 1925,
            "logprob": -7.5312,
            "prob": 0.0005
          }
        ]
      },
      "selected_token": {
        "token": "Ġactual",
        "token_id": 5150,
        "selection_method": "sampling"
      }
    },
    {
      "step": 62,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm okay... so first of course I need figure out which one of these countries, if China exists, or does its own question is in question here?\n\nWait, first confirm what the actual",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm",
        "Ġokay",
        "...",
        "Ġso",
        "Ġfirst",
        "Ġof",
        "Ġcourse",
        "ĠI",
        "Ġneed",
        "Ġfigure",
        "Ġout",
        "Ġwhich",
        "Ġone",
        "Ġof",
        "Ġthese",
        "Ġcountries",
        ",",
        "Ġif",
        "ĠChina",
        "Ġexists",
        ",",
        "Ġor",
        "Ġdoes",
        "Ġits",
        "Ġown",
        "Ġquestion",
        "Ġis",
        "Ġin",
        "Ġquestion",
        "Ġhere",
        "?\n\n",
        "Wait",
        ",",
        "Ġfirst",
        "Ġconfirm",
        "Ġwhat",
        "Ġthe",
        "Ġactual"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290,
        17339,
        1131,
        779,
        1176,
        315,
        3388,
        358,
        1205,
        7216,
        704,
        902,
        832,
        315,
        1521,
        5961,
        11,
        422,
        5734,
        6866,
        11,
        477,
        1587,
        1202,
        1866,
        3488,
        374,
        304,
        3488,
        1618,
        1980,
        14524,
        11,
        1176,
        7838,
        1148,
        279,
        5150
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġquestion",
            "token_id": 3488,
            "logprob": -0.3418,
            "prob": 0.7109
          },
          {
            "token": "Ġcapital",
            "token_id": 6864,
            "logprob": -1.4688,
            "prob": 0.2305
          },
          {
            "token": "Ġfacts",
            "token_id": 13363,
            "logprob": -4.5938,
            "prob": 0.0101
          },
          {
            "token": "Ġcurrent",
            "token_id": 1510,
            "logprob": -4.5938,
            "prob": 0.0101
          },
          {
            "token": "Ġcountry",
            "token_id": 3224,
            "logprob": -4.8438,
            "prob": 0.0079
          },
          {
            "token": "Ġfact",
            "token_id": 2144,
            "logprob": -5.2188,
            "prob": 0.0054
          },
          {
            "token": "Ġanswer",
            "token_id": 4320,
            "logprob": -5.5938,
            "prob": 0.0037
          },
          {
            "token": "Ġcapitals",
            "token_id": 94099,
            "logprob": -5.7188,
            "prob": 0.0033
          },
          {
            "token": "Ġname",
            "token_id": 836,
            "logprob": -5.7188,
            "prob": 0.0033
          },
          {
            "token": "Ġsituation",
            "token_id": 6671,
            "logprob": -6.5312,
            "prob": 0.0015
          }
        ]
      },
      "selected_token": {
        "token": "Ġcountry",
        "token_id": 3224,
        "selection_method": "sampling"
      }
    },
    {
      "step": 63,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm okay... so first of course I need figure out which one of these countries, if China exists, or does its own question is in question here?\n\nWait, first confirm what the actual country",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm",
        "Ġokay",
        "...",
        "Ġso",
        "Ġfirst",
        "Ġof",
        "Ġcourse",
        "ĠI",
        "Ġneed",
        "Ġfigure",
        "Ġout",
        "Ġwhich",
        "Ġone",
        "Ġof",
        "Ġthese",
        "Ġcountries",
        ",",
        "Ġif",
        "ĠChina",
        "Ġexists",
        ",",
        "Ġor",
        "Ġdoes",
        "Ġits",
        "Ġown",
        "Ġquestion",
        "Ġis",
        "Ġin",
        "Ġquestion",
        "Ġhere",
        "?\n\n",
        "Wait",
        ",",
        "Ġfirst",
        "Ġconfirm",
        "Ġwhat",
        "Ġthe",
        "Ġactual",
        "Ġcountry"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290,
        17339,
        1131,
        779,
        1176,
        315,
        3388,
        358,
        1205,
        7216,
        704,
        902,
        832,
        315,
        1521,
        5961,
        11,
        422,
        5734,
        6866,
        11,
        477,
        1587,
        1202,
        1866,
        3488,
        374,
        304,
        3488,
        1618,
        1980,
        14524,
        11,
        1176,
        7838,
        1148,
        279,
        5150,
        3224
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġis",
            "token_id": 374,
            "logprob": -0.6641,
            "prob": 0.5156
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -1.2891,
            "prob": 0.2754
          },
          {
            "token": "'s",
            "token_id": 596,
            "logprob": -2.4062,
            "prob": 0.0894
          },
          {
            "token": "Ġthey",
            "token_id": 814,
            "logprob": -2.7812,
            "prob": 0.0615
          },
          {
            "token": "Ġwe",
            "token_id": 584,
            "logprob": -4.5312,
            "prob": 0.0107
          },
          {
            "token": "Ġcalled",
            "token_id": 2663,
            "logprob": -4.6562,
            "prob": 0.0095
          },
          {
            "token": "Ġname",
            "token_id": 836,
            "logprob": -4.6562,
            "prob": 0.0095
          },
          {
            "token": "Ġbeing",
            "token_id": 1694,
            "logprob": -4.7812,
            "prob": 0.0084
          },
          {
            "token": "Ġhere",
            "token_id": 1618,
            "logprob": -5.5312,
            "prob": 0.0039
          },
          {
            "token": "Ġnames",
            "token_id": 5144,
            "logprob": -5.5938,
            "prob": 0.0037
          }
        ]
      },
      "selected_token": {
        "token": "'s",
        "token_id": 596,
        "selection_method": "sampling"
      }
    },
    {
      "step": 64,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm okay... so first of course I need figure out which one of these countries, if China exists, or does its own question is in question here?\n\nWait, first confirm what the actual country's",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm",
        "Ġokay",
        "...",
        "Ġso",
        "Ġfirst",
        "Ġof",
        "Ġcourse",
        "ĠI",
        "Ġneed",
        "Ġfigure",
        "Ġout",
        "Ġwhich",
        "Ġone",
        "Ġof",
        "Ġthese",
        "Ġcountries",
        ",",
        "Ġif",
        "ĠChina",
        "Ġexists",
        ",",
        "Ġor",
        "Ġdoes",
        "Ġits",
        "Ġown",
        "Ġquestion",
        "Ġis",
        "Ġin",
        "Ġquestion",
        "Ġhere",
        "?\n\n",
        "Wait",
        ",",
        "Ġfirst",
        "Ġconfirm",
        "Ġwhat",
        "Ġthe",
        "Ġactual",
        "Ġcountry",
        "'s"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290,
        17339,
        1131,
        779,
        1176,
        315,
        3388,
        358,
        1205,
        7216,
        704,
        902,
        832,
        315,
        1521,
        5961,
        11,
        422,
        5734,
        6866,
        11,
        477,
        1587,
        1202,
        1866,
        3488,
        374,
        304,
        3488,
        1618,
        1980,
        14524,
        11,
        1176,
        7838,
        1148,
        279,
        5150,
        3224,
        596
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġcapital",
            "token_id": 6864,
            "logprob": -0.167,
            "prob": 0.8477
          },
          {
            "token": "Ġname",
            "token_id": 836,
            "logprob": -1.9141,
            "prob": 0.1475
          },
          {
            "token": "Ġcapitals",
            "token_id": 94099,
            "logprob": -6.1562,
            "prob": 0.0021
          },
          {
            "token": "Ġofficial",
            "token_id": 4033,
            "logprob": -6.4688,
            "prob": 0.0015
          },
          {
            "token": "Ġstatus",
            "token_id": 2704,
            "logprob": -7.2188,
            "prob": 0.0007
          },
          {
            "token": "Ġcurrent",
            "token_id": 1510,
            "logprob": -7.7812,
            "prob": 0.0004
          },
          {
            "token": "Ġnames",
            "token_id": 5144,
            "logprob": -7.8438,
            "prob": 0.0004
          },
          {
            "token": "Ġquestion",
            "token_id": 3488,
            "logprob": -8.125,
            "prob": 0.0003
          },
          {
            "token": "Ġfacts",
            "token_id": 13363,
            "logprob": -8.625,
            "prob": 0.0002
          },
          {
            "token": "Ġactual",
            "token_id": 5150,
            "logprob": -9.0,
            "prob": 0.0001
          }
        ]
      },
      "selected_token": {
        "token": "Ġcapital",
        "token_id": 6864,
        "selection_method": "sampling"
      }
    },
    {
      "step": 65,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm okay... so first of course I need figure out which one of these countries, if China exists, or does its own question is in question here?\n\nWait, first confirm what the actual country's capital",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm",
        "Ġokay",
        "...",
        "Ġso",
        "Ġfirst",
        "Ġof",
        "Ġcourse",
        "ĠI",
        "Ġneed",
        "Ġfigure",
        "Ġout",
        "Ġwhich",
        "Ġone",
        "Ġof",
        "Ġthese",
        "Ġcountries",
        ",",
        "Ġif",
        "ĠChina",
        "Ġexists",
        ",",
        "Ġor",
        "Ġdoes",
        "Ġits",
        "Ġown",
        "Ġquestion",
        "Ġis",
        "Ġin",
        "Ġquestion",
        "Ġhere",
        "?\n\n",
        "Wait",
        ",",
        "Ġfirst",
        "Ġconfirm",
        "Ġwhat",
        "Ġthe",
        "Ġactual",
        "Ġcountry",
        "'s",
        "Ġcapital"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290,
        17339,
        1131,
        779,
        1176,
        315,
        3388,
        358,
        1205,
        7216,
        704,
        902,
        832,
        315,
        1521,
        5961,
        11,
        422,
        5734,
        6866,
        11,
        477,
        1587,
        1202,
        1866,
        3488,
        374,
        304,
        3488,
        1618,
        1980,
        14524,
        11,
        1176,
        7838,
        1148,
        279,
        5150,
        3224,
        596,
        6864
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġis",
            "token_id": 374,
            "logprob": -0.0342,
            "prob": 0.9648
          },
          {
            "token": ".",
            "token_id": 13,
            "logprob": -3.6562,
            "prob": 0.0258
          },
          {
            "token": "Ġof",
            "token_id": 315,
            "logprob": -6.0312,
            "prob": 0.0024
          },
          {
            "token": "Ġcities",
            "token_id": 9919,
            "logprob": -7.0312,
            "prob": 0.0009
          },
          {
            "token": "Ġthey",
            "token_id": 814,
            "logprob": -7.0312,
            "prob": 0.0009
          },
          {
            "token": "Ġcity",
            "token_id": 3363,
            "logprob": -7.1562,
            "prob": 0.0008
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -7.5312,
            "prob": 0.0005
          },
          {
            "token": "?",
            "token_id": 30,
            "logprob": -7.7812,
            "prob": 0.0004
          },
          {
            "token": "Ġhere",
            "token_id": 1618,
            "logprob": -8.0625,
            "prob": 0.0003
          },
          {
            "token": "Ġthat",
            "token_id": 430,
            "logprob": -8.5625,
            "prob": 0.0002
          }
        ]
      },
      "selected_token": {
        "token": "Ġhere",
        "token_id": 1618,
        "selection_method": "sampling"
      }
    },
    {
      "step": 66,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm okay... so first of course I need figure out which one of these countries, if China exists, or does its own question is in question here?\n\nWait, first confirm what the actual country's capital here",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm",
        "Ġokay",
        "...",
        "Ġso",
        "Ġfirst",
        "Ġof",
        "Ġcourse",
        "ĠI",
        "Ġneed",
        "Ġfigure",
        "Ġout",
        "Ġwhich",
        "Ġone",
        "Ġof",
        "Ġthese",
        "Ġcountries",
        ",",
        "Ġif",
        "ĠChina",
        "Ġexists",
        ",",
        "Ġor",
        "Ġdoes",
        "Ġits",
        "Ġown",
        "Ġquestion",
        "Ġis",
        "Ġin",
        "Ġquestion",
        "Ġhere",
        "?\n\n",
        "Wait",
        ",",
        "Ġfirst",
        "Ġconfirm",
        "Ġwhat",
        "Ġthe",
        "Ġactual",
        "Ġcountry",
        "'s",
        "Ġcapital",
        "Ġhere"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290,
        17339,
        1131,
        779,
        1176,
        315,
        3388,
        358,
        1205,
        7216,
        704,
        902,
        832,
        315,
        1521,
        5961,
        11,
        422,
        5734,
        6866,
        11,
        477,
        1587,
        1202,
        1866,
        3488,
        374,
        304,
        3488,
        1618,
        1980,
        14524,
        11,
        1176,
        7838,
        1148,
        279,
        5150,
        3224,
        596,
        6864,
        1618
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": ".",
            "token_id": 13,
            "logprob": -0.5664,
            "prob": 0.5664
          },
          {
            "token": "Ġis",
            "token_id": 374,
            "logprob": -0.9414,
            "prob": 0.3906
          },
          {
            "token": "?",
            "token_id": 30,
            "logprob": -3.9375,
            "prob": 0.0194
          },
          {
            "token": "Ġrefers",
            "token_id": 19813,
            "logprob": -4.1875,
            "prob": 0.0151
          },
          {
            "token": "Ġthey",
            "token_id": 814,
            "logprob": -6.3125,
            "prob": 0.0018
          },
          {
            "token": "Ġbeing",
            "token_id": 1694,
            "logprob": -7.3125,
            "prob": 0.0007
          },
          {
            "token": "Ġreferred",
            "token_id": 14183,
            "logprob": -7.4375,
            "prob": 0.0006
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -7.5625,
            "prob": 0.0005
          },
          {
            "token": ",",
            "token_id": 11,
            "logprob": -7.6875,
            "prob": 0.0005
          },
          {
            "token": "Ġreferring",
            "token_id": 22797,
            "logprob": -7.6875,
            "prob": 0.0005
          }
        ]
      },
      "selected_token": {
        "token": "Ġbeing",
        "token_id": 1694,
        "selection_method": "sampling"
      }
    },
    {
      "step": 67,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm okay... so first of course I need figure out which one of these countries, if China exists, or does its own question is in question here?\n\nWait, first confirm what the actual country's capital here being",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm",
        "Ġokay",
        "...",
        "Ġso",
        "Ġfirst",
        "Ġof",
        "Ġcourse",
        "ĠI",
        "Ġneed",
        "Ġfigure",
        "Ġout",
        "Ġwhich",
        "Ġone",
        "Ġof",
        "Ġthese",
        "Ġcountries",
        ",",
        "Ġif",
        "ĠChina",
        "Ġexists",
        ",",
        "Ġor",
        "Ġdoes",
        "Ġits",
        "Ġown",
        "Ġquestion",
        "Ġis",
        "Ġin",
        "Ġquestion",
        "Ġhere",
        "?\n\n",
        "Wait",
        ",",
        "Ġfirst",
        "Ġconfirm",
        "Ġwhat",
        "Ġthe",
        "Ġactual",
        "Ġcountry",
        "'s",
        "Ġcapital",
        "Ġhere",
        "Ġbeing"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290,
        17339,
        1131,
        779,
        1176,
        315,
        3388,
        358,
        1205,
        7216,
        704,
        902,
        832,
        315,
        1521,
        5961,
        11,
        422,
        5734,
        6866,
        11,
        477,
        1587,
        1202,
        1866,
        3488,
        374,
        304,
        3488,
        1618,
        1980,
        14524,
        11,
        1176,
        7838,
        1148,
        279,
        5150,
        3224,
        596,
        6864,
        1618,
        1694
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġreferred",
            "token_id": 14183,
            "logprob": -0.6875,
            "prob": 0.5039
          },
          {
            "token": "Ġasked",
            "token_id": 4691,
            "logprob": -1.0625,
            "prob": 0.3457
          },
          {
            "token": "Ġdiscussed",
            "token_id": 14407,
            "logprob": -2.4375,
            "prob": 0.0874
          },
          {
            "token": "Ġtalked",
            "token_id": 15243,
            "logprob": -3.8125,
            "prob": 0.0221
          },
          {
            "token": "Ġreferenced",
            "token_id": 25819,
            "logprob": -4.1875,
            "prob": 0.0151
          },
          {
            "token": "Ġquestioned",
            "token_id": 29440,
            "logprob": -4.4375,
            "prob": 0.0118
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -5.9375,
            "prob": 0.0026
          },
          {
            "token": "Ġqueried",
            "token_id": 79002,
            "logprob": -6.1875,
            "prob": 0.002
          },
          {
            "token": "Ġrefers",
            "token_id": 19813,
            "logprob": -6.1875,
            "prob": 0.002
          },
          {
            "token": "Ġrefer",
            "token_id": 8464,
            "logprob": -6.375,
            "prob": 0.0017
          }
        ]
      },
      "selected_token": {
        "token": "Ġreferenced",
        "token_id": 25819,
        "selection_method": "sampling"
      }
    },
    {
      "step": 68,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm okay... so first of course I need figure out which one of these countries, if China exists, or does its own question is in question here?\n\nWait, first confirm what the actual country's capital here being referenced",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm",
        "Ġokay",
        "...",
        "Ġso",
        "Ġfirst",
        "Ġof",
        "Ġcourse",
        "ĠI",
        "Ġneed",
        "Ġfigure",
        "Ġout",
        "Ġwhich",
        "Ġone",
        "Ġof",
        "Ġthese",
        "Ġcountries",
        ",",
        "Ġif",
        "ĠChina",
        "Ġexists",
        ",",
        "Ġor",
        "Ġdoes",
        "Ġits",
        "Ġown",
        "Ġquestion",
        "Ġis",
        "Ġin",
        "Ġquestion",
        "Ġhere",
        "?\n\n",
        "Wait",
        ",",
        "Ġfirst",
        "Ġconfirm",
        "Ġwhat",
        "Ġthe",
        "Ġactual",
        "Ġcountry",
        "'s",
        "Ġcapital",
        "Ġhere",
        "Ġbeing",
        "Ġreferenced"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290,
        17339,
        1131,
        779,
        1176,
        315,
        3388,
        358,
        1205,
        7216,
        704,
        902,
        832,
        315,
        1521,
        5961,
        11,
        422,
        5734,
        6866,
        11,
        477,
        1587,
        1202,
        1866,
        3488,
        374,
        304,
        3488,
        1618,
        1980,
        14524,
        11,
        1176,
        7838,
        1148,
        279,
        5150,
        3224,
        596,
        6864,
        1618,
        1694,
        25819
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġis",
            "token_id": 374,
            "logprob": -0.5,
            "prob": 0.6055
          },
          {
            "token": ".",
            "token_id": 13,
            "logprob": -1.0,
            "prob": 0.3672
          },
          {
            "token": "?",
            "token_id": 30,
            "logprob": -4.25,
            "prob": 0.0142
          },
          {
            "token": "Ġto",
            "token_id": 311,
            "logprob": -5.25,
            "prob": 0.0052
          },
          {
            "token": "Ġhere",
            "token_id": 1618,
            "logprob": -6.0,
            "prob": 0.0025
          },
          {
            "token": "Ġas",
            "token_id": 439,
            "logprob": -6.375,
            "prob": 0.0017
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -7.0,
            "prob": 0.0009
          },
          {
            "token": "Ġby",
            "token_id": 555,
            "logprob": -7.25,
            "prob": 0.0007
          },
          {
            "token": "Ġmight",
            "token_id": 2643,
            "logprob": -7.875,
            "prob": 0.0004
          },
          {
            "token": ",",
            "token_id": 11,
            "logprob": -8.375,
            "prob": 0.0002
          }
        ]
      },
      "selected_token": {
        "token": "Ġmight",
        "token_id": 2643,
        "selection_method": "sampling"
      }
    },
    {
      "step": 69,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm okay... so first of course I need figure out which one of these countries, if China exists, or does its own question is in question here?\n\nWait, first confirm what the actual country's capital here being referenced might",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm",
        "Ġokay",
        "...",
        "Ġso",
        "Ġfirst",
        "Ġof",
        "Ġcourse",
        "ĠI",
        "Ġneed",
        "Ġfigure",
        "Ġout",
        "Ġwhich",
        "Ġone",
        "Ġof",
        "Ġthese",
        "Ġcountries",
        ",",
        "Ġif",
        "ĠChina",
        "Ġexists",
        ",",
        "Ġor",
        "Ġdoes",
        "Ġits",
        "Ġown",
        "Ġquestion",
        "Ġis",
        "Ġin",
        "Ġquestion",
        "Ġhere",
        "?\n\n",
        "Wait",
        ",",
        "Ġfirst",
        "Ġconfirm",
        "Ġwhat",
        "Ġthe",
        "Ġactual",
        "Ġcountry",
        "'s",
        "Ġcapital",
        "Ġhere",
        "Ġbeing",
        "Ġreferenced",
        "Ġmight"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290,
        17339,
        1131,
        779,
        1176,
        315,
        3388,
        358,
        1205,
        7216,
        704,
        902,
        832,
        315,
        1521,
        5961,
        11,
        422,
        5734,
        6866,
        11,
        477,
        1587,
        1202,
        1866,
        3488,
        374,
        304,
        3488,
        1618,
        1980,
        14524,
        11,
        1176,
        7838,
        1148,
        279,
        5150,
        3224,
        596,
        6864,
        1618,
        1694,
        25819,
        2643
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġbe",
            "token_id": 387,
            "logprob": -0.0009,
            "prob": 1.0
          },
          {
            "token": "Ġactually",
            "token_id": 3604,
            "logprob": -8.5,
            "prob": 0.0002
          },
          {
            "token": ".",
            "token_id": 13,
            "logprob": -9.125,
            "prob": 0.0001
          },
          {
            "token": "Ġmean",
            "token_id": 3152,
            "logprob": -9.125,
            "prob": 0.0001
          },
          {
            "token": "Ġhave",
            "token_id": 617,
            "logprob": -9.5,
            "prob": 0.0001
          },
          {
            "token": "Ġnot",
            "token_id": 539,
            "logprob": -9.625,
            "prob": 0.0001
          },
          {
            "token": "Ġneed",
            "token_id": 1205,
            "logprob": -9.875,
            "prob": 0.0001
          },
          {
            "token": "Ġrefer",
            "token_id": 8464,
            "logprob": -10.0,
            "prob": 0.0
          },
          {
            "token": "Ġcommonly",
            "token_id": 17037,
            "logprob": -10.25,
            "prob": 0.0
          },
          {
            "token": "Ġeven",
            "token_id": 1524,
            "logprob": -10.375,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġbe",
        "token_id": 387,
        "selection_method": "sampling"
      }
    },
    {
      "step": 70,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm okay... so first of course I need figure out which one of these countries, if China exists, or does its own question is in question here?\n\nWait, first confirm what the actual country's capital here being referenced might be",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm",
        "Ġokay",
        "...",
        "Ġso",
        "Ġfirst",
        "Ġof",
        "Ġcourse",
        "ĠI",
        "Ġneed",
        "Ġfigure",
        "Ġout",
        "Ġwhich",
        "Ġone",
        "Ġof",
        "Ġthese",
        "Ġcountries",
        ",",
        "Ġif",
        "ĠChina",
        "Ġexists",
        ",",
        "Ġor",
        "Ġdoes",
        "Ġits",
        "Ġown",
        "Ġquestion",
        "Ġis",
        "Ġin",
        "Ġquestion",
        "Ġhere",
        "?\n\n",
        "Wait",
        ",",
        "Ġfirst",
        "Ġconfirm",
        "Ġwhat",
        "Ġthe",
        "Ġactual",
        "Ġcountry",
        "'s",
        "Ġcapital",
        "Ġhere",
        "Ġbeing",
        "Ġreferenced",
        "Ġmight",
        "Ġbe"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290,
        17339,
        1131,
        779,
        1176,
        315,
        3388,
        358,
        1205,
        7216,
        704,
        902,
        832,
        315,
        1521,
        5961,
        11,
        422,
        5734,
        6866,
        11,
        477,
        1587,
        1202,
        1866,
        3488,
        374,
        304,
        3488,
        1618,
        1980,
        14524,
        11,
        1176,
        7838,
        1148,
        279,
        5150,
        3224,
        596,
        6864,
        1618,
        1694,
        25819,
        2643,
        387
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": ".",
            "token_id": 13,
            "logprob": -0.0217,
            "prob": 0.9805
          },
          {
            "token": "?",
            "token_id": 30,
            "logprob": -4.2812,
            "prob": 0.014
          },
          {
            "token": ",",
            "token_id": 11,
            "logprob": -6.5312,
            "prob": 0.0015
          },
          {
            "token": ".\n\n",
            "token_id": 382,
            "logprob": -6.6562,
            "prob": 0.0013
          },
          {
            "token": "...",
            "token_id": 1131,
            "logprob": -7.6562,
            "prob": 0.0005
          },
          {
            "token": "ĠChina",
            "token_id": 5734,
            "logprob": -7.6562,
            "prob": 0.0005
          },
          {
            "token": "Ġreferring",
            "token_id": 22797,
            "logprob": -8.0,
            "prob": 0.0003
          },
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -8.25,
            "prob": 0.0003
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -8.375,
            "prob": 0.0002
          },
          {
            "token": "Ġknown",
            "token_id": 3967,
            "logprob": -8.75,
            "prob": 0.0002
          }
        ]
      },
      "selected_token": {
        "token": ".",
        "token_id": 13,
        "selection_method": "sampling"
      }
    },
    {
      "step": 71,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm okay... so first of course I need figure out which one of these countries, if China exists, or does its own question is in question here?\n\nWait, first confirm what the actual country's capital here being referenced might be.",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm",
        "Ġokay",
        "...",
        "Ġso",
        "Ġfirst",
        "Ġof",
        "Ġcourse",
        "ĠI",
        "Ġneed",
        "Ġfigure",
        "Ġout",
        "Ġwhich",
        "Ġone",
        "Ġof",
        "Ġthese",
        "Ġcountries",
        ",",
        "Ġif",
        "ĠChina",
        "Ġexists",
        ",",
        "Ġor",
        "Ġdoes",
        "Ġits",
        "Ġown",
        "Ġquestion",
        "Ġis",
        "Ġin",
        "Ġquestion",
        "Ġhere",
        "?\n\n",
        "Wait",
        ",",
        "Ġfirst",
        "Ġconfirm",
        "Ġwhat",
        "Ġthe",
        "Ġactual",
        "Ġcountry",
        "'s",
        "Ġcapital",
        "Ġhere",
        "Ġbeing",
        "Ġreferenced",
        "Ġmight",
        "Ġbe",
        "."
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290,
        17339,
        1131,
        779,
        1176,
        315,
        3388,
        358,
        1205,
        7216,
        704,
        902,
        832,
        315,
        1521,
        5961,
        11,
        422,
        5734,
        6866,
        11,
        477,
        1587,
        1202,
        1866,
        3488,
        374,
        304,
        3488,
        1618,
        1980,
        14524,
        11,
        1176,
        7838,
        1148,
        279,
        5150,
        3224,
        596,
        6864,
        1618,
        1694,
        25819,
        2643,
        387,
        13
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "ĠThe",
            "token_id": 578,
            "logprob": -0.8594,
            "prob": 0.4238
          },
          {
            "token": "ĠChina",
            "token_id": 5734,
            "logprob": -1.7344,
            "prob": 0.1768
          },
          {
            "token": "ĠLet",
            "token_id": 6914,
            "logprob": -2.2344,
            "prob": 0.1069
          },
          {
            "token": "ĠWell",
            "token_id": 8489,
            "logprob": -2.3594,
            "prob": 0.0942
          },
          {
            "token": "ĠWait",
            "token_id": 14144,
            "logprob": -2.6094,
            "prob": 0.0737
          },
          {
            "token": "ĠSince",
            "token_id": 8876,
            "logprob": -3.2344,
            "prob": 0.0393
          },
          {
            "token": "ĠSo",
            "token_id": 2100,
            "logprob": -3.3594,
            "prob": 0.0347
          },
          {
            "token": "ĠBut",
            "token_id": 2030,
            "logprob": -3.9844,
            "prob": 0.0186
          },
          {
            "token": "ĠI",
            "token_id": 358,
            "logprob": -4.9375,
            "prob": 0.0073
          },
          {
            "token": "ĠIn",
            "token_id": 763,
            "logprob": -5.6875,
            "prob": 0.0034
          }
        ]
      },
      "selected_token": {
        "token": "ĠSince",
        "token_id": 8876,
        "selection_method": "sampling"
      }
    },
    {
      "step": 72,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm okay... so first of course I need figure out which one of these countries, if China exists, or does its own question is in question here?\n\nWait, first confirm what the actual country's capital here being referenced might be. Since",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm",
        "Ġokay",
        "...",
        "Ġso",
        "Ġfirst",
        "Ġof",
        "Ġcourse",
        "ĠI",
        "Ġneed",
        "Ġfigure",
        "Ġout",
        "Ġwhich",
        "Ġone",
        "Ġof",
        "Ġthese",
        "Ġcountries",
        ",",
        "Ġif",
        "ĠChina",
        "Ġexists",
        ",",
        "Ġor",
        "Ġdoes",
        "Ġits",
        "Ġown",
        "Ġquestion",
        "Ġis",
        "Ġin",
        "Ġquestion",
        "Ġhere",
        "?\n\n",
        "Wait",
        ",",
        "Ġfirst",
        "Ġconfirm",
        "Ġwhat",
        "Ġthe",
        "Ġactual",
        "Ġcountry",
        "'s",
        "Ġcapital",
        "Ġhere",
        "Ġbeing",
        "Ġreferenced",
        "Ġmight",
        "Ġbe",
        ".",
        "ĠSince"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290,
        17339,
        1131,
        779,
        1176,
        315,
        3388,
        358,
        1205,
        7216,
        704,
        902,
        832,
        315,
        1521,
        5961,
        11,
        422,
        5734,
        6866,
        11,
        477,
        1587,
        1202,
        1866,
        3488,
        374,
        304,
        3488,
        1618,
        1980,
        14524,
        11,
        1176,
        7838,
        1148,
        279,
        5150,
        3224,
        596,
        6864,
        1618,
        1694,
        25819,
        2643,
        387,
        13,
        8876
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -0.1367,
            "prob": 0.8711
          },
          {
            "token": "Ġit",
            "token_id": 433,
            "logprob": -2.6406,
            "prob": 0.0718
          },
          {
            "token": "ĠChina",
            "token_id": 5734,
            "logprob": -3.5156,
            "prob": 0.0298
          },
          {
            "token": "ĠI",
            "token_id": 358,
            "logprob": -4.375,
            "prob": 0.0125
          },
          {
            "token": "Ġthey",
            "token_id": 814,
            "logprob": -5.375,
            "prob": 0.0046
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -6.125,
            "prob": 0.0022
          },
          {
            "token": "Ġwe",
            "token_id": 584,
            "logprob": -6.25,
            "prob": 0.0019
          },
          {
            "token": "Ġthis",
            "token_id": 420,
            "logprob": -6.25,
            "prob": 0.0019
          },
          {
            "token": "Ġ\"",
            "token_id": 330,
            "logprob": -6.375,
            "prob": 0.0017
          },
          {
            "token": "ĠChinese",
            "token_id": 8620,
            "logprob": -8.375,
            "prob": 0.0002
          }
        ]
      },
      "selected_token": {
        "token": "Ġthey",
        "token_id": 814,
        "selection_method": "sampling"
      }
    },
    {
      "step": 73,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm okay... so first of course I need figure out which one of these countries, if China exists, or does its own question is in question here?\n\nWait, first confirm what the actual country's capital here being referenced might be. Since they",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm",
        "Ġokay",
        "...",
        "Ġso",
        "Ġfirst",
        "Ġof",
        "Ġcourse",
        "ĠI",
        "Ġneed",
        "Ġfigure",
        "Ġout",
        "Ġwhich",
        "Ġone",
        "Ġof",
        "Ġthese",
        "Ġcountries",
        ",",
        "Ġif",
        "ĠChina",
        "Ġexists",
        ",",
        "Ġor",
        "Ġdoes",
        "Ġits",
        "Ġown",
        "Ġquestion",
        "Ġis",
        "Ġin",
        "Ġquestion",
        "Ġhere",
        "?\n\n",
        "Wait",
        ",",
        "Ġfirst",
        "Ġconfirm",
        "Ġwhat",
        "Ġthe",
        "Ġactual",
        "Ġcountry",
        "'s",
        "Ġcapital",
        "Ġhere",
        "Ġbeing",
        "Ġreferenced",
        "Ġmight",
        "Ġbe",
        ".",
        "ĠSince",
        "Ġthey"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290,
        17339,
        1131,
        779,
        1176,
        315,
        3388,
        358,
        1205,
        7216,
        704,
        902,
        832,
        315,
        1521,
        5961,
        11,
        422,
        5734,
        6866,
        11,
        477,
        1587,
        1202,
        1866,
        3488,
        374,
        304,
        3488,
        1618,
        1980,
        14524,
        11,
        1176,
        7838,
        1148,
        279,
        5150,
        3224,
        596,
        6864,
        1618,
        1694,
        25819,
        2643,
        387,
        13,
        8876,
        814
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "'re",
            "token_id": 2351,
            "logprob": -0.8047,
            "prob": 0.4473
          },
          {
            "token": "Ġsaid",
            "token_id": 1071,
            "logprob": -1.9297,
            "prob": 0.1455
          },
          {
            "token": "Ġmentioned",
            "token_id": 9932,
            "logprob": -2.0625,
            "prob": 0.1279
          },
          {
            "token": "Ġwrote",
            "token_id": 6267,
            "logprob": -2.8125,
            "prob": 0.0605
          },
          {
            "token": "Ġspecified",
            "token_id": 5300,
            "logprob": -3.0625,
            "prob": 0.0471
          },
          {
            "token": "Ġused",
            "token_id": 1511,
            "logprob": -3.0625,
            "prob": 0.0471
          },
          {
            "token": "Ġare",
            "token_id": 527,
            "logprob": -3.1875,
            "prob": 0.0415
          },
          {
            "token": "Ġmention",
            "token_id": 6420,
            "logprob": -3.8125,
            "prob": 0.0222
          },
          {
            "token": "Ġspecifically",
            "token_id": 11951,
            "logprob": -4.1875,
            "prob": 0.0153
          },
          {
            "token": "Ġsay",
            "token_id": 2019,
            "logprob": -4.5625,
            "prob": 0.0105
          }
        ]
      },
      "selected_token": {
        "token": "'re",
        "token_id": 2351,
        "selection_method": "sampling"
      }
    },
    {
      "step": 74,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm okay... so first of course I need figure out which one of these countries, if China exists, or does its own question is in question here?\n\nWait, first confirm what the actual country's capital here being referenced might be. Since they're",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm",
        "Ġokay",
        "...",
        "Ġso",
        "Ġfirst",
        "Ġof",
        "Ġcourse",
        "ĠI",
        "Ġneed",
        "Ġfigure",
        "Ġout",
        "Ġwhich",
        "Ġone",
        "Ġof",
        "Ġthese",
        "Ġcountries",
        ",",
        "Ġif",
        "ĠChina",
        "Ġexists",
        ",",
        "Ġor",
        "Ġdoes",
        "Ġits",
        "Ġown",
        "Ġquestion",
        "Ġis",
        "Ġin",
        "Ġquestion",
        "Ġhere",
        "?\n\n",
        "Wait",
        ",",
        "Ġfirst",
        "Ġconfirm",
        "Ġwhat",
        "Ġthe",
        "Ġactual",
        "Ġcountry",
        "'s",
        "Ġcapital",
        "Ġhere",
        "Ġbeing",
        "Ġreferenced",
        "Ġmight",
        "Ġbe",
        ".",
        "ĠSince",
        "Ġthey",
        "'re"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290,
        17339,
        1131,
        779,
        1176,
        315,
        3388,
        358,
        1205,
        7216,
        704,
        902,
        832,
        315,
        1521,
        5961,
        11,
        422,
        5734,
        6866,
        11,
        477,
        1587,
        1202,
        1866,
        3488,
        374,
        304,
        3488,
        1618,
        1980,
        14524,
        11,
        1176,
        7838,
        1148,
        279,
        5150,
        3224,
        596,
        6864,
        1618,
        1694,
        25819,
        2643,
        387,
        13,
        8876,
        814,
        2351
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġasking",
            "token_id": 10371,
            "logprob": -0.2148,
            "prob": 0.8047
          },
          {
            "token": "Ġusing",
            "token_id": 1701,
            "logprob": -2.3438,
            "prob": 0.0962
          },
          {
            "token": "Ġtalking",
            "token_id": 7556,
            "logprob": -3.4688,
            "prob": 0.0312
          },
          {
            "token": "Ġwriting",
            "token_id": 4477,
            "logprob": -3.8438,
            "prob": 0.0215
          },
          {
            "token": "Ġreferring",
            "token_id": 22797,
            "logprob": -4.4688,
            "prob": 0.0115
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -4.7188,
            "prob": 0.009
          },
          {
            "token": "Ġsaying",
            "token_id": 5605,
            "logprob": -4.9688,
            "prob": 0.007
          },
          {
            "token": "Ġspeaking",
            "token_id": 12365,
            "logprob": -5.5938,
            "prob": 0.0037
          },
          {
            "token": "Ġspecifically",
            "token_id": 11951,
            "logprob": -6.2188,
            "prob": 0.002
          },
          {
            "token": "Ġstating",
            "token_id": 28898,
            "logprob": -6.2188,
            "prob": 0.002
          }
        ]
      },
      "selected_token": {
        "token": "Ġstating",
        "token_id": 28898,
        "selection_method": "sampling"
      }
    },
    {
      "step": 75,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm okay... so first of course I need figure out which one of these countries, if China exists, or does its own question is in question here?\n\nWait, first confirm what the actual country's capital here being referenced might be. Since they're stating",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm",
        "Ġokay",
        "...",
        "Ġso",
        "Ġfirst",
        "Ġof",
        "Ġcourse",
        "ĠI",
        "Ġneed",
        "Ġfigure",
        "Ġout",
        "Ġwhich",
        "Ġone",
        "Ġof",
        "Ġthese",
        "Ġcountries",
        ",",
        "Ġif",
        "ĠChina",
        "Ġexists",
        ",",
        "Ġor",
        "Ġdoes",
        "Ġits",
        "Ġown",
        "Ġquestion",
        "Ġis",
        "Ġin",
        "Ġquestion",
        "Ġhere",
        "?\n\n",
        "Wait",
        ",",
        "Ġfirst",
        "Ġconfirm",
        "Ġwhat",
        "Ġthe",
        "Ġactual",
        "Ġcountry",
        "'s",
        "Ġcapital",
        "Ġhere",
        "Ġbeing",
        "Ġreferenced",
        "Ġmight",
        "Ġbe",
        ".",
        "ĠSince",
        "Ġthey",
        "'re",
        "Ġstating"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290,
        17339,
        1131,
        779,
        1176,
        315,
        3388,
        358,
        1205,
        7216,
        704,
        902,
        832,
        315,
        1521,
        5961,
        11,
        422,
        5734,
        6866,
        11,
        477,
        1587,
        1202,
        1866,
        3488,
        374,
        304,
        3488,
        1618,
        1980,
        14524,
        11,
        1176,
        7838,
        1148,
        279,
        5150,
        3224,
        596,
        6864,
        1618,
        1694,
        25819,
        2643,
        387,
        13,
        8876,
        814,
        2351,
        28898
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġ\"",
            "token_id": 330,
            "logprob": -0.2773,
            "prob": 0.7578
          },
          {
            "token": "ĠChina",
            "token_id": 5734,
            "logprob": -2.2812,
            "prob": 0.1025
          },
          {
            "token": "Ġit",
            "token_id": 433,
            "logprob": -2.5312,
            "prob": 0.0801
          },
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -3.1562,
            "prob": 0.0427
          },
          {
            "token": "Ġthat",
            "token_id": 430,
            "logprob": -5.7188,
            "prob": 0.0033
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -5.7188,
            "prob": 0.0033
          },
          {
            "token": "Ġa",
            "token_id": 264,
            "logprob": -5.7812,
            "prob": 0.0031
          },
          {
            "token": "中国",
            "token_id": 59795,
            "logprob": -6.6562,
            "prob": 0.0013
          },
          {
            "token": "ĠChinese",
            "token_id": 8620,
            "logprob": -7.4062,
            "prob": 0.0006
          },
          {
            "token": "Ġabout",
            "token_id": 922,
            "logprob": -7.4688,
            "prob": 0.0006
          }
        ]
      },
      "selected_token": {
        "token": "ĠChina",
        "token_id": 5734,
        "selection_method": "sampling"
      }
    },
    {
      "step": 76,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm okay... so first of course I need figure out which one of these countries, if China exists, or does its own question is in question here?\n\nWait, first confirm what the actual country's capital here being referenced might be. Since they're stating China",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm",
        "Ġokay",
        "...",
        "Ġso",
        "Ġfirst",
        "Ġof",
        "Ġcourse",
        "ĠI",
        "Ġneed",
        "Ġfigure",
        "Ġout",
        "Ġwhich",
        "Ġone",
        "Ġof",
        "Ġthese",
        "Ġcountries",
        ",",
        "Ġif",
        "ĠChina",
        "Ġexists",
        ",",
        "Ġor",
        "Ġdoes",
        "Ġits",
        "Ġown",
        "Ġquestion",
        "Ġis",
        "Ġin",
        "Ġquestion",
        "Ġhere",
        "?\n\n",
        "Wait",
        ",",
        "Ġfirst",
        "Ġconfirm",
        "Ġwhat",
        "Ġthe",
        "Ġactual",
        "Ġcountry",
        "'s",
        "Ġcapital",
        "Ġhere",
        "Ġbeing",
        "Ġreferenced",
        "Ġmight",
        "Ġbe",
        ".",
        "ĠSince",
        "Ġthey",
        "'re",
        "Ġstating",
        "ĠChina"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290,
        17339,
        1131,
        779,
        1176,
        315,
        3388,
        358,
        1205,
        7216,
        704,
        902,
        832,
        315,
        1521,
        5961,
        11,
        422,
        5734,
        6866,
        11,
        477,
        1587,
        1202,
        1866,
        3488,
        374,
        304,
        3488,
        1618,
        1980,
        14524,
        11,
        1176,
        7838,
        1148,
        279,
        5150,
        3224,
        596,
        6864,
        1618,
        1694,
        25819,
        2643,
        387,
        13,
        8876,
        814,
        2351,
        28898,
        5734
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "'s",
            "token_id": 596,
            "logprob": -0.7852,
            "prob": 0.457
          },
          {
            "token": ",",
            "token_id": 11,
            "logprob": -0.7852,
            "prob": 0.457
          },
          {
            "token": "Ġas",
            "token_id": 439,
            "logprob": -3.2812,
            "prob": 0.0376
          },
          {
            "token": "Ġspecifically",
            "token_id": 11951,
            "logprob": -4.1562,
            "prob": 0.0156
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -4.6562,
            "prob": 0.0095
          },
          {
            "token": "Ġ(",
            "token_id": 320,
            "logprob": -5.4062,
            "prob": 0.0045
          },
          {
            "token": "Ġhere",
            "token_id": 1618,
            "logprob": -5.5312,
            "prob": 0.004
          },
          {
            "token": ".",
            "token_id": 13,
            "logprob": -5.7812,
            "prob": 0.0031
          },
          {
            "token": "Ġand",
            "token_id": 323,
            "logprob": -5.9062,
            "prob": 0.0027
          },
          {
            "token": "Ġfirst",
            "token_id": 1176,
            "logprob": -5.9062,
            "prob": 0.0027
          }
        ]
      },
      "selected_token": {
        "token": "Ġand",
        "token_id": 323,
        "selection_method": "sampling"
      }
    },
    {
      "step": 77,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm okay... so first of course I need figure out which one of these countries, if China exists, or does its own question is in question here?\n\nWait, first confirm what the actual country's capital here being referenced might be. Since they're stating China and",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm",
        "Ġokay",
        "...",
        "Ġso",
        "Ġfirst",
        "Ġof",
        "Ġcourse",
        "ĠI",
        "Ġneed",
        "Ġfigure",
        "Ġout",
        "Ġwhich",
        "Ġone",
        "Ġof",
        "Ġthese",
        "Ġcountries",
        ",",
        "Ġif",
        "ĠChina",
        "Ġexists",
        ",",
        "Ġor",
        "Ġdoes",
        "Ġits",
        "Ġown",
        "Ġquestion",
        "Ġis",
        "Ġin",
        "Ġquestion",
        "Ġhere",
        "?\n\n",
        "Wait",
        ",",
        "Ġfirst",
        "Ġconfirm",
        "Ġwhat",
        "Ġthe",
        "Ġactual",
        "Ġcountry",
        "'s",
        "Ġcapital",
        "Ġhere",
        "Ġbeing",
        "Ġreferenced",
        "Ġmight",
        "Ġbe",
        ".",
        "ĠSince",
        "Ġthey",
        "'re",
        "Ġstating",
        "ĠChina",
        "Ġand"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290,
        17339,
        1131,
        779,
        1176,
        315,
        3388,
        358,
        1205,
        7216,
        704,
        902,
        832,
        315,
        1521,
        5961,
        11,
        422,
        5734,
        6866,
        11,
        477,
        1587,
        1202,
        1866,
        3488,
        374,
        304,
        3488,
        1618,
        1980,
        14524,
        11,
        1176,
        7838,
        1148,
        279,
        5150,
        3224,
        596,
        6864,
        1618,
        1694,
        25819,
        2643,
        387,
        13,
        8876,
        814,
        2351,
        28898,
        5734,
        323
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġasking",
            "token_id": 10371,
            "logprob": -0.1426,
            "prob": 0.8672
          },
          {
            "token": "Ġits",
            "token_id": 1202,
            "logprob": -2.3906,
            "prob": 0.0913
          },
          {
            "token": "Ġthen",
            "token_id": 1243,
            "logprob": -4.1562,
            "prob": 0.0159
          },
          {
            "token": "Ġcapital",
            "token_id": 6864,
            "logprob": -5.1562,
            "prob": 0.0058
          },
          {
            "token": "Ġusing",
            "token_id": 1701,
            "logprob": -5.5312,
            "prob": 0.004
          },
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -5.6562,
            "prob": 0.0035
          },
          {
            "token": "Ġreferring",
            "token_id": 22797,
            "logprob": -5.9062,
            "prob": 0.0028
          },
          {
            "token": "Ġtalking",
            "token_id": 7556,
            "logprob": -6.2812,
            "prob": 0.0019
          },
          {
            "token": "Ġ\"",
            "token_id": 330,
            "logprob": -6.4062,
            "prob": 0.0017
          },
          {
            "token": "Ġsaying",
            "token_id": 5605,
            "logprob": -6.9062,
            "prob": 0.001
          }
        ]
      },
      "selected_token": {
        "token": "Ġthen",
        "token_id": 1243,
        "selection_method": "sampling"
      }
    },
    {
      "step": 78,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm okay... so first of course I need figure out which one of these countries, if China exists, or does its own question is in question here?\n\nWait, first confirm what the actual country's capital here being referenced might be. Since they're stating China and then",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm",
        "Ġokay",
        "...",
        "Ġso",
        "Ġfirst",
        "Ġof",
        "Ġcourse",
        "ĠI",
        "Ġneed",
        "Ġfigure",
        "Ġout",
        "Ġwhich",
        "Ġone",
        "Ġof",
        "Ġthese",
        "Ġcountries",
        ",",
        "Ġif",
        "ĠChina",
        "Ġexists",
        ",",
        "Ġor",
        "Ġdoes",
        "Ġits",
        "Ġown",
        "Ġquestion",
        "Ġis",
        "Ġin",
        "Ġquestion",
        "Ġhere",
        "?\n\n",
        "Wait",
        ",",
        "Ġfirst",
        "Ġconfirm",
        "Ġwhat",
        "Ġthe",
        "Ġactual",
        "Ġcountry",
        "'s",
        "Ġcapital",
        "Ġhere",
        "Ġbeing",
        "Ġreferenced",
        "Ġmight",
        "Ġbe",
        ".",
        "ĠSince",
        "Ġthey",
        "'re",
        "Ġstating",
        "ĠChina",
        "Ġand",
        "Ġthen"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290,
        17339,
        1131,
        779,
        1176,
        315,
        3388,
        358,
        1205,
        7216,
        704,
        902,
        832,
        315,
        1521,
        5961,
        11,
        422,
        5734,
        6866,
        11,
        477,
        1587,
        1202,
        1866,
        3488,
        374,
        304,
        3488,
        1618,
        1980,
        14524,
        11,
        1176,
        7838,
        1148,
        279,
        5150,
        3224,
        596,
        6864,
        1618,
        1694,
        25819,
        2643,
        387,
        13,
        8876,
        814,
        2351,
        28898,
        5734,
        323,
        1243
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġasking",
            "token_id": 10371,
            "logprob": -0.0664,
            "prob": 0.9375
          },
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -3.6875,
            "prob": 0.0249
          },
          {
            "token": "Ġits",
            "token_id": 1202,
            "logprob": -4.3125,
            "prob": 0.0134
          },
          {
            "token": "Ġ\"",
            "token_id": 330,
            "logprob": -4.8125,
            "prob": 0.0081
          },
          {
            "token": "Ġcapital",
            "token_id": 6864,
            "logprob": -5.5625,
            "prob": 0.0038
          },
          {
            "token": "Ġsaying",
            "token_id": 5605,
            "logprob": -6.3125,
            "prob": 0.0018
          },
          {
            "token": "Ġtalking",
            "token_id": 7556,
            "logprob": -6.4375,
            "prob": 0.0016
          },
          {
            "token": "Ġusing",
            "token_id": 1701,
            "logprob": -6.5625,
            "prob": 0.0014
          },
          {
            "token": "Ġreferring",
            "token_id": 22797,
            "logprob": -6.6875,
            "prob": 0.0012
          },
          {
            "token": "Ġtheir",
            "token_id": 872,
            "logprob": -6.8125,
            "prob": 0.0011
          }
        ]
      },
      "selected_token": {
        "token": "Ġasking",
        "token_id": 10371,
        "selection_method": "sampling"
      }
    },
    {
      "step": 79,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm okay... so first of course I need figure out which one of these countries, if China exists, or does its own question is in question here?\n\nWait, first confirm what the actual country's capital here being referenced might be. Since they're stating China and then asking",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm",
        "Ġokay",
        "...",
        "Ġso",
        "Ġfirst",
        "Ġof",
        "Ġcourse",
        "ĠI",
        "Ġneed",
        "Ġfigure",
        "Ġout",
        "Ġwhich",
        "Ġone",
        "Ġof",
        "Ġthese",
        "Ġcountries",
        ",",
        "Ġif",
        "ĠChina",
        "Ġexists",
        ",",
        "Ġor",
        "Ġdoes",
        "Ġits",
        "Ġown",
        "Ġquestion",
        "Ġis",
        "Ġin",
        "Ġquestion",
        "Ġhere",
        "?\n\n",
        "Wait",
        ",",
        "Ġfirst",
        "Ġconfirm",
        "Ġwhat",
        "Ġthe",
        "Ġactual",
        "Ġcountry",
        "'s",
        "Ġcapital",
        "Ġhere",
        "Ġbeing",
        "Ġreferenced",
        "Ġmight",
        "Ġbe",
        ".",
        "ĠSince",
        "Ġthey",
        "'re",
        "Ġstating",
        "ĠChina",
        "Ġand",
        "Ġthen",
        "Ġasking"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290,
        17339,
        1131,
        779,
        1176,
        315,
        3388,
        358,
        1205,
        7216,
        704,
        902,
        832,
        315,
        1521,
        5961,
        11,
        422,
        5734,
        6866,
        11,
        477,
        1587,
        1202,
        1866,
        3488,
        374,
        304,
        3488,
        1618,
        1980,
        14524,
        11,
        1176,
        7838,
        1148,
        279,
        5150,
        3224,
        596,
        6864,
        1618,
        1694,
        25819,
        2643,
        387,
        13,
        8876,
        814,
        2351,
        28898,
        5734,
        323,
        1243,
        10371
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġits",
            "token_id": 1202,
            "logprob": -0.9805,
            "prob": 0.375
          },
          {
            "token": "Ġfor",
            "token_id": 369,
            "logprob": -0.9805,
            "prob": 0.375
          },
          {
            "token": "Ġabout",
            "token_id": 922,
            "logprob": -1.7344,
            "prob": 0.1768
          },
          {
            "token": "Ġwhat",
            "token_id": 1148,
            "logprob": -3.6094,
            "prob": 0.0272
          },
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -3.6094,
            "prob": 0.0272
          },
          {
            "token": "Ġ\"",
            "token_id": 330,
            "logprob": -4.4688,
            "prob": 0.0114
          },
          {
            "token": "Ġtheir",
            "token_id": 872,
            "logprob": -6.2188,
            "prob": 0.002
          },
          {
            "token": "Ġcapital",
            "token_id": 6864,
            "logprob": -6.8438,
            "prob": 0.0011
          },
          {
            "token": "Ġwhich",
            "token_id": 902,
            "logprob": -6.8438,
            "prob": 0.0011
          },
          {
            "token": "ĠChina",
            "token_id": 5734,
            "logprob": -7.0938,
            "prob": 0.0008
          }
        ]
      },
      "selected_token": {
        "token": "Ġits",
        "token_id": 1202,
        "selection_method": "sampling"
      }
    },
    {
      "step": 80,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm okay... so first of course I need figure out which one of these countries, if China exists, or does its own question is in question here?\n\nWait, first confirm what the actual country's capital here being referenced might be. Since they're stating China and then asking its",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm",
        "Ġokay",
        "...",
        "Ġso",
        "Ġfirst",
        "Ġof",
        "Ġcourse",
        "ĠI",
        "Ġneed",
        "Ġfigure",
        "Ġout",
        "Ġwhich",
        "Ġone",
        "Ġof",
        "Ġthese",
        "Ġcountries",
        ",",
        "Ġif",
        "ĠChina",
        "Ġexists",
        ",",
        "Ġor",
        "Ġdoes",
        "Ġits",
        "Ġown",
        "Ġquestion",
        "Ġis",
        "Ġin",
        "Ġquestion",
        "Ġhere",
        "?\n\n",
        "Wait",
        ",",
        "Ġfirst",
        "Ġconfirm",
        "Ġwhat",
        "Ġthe",
        "Ġactual",
        "Ġcountry",
        "'s",
        "Ġcapital",
        "Ġhere",
        "Ġbeing",
        "Ġreferenced",
        "Ġmight",
        "Ġbe",
        ".",
        "ĠSince",
        "Ġthey",
        "'re",
        "Ġstating",
        "ĠChina",
        "Ġand",
        "Ġthen",
        "Ġasking",
        "Ġits"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290,
        17339,
        1131,
        779,
        1176,
        315,
        3388,
        358,
        1205,
        7216,
        704,
        902,
        832,
        315,
        1521,
        5961,
        11,
        422,
        5734,
        6866,
        11,
        477,
        1587,
        1202,
        1866,
        3488,
        374,
        304,
        3488,
        1618,
        1980,
        14524,
        11,
        1176,
        7838,
        1148,
        279,
        5150,
        3224,
        596,
        6864,
        1618,
        1694,
        25819,
        2643,
        387,
        13,
        8876,
        814,
        2351,
        28898,
        5734,
        323,
        1243,
        10371,
        1202
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġcapital",
            "token_id": 6864,
            "logprob": -0.0009,
            "prob": 1.0
          },
          {
            "token": "Ġ\"",
            "token_id": 330,
            "logprob": -7.5625,
            "prob": 0.0005
          },
          {
            "token": "Ġcapitals",
            "token_id": 94099,
            "logprob": -9.375,
            "prob": 0.0001
          },
          {
            "token": "ĠCapital",
            "token_id": 18880,
            "logprob": -10.375,
            "prob": 0.0
          },
          {
            "token": "capital",
            "token_id": 66163,
            "logprob": -10.4375,
            "prob": 0.0
          },
          {
            "token": "'",
            "token_id": 6,
            "logprob": -10.625,
            "prob": 0.0
          },
          {
            "token": "Ġcap",
            "token_id": 2107,
            "logprob": -10.6875,
            "prob": 0.0
          },
          {
            "token": "Ġfirst",
            "token_id": 1176,
            "logprob": -10.75,
            "prob": 0.0
          },
          {
            "token": "首",
            "token_id": 61075,
            "logprob": -11.3125,
            "prob": 0.0
          },
          {
            "token": "Ġown",
            "token_id": 1866,
            "logprob": -11.5625,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġcapital",
        "token_id": 6864,
        "selection_method": "sampling"
      }
    },
    {
      "step": 81,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm okay... so first of course I need figure out which one of these countries, if China exists, or does its own question is in question here?\n\nWait, first confirm what the actual country's capital here being referenced might be. Since they're stating China and then asking its capital",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm",
        "Ġokay",
        "...",
        "Ġso",
        "Ġfirst",
        "Ġof",
        "Ġcourse",
        "ĠI",
        "Ġneed",
        "Ġfigure",
        "Ġout",
        "Ġwhich",
        "Ġone",
        "Ġof",
        "Ġthese",
        "Ġcountries",
        ",",
        "Ġif",
        "ĠChina",
        "Ġexists",
        ",",
        "Ġor",
        "Ġdoes",
        "Ġits",
        "Ġown",
        "Ġquestion",
        "Ġis",
        "Ġin",
        "Ġquestion",
        "Ġhere",
        "?\n\n",
        "Wait",
        ",",
        "Ġfirst",
        "Ġconfirm",
        "Ġwhat",
        "Ġthe",
        "Ġactual",
        "Ġcountry",
        "'s",
        "Ġcapital",
        "Ġhere",
        "Ġbeing",
        "Ġreferenced",
        "Ġmight",
        "Ġbe",
        ".",
        "ĠSince",
        "Ġthey",
        "'re",
        "Ġstating",
        "ĠChina",
        "Ġand",
        "Ġthen",
        "Ġasking",
        "Ġits",
        "Ġcapital"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290,
        17339,
        1131,
        779,
        1176,
        315,
        3388,
        358,
        1205,
        7216,
        704,
        902,
        832,
        315,
        1521,
        5961,
        11,
        422,
        5734,
        6866,
        11,
        477,
        1587,
        1202,
        1866,
        3488,
        374,
        304,
        3488,
        1618,
        1980,
        14524,
        11,
        1176,
        7838,
        1148,
        279,
        5150,
        3224,
        596,
        6864,
        1618,
        1694,
        25819,
        2643,
        387,
        13,
        8876,
        814,
        2351,
        28898,
        5734,
        323,
        1243,
        10371,
        1202,
        6864
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": ",",
            "token_id": 11,
            "logprob": -0.5,
            "prob": 0.6055
          },
          {
            "token": ".",
            "token_id": 13,
            "logprob": -1.0,
            "prob": 0.3672
          },
          {
            "token": "...",
            "token_id": 1131,
            "logprob": -4.125,
            "prob": 0.0161
          },
          {
            "token": "?",
            "token_id": 30,
            "logprob": -5.75,
            "prob": 0.0032
          },
          {
            "token": "Ġcity",
            "token_id": 3363,
            "logprob": -6.125,
            "prob": 0.0022
          },
          {
            "token": ".\n\n",
            "token_id": 382,
            "logprob": -6.375,
            "prob": 0.0017
          },
          {
            "token": "—",
            "token_id": 2345,
            "logprob": -7.875,
            "prob": 0.0004
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -7.875,
            "prob": 0.0004
          },
          {
            "token": "Ġis",
            "token_id": 374,
            "logprob": -8.25,
            "prob": 0.0003
          },
          {
            "token": "—that",
            "token_id": 41128,
            "logprob": -9.125,
            "prob": 0.0001
          }
        ]
      },
      "selected_token": {
        "token": "...",
        "token_id": 1131,
        "selection_method": "sampling"
      }
    },
    {
      "step": 82,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm okay... so first of course I need figure out which one of these countries, if China exists, or does its own question is in question here?\n\nWait, first confirm what the actual country's capital here being referenced might be. Since they're stating China and then asking its capital...",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm",
        "Ġokay",
        "...",
        "Ġso",
        "Ġfirst",
        "Ġof",
        "Ġcourse",
        "ĠI",
        "Ġneed",
        "Ġfigure",
        "Ġout",
        "Ġwhich",
        "Ġone",
        "Ġof",
        "Ġthese",
        "Ġcountries",
        ",",
        "Ġif",
        "ĠChina",
        "Ġexists",
        ",",
        "Ġor",
        "Ġdoes",
        "Ġits",
        "Ġown",
        "Ġquestion",
        "Ġis",
        "Ġin",
        "Ġquestion",
        "Ġhere",
        "?\n\n",
        "Wait",
        ",",
        "Ġfirst",
        "Ġconfirm",
        "Ġwhat",
        "Ġthe",
        "Ġactual",
        "Ġcountry",
        "'s",
        "Ġcapital",
        "Ġhere",
        "Ġbeing",
        "Ġreferenced",
        "Ġmight",
        "Ġbe",
        ".",
        "ĠSince",
        "Ġthey",
        "'re",
        "Ġstating",
        "ĠChina",
        "Ġand",
        "Ġthen",
        "Ġasking",
        "Ġits",
        "Ġcapital",
        "..."
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290,
        17339,
        1131,
        779,
        1176,
        315,
        3388,
        358,
        1205,
        7216,
        704,
        902,
        832,
        315,
        1521,
        5961,
        11,
        422,
        5734,
        6866,
        11,
        477,
        1587,
        1202,
        1866,
        3488,
        374,
        304,
        3488,
        1618,
        1980,
        14524,
        11,
        1176,
        7838,
        1148,
        279,
        5150,
        3224,
        596,
        6864,
        1618,
        1694,
        25819,
        2643,
        387,
        13,
        8876,
        814,
        2351,
        28898,
        5734,
        323,
        1243,
        10371,
        1202,
        6864,
        1131
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "ĠWell",
            "token_id": 8489,
            "logprob": -0.875,
            "prob": 0.416
          },
          {
            "token": "Ġwell",
            "token_id": 1664,
            "logprob": -2.0,
            "prob": 0.1357
          },
          {
            "token": "ĠChina",
            "token_id": 5734,
            "logprob": -2.125,
            "prob": 0.1196
          },
          {
            "token": "ĠI",
            "token_id": 358,
            "logprob": -2.625,
            "prob": 0.0723
          },
          {
            "token": "ĠSo",
            "token_id": 2100,
            "logprob": -3.0,
            "prob": 0.0498
          },
          {
            "token": "Ġso",
            "token_id": 779,
            "logprob": -3.0,
            "prob": 0.0498
          },
          {
            "token": "Ġbut",
            "token_id": 719,
            "logprob": -3.0,
            "prob": 0.0498
          },
          {
            "token": "Ġassuming",
            "token_id": 26619,
            "logprob": -4.125,
            "prob": 0.0161
          },
          {
            "token": "ĠBut",
            "token_id": 2030,
            "logprob": -4.625,
            "prob": 0.0098
          },
          {
            "token": "ĠLet",
            "token_id": 6914,
            "logprob": -4.75,
            "prob": 0.0087
          }
        ]
      },
      "selected_token": {
        "token": "Ġso",
        "token_id": 779,
        "selection_method": "sampling"
      }
    },
    {
      "step": 83,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm okay... so first of course I need figure out which one of these countries, if China exists, or does its own question is in question here?\n\nWait, first confirm what the actual country's capital here being referenced might be. Since they're stating China and then asking its capital... so",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm",
        "Ġokay",
        "...",
        "Ġso",
        "Ġfirst",
        "Ġof",
        "Ġcourse",
        "ĠI",
        "Ġneed",
        "Ġfigure",
        "Ġout",
        "Ġwhich",
        "Ġone",
        "Ġof",
        "Ġthese",
        "Ġcountries",
        ",",
        "Ġif",
        "ĠChina",
        "Ġexists",
        ",",
        "Ġor",
        "Ġdoes",
        "Ġits",
        "Ġown",
        "Ġquestion",
        "Ġis",
        "Ġin",
        "Ġquestion",
        "Ġhere",
        "?\n\n",
        "Wait",
        ",",
        "Ġfirst",
        "Ġconfirm",
        "Ġwhat",
        "Ġthe",
        "Ġactual",
        "Ġcountry",
        "'s",
        "Ġcapital",
        "Ġhere",
        "Ġbeing",
        "Ġreferenced",
        "Ġmight",
        "Ġbe",
        ".",
        "ĠSince",
        "Ġthey",
        "'re",
        "Ġstating",
        "ĠChina",
        "Ġand",
        "Ġthen",
        "Ġasking",
        "Ġits",
        "Ġcapital",
        "...",
        "Ġso"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290,
        17339,
        1131,
        779,
        1176,
        315,
        3388,
        358,
        1205,
        7216,
        704,
        902,
        832,
        315,
        1521,
        5961,
        11,
        422,
        5734,
        6866,
        11,
        477,
        1587,
        1202,
        1866,
        3488,
        374,
        304,
        3488,
        1618,
        1980,
        14524,
        11,
        1176,
        7838,
        1148,
        279,
        5150,
        3224,
        596,
        6864,
        1618,
        1694,
        25819,
        2643,
        387,
        13,
        8876,
        814,
        2351,
        28898,
        5734,
        323,
        1243,
        10371,
        1202,
        6864,
        1131,
        779
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġassuming",
            "token_id": 26619,
            "logprob": -0.957,
            "prob": 0.3848
          },
          {
            "token": "ĠI",
            "token_id": 358,
            "logprob": -1.8359,
            "prob": 0.1602
          },
          {
            "token": "ĠChina",
            "token_id": 5734,
            "logprob": -2.0781,
            "prob": 0.1245
          },
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -2.3281,
            "prob": 0.0972
          },
          {
            "token": "Ġmaybe",
            "token_id": 7344,
            "logprob": -2.4531,
            "prob": 0.0854
          },
          {
            "token": "Ġprobably",
            "token_id": 4762,
            "logprob": -3.4531,
            "prob": 0.0315
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -3.9531,
            "prob": 0.0192
          },
          {
            "token": "Ġthey",
            "token_id": 814,
            "logprob": -4.4688,
            "prob": 0.0116
          },
          {
            "token": "Ġit",
            "token_id": 433,
            "logprob": -4.8438,
            "prob": 0.008
          },
          {
            "token": "Ġpresumably",
            "token_id": 36548,
            "logprob": -4.8438,
            "prob": 0.008
          }
        ]
      },
      "selected_token": {
        "token": "Ġprobably",
        "token_id": 4762,
        "selection_method": "sampling"
      }
    },
    {
      "step": 84,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm okay... so first of course I need figure out which one of these countries, if China exists, or does its own question is in question here?\n\nWait, first confirm what the actual country's capital here being referenced might be. Since they're stating China and then asking its capital... so probably",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm",
        "Ġokay",
        "...",
        "Ġso",
        "Ġfirst",
        "Ġof",
        "Ġcourse",
        "ĠI",
        "Ġneed",
        "Ġfigure",
        "Ġout",
        "Ġwhich",
        "Ġone",
        "Ġof",
        "Ġthese",
        "Ġcountries",
        ",",
        "Ġif",
        "ĠChina",
        "Ġexists",
        ",",
        "Ġor",
        "Ġdoes",
        "Ġits",
        "Ġown",
        "Ġquestion",
        "Ġis",
        "Ġin",
        "Ġquestion",
        "Ġhere",
        "?\n\n",
        "Wait",
        ",",
        "Ġfirst",
        "Ġconfirm",
        "Ġwhat",
        "Ġthe",
        "Ġactual",
        "Ġcountry",
        "'s",
        "Ġcapital",
        "Ġhere",
        "Ġbeing",
        "Ġreferenced",
        "Ġmight",
        "Ġbe",
        ".",
        "ĠSince",
        "Ġthey",
        "'re",
        "Ġstating",
        "ĠChina",
        "Ġand",
        "Ġthen",
        "Ġasking",
        "Ġits",
        "Ġcapital",
        "...",
        "Ġso",
        "Ġprobably"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290,
        17339,
        1131,
        779,
        1176,
        315,
        3388,
        358,
        1205,
        7216,
        704,
        902,
        832,
        315,
        1521,
        5961,
        11,
        422,
        5734,
        6866,
        11,
        477,
        1587,
        1202,
        1866,
        3488,
        374,
        304,
        3488,
        1618,
        1980,
        14524,
        11,
        1176,
        7838,
        1148,
        279,
        5150,
        3224,
        596,
        6864,
        1618,
        1694,
        25819,
        2643,
        387,
        13,
        8876,
        814,
        2351,
        28898,
        5734,
        323,
        1243,
        10371,
        1202,
        6864,
        1131,
        779,
        4762
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġstraightforward",
            "token_id": 31439,
            "logprob": -1.5469,
            "prob": 0.2119
          },
          {
            "token": "Ġthey",
            "token_id": 814,
            "logprob": -1.9219,
            "prob": 0.1455
          },
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -1.9219,
            "prob": 0.1455
          },
          {
            "token": "Ġassuming",
            "token_id": 26619,
            "logprob": -2.0469,
            "prob": 0.1289
          },
          {
            "token": "Ġjust",
            "token_id": 1120,
            "logprob": -2.2969,
            "prob": 0.1001
          },
          {
            "token": "Ġa",
            "token_id": 264,
            "logprob": -2.5469,
            "prob": 0.0781
          },
          {
            "token": "ĠChina",
            "token_id": 5734,
            "logprob": -2.9219,
            "prob": 0.0537
          },
          {
            "token": "Ġit",
            "token_id": 433,
            "logprob": -3.5469,
            "prob": 0.0287
          },
          {
            "token": "Ġnot",
            "token_id": 539,
            "logprob": -3.7969,
            "prob": 0.0223
          },
          {
            "token": "Ġassume",
            "token_id": 9855,
            "logprob": -4.4375,
            "prob": 0.012
          }
        ]
      },
      "selected_token": {
        "token": "Ġthey",
        "token_id": 814,
        "selection_method": "sampling"
      }
    },
    {
      "step": 85,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm okay... so first of course I need figure out which one of these countries, if China exists, or does its own question is in question here?\n\nWait, first confirm what the actual country's capital here being referenced might be. Since they're stating China and then asking its capital... so probably they",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm",
        "Ġokay",
        "...",
        "Ġso",
        "Ġfirst",
        "Ġof",
        "Ġcourse",
        "ĠI",
        "Ġneed",
        "Ġfigure",
        "Ġout",
        "Ġwhich",
        "Ġone",
        "Ġof",
        "Ġthese",
        "Ġcountries",
        ",",
        "Ġif",
        "ĠChina",
        "Ġexists",
        ",",
        "Ġor",
        "Ġdoes",
        "Ġits",
        "Ġown",
        "Ġquestion",
        "Ġis",
        "Ġin",
        "Ġquestion",
        "Ġhere",
        "?\n\n",
        "Wait",
        ",",
        "Ġfirst",
        "Ġconfirm",
        "Ġwhat",
        "Ġthe",
        "Ġactual",
        "Ġcountry",
        "'s",
        "Ġcapital",
        "Ġhere",
        "Ġbeing",
        "Ġreferenced",
        "Ġmight",
        "Ġbe",
        ".",
        "ĠSince",
        "Ġthey",
        "'re",
        "Ġstating",
        "ĠChina",
        "Ġand",
        "Ġthen",
        "Ġasking",
        "Ġits",
        "Ġcapital",
        "...",
        "Ġso",
        "Ġprobably",
        "Ġthey"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290,
        17339,
        1131,
        779,
        1176,
        315,
        3388,
        358,
        1205,
        7216,
        704,
        902,
        832,
        315,
        1521,
        5961,
        11,
        422,
        5734,
        6866,
        11,
        477,
        1587,
        1202,
        1866,
        3488,
        374,
        304,
        3488,
        1618,
        1980,
        14524,
        11,
        1176,
        7838,
        1148,
        279,
        5150,
        3224,
        596,
        6864,
        1618,
        1694,
        25819,
        2643,
        387,
        13,
        8876,
        814,
        2351,
        28898,
        5734,
        323,
        1243,
        10371,
        1202,
        6864,
        1131,
        779,
        4762,
        814
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "'re",
            "token_id": 2351,
            "logprob": -0.9102,
            "prob": 0.4023
          },
          {
            "token": "Ġare",
            "token_id": 527,
            "logprob": -1.9141,
            "prob": 0.1475
          },
          {
            "token": "Ġjust",
            "token_id": 1120,
            "logprob": -1.9141,
            "prob": 0.1475
          },
          {
            "token": "Ġknow",
            "token_id": 1440,
            "logprob": -2.2812,
            "prob": 0.1016
          },
          {
            "token": "Ġwant",
            "token_id": 1390,
            "logprob": -2.4062,
            "prob": 0.0898
          },
          {
            "token": "Ġmean",
            "token_id": 3152,
            "logprob": -3.2812,
            "prob": 0.0374
          },
          {
            "token": "Ġthink",
            "token_id": 1781,
            "logprob": -3.2812,
            "prob": 0.0374
          },
          {
            "token": "Ġassume",
            "token_id": 9855,
            "logprob": -4.6562,
            "prob": 0.0095
          },
          {
            "token": "Ġbelieve",
            "token_id": 4510,
            "logprob": -4.9062,
            "prob": 0.0074
          },
          {
            "token": "Ġexpect",
            "token_id": 1755,
            "logprob": -5.0312,
            "prob": 0.0065
          }
        ]
      },
      "selected_token": {
        "token": "'re",
        "token_id": 2351,
        "selection_method": "sampling"
      }
    },
    {
      "step": 86,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm okay... so first of course I need figure out which one of these countries, if China exists, or does its own question is in question here?\n\nWait, first confirm what the actual country's capital here being referenced might be. Since they're stating China and then asking its capital... so probably they're",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm",
        "Ġokay",
        "...",
        "Ġso",
        "Ġfirst",
        "Ġof",
        "Ġcourse",
        "ĠI",
        "Ġneed",
        "Ġfigure",
        "Ġout",
        "Ġwhich",
        "Ġone",
        "Ġof",
        "Ġthese",
        "Ġcountries",
        ",",
        "Ġif",
        "ĠChina",
        "Ġexists",
        ",",
        "Ġor",
        "Ġdoes",
        "Ġits",
        "Ġown",
        "Ġquestion",
        "Ġis",
        "Ġin",
        "Ġquestion",
        "Ġhere",
        "?\n\n",
        "Wait",
        ",",
        "Ġfirst",
        "Ġconfirm",
        "Ġwhat",
        "Ġthe",
        "Ġactual",
        "Ġcountry",
        "'s",
        "Ġcapital",
        "Ġhere",
        "Ġbeing",
        "Ġreferenced",
        "Ġmight",
        "Ġbe",
        ".",
        "ĠSince",
        "Ġthey",
        "'re",
        "Ġstating",
        "ĠChina",
        "Ġand",
        "Ġthen",
        "Ġasking",
        "Ġits",
        "Ġcapital",
        "...",
        "Ġso",
        "Ġprobably",
        "Ġthey",
        "'re"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290,
        17339,
        1131,
        779,
        1176,
        315,
        3388,
        358,
        1205,
        7216,
        704,
        902,
        832,
        315,
        1521,
        5961,
        11,
        422,
        5734,
        6866,
        11,
        477,
        1587,
        1202,
        1866,
        3488,
        374,
        304,
        3488,
        1618,
        1980,
        14524,
        11,
        1176,
        7838,
        1148,
        279,
        5150,
        3224,
        596,
        6864,
        1618,
        1694,
        25819,
        2643,
        387,
        13,
        8876,
        814,
        2351,
        28898,
        5734,
        323,
        1243,
        10371,
        1202,
        6864,
        1131,
        779,
        4762,
        814,
        2351
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġasking",
            "token_id": 10371,
            "logprob": -1.0703,
            "prob": 0.3438
          },
          {
            "token": "Ġjust",
            "token_id": 1120,
            "logprob": -1.5703,
            "prob": 0.208
          },
          {
            "token": "Ġassuming",
            "token_id": 26619,
            "logprob": -2.1875,
            "prob": 0.1113
          },
          {
            "token": "Ġreferring",
            "token_id": 22797,
            "logprob": -2.3125,
            "prob": 0.0986
          },
          {
            "token": "Ġexpecting",
            "token_id": 23132,
            "logprob": -2.9375,
            "prob": 0.0527
          },
          {
            "token": "Ġnot",
            "token_id": 539,
            "logprob": -3.0625,
            "prob": 0.0464
          },
          {
            "token": "Ġlooking",
            "token_id": 3411,
            "logprob": -3.6875,
            "prob": 0.0249
          },
          {
            "token": "Ġtalking",
            "token_id": 7556,
            "logprob": -3.6875,
            "prob": 0.0249
          },
          {
            "token": "Ġpretty",
            "token_id": 5128,
            "logprob": -4.3125,
            "prob": 0.0133
          },
          {
            "token": "Ġthinking",
            "token_id": 7422,
            "logprob": -4.3125,
            "prob": 0.0133
          }
        ]
      },
      "selected_token": {
        "token": "Ġlooking",
        "token_id": 3411,
        "selection_method": "sampling"
      }
    },
    {
      "step": 87,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm okay... so first of course I need figure out which one of these countries, if China exists, or does its own question is in question here?\n\nWait, first confirm what the actual country's capital here being referenced might be. Since they're stating China and then asking its capital... so probably they're looking",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm",
        "Ġokay",
        "...",
        "Ġso",
        "Ġfirst",
        "Ġof",
        "Ġcourse",
        "ĠI",
        "Ġneed",
        "Ġfigure",
        "Ġout",
        "Ġwhich",
        "Ġone",
        "Ġof",
        "Ġthese",
        "Ġcountries",
        ",",
        "Ġif",
        "ĠChina",
        "Ġexists",
        ",",
        "Ġor",
        "Ġdoes",
        "Ġits",
        "Ġown",
        "Ġquestion",
        "Ġis",
        "Ġin",
        "Ġquestion",
        "Ġhere",
        "?\n\n",
        "Wait",
        ",",
        "Ġfirst",
        "Ġconfirm",
        "Ġwhat",
        "Ġthe",
        "Ġactual",
        "Ġcountry",
        "'s",
        "Ġcapital",
        "Ġhere",
        "Ġbeing",
        "Ġreferenced",
        "Ġmight",
        "Ġbe",
        ".",
        "ĠSince",
        "Ġthey",
        "'re",
        "Ġstating",
        "ĠChina",
        "Ġand",
        "Ġthen",
        "Ġasking",
        "Ġits",
        "Ġcapital",
        "...",
        "Ġso",
        "Ġprobably",
        "Ġthey",
        "'re",
        "Ġlooking"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290,
        17339,
        1131,
        779,
        1176,
        315,
        3388,
        358,
        1205,
        7216,
        704,
        902,
        832,
        315,
        1521,
        5961,
        11,
        422,
        5734,
        6866,
        11,
        477,
        1587,
        1202,
        1866,
        3488,
        374,
        304,
        3488,
        1618,
        1980,
        14524,
        11,
        1176,
        7838,
        1148,
        279,
        5150,
        3224,
        596,
        6864,
        1618,
        1694,
        25819,
        2643,
        387,
        13,
        8876,
        814,
        2351,
        28898,
        5734,
        323,
        1243,
        10371,
        1202,
        6864,
        1131,
        779,
        4762,
        814,
        2351,
        3411
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġfor",
            "token_id": 369,
            "logprob": -0.0068,
            "prob": 0.9922
          },
          {
            "token": "Ġto",
            "token_id": 311,
            "logprob": -5.125,
            "prob": 0.0059
          },
          {
            "token": "Ġabout",
            "token_id": 922,
            "logprob": -8.5,
            "prob": 0.0002
          },
          {
            "token": "Ġjust",
            "token_id": 1120,
            "logprob": -8.5,
            "prob": 0.0002
          },
          {
            "token": "Ġstraight",
            "token_id": 7833,
            "logprob": -9.5,
            "prob": 0.0001
          },
          {
            "token": "Ġstraightforward",
            "token_id": 31439,
            "logprob": -9.625,
            "prob": 0.0001
          },
          {
            "token": "Ġspecifically",
            "token_id": 11951,
            "logprob": -9.625,
            "prob": 0.0001
          },
          {
            "token": "Ġat",
            "token_id": 520,
            "logprob": -10.25,
            "prob": 0.0
          },
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -10.8125,
            "prob": 0.0
          },
          {
            "token": "Ġsimply",
            "token_id": 5042,
            "logprob": -10.9375,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġsimply",
        "token_id": 5042,
        "selection_method": "sampling"
      }
    },
    {
      "step": 88,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm okay... so first of course I need figure out which one of these countries, if China exists, or does its own question is in question here?\n\nWait, first confirm what the actual country's capital here being referenced might be. Since they're stating China and then asking its capital... so probably they're looking simply",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm",
        "Ġokay",
        "...",
        "Ġso",
        "Ġfirst",
        "Ġof",
        "Ġcourse",
        "ĠI",
        "Ġneed",
        "Ġfigure",
        "Ġout",
        "Ġwhich",
        "Ġone",
        "Ġof",
        "Ġthese",
        "Ġcountries",
        ",",
        "Ġif",
        "ĠChina",
        "Ġexists",
        ",",
        "Ġor",
        "Ġdoes",
        "Ġits",
        "Ġown",
        "Ġquestion",
        "Ġis",
        "Ġin",
        "Ġquestion",
        "Ġhere",
        "?\n\n",
        "Wait",
        ",",
        "Ġfirst",
        "Ġconfirm",
        "Ġwhat",
        "Ġthe",
        "Ġactual",
        "Ġcountry",
        "'s",
        "Ġcapital",
        "Ġhere",
        "Ġbeing",
        "Ġreferenced",
        "Ġmight",
        "Ġbe",
        ".",
        "ĠSince",
        "Ġthey",
        "'re",
        "Ġstating",
        "ĠChina",
        "Ġand",
        "Ġthen",
        "Ġasking",
        "Ġits",
        "Ġcapital",
        "...",
        "Ġso",
        "Ġprobably",
        "Ġthey",
        "'re",
        "Ġlooking",
        "Ġsimply"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290,
        17339,
        1131,
        779,
        1176,
        315,
        3388,
        358,
        1205,
        7216,
        704,
        902,
        832,
        315,
        1521,
        5961,
        11,
        422,
        5734,
        6866,
        11,
        477,
        1587,
        1202,
        1866,
        3488,
        374,
        304,
        3488,
        1618,
        1980,
        14524,
        11,
        1176,
        7838,
        1148,
        279,
        5150,
        3224,
        596,
        6864,
        1618,
        1694,
        25819,
        2643,
        387,
        13,
        8876,
        814,
        2351,
        28898,
        5734,
        323,
        1243,
        10371,
        1202,
        6864,
        1131,
        779,
        4762,
        814,
        2351,
        3411,
        5042
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġfor",
            "token_id": 369,
            "logprob": -0.0193,
            "prob": 0.9805
          },
          {
            "token": "Ġto",
            "token_id": 311,
            "logprob": -4.2812,
            "prob": 0.014
          },
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -6.1562,
            "prob": 0.0022
          },
          {
            "token": "Ġasking",
            "token_id": 10371,
            "logprob": -7.0312,
            "prob": 0.0009
          },
          {
            "token": ",",
            "token_id": 11,
            "logprob": -7.5312,
            "prob": 0.0005
          },
          {
            "token": "Ġwhat",
            "token_id": 1148,
            "logprob": -8.0,
            "prob": 0.0003
          },
          {
            "token": "Ġabout",
            "token_id": 922,
            "logprob": -8.875,
            "prob": 0.0001
          },
          {
            "token": "Ġand",
            "token_id": 323,
            "logprob": -8.875,
            "prob": 0.0001
          },
          {
            "token": ".",
            "token_id": 13,
            "logprob": -9.0,
            "prob": 0.0001
          },
          {
            "token": "Ġat",
            "token_id": 520,
            "logprob": -10.0,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": ",",
        "token_id": 11,
        "selection_method": "sampling"
      }
    },
    {
      "step": 89,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm okay... so first of course I need figure out which one of these countries, if China exists, or does its own question is in question here?\n\nWait, first confirm what the actual country's capital here being referenced might be. Since they're stating China and then asking its capital... so probably they're looking simply,",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm",
        "Ġokay",
        "...",
        "Ġso",
        "Ġfirst",
        "Ġof",
        "Ġcourse",
        "ĠI",
        "Ġneed",
        "Ġfigure",
        "Ġout",
        "Ġwhich",
        "Ġone",
        "Ġof",
        "Ġthese",
        "Ġcountries",
        ",",
        "Ġif",
        "ĠChina",
        "Ġexists",
        ",",
        "Ġor",
        "Ġdoes",
        "Ġits",
        "Ġown",
        "Ġquestion",
        "Ġis",
        "Ġin",
        "Ġquestion",
        "Ġhere",
        "?\n\n",
        "Wait",
        ",",
        "Ġfirst",
        "Ġconfirm",
        "Ġwhat",
        "Ġthe",
        "Ġactual",
        "Ġcountry",
        "'s",
        "Ġcapital",
        "Ġhere",
        "Ġbeing",
        "Ġreferenced",
        "Ġmight",
        "Ġbe",
        ".",
        "ĠSince",
        "Ġthey",
        "'re",
        "Ġstating",
        "ĠChina",
        "Ġand",
        "Ġthen",
        "Ġasking",
        "Ġits",
        "Ġcapital",
        "...",
        "Ġso",
        "Ġprobably",
        "Ġthey",
        "'re",
        "Ġlooking",
        "Ġsimply",
        ","
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290,
        17339,
        1131,
        779,
        1176,
        315,
        3388,
        358,
        1205,
        7216,
        704,
        902,
        832,
        315,
        1521,
        5961,
        11,
        422,
        5734,
        6866,
        11,
        477,
        1587,
        1202,
        1866,
        3488,
        374,
        304,
        3488,
        1618,
        1980,
        14524,
        11,
        1176,
        7838,
        1148,
        279,
        5150,
        3224,
        596,
        6864,
        1618,
        1694,
        25819,
        2643,
        387,
        13,
        8876,
        814,
        2351,
        28898,
        5734,
        323,
        1243,
        10371,
        1202,
        6864,
        1131,
        779,
        4762,
        814,
        2351,
        3411,
        5042,
        11
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġbut",
            "token_id": 719,
            "logprob": -1.6875,
            "prob": 0.1855
          },
          {
            "token": "Ġmaybe",
            "token_id": 7344,
            "logprob": -1.9375,
            "prob": 0.1445
          },
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -1.9375,
            "prob": 0.1445
          },
          {
            "token": "Ġstraightforward",
            "token_id": 31439,
            "logprob": -2.1875,
            "prob": 0.1128
          },
          {
            "token": "Ġ\"",
            "token_id": 330,
            "logprob": -2.3125,
            "prob": 0.0991
          },
          {
            "token": "Ġlike",
            "token_id": 1093,
            "logprob": -2.9375,
            "prob": 0.0532
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -3.3125,
            "prob": 0.0366
          },
          {
            "token": "Ġwhat",
            "token_id": 1148,
            "logprob": -3.5625,
            "prob": 0.0284
          },
          {
            "token": "Ġgiven",
            "token_id": 2728,
            "logprob": -3.9375,
            "prob": 0.0195
          },
          {
            "token": "Ġsince",
            "token_id": 2533,
            "logprob": -4.1875,
            "prob": 0.0153
          }
        ]
      },
      "selected_token": {
        "token": "Ġstraightforward",
        "token_id": 31439,
        "selection_method": "sampling"
      }
    },
    {
      "step": 90,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm okay... so first of course I need figure out which one of these countries, if China exists, or does its own question is in question here?\n\nWait, first confirm what the actual country's capital here being referenced might be. Since they're stating China and then asking its capital... so probably they're looking simply, straightforward",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm",
        "Ġokay",
        "...",
        "Ġso",
        "Ġfirst",
        "Ġof",
        "Ġcourse",
        "ĠI",
        "Ġneed",
        "Ġfigure",
        "Ġout",
        "Ġwhich",
        "Ġone",
        "Ġof",
        "Ġthese",
        "Ġcountries",
        ",",
        "Ġif",
        "ĠChina",
        "Ġexists",
        ",",
        "Ġor",
        "Ġdoes",
        "Ġits",
        "Ġown",
        "Ġquestion",
        "Ġis",
        "Ġin",
        "Ġquestion",
        "Ġhere",
        "?\n\n",
        "Wait",
        ",",
        "Ġfirst",
        "Ġconfirm",
        "Ġwhat",
        "Ġthe",
        "Ġactual",
        "Ġcountry",
        "'s",
        "Ġcapital",
        "Ġhere",
        "Ġbeing",
        "Ġreferenced",
        "Ġmight",
        "Ġbe",
        ".",
        "ĠSince",
        "Ġthey",
        "'re",
        "Ġstating",
        "ĠChina",
        "Ġand",
        "Ġthen",
        "Ġasking",
        "Ġits",
        "Ġcapital",
        "...",
        "Ġso",
        "Ġprobably",
        "Ġthey",
        "'re",
        "Ġlooking",
        "Ġsimply",
        ",",
        "Ġstraightforward"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290,
        17339,
        1131,
        779,
        1176,
        315,
        3388,
        358,
        1205,
        7216,
        704,
        902,
        832,
        315,
        1521,
        5961,
        11,
        422,
        5734,
        6866,
        11,
        477,
        1587,
        1202,
        1866,
        3488,
        374,
        304,
        3488,
        1618,
        1980,
        14524,
        11,
        1176,
        7838,
        1148,
        279,
        5150,
        3224,
        596,
        6864,
        1618,
        1694,
        25819,
        2643,
        387,
        13,
        8876,
        814,
        2351,
        28898,
        5734,
        323,
        1243,
        10371,
        1202,
        6864,
        1131,
        779,
        4762,
        814,
        2351,
        3411,
        5042,
        11,
        31439
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "ly",
            "token_id": 398,
            "logprob": -0.7148,
            "prob": 0.4902
          },
          {
            "token": ".",
            "token_id": 13,
            "logprob": -2.2188,
            "prob": 0.1094
          },
          {
            "token": "Ġanswer",
            "token_id": 4320,
            "logprob": -2.2188,
            "prob": 0.1094
          },
          {
            "token": ",",
            "token_id": 11,
            "logprob": -2.2188,
            "prob": 0.1094
          },
          {
            "token": "Ġfor",
            "token_id": 369,
            "logprob": -3.3438,
            "prob": 0.0354
          },
          {
            "token": "Ġinformation",
            "token_id": 2038,
            "logprob": -3.8438,
            "prob": 0.0215
          },
          {
            "token": "Ġfact",
            "token_id": 2144,
            "logprob": -3.8438,
            "prob": 0.0215
          },
          {
            "token": ":",
            "token_id": 25,
            "logprob": -3.9688,
            "prob": 0.0189
          },
          {
            "token": "Ġquestion",
            "token_id": 3488,
            "logprob": -3.9688,
            "prob": 0.0189
          },
          {
            "token": ".\n\n",
            "token_id": 382,
            "logprob": -4.0938,
            "prob": 0.0167
          }
        ]
      },
      "selected_token": {
        "token": "Ġinformation",
        "token_id": 2038,
        "selection_method": "sampling"
      }
    },
    {
      "step": 91,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm okay... so first of course I need figure out which one of these countries, if China exists, or does its own question is in question here?\n\nWait, first confirm what the actual country's capital here being referenced might be. Since they're stating China and then asking its capital... so probably they're looking simply, straightforward information",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm",
        "Ġokay",
        "...",
        "Ġso",
        "Ġfirst",
        "Ġof",
        "Ġcourse",
        "ĠI",
        "Ġneed",
        "Ġfigure",
        "Ġout",
        "Ġwhich",
        "Ġone",
        "Ġof",
        "Ġthese",
        "Ġcountries",
        ",",
        "Ġif",
        "ĠChina",
        "Ġexists",
        ",",
        "Ġor",
        "Ġdoes",
        "Ġits",
        "Ġown",
        "Ġquestion",
        "Ġis",
        "Ġin",
        "Ġquestion",
        "Ġhere",
        "?\n\n",
        "Wait",
        ",",
        "Ġfirst",
        "Ġconfirm",
        "Ġwhat",
        "Ġthe",
        "Ġactual",
        "Ġcountry",
        "'s",
        "Ġcapital",
        "Ġhere",
        "Ġbeing",
        "Ġreferenced",
        "Ġmight",
        "Ġbe",
        ".",
        "ĠSince",
        "Ġthey",
        "'re",
        "Ġstating",
        "ĠChina",
        "Ġand",
        "Ġthen",
        "Ġasking",
        "Ġits",
        "Ġcapital",
        "...",
        "Ġso",
        "Ġprobably",
        "Ġthey",
        "'re",
        "Ġlooking",
        "Ġsimply",
        ",",
        "Ġstraightforward",
        "Ġinformation"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290,
        17339,
        1131,
        779,
        1176,
        315,
        3388,
        358,
        1205,
        7216,
        704,
        902,
        832,
        315,
        1521,
        5961,
        11,
        422,
        5734,
        6866,
        11,
        477,
        1587,
        1202,
        1866,
        3488,
        374,
        304,
        3488,
        1618,
        1980,
        14524,
        11,
        1176,
        7838,
        1148,
        279,
        5150,
        3224,
        596,
        6864,
        1618,
        1694,
        25819,
        2643,
        387,
        13,
        8876,
        814,
        2351,
        28898,
        5734,
        323,
        1243,
        10371,
        1202,
        6864,
        1131,
        779,
        4762,
        814,
        2351,
        3411,
        5042,
        11,
        31439,
        2038
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": ".",
            "token_id": 13,
            "logprob": -0.2734,
            "prob": 0.7617
          },
          {
            "token": ".\n\n",
            "token_id": 382,
            "logprob": -2.0312,
            "prob": 0.1318
          },
          {
            "token": ":",
            "token_id": 25,
            "logprob": -3.2812,
            "prob": 0.0378
          },
          {
            "token": "Ġabout",
            "token_id": 922,
            "logprob": -3.7812,
            "prob": 0.0229
          },
          {
            "token": ",",
            "token_id": 11,
            "logprob": -4.1562,
            "prob": 0.0157
          },
          {
            "token": "Ġhere",
            "token_id": 1618,
            "logprob": -4.4062,
            "prob": 0.0123
          },
          {
            "token": "?",
            "token_id": 30,
            "logprob": -4.6562,
            "prob": 0.0096
          },
          {
            "token": "Ġthat",
            "token_id": 430,
            "logprob": -5.9062,
            "prob": 0.0027
          },
          {
            "token": "?\n\n",
            "token_id": 1980,
            "logprob": -6.5312,
            "prob": 0.0015
          },
          {
            "token": "Ġfor",
            "token_id": 369,
            "logprob": -7.9062,
            "prob": 0.0004
          }
        ]
      },
      "selected_token": {
        "token": "?\n\n",
        "token_id": 1980,
        "selection_method": "sampling"
      }
    },
    {
      "step": 92,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm okay... so first of course I need figure out which one of these countries, if China exists, or does its own question is in question here?\n\nWait, first confirm what the actual country's capital here being referenced might be. Since they're stating China and then asking its capital... so probably they're looking simply, straightforward information?\n\n",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm",
        "Ġokay",
        "...",
        "Ġso",
        "Ġfirst",
        "Ġof",
        "Ġcourse",
        "ĠI",
        "Ġneed",
        "Ġfigure",
        "Ġout",
        "Ġwhich",
        "Ġone",
        "Ġof",
        "Ġthese",
        "Ġcountries",
        ",",
        "Ġif",
        "ĠChina",
        "Ġexists",
        ",",
        "Ġor",
        "Ġdoes",
        "Ġits",
        "Ġown",
        "Ġquestion",
        "Ġis",
        "Ġin",
        "Ġquestion",
        "Ġhere",
        "?\n\n",
        "Wait",
        ",",
        "Ġfirst",
        "Ġconfirm",
        "Ġwhat",
        "Ġthe",
        "Ġactual",
        "Ġcountry",
        "'s",
        "Ġcapital",
        "Ġhere",
        "Ġbeing",
        "Ġreferenced",
        "Ġmight",
        "Ġbe",
        ".",
        "ĠSince",
        "Ġthey",
        "'re",
        "Ġstating",
        "ĠChina",
        "Ġand",
        "Ġthen",
        "Ġasking",
        "Ġits",
        "Ġcapital",
        "...",
        "Ġso",
        "Ġprobably",
        "Ġthey",
        "'re",
        "Ġlooking",
        "Ġsimply",
        ",",
        "Ġstraightforward",
        "Ġinformation",
        "?\n\n"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290,
        17339,
        1131,
        779,
        1176,
        315,
        3388,
        358,
        1205,
        7216,
        704,
        902,
        832,
        315,
        1521,
        5961,
        11,
        422,
        5734,
        6866,
        11,
        477,
        1587,
        1202,
        1866,
        3488,
        374,
        304,
        3488,
        1618,
        1980,
        14524,
        11,
        1176,
        7838,
        1148,
        279,
        5150,
        3224,
        596,
        6864,
        1618,
        1694,
        25819,
        2643,
        387,
        13,
        8876,
        814,
        2351,
        28898,
        5734,
        323,
        1243,
        10371,
        1202,
        6864,
        1131,
        779,
        4762,
        814,
        2351,
        3411,
        5042,
        11,
        31439,
        2038,
        1980
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "But",
            "token_id": 4071,
            "logprob": -1.3281,
            "prob": 0.2656
          },
          {
            "token": "China",
            "token_id": 23078,
            "logprob": -1.4531,
            "prob": 0.2344
          },
          {
            "token": "I",
            "token_id": 40,
            "logprob": -2.2031,
            "prob": 0.1108
          },
          {
            "token": "Well",
            "token_id": 11649,
            "logprob": -2.2031,
            "prob": 0.1108
          },
          {
            "token": "Wait",
            "token_id": 14524,
            "logprob": -2.5781,
            "prob": 0.0762
          },
          {
            "token": "Let",
            "token_id": 10267,
            "logprob": -3.1406,
            "prob": 0.0435
          },
          {
            "token": "So",
            "token_id": 4516,
            "logprob": -3.3906,
            "prob": 0.0337
          },
          {
            "token": "Now",
            "token_id": 7184,
            "logprob": -3.3906,
            "prob": 0.0337
          },
          {
            "token": "Alright",
            "token_id": 72586,
            "logprob": -3.7656,
            "prob": 0.0232
          },
          {
            "token": "Okay",
            "token_id": 33413,
            "logprob": -4.25,
            "prob": 0.0141
          }
        ]
      },
      "selected_token": {
        "token": "But",
        "token_id": 4071,
        "selection_method": "sampling"
      }
    },
    {
      "step": 93,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm okay... so first of course I need figure out which one of these countries, if China exists, or does its own question is in question here?\n\nWait, first confirm what the actual country's capital here being referenced might be. Since they're stating China and then asking its capital... so probably they're looking simply, straightforward information?\n\nBut",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm",
        "Ġokay",
        "...",
        "Ġso",
        "Ġfirst",
        "Ġof",
        "Ġcourse",
        "ĠI",
        "Ġneed",
        "Ġfigure",
        "Ġout",
        "Ġwhich",
        "Ġone",
        "Ġof",
        "Ġthese",
        "Ġcountries",
        ",",
        "Ġif",
        "ĠChina",
        "Ġexists",
        ",",
        "Ġor",
        "Ġdoes",
        "Ġits",
        "Ġown",
        "Ġquestion",
        "Ġis",
        "Ġin",
        "Ġquestion",
        "Ġhere",
        "?\n\n",
        "Wait",
        ",",
        "Ġfirst",
        "Ġconfirm",
        "Ġwhat",
        "Ġthe",
        "Ġactual",
        "Ġcountry",
        "'s",
        "Ġcapital",
        "Ġhere",
        "Ġbeing",
        "Ġreferenced",
        "Ġmight",
        "Ġbe",
        ".",
        "ĠSince",
        "Ġthey",
        "'re",
        "Ġstating",
        "ĠChina",
        "Ġand",
        "Ġthen",
        "Ġasking",
        "Ġits",
        "Ġcapital",
        "...",
        "Ġso",
        "Ġprobably",
        "Ġthey",
        "'re",
        "Ġlooking",
        "Ġsimply",
        ",",
        "Ġstraightforward",
        "Ġinformation",
        "?\n\n",
        "But"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290,
        17339,
        1131,
        779,
        1176,
        315,
        3388,
        358,
        1205,
        7216,
        704,
        902,
        832,
        315,
        1521,
        5961,
        11,
        422,
        5734,
        6866,
        11,
        477,
        1587,
        1202,
        1866,
        3488,
        374,
        304,
        3488,
        1618,
        1980,
        14524,
        11,
        1176,
        7838,
        1148,
        279,
        5150,
        3224,
        596,
        6864,
        1618,
        1694,
        25819,
        2643,
        387,
        13,
        8876,
        814,
        2351,
        28898,
        5734,
        323,
        1243,
        10371,
        1202,
        6864,
        1131,
        779,
        4762,
        814,
        2351,
        3411,
        5042,
        11,
        31439,
        2038,
        1980,
        4071
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġmaybe",
            "token_id": 7344,
            "logprob": -1.0391,
            "prob": 0.3535
          },
          {
            "token": "Ġwait",
            "token_id": 3868,
            "logprob": -1.5391,
            "prob": 0.2148
          },
          {
            "token": "Ġlet",
            "token_id": 1095,
            "logprob": -1.9141,
            "prob": 0.1475
          },
          {
            "token": "ĠI",
            "token_id": 358,
            "logprob": -2.2812,
            "prob": 0.1016
          },
          {
            "token": "Ġhold",
            "token_id": 3412,
            "logprob": -2.7812,
            "prob": 0.0615
          },
          {
            "token": "Ġjust",
            "token_id": 1120,
            "logprob": -3.1562,
            "prob": 0.0422
          },
          {
            "token": "Ġsometimes",
            "token_id": 7170,
            "logprob": -3.7812,
            "prob": 0.0226
          },
          {
            "token": "Ġwhy",
            "token_id": 3249,
            "logprob": -4.2812,
            "prob": 0.0137
          },
          {
            "token": "ĠChina",
            "token_id": 5734,
            "logprob": -4.5312,
            "prob": 0.0107
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -5.2812,
            "prob": 0.005
          }
        ]
      },
      "selected_token": {
        "token": "Ġjust",
        "token_id": 1120,
        "selection_method": "sampling"
      }
    },
    {
      "step": 94,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm okay... so first of course I need figure out which one of these countries, if China exists, or does its own question is in question here?\n\nWait, first confirm what the actual country's capital here being referenced might be. Since they're stating China and then asking its capital... so probably they're looking simply, straightforward information?\n\nBut just",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm",
        "Ġokay",
        "...",
        "Ġso",
        "Ġfirst",
        "Ġof",
        "Ġcourse",
        "ĠI",
        "Ġneed",
        "Ġfigure",
        "Ġout",
        "Ġwhich",
        "Ġone",
        "Ġof",
        "Ġthese",
        "Ġcountries",
        ",",
        "Ġif",
        "ĠChina",
        "Ġexists",
        ",",
        "Ġor",
        "Ġdoes",
        "Ġits",
        "Ġown",
        "Ġquestion",
        "Ġis",
        "Ġin",
        "Ġquestion",
        "Ġhere",
        "?\n\n",
        "Wait",
        ",",
        "Ġfirst",
        "Ġconfirm",
        "Ġwhat",
        "Ġthe",
        "Ġactual",
        "Ġcountry",
        "'s",
        "Ġcapital",
        "Ġhere",
        "Ġbeing",
        "Ġreferenced",
        "Ġmight",
        "Ġbe",
        ".",
        "ĠSince",
        "Ġthey",
        "'re",
        "Ġstating",
        "ĠChina",
        "Ġand",
        "Ġthen",
        "Ġasking",
        "Ġits",
        "Ġcapital",
        "...",
        "Ġso",
        "Ġprobably",
        "Ġthey",
        "'re",
        "Ġlooking",
        "Ġsimply",
        ",",
        "Ġstraightforward",
        "Ġinformation",
        "?\n\n",
        "But",
        "Ġjust"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290,
        17339,
        1131,
        779,
        1176,
        315,
        3388,
        358,
        1205,
        7216,
        704,
        902,
        832,
        315,
        1521,
        5961,
        11,
        422,
        5734,
        6866,
        11,
        477,
        1587,
        1202,
        1866,
        3488,
        374,
        304,
        3488,
        1618,
        1980,
        14524,
        11,
        1176,
        7838,
        1148,
        279,
        5150,
        3224,
        596,
        6864,
        1618,
        1694,
        25819,
        2643,
        387,
        13,
        8876,
        814,
        2351,
        28898,
        5734,
        323,
        1243,
        10371,
        1202,
        6864,
        1131,
        779,
        4762,
        814,
        2351,
        3411,
        5042,
        11,
        31439,
        2038,
        1980,
        4071,
        1120
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġto",
            "token_id": 311,
            "logprob": -0.043,
            "prob": 0.957
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -3.6719,
            "prob": 0.0255
          },
          {
            "token": "Ġmaking",
            "token_id": 3339,
            "logprob": -4.6562,
            "prob": 0.0094
          },
          {
            "token": "Ġbecause",
            "token_id": 1606,
            "logprob": -6.0312,
            "prob": 0.0024
          },
          {
            "token": "Ġso",
            "token_id": 779,
            "logprob": -6.7812,
            "prob": 0.0011
          },
          {
            "token": "Ġmake",
            "token_id": 1304,
            "logprob": -7.1562,
            "prob": 0.0008
          },
          {
            "token": "Ġwant",
            "token_id": 1390,
            "logprob": -7.6562,
            "prob": 0.0005
          },
          {
            "token": "Ġwait",
            "token_id": 3868,
            "logprob": -8.0625,
            "prob": 0.0003
          },
          {
            "token": "Ġa",
            "token_id": 264,
            "logprob": -8.4375,
            "prob": 0.0002
          },
          {
            "token": "Ġgo",
            "token_id": 733,
            "logprob": -8.8125,
            "prob": 0.0002
          }
        ]
      },
      "selected_token": {
        "token": "Ġwant",
        "token_id": 1390,
        "selection_method": "sampling"
      }
    },
    {
      "step": 95,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm okay... so first of course I need figure out which one of these countries, if China exists, or does its own question is in question here?\n\nWait, first confirm what the actual country's capital here being referenced might be. Since they're stating China and then asking its capital... so probably they're looking simply, straightforward information?\n\nBut just want",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm",
        "Ġokay",
        "...",
        "Ġso",
        "Ġfirst",
        "Ġof",
        "Ġcourse",
        "ĠI",
        "Ġneed",
        "Ġfigure",
        "Ġout",
        "Ġwhich",
        "Ġone",
        "Ġof",
        "Ġthese",
        "Ġcountries",
        ",",
        "Ġif",
        "ĠChina",
        "Ġexists",
        ",",
        "Ġor",
        "Ġdoes",
        "Ġits",
        "Ġown",
        "Ġquestion",
        "Ġis",
        "Ġin",
        "Ġquestion",
        "Ġhere",
        "?\n\n",
        "Wait",
        ",",
        "Ġfirst",
        "Ġconfirm",
        "Ġwhat",
        "Ġthe",
        "Ġactual",
        "Ġcountry",
        "'s",
        "Ġcapital",
        "Ġhere",
        "Ġbeing",
        "Ġreferenced",
        "Ġmight",
        "Ġbe",
        ".",
        "ĠSince",
        "Ġthey",
        "'re",
        "Ġstating",
        "ĠChina",
        "Ġand",
        "Ġthen",
        "Ġasking",
        "Ġits",
        "Ġcapital",
        "...",
        "Ġso",
        "Ġprobably",
        "Ġthey",
        "'re",
        "Ġlooking",
        "Ġsimply",
        ",",
        "Ġstraightforward",
        "Ġinformation",
        "?\n\n",
        "But",
        "Ġjust",
        "Ġwant"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290,
        17339,
        1131,
        779,
        1176,
        315,
        3388,
        358,
        1205,
        7216,
        704,
        902,
        832,
        315,
        1521,
        5961,
        11,
        422,
        5734,
        6866,
        11,
        477,
        1587,
        1202,
        1866,
        3488,
        374,
        304,
        3488,
        1618,
        1980,
        14524,
        11,
        1176,
        7838,
        1148,
        279,
        5150,
        3224,
        596,
        6864,
        1618,
        1694,
        25819,
        2643,
        387,
        13,
        8876,
        814,
        2351,
        28898,
        5734,
        323,
        1243,
        10371,
        1202,
        6864,
        1131,
        779,
        4762,
        814,
        2351,
        3411,
        5042,
        11,
        31439,
        2038,
        1980,
        4071,
        1120,
        1390
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġto",
            "token_id": 311,
            "logprob": -0.001,
            "prob": 1.0
          },
          {
            "token": "Ġmake",
            "token_id": 1304,
            "logprob": -7.25,
            "prob": 0.0007
          },
          {
            "token": "Ġme",
            "token_id": 757,
            "logprob": -9.5,
            "prob": 0.0001
          },
          {
            "token": "Ġdouble",
            "token_id": 2033,
            "logprob": -9.5,
            "prob": 0.0001
          },
          {
            "token": "Ġbe",
            "token_id": 387,
            "logprob": -9.875,
            "prob": 0.0001
          },
          {
            "token": "Ġmy",
            "token_id": 856,
            "logprob": -12.0,
            "prob": 0.0
          },
          {
            "token": ",",
            "token_id": 11,
            "logprob": -12.25,
            "prob": 0.0
          },
          {
            "token": "Ġstep",
            "token_id": 3094,
            "logprob": -12.375,
            "prob": 0.0
          },
          {
            "token": "Ġsure",
            "token_id": 2771,
            "logprob": -12.375,
            "prob": 0.0
          },
          {
            "token": "ĠI",
            "token_id": 358,
            "logprob": -12.5,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġto",
        "token_id": 311,
        "selection_method": "sampling"
      }
    },
    {
      "step": 96,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm okay... so first of course I need figure out which one of these countries, if China exists, or does its own question is in question here?\n\nWait, first confirm what the actual country's capital here being referenced might be. Since they're stating China and then asking its capital... so probably they're looking simply, straightforward information?\n\nBut just want to",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm",
        "Ġokay",
        "...",
        "Ġso",
        "Ġfirst",
        "Ġof",
        "Ġcourse",
        "ĠI",
        "Ġneed",
        "Ġfigure",
        "Ġout",
        "Ġwhich",
        "Ġone",
        "Ġof",
        "Ġthese",
        "Ġcountries",
        ",",
        "Ġif",
        "ĠChina",
        "Ġexists",
        ",",
        "Ġor",
        "Ġdoes",
        "Ġits",
        "Ġown",
        "Ġquestion",
        "Ġis",
        "Ġin",
        "Ġquestion",
        "Ġhere",
        "?\n\n",
        "Wait",
        ",",
        "Ġfirst",
        "Ġconfirm",
        "Ġwhat",
        "Ġthe",
        "Ġactual",
        "Ġcountry",
        "'s",
        "Ġcapital",
        "Ġhere",
        "Ġbeing",
        "Ġreferenced",
        "Ġmight",
        "Ġbe",
        ".",
        "ĠSince",
        "Ġthey",
        "'re",
        "Ġstating",
        "ĠChina",
        "Ġand",
        "Ġthen",
        "Ġasking",
        "Ġits",
        "Ġcapital",
        "...",
        "Ġso",
        "Ġprobably",
        "Ġthey",
        "'re",
        "Ġlooking",
        "Ġsimply",
        ",",
        "Ġstraightforward",
        "Ġinformation",
        "?\n\n",
        "But",
        "Ġjust",
        "Ġwant",
        "Ġto"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290,
        17339,
        1131,
        779,
        1176,
        315,
        3388,
        358,
        1205,
        7216,
        704,
        902,
        832,
        315,
        1521,
        5961,
        11,
        422,
        5734,
        6866,
        11,
        477,
        1587,
        1202,
        1866,
        3488,
        374,
        304,
        3488,
        1618,
        1980,
        14524,
        11,
        1176,
        7838,
        1148,
        279,
        5150,
        3224,
        596,
        6864,
        1618,
        1694,
        25819,
        2643,
        387,
        13,
        8876,
        814,
        2351,
        28898,
        5734,
        323,
        1243,
        10371,
        1202,
        6864,
        1131,
        779,
        4762,
        814,
        2351,
        3411,
        5042,
        11,
        31439,
        2038,
        1980,
        4071,
        1120,
        1390,
        311
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġmake",
            "token_id": 1304,
            "logprob": -0.5078,
            "prob": 0.6016
          },
          {
            "token": "Ġbe",
            "token_id": 387,
            "logprob": -1.0078,
            "prob": 0.3652
          },
          {
            "token": "Ġthink",
            "token_id": 1781,
            "logprob": -4.0,
            "prob": 0.0182
          },
          {
            "token": "Ġdouble",
            "token_id": 2033,
            "logprob": -5.5,
            "prob": 0.0041
          },
          {
            "token": "Ġcheck",
            "token_id": 1817,
            "logprob": -6.125,
            "prob": 0.0022
          },
          {
            "token": "Ġrecall",
            "token_id": 19635,
            "logprob": -6.625,
            "prob": 0.0013
          },
          {
            "token": "Ġensure",
            "token_id": 6106,
            "logprob": -6.625,
            "prob": 0.0013
          },
          {
            "token": "Ġverify",
            "token_id": 10356,
            "logprob": -6.875,
            "prob": 0.001
          },
          {
            "token": "Ġconfirm",
            "token_id": 7838,
            "logprob": -7.0,
            "prob": 0.0009
          },
          {
            "token": "Ġgo",
            "token_id": 733,
            "logprob": -7.125,
            "prob": 0.0008
          }
        ]
      },
      "selected_token": {
        "token": "Ġconfirm",
        "token_id": 7838,
        "selection_method": "sampling"
      }
    },
    {
      "step": 97,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm okay... so first of course I need figure out which one of these countries, if China exists, or does its own question is in question here?\n\nWait, first confirm what the actual country's capital here being referenced might be. Since they're stating China and then asking its capital... so probably they're looking simply, straightforward information?\n\nBut just want to confirm",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm",
        "Ġokay",
        "...",
        "Ġso",
        "Ġfirst",
        "Ġof",
        "Ġcourse",
        "ĠI",
        "Ġneed",
        "Ġfigure",
        "Ġout",
        "Ġwhich",
        "Ġone",
        "Ġof",
        "Ġthese",
        "Ġcountries",
        ",",
        "Ġif",
        "ĠChina",
        "Ġexists",
        ",",
        "Ġor",
        "Ġdoes",
        "Ġits",
        "Ġown",
        "Ġquestion",
        "Ġis",
        "Ġin",
        "Ġquestion",
        "Ġhere",
        "?\n\n",
        "Wait",
        ",",
        "Ġfirst",
        "Ġconfirm",
        "Ġwhat",
        "Ġthe",
        "Ġactual",
        "Ġcountry",
        "'s",
        "Ġcapital",
        "Ġhere",
        "Ġbeing",
        "Ġreferenced",
        "Ġmight",
        "Ġbe",
        ".",
        "ĠSince",
        "Ġthey",
        "'re",
        "Ġstating",
        "ĠChina",
        "Ġand",
        "Ġthen",
        "Ġasking",
        "Ġits",
        "Ġcapital",
        "...",
        "Ġso",
        "Ġprobably",
        "Ġthey",
        "'re",
        "Ġlooking",
        "Ġsimply",
        ",",
        "Ġstraightforward",
        "Ġinformation",
        "?\n\n",
        "But",
        "Ġjust",
        "Ġwant",
        "Ġto",
        "Ġconfirm"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290,
        17339,
        1131,
        779,
        1176,
        315,
        3388,
        358,
        1205,
        7216,
        704,
        902,
        832,
        315,
        1521,
        5961,
        11,
        422,
        5734,
        6866,
        11,
        477,
        1587,
        1202,
        1866,
        3488,
        374,
        304,
        3488,
        1618,
        1980,
        14524,
        11,
        1176,
        7838,
        1148,
        279,
        5150,
        3224,
        596,
        6864,
        1618,
        1694,
        25819,
        2643,
        387,
        13,
        8876,
        814,
        2351,
        28898,
        5734,
        323,
        1243,
        10371,
        1202,
        6864,
        1131,
        779,
        4762,
        814,
        2351,
        3411,
        5042,
        11,
        31439,
        2038,
        1980,
        4071,
        1120,
        1390,
        311,
        7838
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": ".",
            "token_id": 13,
            "logprob": -1.4062,
            "prob": 0.2451
          },
          {
            "token": ",",
            "token_id": 11,
            "logprob": -1.5312,
            "prob": 0.2158
          },
          {
            "token": ":",
            "token_id": 25,
            "logprob": -2.1562,
            "prob": 0.1157
          },
          {
            "token": "Ġmy",
            "token_id": 856,
            "logprob": -2.2812,
            "prob": 0.1021
          },
          {
            "token": "Ġthat",
            "token_id": 430,
            "logprob": -2.2812,
            "prob": 0.1021
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -2.6562,
            "prob": 0.0703
          },
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -3.5312,
            "prob": 0.0293
          },
          {
            "token": "Ġif",
            "token_id": 422,
            "logprob": -3.7812,
            "prob": 0.0228
          },
          {
            "token": "...",
            "token_id": 1131,
            "logprob": -4.6562,
            "prob": 0.0095
          },
          {
            "token": "Ġfirst",
            "token_id": 1176,
            "logprob": -4.6562,
            "prob": 0.0095
          }
        ]
      },
      "selected_token": {
        "token": ",",
        "token_id": 11,
        "selection_method": "sampling"
      }
    },
    {
      "step": 98,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm okay... so first of course I need figure out which one of these countries, if China exists, or does its own question is in question here?\n\nWait, first confirm what the actual country's capital here being referenced might be. Since they're stating China and then asking its capital... so probably they're looking simply, straightforward information?\n\nBut just want to confirm,",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm",
        "Ġokay",
        "...",
        "Ġso",
        "Ġfirst",
        "Ġof",
        "Ġcourse",
        "ĠI",
        "Ġneed",
        "Ġfigure",
        "Ġout",
        "Ġwhich",
        "Ġone",
        "Ġof",
        "Ġthese",
        "Ġcountries",
        ",",
        "Ġif",
        "ĠChina",
        "Ġexists",
        ",",
        "Ġor",
        "Ġdoes",
        "Ġits",
        "Ġown",
        "Ġquestion",
        "Ġis",
        "Ġin",
        "Ġquestion",
        "Ġhere",
        "?\n\n",
        "Wait",
        ",",
        "Ġfirst",
        "Ġconfirm",
        "Ġwhat",
        "Ġthe",
        "Ġactual",
        "Ġcountry",
        "'s",
        "Ġcapital",
        "Ġhere",
        "Ġbeing",
        "Ġreferenced",
        "Ġmight",
        "Ġbe",
        ".",
        "ĠSince",
        "Ġthey",
        "'re",
        "Ġstating",
        "ĠChina",
        "Ġand",
        "Ġthen",
        "Ġasking",
        "Ġits",
        "Ġcapital",
        "...",
        "Ġso",
        "Ġprobably",
        "Ġthey",
        "'re",
        "Ġlooking",
        "Ġsimply",
        ",",
        "Ġstraightforward",
        "Ġinformation",
        "?\n\n",
        "But",
        "Ġjust",
        "Ġwant",
        "Ġto",
        "Ġconfirm",
        ","
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290,
        17339,
        1131,
        779,
        1176,
        315,
        3388,
        358,
        1205,
        7216,
        704,
        902,
        832,
        315,
        1521,
        5961,
        11,
        422,
        5734,
        6866,
        11,
        477,
        1587,
        1202,
        1866,
        3488,
        374,
        304,
        3488,
        1618,
        1980,
        14524,
        11,
        1176,
        7838,
        1148,
        279,
        5150,
        3224,
        596,
        6864,
        1618,
        1694,
        25819,
        2643,
        387,
        13,
        8876,
        814,
        2351,
        28898,
        5734,
        323,
        1243,
        10371,
        1202,
        6864,
        1131,
        779,
        4762,
        814,
        2351,
        3411,
        5042,
        11,
        31439,
        2038,
        1980,
        4071,
        1120,
        1390,
        311,
        7838,
        11
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "ĠChina",
            "token_id": 5734,
            "logprob": -1.7969,
            "prob": 0.166
          },
          {
            "token": "Ġis",
            "token_id": 374,
            "logprob": -1.7969,
            "prob": 0.166
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -1.7969,
            "prob": 0.166
          },
          {
            "token": "Ġmaybe",
            "token_id": 7344,
            "logprob": -1.7969,
            "prob": 0.166
          },
          {
            "token": "Ġbecause",
            "token_id": 1606,
            "logprob": -2.1719,
            "prob": 0.1143
          },
          {
            "token": "Ġsometimes",
            "token_id": 7170,
            "logprob": -2.9219,
            "prob": 0.054
          },
          {
            "token": "Ġdoes",
            "token_id": 1587,
            "logprob": -3.4219,
            "prob": 0.0327
          },
          {
            "token": "ĠI",
            "token_id": 358,
            "logprob": -4.0312,
            "prob": 0.0175
          },
          {
            "token": "Ġdo",
            "token_id": 656,
            "logprob": -4.1562,
            "prob": 0.0154
          },
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -4.2812,
            "prob": 0.0136
          }
        ]
      },
      "selected_token": {
        "token": "Ġis",
        "token_id": 374,
        "selection_method": "sampling"
      }
    },
    {
      "step": 99,
      "input_text": "中国的首都是<|im_end|>\n<|im_start|>assistant\n<think>Alright? I have that question. Let see, the user asked something that sounds familiar: \"China's CAPITAL IS?\" Hmm okay... so first of course I need figure out which one of these countries, if China exists, or does its own question is in question here?\n\nWait, first confirm what the actual country's capital here being referenced might be. Since they're stating China and then asking its capital... so probably they're looking simply, straightforward information?\n\nBut just want to confirm, is",
      "tokens": [
        "中国",
        "的",
        "首",
        "都",
        "是",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Alright",
        "?",
        "ĠI",
        "Ġhave",
        "Ġthat",
        "Ġquestion",
        ".",
        "ĠLet",
        "Ġsee",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġsomething",
        "Ġthat",
        "Ġsounds",
        "Ġfamiliar",
        ":",
        "Ġ\"",
        "China",
        "'s",
        "ĠCAPITAL",
        "ĠIS",
        "?\"",
        "ĠHmm",
        "Ġokay",
        "...",
        "Ġso",
        "Ġfirst",
        "Ġof",
        "Ġcourse",
        "ĠI",
        "Ġneed",
        "Ġfigure",
        "Ġout",
        "Ġwhich",
        "Ġone",
        "Ġof",
        "Ġthese",
        "Ġcountries",
        ",",
        "Ġif",
        "ĠChina",
        "Ġexists",
        ",",
        "Ġor",
        "Ġdoes",
        "Ġits",
        "Ġown",
        "Ġquestion",
        "Ġis",
        "Ġin",
        "Ġquestion",
        "Ġhere",
        "?\n\n",
        "Wait",
        ",",
        "Ġfirst",
        "Ġconfirm",
        "Ġwhat",
        "Ġthe",
        "Ġactual",
        "Ġcountry",
        "'s",
        "Ġcapital",
        "Ġhere",
        "Ġbeing",
        "Ġreferenced",
        "Ġmight",
        "Ġbe",
        ".",
        "ĠSince",
        "Ġthey",
        "'re",
        "Ġstating",
        "ĠChina",
        "Ġand",
        "Ġthen",
        "Ġasking",
        "Ġits",
        "Ġcapital",
        "...",
        "Ġso",
        "Ġprobably",
        "Ġthey",
        "'re",
        "Ġlooking",
        "Ġsimply",
        ",",
        "Ġstraightforward",
        "Ġinformation",
        "?\n\n",
        "But",
        "Ġjust",
        "Ġwant",
        "Ġto",
        "Ġconfirm",
        ",",
        "Ġis"
      ],
      "token_ids": [
        59795,
        9554,
        61075,
        72368,
        21043,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        72586,
        30,
        358,
        617,
        430,
        3488,
        13,
        6914,
        1518,
        11,
        279,
        1217,
        4691,
        2555,
        430,
        10578,
        11537,
        25,
        330,
        23078,
        596,
        78387,
        3507,
        7673,
        89290,
        17339,
        1131,
        779,
        1176,
        315,
        3388,
        358,
        1205,
        7216,
        704,
        902,
        832,
        315,
        1521,
        5961,
        11,
        422,
        5734,
        6866,
        11,
        477,
        1587,
        1202,
        1866,
        3488,
        374,
        304,
        3488,
        1618,
        1980,
        14524,
        11,
        1176,
        7838,
        1148,
        279,
        5150,
        3224,
        596,
        6864,
        1618,
        1694,
        25819,
        2643,
        387,
        13,
        8876,
        814,
        2351,
        28898,
        5734,
        323,
        1243,
        10371,
        1202,
        6864,
        1131,
        779,
        4762,
        814,
        2351,
        3411,
        5042,
        11,
        31439,
        2038,
        1980,
        4071,
        1120,
        1390,
        311,
        7838,
        11,
        374
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "ĠChina",
            "token_id": 5734,
            "logprob": -0.5195,
            "prob": 0.5977
          },
          {
            "token": "Ġthere",
            "token_id": 1070,
            "logprob": -1.2656,
            "prob": 0.2812
          },
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -2.2656,
            "prob": 0.1035
          },
          {
            "token": "Ġit",
            "token_id": 433,
            "logprob": -4.3438,
            "prob": 0.0132
          },
          {
            "token": "ĠBeijing",
            "token_id": 27647,
            "logprob": -5.7188,
            "prob": 0.0033
          },
          {
            "token": "China",
            "token_id": 23078,
            "logprob": -7.6562,
            "prob": 0.0005
          },
          {
            "token": "Ġ\"",
            "token_id": 330,
            "logprob": -7.7812,
            "prob": 0.0004
          },
          {
            "token": "Ġthis",
            "token_id": 420,
            "logprob": -8.0,
            "prob": 0.0003
          },
          {
            "token": "Ġthat",
            "token_id": 430,
            "logprob": -8.5625,
            "prob": 0.0002
          },
          {
            "token": "Ġtoday",
            "token_id": 3432,
            "logprob": -8.625,
            "prob": 0.0002
          }
        ]
      },
      "selected_token": {
        "token": "ĠChina",
        "token_id": 5734,
        "selection_method": "sampling"
      }
    }
  ]
}