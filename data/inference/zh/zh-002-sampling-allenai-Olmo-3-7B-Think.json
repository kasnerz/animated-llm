{
  "id": "generated_20251203_121258",
  "prompt": "给我讲个笑话",
  "language": "zh",
  "temperature": 1.0,
  "top_k": 10,
  "model_info": {
    "name": "allenai/Olmo-3-7B-Think",
    "num_layers": 32,
    "hidden_size": 4096,
    "num_attention_heads": 32,
    "vocab_size": 100278
  },
  "generation_steps": [
    {
      "step": 0,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Okay",
            "token_id": 33413,
            "logprob": -0.1465,
            "prob": 0.8633
          },
          {
            "token": "�",
            "token_id": 161,
            "logprob": -2.5156,
            "prob": 0.0806
          },
          {
            "token": "Alright",
            "token_id": 72586,
            "logprob": -3.3906,
            "prob": 0.0334
          },
          {
            "token": "好",
            "token_id": 53901,
            "logprob": -4.2188,
            "prob": 0.0149
          },
          {
            "token": "ĠOkay",
            "token_id": 36539,
            "logprob": -7.0312,
            "prob": 0.0009
          },
          {
            "token": "Ġ�",
            "token_id": 4996,
            "logprob": -7.0312,
            "prob": 0.0009
          },
          {
            "token": "ĠAlright",
            "token_id": 98693,
            "logprob": -7.0938,
            "prob": 0.0008
          },
          {
            "token": "当",
            "token_id": 40265,
            "logprob": -7.8438,
            "prob": 0.0004
          },
          {
            "token": "�",
            "token_id": 45932,
            "logprob": -8.125,
            "prob": 0.0003
          },
          {
            "token": "你",
            "token_id": 57668,
            "logprob": -8.5625,
            "prob": 0.0002
          }
        ]
      },
      "selected_token": {
        "token": "Okay",
        "token_id": 33413,
        "selection_method": "sampling"
      }
    },
    {
      "step": 1,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": ",",
            "token_id": 11,
            "logprob": -0.0017,
            "prob": 1.0
          },
          {
            "token": "，",
            "token_id": 3922,
            "logprob": -6.625,
            "prob": 0.0013
          },
          {
            "token": "!",
            "token_id": 0,
            "logprob": -11.75,
            "prob": 0.0
          },
          {
            "token": "所",
            "token_id": 32938,
            "logprob": -11.9375,
            "prob": 0.0
          },
          {
            "token": "是",
            "token_id": 21043,
            "logprob": -12.0,
            "prob": 0.0
          },
          {
            "token": "不",
            "token_id": 16937,
            "logprob": -12.0625,
            "prob": 0.0
          },
          {
            "token": "一",
            "token_id": 15120,
            "logprob": -12.125,
            "prob": 0.0
          },
          {
            "token": "Ġso",
            "token_id": 779,
            "logprob": -12.1875,
            "prob": 0.0
          },
          {
            "token": "�",
            "token_id": 8067,
            "logprob": -12.1875,
            "prob": 0.0
          },
          {
            "token": "�",
            "token_id": 45114,
            "logprob": -12.25,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": ",",
        "token_id": 11,
        "selection_method": "sampling"
      }
    },
    {
      "step": 2,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay,",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ","
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -0.0087,
            "prob": 0.9922
          },
          {
            "token": "用户",
            "token_id": 20600,
            "logprob": -5.5,
            "prob": 0.0041
          },
          {
            "token": "ĠI",
            "token_id": 358,
            "logprob": -6.0,
            "prob": 0.0025
          },
          {
            "token": "Ġ用户",
            "token_id": 51142,
            "logprob": -7.5,
            "prob": 0.0005
          },
          {
            "token": "Ġso",
            "token_id": 779,
            "logprob": -8.0625,
            "prob": 0.0003
          },
          {
            "token": "我",
            "token_id": 37046,
            "logprob": -8.3125,
            "prob": 0.0002
          },
          {
            "token": "Ġuser",
            "token_id": 1217,
            "logprob": -9.0625,
            "prob": 0.0001
          },
          {
            "token": "Ġ�",
            "token_id": 47850,
            "logprob": -9.125,
            "prob": 0.0001
          },
          {
            "token": "Ġ�",
            "token_id": 33281,
            "logprob": -9.1875,
            "prob": 0.0001
          },
          {
            "token": "Ġ�",
            "token_id": 50534,
            "logprob": -9.25,
            "prob": 0.0001
          }
        ]
      },
      "selected_token": {
        "token": "Ġthe",
        "token_id": 279,
        "selection_method": "sampling"
      }
    },
    {
      "step": 3,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġuser",
            "token_id": 1217,
            "logprob": -0.0,
            "prob": 1.0
          },
          {
            "token": "用户",
            "token_id": 20600,
            "logprob": -11.25,
            "prob": 0.0
          },
          {
            "token": "user",
            "token_id": 882,
            "logprob": -13.625,
            "prob": 0.0
          },
          {
            "token": "Ġusers",
            "token_id": 3932,
            "logprob": -14.375,
            "prob": 0.0
          },
          {
            "token": "Ġcustomer",
            "token_id": 6130,
            "logprob": -14.75,
            "prob": 0.0
          },
          {
            "token": "Ġperson",
            "token_id": 1732,
            "logprob": -15.125,
            "prob": 0.0
          },
          {
            "token": "Ġ用户",
            "token_id": 51142,
            "logprob": -15.5625,
            "prob": 0.0
          },
          {
            "token": "Ġteacher",
            "token_id": 11326,
            "logprob": -16.0,
            "prob": 0.0
          },
          {
            "token": "Ġcomputer",
            "token_id": 6500,
            "logprob": -16.375,
            "prob": 0.0
          },
          {
            "token": "Ġman",
            "token_id": 893,
            "logprob": -16.625,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġuser",
        "token_id": 1217,
        "selection_method": "sampling"
      }
    },
    {
      "step": 4,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġasked",
            "token_id": 4691,
            "logprob": -0.2314,
            "prob": 0.793
          },
          {
            "token": "Ġwants",
            "token_id": 6944,
            "logprob": -1.7344,
            "prob": 0.1768
          },
          {
            "token": "Ġis",
            "token_id": 374,
            "logprob": -3.8594,
            "prob": 0.0211
          },
          {
            "token": "Ġjust",
            "token_id": 1120,
            "logprob": -4.8438,
            "prob": 0.0078
          },
          {
            "token": "Ġwanted",
            "token_id": 4934,
            "logprob": -9.1875,
            "prob": 0.0001
          },
          {
            "token": "Ġwant",
            "token_id": 1390,
            "logprob": -10.875,
            "prob": 0.0
          },
          {
            "token": "Ġasking",
            "token_id": 10371,
            "logprob": -10.9375,
            "prob": 0.0
          },
          {
            "token": "Ġrequested",
            "token_id": 11472,
            "logprob": -11.0,
            "prob": 0.0
          },
          {
            "token": "Ġasks",
            "token_id": 17501,
            "logprob": -11.375,
            "prob": 0.0
          },
          {
            "token": "Ġhas",
            "token_id": 706,
            "logprob": -12.5,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġasked",
        "token_id": 4691,
        "selection_method": "sampling"
      }
    },
    {
      "step": 5,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġfor",
            "token_id": 369,
            "logprob": -0.0703,
            "prob": 0.9336
          },
          {
            "token": "Ġme",
            "token_id": 757,
            "logprob": -2.6875,
            "prob": 0.0674
          },
          {
            "token": "我",
            "token_id": 37046,
            "logprob": -8.875,
            "prob": 0.0001
          },
          {
            "token": "Ġ�",
            "token_id": 50534,
            "logprob": -11.5,
            "prob": 0.0
          },
          {
            "token": ",",
            "token_id": 11,
            "logprob": -12.4375,
            "prob": 0.0
          },
          {
            "token": "for",
            "token_id": 2000,
            "logprob": -13.625,
            "prob": 0.0
          },
          {
            "token": "me",
            "token_id": 2727,
            "logprob": -14.375,
            "prob": 0.0
          },
          {
            "token": "-me",
            "token_id": 35073,
            "logprob": -14.4375,
            "prob": 0.0
          },
          {
            "token": "�",
            "token_id": 10414,
            "logprob": -14.75,
            "prob": 0.0
          },
          {
            "token": "Ġto",
            "token_id": 311,
            "logprob": -15.375,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġfor",
        "token_id": 369,
        "selection_method": "sampling"
      }
    },
    {
      "step": 6,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġa",
            "token_id": 264,
            "logprob": -0.0001,
            "prob": 1.0
          },
          {
            "token": "Ġan",
            "token_id": 459,
            "logprob": -10.125,
            "prob": 0.0
          },
          {
            "token": "Ġme",
            "token_id": 757,
            "logprob": -10.3125,
            "prob": 0.0
          },
          {
            "token": "一个",
            "token_id": 48044,
            "logprob": -12.75,
            "prob": 0.0
          },
          {
            "token": "我",
            "token_id": 37046,
            "logprob": -13.8125,
            "prob": 0.0
          },
          {
            "token": "a",
            "token_id": 64,
            "logprob": -14.8125,
            "prob": 0.0
          },
          {
            "token": "Ġ�",
            "token_id": 50534,
            "logprob": -14.875,
            "prob": 0.0
          },
          {
            "token": "Ġmy",
            "token_id": 856,
            "logprob": -15.6875,
            "prob": 0.0
          },
          {
            "token": "Ġ\"",
            "token_id": 330,
            "logprob": -15.6875,
            "prob": 0.0
          },
          {
            "token": "Ġsome",
            "token_id": 1063,
            "logprob": -15.875,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġa",
        "token_id": 264,
        "selection_method": "sampling"
      }
    },
    {
      "step": 7,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġjoke",
            "token_id": 22380,
            "logprob": -0.017,
            "prob": 0.9844
          },
          {
            "token": "Ġlaugh",
            "token_id": 12835,
            "logprob": -5.0312,
            "prob": 0.0066
          },
          {
            "token": "Ġfunny",
            "token_id": 15526,
            "logprob": -5.5312,
            "prob": 0.004
          },
          {
            "token": "Ġjokes",
            "token_id": 32520,
            "logprob": -5.8438,
            "prob": 0.0029
          },
          {
            "token": "笑",
            "token_id": 49838,
            "logprob": -7.0312,
            "prob": 0.0009
          },
          {
            "token": "Ġknock",
            "token_id": 14459,
            "logprob": -7.8438,
            "prob": 0.0004
          },
          {
            "token": "ĠChinese",
            "token_id": 8620,
            "logprob": -8.1875,
            "prob": 0.0003
          },
          {
            "token": "Ġsmile",
            "token_id": 15648,
            "logprob": -8.625,
            "prob": 0.0002
          },
          {
            "token": "Ġlaughing",
            "token_id": 32681,
            "logprob": -9.1875,
            "prob": 0.0001
          },
          {
            "token": "Ġstory",
            "token_id": 3446,
            "logprob": -9.375,
            "prob": 0.0001
          }
        ]
      },
      "selected_token": {
        "token": "Ġjoke",
        "token_id": 22380,
        "selection_method": "sampling"
      }
    },
    {
      "step": 8,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -0.3887,
            "prob": 0.6797
          },
          {
            "token": ".",
            "token_id": 13,
            "logprob": -1.1406,
            "prob": 0.3203
          },
          {
            "token": ",",
            "token_id": 11,
            "logprob": -7.75,
            "prob": 0.0004
          },
          {
            "token": "Ġto",
            "token_id": 311,
            "logprob": -7.8125,
            "prob": 0.0004
          },
          {
            "token": "Ġ(\"",
            "token_id": 3573,
            "logprob": -12.0625,
            "prob": 0.0
          },
          {
            "token": ":",
            "token_id": 25,
            "logprob": -12.625,
            "prob": 0.0
          },
          {
            "token": "in",
            "token_id": 258,
            "logprob": -12.9375,
            "prob": 0.0
          },
          {
            "token": "!",
            "token_id": 0,
            "logprob": -13.0,
            "prob": 0.0
          },
          {
            "token": ".\"",
            "token_id": 1210,
            "logprob": -13.0,
            "prob": 0.0
          },
          {
            "token": "Ġand",
            "token_id": 323,
            "logprob": -13.25,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": ".",
        "token_id": 13,
        "selection_method": "sampling"
      }
    },
    {
      "step": 9,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke.",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        "."
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "ĠLet",
            "token_id": 6914,
            "logprob": -0.0076,
            "prob": 0.9922
          },
          {
            "token": "ĠI",
            "token_id": 358,
            "logprob": -5.25,
            "prob": 0.0052
          },
          {
            "token": "ĠHmm",
            "token_id": 89290,
            "logprob": -7.0,
            "prob": 0.0009
          },
          {
            "token": "ĠAlright",
            "token_id": 98693,
            "logprob": -7.25,
            "prob": 0.0007
          },
          {
            "token": "ĠSince",
            "token_id": 8876,
            "logprob": -7.25,
            "prob": 0.0007
          },
          {
            "token": "ĠThey",
            "token_id": 2435,
            "logprob": -10.0625,
            "prob": 0.0
          },
          {
            "token": "Ġlet",
            "token_id": 1095,
            "logprob": -12.0,
            "prob": 0.0
          },
          {
            "token": "ĠFirst",
            "token_id": 5629,
            "logprob": -12.25,
            "prob": 0.0
          },
          {
            "token": "ĠWell",
            "token_id": 8489,
            "logprob": -12.375,
            "prob": 0.0
          },
          {
            "token": "Let",
            "token_id": 10267,
            "logprob": -12.9375,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "ĠLet",
        "token_id": 6914,
        "selection_method": "sampling"
      }
    },
    {
      "step": 10,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġme",
            "token_id": 757,
            "logprob": -0.0003,
            "prob": 1.0
          },
          {
            "token": "'s",
            "token_id": 596,
            "logprob": -8.125,
            "prob": 0.0003
          },
          {
            "token": "me",
            "token_id": 2727,
            "logprob": -13.0,
            "prob": 0.0
          },
          {
            "token": "-me",
            "token_id": 35073,
            "logprob": -15.25,
            "prob": 0.0
          },
          {
            "token": "我",
            "token_id": 37046,
            "logprob": -15.9375,
            "prob": 0.0
          },
          {
            "token": "Ġmy",
            "token_id": 856,
            "logprob": -16.0,
            "prob": 0.0
          },
          {
            "token": "'m",
            "token_id": 2846,
            "logprob": -16.25,
            "prob": 0.0
          },
          {
            "token": "Ġmet",
            "token_id": 2322,
            "logprob": -16.5,
            "prob": 0.0
          },
          {
            "token": "ĠMe",
            "token_id": 2206,
            "logprob": -16.875,
            "prob": 0.0
          },
          {
            "token": "'t",
            "token_id": 956,
            "logprob": -17.625,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġme",
        "token_id": 757,
        "selection_method": "sampling"
      }
    },
    {
      "step": 11,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġthink",
            "token_id": 1781,
            "logprob": -0.0001,
            "prob": 1.0
          },
          {
            "token": "Ġsee",
            "token_id": 1518,
            "logprob": -9.8125,
            "prob": 0.0001
          },
          {
            "token": "Ġthinks",
            "token_id": 15849,
            "logprob": -12.3125,
            "prob": 0.0
          },
          {
            "token": "Ġbrainstorm",
            "token_id": 87881,
            "logprob": -12.3125,
            "prob": 0.0
          },
          {
            "token": "Ġrecall",
            "token_id": 19635,
            "logprob": -12.5625,
            "prob": 0.0
          },
          {
            "token": "Ġthinking",
            "token_id": 7422,
            "logprob": -12.875,
            "prob": 0.0
          },
          {
            "token": "think",
            "token_id": 27963,
            "logprob": -13.25,
            "prob": 0.0
          },
          {
            "token": "Ġpick",
            "token_id": 3820,
            "logprob": -13.375,
            "prob": 0.0
          },
          {
            "token": "Ġtry",
            "token_id": 1456,
            "logprob": -13.75,
            "prob": 0.0
          },
          {
            "token": "Ġquickly",
            "token_id": 6288,
            "logprob": -14.0625,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġthink",
        "token_id": 1781,
        "selection_method": "sampling"
      }
    },
    {
      "step": 12,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġof",
            "token_id": 315,
            "logprob": -0.0123,
            "prob": 0.9883
          },
          {
            "token": "Ġabout",
            "token_id": 922,
            "logprob": -4.625,
            "prob": 0.0097
          },
          {
            "token": "...",
            "token_id": 1131,
            "logprob": -6.0625,
            "prob": 0.0023
          },
          {
            "token": ".",
            "token_id": 13,
            "logprob": -8.125,
            "prob": 0.0003
          },
          {
            "token": "…",
            "token_id": 1981,
            "logprob": -13.0,
            "prob": 0.0
          },
          {
            "token": "Ġquickly",
            "token_id": 6288,
            "logprob": -14.125,
            "prob": 0.0
          },
          {
            "token": "Ġup",
            "token_id": 709,
            "logprob": -15.0,
            "prob": 0.0
          },
          {
            "token": "Ġ...",
            "token_id": 2564,
            "logprob": -15.0625,
            "prob": 0.0
          },
          {
            "token": "of",
            "token_id": 1073,
            "logprob": -15.4375,
            "prob": 0.0
          },
          {
            "token": "about",
            "token_id": 9274,
            "logprob": -15.6875,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġof",
        "token_id": 315,
        "selection_method": "sampling"
      }
    },
    {
      "step": 13,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġa",
            "token_id": 264,
            "logprob": -0.582,
            "prob": 0.5586
          },
          {
            "token": "Ġsomething",
            "token_id": 2555,
            "logprob": -0.832,
            "prob": 0.4336
          },
          {
            "token": "Ġone",
            "token_id": 832,
            "logprob": -4.9688,
            "prob": 0.007
          },
          {
            "token": "Ġsome",
            "token_id": 1063,
            "logprob": -7.4688,
            "prob": 0.0006
          },
          {
            "token": "一个",
            "token_id": 48044,
            "logprob": -11.4375,
            "prob": 0.0
          },
          {
            "token": "something",
            "token_id": 34431,
            "logprob": -11.875,
            "prob": 0.0
          },
          {
            "token": "Ġsom",
            "token_id": 1794,
            "logprob": -11.9375,
            "prob": 0.0
          },
          {
            "token": "Ġan",
            "token_id": 459,
            "logprob": -12.5625,
            "prob": 0.0
          },
          {
            "token": "Ġsomewhere",
            "token_id": 15038,
            "logprob": -13.375,
            "prob": 0.0
          },
          {
            "token": "Ġsomeone",
            "token_id": 4423,
            "logprob": -14.0625,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġa",
        "token_id": 264,
        "selection_method": "sampling"
      }
    },
    {
      "step": 14,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġgood",
            "token_id": 1695,
            "logprob": -0.1104,
            "prob": 0.8945
          },
          {
            "token": "Ġlight",
            "token_id": 3177,
            "logprob": -2.9219,
            "prob": 0.0537
          },
          {
            "token": "Ġsimple",
            "token_id": 4382,
            "logprob": -3.5469,
            "prob": 0.0288
          },
          {
            "token": "Ġclassic",
            "token_id": 11670,
            "logprob": -4.25,
            "prob": 0.0145
          },
          {
            "token": "Ġclean",
            "token_id": 4335,
            "logprob": -5.625,
            "prob": 0.0037
          },
          {
            "token": "Ġfunny",
            "token_id": 15526,
            "logprob": -7.0,
            "prob": 0.0009
          },
          {
            "token": "Ġquick",
            "token_id": 4062,
            "logprob": -7.25,
            "prob": 0.0007
          },
          {
            "token": "Ġshort",
            "token_id": 2875,
            "logprob": -7.875,
            "prob": 0.0004
          },
          {
            "token": "Ġfew",
            "token_id": 2478,
            "logprob": -8.5,
            "prob": 0.0002
          },
          {
            "token": "Ġcommon",
            "token_id": 4279,
            "logprob": -8.5625,
            "prob": 0.0002
          }
        ]
      },
      "selected_token": {
        "token": "Ġgood",
        "token_id": 1695,
        "selection_method": "sampling"
      }
    },
    {
      "step": 15,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġone",
            "token_id": 832,
            "logprob": -0.0008,
            "prob": 1.0
          },
          {
            "token": ",",
            "token_id": 11,
            "logprob": -7.25,
            "prob": 0.0007
          },
          {
            "token": "Ġones",
            "token_id": 6305,
            "logprob": -10.5,
            "prob": 0.0
          },
          {
            "token": "One",
            "token_id": 4054,
            "logprob": -11.3125,
            "prob": 0.0
          },
          {
            "token": "Ġpun",
            "token_id": 8474,
            "logprob": -11.4375,
            "prob": 0.0
          },
          {
            "token": "ĠOne",
            "token_id": 3861,
            "logprob": -12.0,
            "prob": 0.0
          },
          {
            "token": "Ġjoke",
            "token_id": 22380,
            "logprob": -12.4375,
            "prob": 0.0
          },
          {
            "token": "one",
            "token_id": 606,
            "logprob": -13.0,
            "prob": 0.0
          },
          {
            "token": "Ġclean",
            "token_id": 4335,
            "logprob": -13.0,
            "prob": 0.0
          },
          {
            "token": "Ġsetup",
            "token_id": 6642,
            "logprob": -13.5625,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġone",
        "token_id": 832,
        "selection_method": "sampling"
      }
    },
    {
      "step": 16,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": ".",
            "token_id": 13,
            "logprob": -0.0791,
            "prob": 0.9258
          },
          {
            "token": "Ġthat",
            "token_id": 430,
            "logprob": -2.5781,
            "prob": 0.0757
          },
          {
            "token": ".\n\n",
            "token_id": 382,
            "logprob": -8.4375,
            "prob": 0.0002
          },
          {
            "token": "Ġto",
            "token_id": 311,
            "logprob": -10.5625,
            "prob": 0.0
          },
          {
            "token": "...",
            "token_id": 1131,
            "logprob": -10.9375,
            "prob": 0.0
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -13.0,
            "prob": 0.0
          },
          {
            "token": "that",
            "token_id": 9210,
            "logprob": -13.9375,
            "prob": 0.0
          },
          {
            "token": "ĠThat",
            "token_id": 3011,
            "logprob": -14.0,
            "prob": 0.0
          },
          {
            "token": "That",
            "token_id": 4897,
            "logprob": -14.375,
            "prob": 0.0
          },
          {
            "token": "Ġhere",
            "token_id": 1618,
            "logprob": -14.75,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": ".",
        "token_id": 13,
        "selection_method": "sampling"
      }
    },
    {
      "step": 17,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one.",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        "."
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "ĠI",
            "token_id": 358,
            "logprob": -0.5234,
            "prob": 0.5938
          },
          {
            "token": "ĠMaybe",
            "token_id": 10926,
            "logprob": -1.5859,
            "prob": 0.2051
          },
          {
            "token": "ĠHmm",
            "token_id": 89290,
            "logprob": -1.9609,
            "prob": 0.1406
          },
          {
            "token": "ĠIt",
            "token_id": 1102,
            "logprob": -3.0781,
            "prob": 0.0457
          },
          {
            "token": "ĠShould",
            "token_id": 12540,
            "logprob": -5.6562,
            "prob": 0.0035
          },
          {
            "token": "ĠJ",
            "token_id": 622,
            "logprob": -5.7812,
            "prob": 0.0031
          },
          {
            "token": "ĠThey",
            "token_id": 2435,
            "logprob": -5.9688,
            "prob": 0.0026
          },
          {
            "token": "ĠFirst",
            "token_id": 5629,
            "logprob": -7.2188,
            "prob": 0.0007
          },
          {
            "token": "ĠH",
            "token_id": 473,
            "logprob": -7.3438,
            "prob": 0.0007
          },
          {
            "token": "ĠSince",
            "token_id": 8876,
            "logprob": -7.6562,
            "prob": 0.0005
          }
        ]
      },
      "selected_token": {
        "token": "ĠI",
        "token_id": 358,
        "selection_method": "sampling"
      }
    },
    {
      "step": 18,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġshould",
            "token_id": 1288,
            "logprob": -0.2275,
            "prob": 0.7969
          },
          {
            "token": "Ġneed",
            "token_id": 1205,
            "logprob": -1.6016,
            "prob": 0.2012
          },
          {
            "token": "Ġwant",
            "token_id": 1390,
            "logprob": -6.3438,
            "prob": 0.0017
          },
          {
            "token": "Ġremember",
            "token_id": 6227,
            "logprob": -8.0625,
            "prob": 0.0003
          },
          {
            "token": "Ġshouldn",
            "token_id": 13434,
            "logprob": -10.75,
            "prob": 0.0
          },
          {
            "token": "Ġknow",
            "token_id": 1440,
            "logprob": -11.125,
            "prob": 0.0
          },
          {
            "token": "'ve",
            "token_id": 3077,
            "logprob": -11.3125,
            "prob": 0.0
          },
          {
            "token": "Ġguess",
            "token_id": 8101,
            "logprob": -11.5625,
            "prob": 0.0
          },
          {
            "token": "should",
            "token_id": 5562,
            "logprob": -11.9375,
            "prob": 0.0
          },
          {
            "token": "Ġusually",
            "token_id": 6118,
            "logprob": -12.0,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġshould",
        "token_id": 1288,
        "selection_method": "sampling"
      }
    },
    {
      "step": 19,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġpick",
            "token_id": 3820,
            "logprob": -0.2793,
            "prob": 0.7539
          },
          {
            "token": "Ġkeep",
            "token_id": 2567,
            "logprob": -2.0312,
            "prob": 0.1309
          },
          {
            "token": "Ġmake",
            "token_id": 1304,
            "logprob": -3.0312,
            "prob": 0.0483
          },
          {
            "token": "Ġgo",
            "token_id": 733,
            "logprob": -3.1562,
            "prob": 0.0427
          },
          {
            "token": "Ġconsider",
            "token_id": 2980,
            "logprob": -4.2812,
            "prob": 0.0139
          },
          {
            "token": "Ġchoose",
            "token_id": 5268,
            "logprob": -5.0312,
            "prob": 0.0065
          },
          {
            "token": "Ġprobably",
            "token_id": 4762,
            "logprob": -7.5938,
            "prob": 0.0005
          },
          {
            "token": "Ġtry",
            "token_id": 1456,
            "logprob": -8.125,
            "prob": 0.0003
          },
          {
            "token": "Ġavoid",
            "token_id": 5766,
            "logprob": -8.25,
            "prob": 0.0003
          },
          {
            "token": "Ġsomething",
            "token_id": 2555,
            "logprob": -9.0,
            "prob": 0.0001
          }
        ]
      },
      "selected_token": {
        "token": "Ġpick",
        "token_id": 3820,
        "selection_method": "sampling"
      }
    },
    {
      "step": 20,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġsomething",
            "token_id": 2555,
            "logprob": -0.0002,
            "prob": 1.0
          },
          {
            "token": "Ġa",
            "token_id": 264,
            "logprob": -8.625,
            "prob": 0.0002
          },
          {
            "token": "Ġone",
            "token_id": 832,
            "logprob": -10.25,
            "prob": 0.0
          },
          {
            "token": "something",
            "token_id": 34431,
            "logprob": -12.375,
            "prob": 0.0
          },
          {
            "token": "ĠSomething",
            "token_id": 25681,
            "logprob": -13.4375,
            "prob": 0.0
          },
          {
            "token": "Ġsom",
            "token_id": 1794,
            "logprob": -14.4375,
            "prob": 0.0
          },
          {
            "token": "Ġan",
            "token_id": 459,
            "logprob": -15.0625,
            "prob": 0.0
          },
          {
            "token": "Ġsomeone",
            "token_id": 4423,
            "logprob": -15.4375,
            "prob": 0.0
          },
          {
            "token": "omething",
            "token_id": 11793,
            "logprob": -15.6875,
            "prob": 0.0
          },
          {
            "token": "Ġsomewhere",
            "token_id": 15038,
            "logprob": -15.6875,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġsomething",
        "token_id": 2555,
        "selection_method": "sampling"
      }
    },
    {
      "step": 21,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġlight",
            "token_id": 3177,
            "logprob": -0.5977,
            "prob": 0.5508
          },
          {
            "token": "Ġthat",
            "token_id": 430,
            "logprob": -0.9727,
            "prob": 0.377
          },
          {
            "token": "Ġsimple",
            "token_id": 4382,
            "logprob": -3.2812,
            "prob": 0.0374
          },
          {
            "token": "Ġuniversal",
            "token_id": 20789,
            "logprob": -3.8438,
            "prob": 0.0214
          },
          {
            "token": "Ġclassic",
            "token_id": 11670,
            "logprob": -5.4062,
            "prob": 0.0045
          },
          {
            "token": "Ġclean",
            "token_id": 4335,
            "logprob": -5.7812,
            "prob": 0.0031
          },
          {
            "token": "Ġuniversally",
            "token_id": 61528,
            "logprob": -6.3438,
            "prob": 0.0017
          },
          {
            "token": "Ġl",
            "token_id": 326,
            "logprob": -6.7812,
            "prob": 0.0011
          },
          {
            "token": "Ġshort",
            "token_id": 2875,
            "logprob": -8.125,
            "prob": 0.0003
          },
          {
            "token": "Ġeasy",
            "token_id": 4228,
            "logprob": -8.375,
            "prob": 0.0002
          }
        ]
      },
      "selected_token": {
        "token": "Ġlight",
        "token_id": 3177,
        "selection_method": "sampling"
      }
    },
    {
      "step": 22,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "-hearted",
            "token_id": 70395,
            "logprob": -0.252,
            "prob": 0.7773
          },
          {
            "token": "Ġand",
            "token_id": 323,
            "logprob": -1.5,
            "prob": 0.2227
          },
          {
            "token": ",",
            "token_id": 11,
            "logprob": -8.125,
            "prob": 0.0003
          },
          {
            "token": "Ġheart",
            "token_id": 4851,
            "logprob": -10.4375,
            "prob": 0.0
          },
          {
            "token": "heart",
            "token_id": 18207,
            "logprob": -11.1875,
            "prob": 0.0
          },
          {
            "token": "-colored",
            "token_id": 58822,
            "logprob": -12.0,
            "prob": 0.0
          },
          {
            "token": "和",
            "token_id": 34208,
            "logprob": -12.25,
            "prob": 0.0
          },
          {
            "token": "-heart",
            "token_id": 43344,
            "logprob": -12.8125,
            "prob": 0.0
          },
          {
            "token": "-minded",
            "token_id": 34423,
            "logprob": -13.0,
            "prob": 0.0
          },
          {
            "token": "-handed",
            "token_id": 37240,
            "logprob": -13.0625,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "-hearted",
        "token_id": 70395,
        "selection_method": "sampling"
      }
    },
    {
      "step": 23,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġand",
            "token_id": 323,
            "logprob": -0.0043,
            "prob": 0.9961
          },
          {
            "token": "Ġthat",
            "token_id": 430,
            "logprob": -5.625,
            "prob": 0.0036
          },
          {
            "token": ",",
            "token_id": 11,
            "logprob": -7.25,
            "prob": 0.0007
          },
          {
            "token": "Ġwith",
            "token_id": 449,
            "logprob": -13.5,
            "prob": 0.0
          },
          {
            "token": "Ġbut",
            "token_id": 719,
            "logprob": -14.5625,
            "prob": 0.0
          },
          {
            "token": "that",
            "token_id": 9210,
            "logprob": -15.6875,
            "prob": 0.0
          },
          {
            "token": "Ġsince",
            "token_id": 2533,
            "logprob": -15.875,
            "prob": 0.0
          },
          {
            "token": "Ġmaybe",
            "token_id": 7344,
            "logprob": -15.875,
            "prob": 0.0
          },
          {
            "token": ".",
            "token_id": 13,
            "logprob": -16.0,
            "prob": 0.0
          },
          {
            "token": "和",
            "token_id": 34208,
            "logprob": -16.125,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġand",
        "token_id": 323,
        "selection_method": "sampling"
      }
    },
    {
      "step": 24,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġnot",
            "token_id": 539,
            "logprob": -0.2441,
            "prob": 0.7812
          },
          {
            "token": "Ġuniversal",
            "token_id": 20789,
            "logprob": -1.9922,
            "prob": 0.1357
          },
          {
            "token": "Ġuniversally",
            "token_id": 61528,
            "logprob": -3.25,
            "prob": 0.0391
          },
          {
            "token": "Ġeasy",
            "token_id": 4228,
            "logprob": -3.9375,
            "prob": 0.0197
          },
          {
            "token": "Ġmaybe",
            "token_id": 7344,
            "logprob": -4.5625,
            "prob": 0.0105
          },
          {
            "token": "Ġfamily",
            "token_id": 3070,
            "logprob": -6.125,
            "prob": 0.0022
          },
          {
            "token": "Ġappropriate",
            "token_id": 8475,
            "logprob": -6.3125,
            "prob": 0.0018
          },
          {
            "token": "Ġsimple",
            "token_id": 4382,
            "logprob": -6.5625,
            "prob": 0.0014
          },
          {
            "token": "Ġunderstandable",
            "token_id": 49839,
            "logprob": -7.5625,
            "prob": 0.0005
          },
          {
            "token": "Ġrel",
            "token_id": 1375,
            "logprob": -7.8125,
            "prob": 0.0004
          }
        ]
      },
      "selected_token": {
        "token": "Ġuniversal",
        "token_id": 20789,
        "selection_method": "sampling"
      }
    },
    {
      "step": 25,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġso",
            "token_id": 779,
            "logprob": -0.3379,
            "prob": 0.7148
          },
          {
            "token": ".",
            "token_id": 13,
            "logprob": -1.4609,
            "prob": 0.2314
          },
          {
            "token": ",",
            "token_id": 11,
            "logprob": -3.5938,
            "prob": 0.0277
          },
          {
            "token": "Ġto",
            "token_id": 311,
            "logprob": -4.2188,
            "prob": 0.0148
          },
          {
            "token": "Ġsince",
            "token_id": 2533,
            "logprob": -5.3438,
            "prob": 0.0048
          },
          {
            "token": "Ġthat",
            "token_id": 430,
            "logprob": -5.7188,
            "prob": 0.0033
          },
          {
            "token": "Ġenough",
            "token_id": 3403,
            "logprob": -5.9688,
            "prob": 0.0026
          },
          {
            "token": "Ġbecause",
            "token_id": 1606,
            "logprob": -6.9688,
            "prob": 0.0009
          },
          {
            "token": "Ġmaybe",
            "token_id": 7344,
            "logprob": -8.125,
            "prob": 0.0003
          },
          {
            "token": "—",
            "token_id": 2345,
            "logprob": -8.9375,
            "prob": 0.0001
          }
        ]
      },
      "selected_token": {
        "token": "Ġso",
        "token_id": 779,
        "selection_method": "sampling"
      }
    },
    {
      "step": 26,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal so",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal",
        "Ġso"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789,
        779
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġit",
            "token_id": 433,
            "logprob": -0.1582,
            "prob": 0.8516
          },
          {
            "token": "Ġeveryone",
            "token_id": 5127,
            "logprob": -2.5312,
            "prob": 0.0796
          },
          {
            "token": "Ġanyone",
            "token_id": 5606,
            "logprob": -3.4688,
            "prob": 0.0311
          },
          {
            "token": "Ġthat",
            "token_id": 430,
            "logprob": -3.6562,
            "prob": 0.0258
          },
          {
            "token": "Ġmost",
            "token_id": 1455,
            "logprob": -4.8438,
            "prob": 0.0079
          },
          {
            "token": "Ġpeople",
            "token_id": 1274,
            "logprob": -6.5312,
            "prob": 0.0015
          },
          {
            "token": "Ġdifferent",
            "token_id": 2204,
            "logprob": -8.1875,
            "prob": 0.0003
          },
          {
            "token": "Ġmore",
            "token_id": 810,
            "logprob": -8.4375,
            "prob": 0.0002
          },
          {
            "token": "Ġalmost",
            "token_id": 4661,
            "logprob": -8.4375,
            "prob": 0.0002
          },
          {
            "token": "Ġnobody",
            "token_id": 19093,
            "logprob": -10.75,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġit",
        "token_id": 433,
        "selection_method": "sampling"
      }
    },
    {
      "step": 27,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal so it",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal",
        "Ġso",
        "Ġit"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789,
        779,
        433
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "'s",
            "token_id": 596,
            "logprob": -0.9336,
            "prob": 0.3926
          },
          {
            "token": "Ġreson",
            "token_id": 29280,
            "logprob": -1.3047,
            "prob": 0.2695
          },
          {
            "token": "Ġworks",
            "token_id": 4375,
            "logprob": -1.9297,
            "prob": 0.1445
          },
          {
            "token": "Ġcan",
            "token_id": 649,
            "logprob": -2.8125,
            "prob": 0.0603
          },
          {
            "token": "Ġlands",
            "token_id": 12098,
            "logprob": -3.1875,
            "prob": 0.0415
          },
          {
            "token": "Ġdoesn",
            "token_id": 3250,
            "logprob": -3.6875,
            "prob": 0.0251
          },
          {
            "token": "Ġappeals",
            "token_id": 35730,
            "logprob": -4.25,
            "prob": 0.0143
          },
          {
            "token": "Ġtranslates",
            "token_id": 48018,
            "logprob": -4.25,
            "prob": 0.0143
          },
          {
            "token": "Ġrelates",
            "token_id": 36716,
            "logprob": -5.1875,
            "prob": 0.0056
          },
          {
            "token": "Ġapplies",
            "token_id": 17208,
            "logprob": -5.4375,
            "prob": 0.0044
          }
        ]
      },
      "selected_token": {
        "token": "Ġworks",
        "token_id": 4375,
        "selection_method": "sampling"
      }
    },
    {
      "step": 28,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal so it works",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal",
        "Ġso",
        "Ġit",
        "Ġworks"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789,
        779,
        433,
        4375
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġacross",
            "token_id": 4028,
            "logprob": -0.6367,
            "prob": 0.5273
          },
          {
            "token": "Ġfor",
            "token_id": 369,
            "logprob": -0.7617,
            "prob": 0.4668
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -6.125,
            "prob": 0.0022
          },
          {
            "token": "Ġwell",
            "token_id": 1664,
            "logprob": -6.75,
            "prob": 0.0012
          },
          {
            "token": "Ġwith",
            "token_id": 449,
            "logprob": -7.5625,
            "prob": 0.0005
          },
          {
            "token": "Ġregardless",
            "token_id": 15851,
            "logprob": -9.875,
            "prob": 0.0001
          },
          {
            "token": "Ġon",
            "token_id": 389,
            "logprob": -10.0625,
            "prob": 0.0
          },
          {
            "token": "Ġpretty",
            "token_id": 5128,
            "logprob": -10.375,
            "prob": 0.0
          },
          {
            "token": "Ġwithout",
            "token_id": 2085,
            "logprob": -10.5625,
            "prob": 0.0
          },
          {
            "token": "Ġwidely",
            "token_id": 13882,
            "logprob": -10.75,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġacross",
        "token_id": 4028,
        "selection_method": "sampling"
      }
    },
    {
      "step": 29,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal so it works across",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal",
        "Ġso",
        "Ġit",
        "Ġworks",
        "Ġacross"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789,
        779,
        433,
        4375,
        4028
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġdifferent",
            "token_id": 2204,
            "logprob": -0.2324,
            "prob": 0.793
          },
          {
            "token": "Ġcultures",
            "token_id": 27833,
            "logprob": -1.7344,
            "prob": 0.1768
          },
          {
            "token": "Ġages",
            "token_id": 17051,
            "logprob": -4.3438,
            "prob": 0.0128
          },
          {
            "token": "Ġage",
            "token_id": 4325,
            "logprob": -4.5938,
            "prob": 0.01
          },
          {
            "token": "Ġlanguages",
            "token_id": 15823,
            "logprob": -5.0312,
            "prob": 0.0064
          },
          {
            "token": "Ġmost",
            "token_id": 1455,
            "logprob": -8.125,
            "prob": 0.0003
          },
          {
            "token": "Ġvarious",
            "token_id": 5370,
            "logprob": -8.375,
            "prob": 0.0002
          },
          {
            "token": "Ġlanguage",
            "token_id": 4221,
            "logprob": -8.8125,
            "prob": 0.0002
          },
          {
            "token": "different",
            "token_id": 64750,
            "logprob": -9.375,
            "prob": 0.0001
          },
          {
            "token": "ĠDifferent",
            "token_id": 34496,
            "logprob": -9.6875,
            "prob": 0.0001
          }
        ]
      },
      "selected_token": {
        "token": "Ġdifferent",
        "token_id": 2204,
        "selection_method": "sampling"
      }
    },
    {
      "step": 30,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal so it works across different",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal",
        "Ġso",
        "Ġit",
        "Ġworks",
        "Ġacross",
        "Ġdifferent"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789,
        779,
        433,
        4375,
        4028,
        2204
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġcultures",
            "token_id": 27833,
            "logprob": -0.4102,
            "prob": 0.6641
          },
          {
            "token": "Ġages",
            "token_id": 17051,
            "logprob": -1.5391,
            "prob": 0.2158
          },
          {
            "token": "Ġage",
            "token_id": 4325,
            "logprob": -2.5312,
            "prob": 0.0791
          },
          {
            "token": "Ġlanguages",
            "token_id": 15823,
            "logprob": -3.5312,
            "prob": 0.0292
          },
          {
            "token": "Ġaudiences",
            "token_id": 30994,
            "logprob": -5.0312,
            "prob": 0.0065
          },
          {
            "token": "Ġgroups",
            "token_id": 5315,
            "logprob": -5.6562,
            "prob": 0.0035
          },
          {
            "token": "Ġpeople",
            "token_id": 1274,
            "logprob": -7.1562,
            "prob": 0.0008
          },
          {
            "token": "Ġbackgrounds",
            "token_id": 36576,
            "logprob": -7.7188,
            "prob": 0.0004
          },
          {
            "token": "Ġcontexts",
            "token_id": 38697,
            "logprob": -8.25,
            "prob": 0.0003
          },
          {
            "token": "Ġcultural",
            "token_id": 13042,
            "logprob": -8.4375,
            "prob": 0.0002
          }
        ]
      },
      "selected_token": {
        "token": "Ġcultures",
        "token_id": 27833,
        "selection_method": "sampling"
      }
    },
    {
      "step": 31,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal so it works across different cultures",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal",
        "Ġso",
        "Ġit",
        "Ġworks",
        "Ġacross",
        "Ġdifferent",
        "Ġcultures"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789,
        779,
        433,
        4375,
        4028,
        2204,
        27833
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": ".",
            "token_id": 13,
            "logprob": -0.3691,
            "prob": 0.6914
          },
          {
            "token": "Ġand",
            "token_id": 323,
            "logprob": -2.375,
            "prob": 0.0938
          },
          {
            "token": "Ġmaybe",
            "token_id": 7344,
            "logprob": -2.625,
            "prob": 0.0728
          },
          {
            "token": "Ġor",
            "token_id": 477,
            "logprob": -2.625,
            "prob": 0.0728
          },
          {
            "token": ",",
            "token_id": 11,
            "logprob": -3.125,
            "prob": 0.0442
          },
          {
            "token": "Ġif",
            "token_id": 422,
            "logprob": -3.875,
            "prob": 0.0209
          },
          {
            "token": ".\n\n",
            "token_id": 382,
            "logprob": -6.375,
            "prob": 0.0017
          },
          {
            "token": "Ġprobably",
            "token_id": 4762,
            "logprob": -7.75,
            "prob": 0.0004
          },
          {
            "token": "Ġwithout",
            "token_id": 2085,
            "logprob": -8.125,
            "prob": 0.0003
          },
          {
            "token": "Ġunless",
            "token_id": 7389,
            "logprob": -8.125,
            "prob": 0.0003
          }
        ]
      },
      "selected_token": {
        "token": ".",
        "token_id": 13,
        "selection_method": "sampling"
      }
    },
    {
      "step": 32,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal so it works across different cultures.",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal",
        "Ġso",
        "Ġit",
        "Ġworks",
        "Ġacross",
        "Ġdifferent",
        "Ġcultures",
        "."
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789,
        779,
        433,
        4375,
        4028,
        2204,
        27833,
        13
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "ĠMaybe",
            "token_id": 10926,
            "logprob": -0.0125,
            "prob": 0.9883
          },
          {
            "token": "ĠP",
            "token_id": 393,
            "logprob": -4.75,
            "prob": 0.0085
          },
          {
            "token": "ĠHmm",
            "token_id": 89290,
            "logprob": -6.125,
            "prob": 0.0022
          },
          {
            "token": "ĠLet",
            "token_id": 6914,
            "logprob": -8.25,
            "prob": 0.0003
          },
          {
            "token": "ĠAvoid",
            "token_id": 35106,
            "logprob": -8.4375,
            "prob": 0.0002
          },
          {
            "token": "Maybe",
            "token_id": 22105,
            "logprob": -8.5,
            "prob": 0.0002
          },
          {
            "token": "ĠProbably",
            "token_id": 38254,
            "logprob": -9.0,
            "prob": 0.0001
          },
          {
            "token": "ĠAnimals",
            "token_id": 47966,
            "logprob": -9.25,
            "prob": 0.0001
          },
          {
            "token": "Ġmaybe",
            "token_id": 7344,
            "logprob": -9.3125,
            "prob": 0.0001
          },
          {
            "token": "maybe",
            "token_id": 37860,
            "logprob": -10.375,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "ĠMaybe",
        "token_id": 10926,
        "selection_method": "sampling"
      }
    },
    {
      "step": 33,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal so it works across different cultures. Maybe",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal",
        "Ġso",
        "Ġit",
        "Ġworks",
        "Ġacross",
        "Ġdifferent",
        "Ġcultures",
        ".",
        "ĠMaybe"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789,
        779,
        433,
        4375,
        4028,
        2204,
        27833,
        13,
        10926
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġa",
            "token_id": 264,
            "logprob": -0.0364,
            "prob": 0.9648
          },
          {
            "token": "Ġsomething",
            "token_id": 2555,
            "logprob": -3.8438,
            "prob": 0.0214
          },
          {
            "token": "Ġplay",
            "token_id": 1514,
            "logprob": -4.4062,
            "prob": 0.0121
          },
          {
            "token": "Ġstart",
            "token_id": 1212,
            "logprob": -7.0312,
            "prob": 0.0009
          },
          {
            "token": "Ġplaying",
            "token_id": 5737,
            "logprob": -7.8438,
            "prob": 0.0004
          },
          {
            "token": "Ġplays",
            "token_id": 11335,
            "logprob": -8.4375,
            "prob": 0.0002
          },
          {
            "token": "Ġword",
            "token_id": 3492,
            "logprob": -8.8125,
            "prob": 0.0002
          },
          {
            "token": "Ġinvolving",
            "token_id": 16239,
            "logprob": -9.25,
            "prob": 0.0001
          },
          {
            "token": "Ġpun",
            "token_id": 8474,
            "logprob": -9.3125,
            "prob": 0.0001
          },
          {
            "token": "Ġanimal",
            "token_id": 10065,
            "logprob": -9.375,
            "prob": 0.0001
          }
        ]
      },
      "selected_token": {
        "token": "Ġa",
        "token_id": 264,
        "selection_method": "sampling"
      }
    },
    {
      "step": 34,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal so it works across different cultures. Maybe a",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal",
        "Ġso",
        "Ġit",
        "Ġworks",
        "Ġacross",
        "Ġdifferent",
        "Ġcultures",
        ".",
        "ĠMaybe",
        "Ġa"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789,
        779,
        433,
        4375,
        4028,
        2204,
        27833,
        13,
        10926,
        264
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġplay",
            "token_id": 1514,
            "logprob": -0.4922,
            "prob": 0.6094
          },
          {
            "token": "Ġclassic",
            "token_id": 11670,
            "logprob": -1.1797,
            "prob": 0.3066
          },
          {
            "token": "Ġpun",
            "token_id": 8474,
            "logprob": -2.625,
            "prob": 0.0728
          },
          {
            "token": "Ġplays",
            "token_id": 11335,
            "logprob": -6.0625,
            "prob": 0.0023
          },
          {
            "token": "Ġplayful",
            "token_id": 57169,
            "logprob": -6.875,
            "prob": 0.001
          },
          {
            "token": "Ġclassics",
            "token_id": 53805,
            "logprob": -7.25,
            "prob": 0.0007
          },
          {
            "token": "play",
            "token_id": 1387,
            "logprob": -7.375,
            "prob": 0.0006
          },
          {
            "token": "Ġmath",
            "token_id": 7033,
            "logprob": -7.6875,
            "prob": 0.0005
          },
          {
            "token": "Ġsimple",
            "token_id": 4382,
            "logprob": -7.75,
            "prob": 0.0004
          },
          {
            "token": "Ġplaying",
            "token_id": 5737,
            "logprob": -7.9375,
            "prob": 0.0004
          }
        ]
      },
      "selected_token": {
        "token": "Ġclassic",
        "token_id": 11670,
        "selection_method": "sampling"
      }
    },
    {
      "step": 35,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal so it works across different cultures. Maybe a classic",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal",
        "Ġso",
        "Ġit",
        "Ġworks",
        "Ġacross",
        "Ġdifferent",
        "Ġcultures",
        ".",
        "ĠMaybe",
        "Ġa",
        "Ġclassic"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789,
        779,
        433,
        4375,
        4028,
        2204,
        27833,
        13,
        10926,
        264,
        11670
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġsetup",
            "token_id": 6642,
            "logprob": -0.0723,
            "prob": 0.9297
          },
          {
            "token": "Ġpun",
            "token_id": 8474,
            "logprob": -3.3906,
            "prob": 0.0339
          },
          {
            "token": "Ġsetups",
            "token_id": 84823,
            "logprob": -4.8125,
            "prob": 0.0081
          },
          {
            "token": "Ġformat",
            "token_id": 3645,
            "logprob": -5.0625,
            "prob": 0.0063
          },
          {
            "token": "Ġset",
            "token_id": 743,
            "logprob": -5.4375,
            "prob": 0.0043
          },
          {
            "token": "Ġjoke",
            "token_id": 22380,
            "logprob": -5.75,
            "prob": 0.0032
          },
          {
            "token": "Ġanimal",
            "token_id": 10065,
            "logprob": -6.5,
            "prob": 0.0015
          },
          {
            "token": "Ġplay",
            "token_id": 1514,
            "logprob": -6.5938,
            "prob": 0.0014
          },
          {
            "token": "Ġstructure",
            "token_id": 6070,
            "logprob": -7.0938,
            "prob": 0.0008
          },
          {
            "token": "setup",
            "token_id": 15543,
            "logprob": -7.125,
            "prob": 0.0008
          }
        ]
      },
      "selected_token": {
        "token": "Ġsetup",
        "token_id": 6642,
        "selection_method": "sampling"
      }
    },
    {
      "step": 36,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal so it works across different cultures. Maybe a classic setup",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal",
        "Ġso",
        "Ġit",
        "Ġworks",
        "Ġacross",
        "Ġdifferent",
        "Ġcultures",
        ".",
        "ĠMaybe",
        "Ġa",
        "Ġclassic",
        "Ġsetup"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789,
        779,
        433,
        4375,
        4028,
        2204,
        27833,
        13,
        10926,
        264,
        11670,
        6642
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġand",
            "token_id": 323,
            "logprob": -0.0786,
            "prob": 0.9258
          },
          {
            "token": "Ġwith",
            "token_id": 449,
            "logprob": -3.5156,
            "prob": 0.0297
          },
          {
            "token": "-p",
            "token_id": 2320,
            "logprob": -3.8281,
            "prob": 0.0217
          },
          {
            "token": "-and",
            "token_id": 9976,
            "logprob": -3.9531,
            "prob": 0.0192
          },
          {
            "token": ".",
            "token_id": 13,
            "logprob": -8.0,
            "prob": 0.0003
          },
          {
            "token": "和",
            "token_id": 34208,
            "logprob": -8.1875,
            "prob": 0.0003
          },
          {
            "token": "Ġthat",
            "token_id": 430,
            "logprob": -8.6875,
            "prob": 0.0002
          },
          {
            "token": "Ġinvolving",
            "token_id": 16239,
            "logprob": -9.125,
            "prob": 0.0001
          },
          {
            "token": "과",
            "token_id": 54780,
            "logprob": -9.1875,
            "prob": 0.0001
          },
          {
            "token": "Ġpunch",
            "token_id": 21004,
            "logprob": -9.375,
            "prob": 0.0001
          }
        ]
      },
      "selected_token": {
        "token": "Ġand",
        "token_id": 323,
        "selection_method": "sampling"
      }
    },
    {
      "step": 37,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal so it works across different cultures. Maybe a classic setup and",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal",
        "Ġso",
        "Ġit",
        "Ġworks",
        "Ġacross",
        "Ġdifferent",
        "Ġcultures",
        ".",
        "ĠMaybe",
        "Ġa",
        "Ġclassic",
        "Ġsetup",
        "Ġand"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789,
        779,
        433,
        4375,
        4028,
        2204,
        27833,
        13,
        10926,
        264,
        11670,
        6642,
        323
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġpunch",
            "token_id": 21004,
            "logprob": -0.0102,
            "prob": 0.9883
          },
          {
            "token": "Ġa",
            "token_id": 264,
            "logprob": -4.6875,
            "prob": 0.0091
          },
          {
            "token": "Ġtwist",
            "token_id": 27744,
            "logprob": -7.375,
            "prob": 0.0006
          },
          {
            "token": "Ġpun",
            "token_id": 8474,
            "logprob": -8.25,
            "prob": 0.0003
          },
          {
            "token": "Ġunexpected",
            "token_id": 16907,
            "logprob": -10.125,
            "prob": 0.0
          },
          {
            "token": "Ġsurprise",
            "token_id": 13051,
            "logprob": -11.0625,
            "prob": 0.0
          },
          {
            "token": "ĠPunch",
            "token_id": 54275,
            "logprob": -11.0625,
            "prob": 0.0
          },
          {
            "token": "Ġpayoff",
            "token_id": 85365,
            "logprob": -11.5625,
            "prob": 0.0
          },
          {
            "token": "Ġplay",
            "token_id": 1514,
            "logprob": -12.625,
            "prob": 0.0
          },
          {
            "token": "Ġpivot",
            "token_id": 27137,
            "logprob": -12.625,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġpunch",
        "token_id": 21004,
        "selection_method": "sampling"
      }
    },
    {
      "step": 38,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal so it works across different cultures. Maybe a classic setup and punch",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal",
        "Ġso",
        "Ġit",
        "Ġworks",
        "Ġacross",
        "Ġdifferent",
        "Ġcultures",
        ".",
        "ĠMaybe",
        "Ġa",
        "Ġclassic",
        "Ġsetup",
        "Ġand",
        "Ġpunch"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789,
        779,
        433,
        4375,
        4028,
        2204,
        27833,
        13,
        10926,
        264,
        11670,
        6642,
        323,
        21004
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "line",
            "token_id": 1074,
            "logprob": -0.0,
            "prob": 1.0
          },
          {
            "token": "lines",
            "token_id": 8128,
            "logprob": -11.875,
            "prob": 0.0
          },
          {
            "token": "-line",
            "token_id": 8614,
            "logprob": -15.0,
            "prob": 0.0
          },
          {
            "token": "Ġline",
            "token_id": 1584,
            "logprob": -15.375,
            "prob": 0.0
          },
          {
            "token": "LINE",
            "token_id": 8429,
            "logprob": -16.5,
            "prob": 0.0
          },
          {
            "token": "list",
            "token_id": 1638,
            "logprob": -17.125,
            "prob": 0.0
          },
          {
            "token": "lin",
            "token_id": 3817,
            "logprob": -17.25,
            "prob": 0.0
          },
          {
            "token": "hole",
            "token_id": 31520,
            "logprob": -17.375,
            "prob": 0.0
          },
          {
            "token": "Line",
            "token_id": 2519,
            "logprob": -17.625,
            "prob": 0.0
          },
          {
            "token": "light",
            "token_id": 4238,
            "logprob": -18.125,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "line",
        "token_id": 1074,
        "selection_method": "sampling"
      }
    },
    {
      "step": 39,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal so it works across different cultures. Maybe a classic setup and punchline",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal",
        "Ġso",
        "Ġit",
        "Ġworks",
        "Ġacross",
        "Ġdifferent",
        "Ġcultures",
        ".",
        "ĠMaybe",
        "Ġa",
        "Ġclassic",
        "Ġsetup",
        "Ġand",
        "Ġpunch",
        "line"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789,
        779,
        433,
        4375,
        4028,
        2204,
        27833,
        13,
        10926,
        264,
        11670,
        6642,
        323,
        21004,
        1074
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": ".",
            "token_id": 13,
            "logprob": -0.2354,
            "prob": 0.7891
          },
          {
            "token": ".\n\n",
            "token_id": 382,
            "logprob": -2.2344,
            "prob": 0.1069
          },
          {
            "token": "Ġstructure",
            "token_id": 6070,
            "logprob": -2.4844,
            "prob": 0.0835
          },
          {
            "token": "?",
            "token_id": 30,
            "logprob": -4.625,
            "prob": 0.0099
          },
          {
            "token": "Ġformat",
            "token_id": 3645,
            "logprob": -5.125,
            "prob": 0.006
          },
          {
            "token": "?\n\n",
            "token_id": 1980,
            "logprob": -6.375,
            "prob": 0.0017
          },
          {
            "token": "Ġwith",
            "token_id": 449,
            "logprob": -6.875,
            "prob": 0.001
          },
          {
            "token": "Ġtype",
            "token_id": 955,
            "logprob": -8.75,
            "prob": 0.0002
          },
          {
            "token": "Ġcombo",
            "token_id": 23569,
            "logprob": -8.8125,
            "prob": 0.0002
          },
          {
            "token": "Ġthat",
            "token_id": 430,
            "logprob": -8.9375,
            "prob": 0.0001
          }
        ]
      },
      "selected_token": {
        "token": ".",
        "token_id": 13,
        "selection_method": "sampling"
      }
    },
    {
      "step": 40,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal so it works across different cultures. Maybe a classic setup and punchline.",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal",
        "Ġso",
        "Ġit",
        "Ġworks",
        "Ġacross",
        "Ġdifferent",
        "Ġcultures",
        ".",
        "ĠMaybe",
        "Ġa",
        "Ġclassic",
        "Ġsetup",
        "Ġand",
        "Ġpunch",
        "line",
        "."
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789,
        779,
        433,
        4375,
        4028,
        2204,
        27833,
        13,
        10926,
        264,
        11670,
        6642,
        323,
        21004,
        1074,
        13
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "ĠLet",
            "token_id": 6914,
            "logprob": -0.4414,
            "prob": 0.6445
          },
          {
            "token": "ĠAnimals",
            "token_id": 47966,
            "logprob": -2.0,
            "prob": 0.1348
          },
          {
            "token": "ĠHmm",
            "token_id": 89290,
            "logprob": -2.375,
            "prob": 0.0928
          },
          {
            "token": "ĠP",
            "token_id": 393,
            "logprob": -3.1875,
            "prob": 0.041
          },
          {
            "token": "ĠHow",
            "token_id": 2650,
            "logprob": -3.5625,
            "prob": 0.0283
          },
          {
            "token": "Ġanimals",
            "token_id": 10099,
            "logprob": -4.25,
            "prob": 0.0142
          },
          {
            "token": "ĠWhat",
            "token_id": 3639,
            "logprob": -4.5625,
            "prob": 0.0104
          },
          {
            "token": "Ġanimal",
            "token_id": 10065,
            "logprob": -5.75,
            "prob": 0.0032
          },
          {
            "token": "ĠOh",
            "token_id": 8840,
            "logprob": -6.0625,
            "prob": 0.0023
          },
          {
            "token": "ĠWait",
            "token_id": 14144,
            "logprob": -6.25,
            "prob": 0.0019
          }
        ]
      },
      "selected_token": {
        "token": "ĠHmm",
        "token_id": 89290,
        "selection_method": "sampling"
      }
    },
    {
      "step": 41,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal so it works across different cultures. Maybe a classic setup and punchline. Hmm",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal",
        "Ġso",
        "Ġit",
        "Ġworks",
        "Ġacross",
        "Ġdifferent",
        "Ġcultures",
        ".",
        "ĠMaybe",
        "Ġa",
        "Ġclassic",
        "Ġsetup",
        "Ġand",
        "Ġpunch",
        "line",
        ".",
        "ĠHmm"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789,
        779,
        433,
        4375,
        4028,
        2204,
        27833,
        13,
        10926,
        264,
        11670,
        6642,
        323,
        21004,
        1074,
        13,
        89290
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": ",",
            "token_id": 11,
            "logprob": -0.207,
            "prob": 0.8125
          },
          {
            "token": "...",
            "token_id": 1131,
            "logprob": -1.8359,
            "prob": 0.1602
          },
          {
            "token": ".\n\n",
            "token_id": 382,
            "logprob": -4.5938,
            "prob": 0.0103
          },
          {
            "token": ".",
            "token_id": 13,
            "logprob": -4.7188,
            "prob": 0.009
          },
          {
            "token": "...\n\n",
            "token_id": 2195,
            "logprob": -4.8438,
            "prob": 0.008
          },
          {
            "token": ",...\n\n",
            "token_id": 39325,
            "logprob": -9.9375,
            "prob": 0.0
          },
          {
            "token": "…",
            "token_id": 1981,
            "logprob": -10.0625,
            "prob": 0.0
          },
          {
            "token": ",...",
            "token_id": 29775,
            "logprob": -10.9375,
            "prob": 0.0
          },
          {
            "token": "..",
            "token_id": 497,
            "logprob": -11.625,
            "prob": 0.0
          },
          {
            "token": "…\n\n",
            "token_id": 5551,
            "logprob": -11.625,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": ",",
        "token_id": 11,
        "selection_method": "sampling"
      }
    },
    {
      "step": 42,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal so it works across different cultures. Maybe a classic setup and punchline. Hmm,",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal",
        "Ġso",
        "Ġit",
        "Ġworks",
        "Ġacross",
        "Ġdifferent",
        "Ġcultures",
        ".",
        "ĠMaybe",
        "Ġa",
        "Ġclassic",
        "Ġsetup",
        "Ġand",
        "Ġpunch",
        "line",
        ".",
        "ĠHmm",
        ","
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789,
        779,
        433,
        4375,
        4028,
        2204,
        27833,
        13,
        10926,
        264,
        11670,
        6642,
        323,
        21004,
        1074,
        13,
        89290,
        11
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġanimals",
            "token_id": 10099,
            "logprob": -1.1953,
            "prob": 0.3027
          },
          {
            "token": "Ġwhy",
            "token_id": 3249,
            "logprob": -1.2578,
            "prob": 0.2832
          },
          {
            "token": "Ġwhat",
            "token_id": 1148,
            "logprob": -2.3281,
            "prob": 0.0981
          },
          {
            "token": "Ġpun",
            "token_id": 8474,
            "logprob": -2.7031,
            "prob": 0.0674
          },
          {
            "token": "Ġanimal",
            "token_id": 10065,
            "logprob": -2.7031,
            "prob": 0.0674
          },
          {
            "token": "Ġhow",
            "token_id": 1268,
            "logprob": -3.5781,
            "prob": 0.0281
          },
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -3.6406,
            "prob": 0.0264
          },
          {
            "token": "ĠI",
            "token_id": 358,
            "logprob": -4.0625,
            "prob": 0.0171
          },
          {
            "token": "Ġmaybe",
            "token_id": 7344,
            "logprob": -4.0625,
            "prob": 0.0171
          },
          {
            "token": "Ġword",
            "token_id": 3492,
            "logprob": -4.375,
            "prob": 0.0125
          }
        ]
      },
      "selected_token": {
        "token": "Ġanimals",
        "token_id": 10099,
        "selection_method": "sampling"
      }
    },
    {
      "step": 43,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal so it works across different cultures. Maybe a classic setup and punchline. Hmm, animals",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal",
        "Ġso",
        "Ġit",
        "Ġworks",
        "Ġacross",
        "Ġdifferent",
        "Ġcultures",
        ".",
        "ĠMaybe",
        "Ġa",
        "Ġclassic",
        "Ġsetup",
        "Ġand",
        "Ġpunch",
        "line",
        ".",
        "ĠHmm",
        ",",
        "Ġanimals"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789,
        779,
        433,
        4375,
        4028,
        2204,
        27833,
        13,
        10926,
        264,
        11670,
        6642,
        323,
        21004,
        1074,
        13,
        89290,
        11,
        10099
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġare",
            "token_id": 527,
            "logprob": -0.0282,
            "prob": 0.9727
          },
          {
            "token": "Ġoften",
            "token_id": 3629,
            "logprob": -3.8438,
            "prob": 0.0215
          },
          {
            "token": "Ġor",
            "token_id": 477,
            "logprob": -5.7812,
            "prob": 0.0031
          },
          {
            "token": "Ġusually",
            "token_id": 6118,
            "logprob": -6.4688,
            "prob": 0.0016
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -7.1562,
            "prob": 0.0008
          },
          {
            "token": "Ġalways",
            "token_id": 2744,
            "logprob": -8.4375,
            "prob": 0.0002
          },
          {
            "token": "Ġtend",
            "token_id": 8541,
            "logprob": -8.6875,
            "prob": 0.0002
          },
          {
            "token": "?",
            "token_id": 30,
            "logprob": -8.75,
            "prob": 0.0002
          },
          {
            "token": "Ġmake",
            "token_id": 1304,
            "logprob": -10.0,
            "prob": 0.0
          },
          {
            "token": "Ġnever",
            "token_id": 2646,
            "logprob": -10.3125,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġare",
        "token_id": 527,
        "selection_method": "sampling"
      }
    },
    {
      "step": 44,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal so it works across different cultures. Maybe a classic setup and punchline. Hmm, animals are",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal",
        "Ġso",
        "Ġit",
        "Ġworks",
        "Ġacross",
        "Ġdifferent",
        "Ġcultures",
        ".",
        "ĠMaybe",
        "Ġa",
        "Ġclassic",
        "Ġsetup",
        "Ġand",
        "Ġpunch",
        "line",
        ".",
        "ĠHmm",
        ",",
        "Ġanimals",
        "Ġare"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789,
        779,
        433,
        4375,
        4028,
        2204,
        27833,
        13,
        10926,
        264,
        11670,
        6642,
        323,
        21004,
        1074,
        13,
        89290,
        11,
        10099,
        527
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġusually",
            "token_id": 6118,
            "logprob": -0.7266,
            "prob": 0.4844
          },
          {
            "token": "Ġalways",
            "token_id": 2744,
            "logprob": -0.8516,
            "prob": 0.4277
          },
          {
            "token": "Ġoften",
            "token_id": 3629,
            "logprob": -2.4688,
            "prob": 0.084
          },
          {
            "token": "Ġa",
            "token_id": 264,
            "logprob": -5.9062,
            "prob": 0.0027
          },
          {
            "token": "Ġgenerally",
            "token_id": 8965,
            "logprob": -8.0625,
            "prob": 0.0003
          },
          {
            "token": "Ġcommonly",
            "token_id": 17037,
            "logprob": -8.125,
            "prob": 0.0003
          },
          {
            "token": "always",
            "token_id": 33222,
            "logprob": -8.3125,
            "prob": 0.0003
          },
          {
            "token": "Ġtypically",
            "token_id": 11383,
            "logprob": -9.0625,
            "prob": 0.0001
          },
          {
            "token": "ĠAlways",
            "token_id": 24119,
            "logprob": -9.25,
            "prob": 0.0001
          },
          {
            "token": "Ġsafe",
            "token_id": 6220,
            "logprob": -9.5,
            "prob": 0.0001
          }
        ]
      },
      "selected_token": {
        "token": "Ġalways",
        "token_id": 2744,
        "selection_method": "sampling"
      }
    },
    {
      "step": 45,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal so it works across different cultures. Maybe a classic setup and punchline. Hmm, animals are always",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal",
        "Ġso",
        "Ġit",
        "Ġworks",
        "Ġacross",
        "Ġdifferent",
        "Ġcultures",
        ".",
        "ĠMaybe",
        "Ġa",
        "Ġclassic",
        "Ġsetup",
        "Ġand",
        "Ġpunch",
        "line",
        ".",
        "ĠHmm",
        ",",
        "Ġanimals",
        "Ġare",
        "Ġalways"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789,
        779,
        433,
        4375,
        4028,
        2204,
        27833,
        13,
        10926,
        264,
        11670,
        6642,
        323,
        21004,
        1074,
        13,
        89290,
        11,
        10099,
        527,
        2744
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġa",
            "token_id": 264,
            "logprob": -0.3164,
            "prob": 0.7266
          },
          {
            "token": "Ġsafe",
            "token_id": 6220,
            "logprob": -1.4453,
            "prob": 0.2363
          },
          {
            "token": "Ġfunny",
            "token_id": 15526,
            "logprob": -3.9375,
            "prob": 0.0194
          },
          {
            "token": "Ġreliable",
            "token_id": 15062,
            "logprob": -5.0625,
            "prob": 0.0063
          },
          {
            "token": "Ġpopular",
            "token_id": 5526,
            "logprob": -5.5625,
            "prob": 0.0038
          },
          {
            "token": "Ġfun",
            "token_id": 2523,
            "logprob": -6.4375,
            "prob": 0.0016
          },
          {
            "token": "Ġhit",
            "token_id": 4295,
            "logprob": -7.375,
            "prob": 0.0006
          },
          {
            "token": "safe",
            "token_id": 19193,
            "logprob": -7.625,
            "prob": 0.0005
          },
          {
            "token": "Ġgood",
            "token_id": 1695,
            "logprob": -7.6875,
            "prob": 0.0005
          },
          {
            "token": "Ġeasy",
            "token_id": 4228,
            "logprob": -8.0625,
            "prob": 0.0003
          }
        ]
      },
      "selected_token": {
        "token": "Ġsafe",
        "token_id": 6220,
        "selection_method": "sampling"
      }
    },
    {
      "step": 46,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal so it works across different cultures. Maybe a classic setup and punchline. Hmm, animals are always safe",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal",
        "Ġso",
        "Ġit",
        "Ġworks",
        "Ġacross",
        "Ġdifferent",
        "Ġcultures",
        ".",
        "ĠMaybe",
        "Ġa",
        "Ġclassic",
        "Ġsetup",
        "Ġand",
        "Ġpunch",
        "line",
        ".",
        "ĠHmm",
        ",",
        "Ġanimals",
        "Ġare",
        "Ġalways",
        "Ġsafe"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789,
        779,
        433,
        4375,
        4028,
        2204,
        27833,
        13,
        10926,
        264,
        11670,
        6642,
        323,
        21004,
        1074,
        13,
        89290,
        11,
        10099,
        527,
        2744,
        6220
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġbets",
            "token_id": 43177,
            "logprob": -0.5508,
            "prob": 0.5742
          },
          {
            "token": ".",
            "token_id": 13,
            "logprob": -1.0547,
            "prob": 0.3496
          },
          {
            "token": "Ġchoices",
            "token_id": 11709,
            "logprob": -3.3594,
            "prob": 0.0347
          },
          {
            "token": "Ġand",
            "token_id": 323,
            "logprob": -4.25,
            "prob": 0.0144
          },
          {
            "token": "Ġbet",
            "token_id": 1297,
            "logprob": -5.125,
            "prob": 0.006
          },
          {
            "token": "Ġfor",
            "token_id": 369,
            "logprob": -5.1875,
            "prob": 0.0056
          },
          {
            "token": ",",
            "token_id": 11,
            "logprob": -5.5,
            "prob": 0.0041
          },
          {
            "token": "Ġlaughs",
            "token_id": 49336,
            "logprob": -6.75,
            "prob": 0.0012
          },
          {
            "token": "Ġfunny",
            "token_id": 15526,
            "logprob": -6.9375,
            "prob": 0.001
          },
          {
            "token": "Ġwith",
            "token_id": 449,
            "logprob": -7.3125,
            "prob": 0.0007
          }
        ]
      },
      "selected_token": {
        "token": "Ġbets",
        "token_id": 43177,
        "selection_method": "sampling"
      }
    },
    {
      "step": 47,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal so it works across different cultures. Maybe a classic setup and punchline. Hmm, animals are always safe bets",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal",
        "Ġso",
        "Ġit",
        "Ġworks",
        "Ġacross",
        "Ġdifferent",
        "Ġcultures",
        ".",
        "ĠMaybe",
        "Ġa",
        "Ġclassic",
        "Ġsetup",
        "Ġand",
        "Ġpunch",
        "line",
        ".",
        "ĠHmm",
        ",",
        "Ġanimals",
        "Ġare",
        "Ġalways",
        "Ġsafe",
        "Ġbets"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789,
        779,
        433,
        4375,
        4028,
        2204,
        27833,
        13,
        10926,
        264,
        11670,
        6642,
        323,
        21004,
        1074,
        13,
        89290,
        11,
        10099,
        527,
        2744,
        6220,
        43177
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": ".",
            "token_id": 13,
            "logprob": -0.0447,
            "prob": 0.957
          },
          {
            "token": ".\n\n",
            "token_id": 382,
            "logprob": -3.6719,
            "prob": 0.0255
          },
          {
            "token": "Ġfor",
            "token_id": 369,
            "logprob": -4.1562,
            "prob": 0.0154
          },
          {
            "token": ",",
            "token_id": 11,
            "logprob": -6.4062,
            "prob": 0.0016
          },
          {
            "token": "Ġbecause",
            "token_id": 1606,
            "logprob": -7.1562,
            "prob": 0.0008
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -8.5625,
            "prob": 0.0002
          },
          {
            "token": "—",
            "token_id": 2345,
            "logprob": -10.0625,
            "prob": 0.0
          },
          {
            "token": "Ġwith",
            "token_id": 449,
            "logprob": -10.5625,
            "prob": 0.0
          },
          {
            "token": "Ġsince",
            "token_id": 2533,
            "logprob": -11.3125,
            "prob": 0.0
          },
          {
            "token": "Ġunless",
            "token_id": 7389,
            "logprob": -11.9375,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": ".",
        "token_id": 13,
        "selection_method": "sampling"
      }
    },
    {
      "step": 48,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal so it works across different cultures. Maybe a classic setup and punchline. Hmm, animals are always safe bets.",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal",
        "Ġso",
        "Ġit",
        "Ġworks",
        "Ġacross",
        "Ġdifferent",
        "Ġcultures",
        ".",
        "ĠMaybe",
        "Ġa",
        "Ġclassic",
        "Ġsetup",
        "Ġand",
        "Ġpunch",
        "line",
        ".",
        "ĠHmm",
        ",",
        "Ġanimals",
        "Ġare",
        "Ġalways",
        "Ġsafe",
        "Ġbets",
        "."
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789,
        779,
        433,
        4375,
        4028,
        2204,
        27833,
        13,
        10926,
        264,
        11670,
        6642,
        323,
        21004,
        1074,
        13,
        89290,
        11,
        10099,
        527,
        2744,
        6220,
        43177,
        13
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "ĠHow",
            "token_id": 2650,
            "logprob": -0.0554,
            "prob": 0.9453
          },
          {
            "token": "ĠOh",
            "token_id": 8840,
            "logprob": -3.875,
            "prob": 0.0209
          },
          {
            "token": "ĠLet",
            "token_id": 6914,
            "logprob": -4.1875,
            "prob": 0.0153
          },
          {
            "token": "ĠLike",
            "token_id": 9086,
            "logprob": -5.125,
            "prob": 0.006
          },
          {
            "token": "ĠWait",
            "token_id": 14144,
            "logprob": -6.9375,
            "prob": 0.001
          },
          {
            "token": "ĠWhat",
            "token_id": 3639,
            "logprob": -7.2188,
            "prob": 0.0007
          },
          {
            "token": "ĠP",
            "token_id": 393,
            "logprob": -7.25,
            "prob": 0.0007
          },
          {
            "token": "How",
            "token_id": 4438,
            "logprob": -7.2812,
            "prob": 0.0007
          },
          {
            "token": "ĠAnimals",
            "token_id": 47966,
            "logprob": -7.5938,
            "prob": 0.0005
          },
          {
            "token": "ĠWhy",
            "token_id": 8595,
            "logprob": -7.6875,
            "prob": 0.0005
          }
        ]
      },
      "selected_token": {
        "token": "ĠHow",
        "token_id": 2650,
        "selection_method": "sampling"
      }
    },
    {
      "step": 49,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal so it works across different cultures. Maybe a classic setup and punchline. Hmm, animals are always safe bets. How",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal",
        "Ġso",
        "Ġit",
        "Ġworks",
        "Ġacross",
        "Ġdifferent",
        "Ġcultures",
        ".",
        "ĠMaybe",
        "Ġa",
        "Ġclassic",
        "Ġsetup",
        "Ġand",
        "Ġpunch",
        "line",
        ".",
        "ĠHmm",
        ",",
        "Ġanimals",
        "Ġare",
        "Ġalways",
        "Ġsafe",
        "Ġbets",
        ".",
        "ĠHow"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789,
        779,
        433,
        4375,
        4028,
        2204,
        27833,
        13,
        10926,
        264,
        11670,
        6642,
        323,
        21004,
        1074,
        13,
        89290,
        11,
        10099,
        527,
        2744,
        6220,
        43177,
        13,
        2650
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġabout",
            "token_id": 922,
            "logprob": -0.0,
            "prob": 1.0
          },
          {
            "token": "about",
            "token_id": 9274,
            "logprob": -11.8125,
            "prob": 0.0
          },
          {
            "token": "Ġbout",
            "token_id": 25646,
            "logprob": -14.25,
            "prob": 0.0
          },
          {
            "token": "ĠAbout",
            "token_id": 10180,
            "logprob": -14.6875,
            "prob": 0.0
          },
          {
            "token": "Ġ'",
            "token_id": 364,
            "logprob": -15.625,
            "prob": 0.0
          },
          {
            "token": "ĠABOUT",
            "token_id": 52912,
            "logprob": -15.8125,
            "prob": 0.0
          },
          {
            "token": "_about",
            "token_id": 59075,
            "logprob": -16.5,
            "prob": 0.0
          },
          {
            "token": "About",
            "token_id": 10714,
            "logprob": -16.625,
            "prob": 0.0
          },
          {
            "token": "bout",
            "token_id": 74001,
            "logprob": -16.75,
            "prob": 0.0
          },
          {
            "token": "abouts",
            "token_id": 64399,
            "logprob": -17.375,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġabout",
        "token_id": 922,
        "selection_method": "sampling"
      }
    },
    {
      "step": 50,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal so it works across different cultures. Maybe a classic setup and punchline. Hmm, animals are always safe bets. How about",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal",
        "Ġso",
        "Ġit",
        "Ġworks",
        "Ġacross",
        "Ġdifferent",
        "Ġcultures",
        ".",
        "ĠMaybe",
        "Ġa",
        "Ġclassic",
        "Ġsetup",
        "Ġand",
        "Ġpunch",
        "line",
        ".",
        "ĠHmm",
        ",",
        "Ġanimals",
        "Ġare",
        "Ġalways",
        "Ġsafe",
        "Ġbets",
        ".",
        "ĠHow",
        "Ġabout"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789,
        779,
        433,
        4375,
        4028,
        2204,
        27833,
        13,
        10926,
        264,
        11670,
        6642,
        323,
        21004,
        1074,
        13,
        89290,
        11,
        10099,
        527,
        2744,
        6220,
        43177,
        13,
        2650,
        922
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġa",
            "token_id": 264,
            "logprob": -0.0267,
            "prob": 0.9727
          },
          {
            "token": "Ġsomething",
            "token_id": 2555,
            "logprob": -4.0312,
            "prob": 0.0178
          },
          {
            "token": "Ġinvolving",
            "token_id": 16239,
            "logprob": -5.7188,
            "prob": 0.0033
          },
          {
            "token": "Ġdogs",
            "token_id": 12875,
            "logprob": -6.3438,
            "prob": 0.0018
          },
          {
            "token": "Ġcats",
            "token_id": 19987,
            "logprob": -7.0938,
            "prob": 0.0008
          },
          {
            "token": "Ġanimals",
            "token_id": 10099,
            "logprob": -7.0938,
            "prob": 0.0008
          },
          {
            "token": "Ġchickens",
            "token_id": 54623,
            "logprob": -8.8125,
            "prob": 0.0001
          },
          {
            "token": "Ġthat",
            "token_id": 430,
            "logprob": -9.8125,
            "prob": 0.0001
          },
          {
            "token": "Ġpigs",
            "token_id": 49910,
            "logprob": -9.8125,
            "prob": 0.0001
          },
          {
            "token": "Ġinvolves",
            "token_id": 18065,
            "logprob": -9.9375,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġa",
        "token_id": 264,
        "selection_method": "sampling"
      }
    },
    {
      "step": 51,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal so it works across different cultures. Maybe a classic setup and punchline. Hmm, animals are always safe bets. How about a",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal",
        "Ġso",
        "Ġit",
        "Ġworks",
        "Ġacross",
        "Ġdifferent",
        "Ġcultures",
        ".",
        "ĠMaybe",
        "Ġa",
        "Ġclassic",
        "Ġsetup",
        "Ġand",
        "Ġpunch",
        "line",
        ".",
        "ĠHmm",
        ",",
        "Ġanimals",
        "Ġare",
        "Ġalways",
        "Ġsafe",
        "Ġbets",
        ".",
        "ĠHow",
        "Ġabout",
        "Ġa"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789,
        779,
        433,
        4375,
        4028,
        2204,
        27833,
        13,
        10926,
        264,
        11670,
        6642,
        323,
        21004,
        1074,
        13,
        89290,
        11,
        10099,
        527,
        2744,
        6220,
        43177,
        13,
        2650,
        922,
        264
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġdog",
            "token_id": 5679,
            "logprob": -0.2109,
            "prob": 0.8086
          },
          {
            "token": "Ġchicken",
            "token_id": 16553,
            "logprob": -2.7188,
            "prob": 0.0664
          },
          {
            "token": "Ġhorse",
            "token_id": 15580,
            "logprob": -4.0312,
            "prob": 0.0179
          },
          {
            "token": "Ġduck",
            "token_id": 37085,
            "logprob": -4.0312,
            "prob": 0.0179
          },
          {
            "token": "Ġcat",
            "token_id": 8415,
            "logprob": -4.0312,
            "prob": 0.0179
          },
          {
            "token": "Ġmonkey",
            "token_id": 39803,
            "logprob": -4.8438,
            "prob": 0.0079
          },
          {
            "token": "Ġpun",
            "token_id": 8474,
            "logprob": -4.9062,
            "prob": 0.0074
          },
          {
            "token": "Ġelephant",
            "token_id": 46840,
            "logprob": -5.3438,
            "prob": 0.0048
          },
          {
            "token": "Ġfrog",
            "token_id": 60981,
            "logprob": -5.4062,
            "prob": 0.0045
          },
          {
            "token": "Ġjoke",
            "token_id": 22380,
            "logprob": -5.75,
            "prob": 0.0032
          }
        ]
      },
      "selected_token": {
        "token": "Ġdog",
        "token_id": 5679,
        "selection_method": "sampling"
      }
    },
    {
      "step": 52,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal so it works across different cultures. Maybe a classic setup and punchline. Hmm, animals are always safe bets. How about a dog",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal",
        "Ġso",
        "Ġit",
        "Ġworks",
        "Ġacross",
        "Ġdifferent",
        "Ġcultures",
        ".",
        "ĠMaybe",
        "Ġa",
        "Ġclassic",
        "Ġsetup",
        "Ġand",
        "Ġpunch",
        "line",
        ".",
        "ĠHmm",
        ",",
        "Ġanimals",
        "Ġare",
        "Ġalways",
        "Ġsafe",
        "Ġbets",
        ".",
        "ĠHow",
        "Ġabout",
        "Ġa",
        "Ġdog"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789,
        779,
        433,
        4375,
        4028,
        2204,
        27833,
        13,
        10926,
        264,
        11670,
        6642,
        323,
        21004,
        1074,
        13,
        89290,
        11,
        10099,
        527,
        2744,
        6220,
        43177,
        13,
        2650,
        922,
        264,
        5679
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġjoke",
            "token_id": 22380,
            "logprob": -0.459,
            "prob": 0.6328
          },
          {
            "token": "?",
            "token_id": 30,
            "logprob": -1.3984,
            "prob": 0.2471
          },
          {
            "token": "Ġand",
            "token_id": 323,
            "logprob": -2.2656,
            "prob": 0.103
          },
          {
            "token": "Ġpun",
            "token_id": 8474,
            "logprob": -5.5938,
            "prob": 0.0038
          },
          {
            "token": "-related",
            "token_id": 14228,
            "logprob": -5.9062,
            "prob": 0.0027
          },
          {
            "token": "?\n\n",
            "token_id": 1980,
            "logprob": -6.0312,
            "prob": 0.0024
          },
          {
            "token": "Ġwalking",
            "token_id": 11689,
            "logprob": -6.5938,
            "prob": 0.0014
          },
          {
            "token": "Ġjokes",
            "token_id": 32520,
            "logprob": -7.0938,
            "prob": 0.0008
          },
          {
            "token": "Ġor",
            "token_id": 477,
            "logprob": -7.3438,
            "prob": 0.0007
          },
          {
            "token": "Ġsince",
            "token_id": 2533,
            "logprob": -7.5938,
            "prob": 0.0005
          }
        ]
      },
      "selected_token": {
        "token": "Ġjoke",
        "token_id": 22380,
        "selection_method": "sampling"
      }
    },
    {
      "step": 53,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal so it works across different cultures. Maybe a classic setup and punchline. Hmm, animals are always safe bets. How about a dog joke",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal",
        "Ġso",
        "Ġit",
        "Ġworks",
        "Ġacross",
        "Ġdifferent",
        "Ġcultures",
        ".",
        "ĠMaybe",
        "Ġa",
        "Ġclassic",
        "Ġsetup",
        "Ġand",
        "Ġpunch",
        "line",
        ".",
        "ĠHmm",
        ",",
        "Ġanimals",
        "Ġare",
        "Ġalways",
        "Ġsafe",
        "Ġbets",
        ".",
        "ĠHow",
        "Ġabout",
        "Ġa",
        "Ġdog",
        "Ġjoke"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789,
        779,
        433,
        4375,
        4028,
        2204,
        27833,
        13,
        10926,
        264,
        11670,
        6642,
        323,
        21004,
        1074,
        13,
        89290,
        11,
        10099,
        527,
        2744,
        6220,
        43177,
        13,
        2650,
        922,
        264,
        5679,
        22380
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "?",
            "token_id": 30,
            "logprob": -0.0234,
            "prob": 0.9766
          },
          {
            "token": "?\n\n",
            "token_id": 1980,
            "logprob": -3.7812,
            "prob": 0.0229
          },
          {
            "token": "Ġsince",
            "token_id": 2533,
            "logprob": -8.5,
            "prob": 0.0002
          },
          {
            "token": "Ġbecause",
            "token_id": 1606,
            "logprob": -10.625,
            "prob": 0.0
          },
          {
            "token": "?\n",
            "token_id": 5380,
            "logprob": -12.125,
            "prob": 0.0
          },
          {
            "token": "？",
            "token_id": 11571,
            "logprob": -14.0,
            "prob": 0.0
          },
          {
            "token": "Ġagain",
            "token_id": 1578,
            "logprob": -14.125,
            "prob": 0.0
          },
          {
            "token": "...",
            "token_id": 1131,
            "logprob": -14.4375,
            "prob": 0.0
          },
          {
            "token": "？\n\n",
            "token_id": 27948,
            "logprob": -15.375,
            "prob": 0.0
          },
          {
            "token": "?)\n\n",
            "token_id": 58877,
            "logprob": -15.375,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "?",
        "token_id": 30,
        "selection_method": "sampling"
      }
    },
    {
      "step": 54,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal so it works across different cultures. Maybe a classic setup and punchline. Hmm, animals are always safe bets. How about a dog joke?",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal",
        "Ġso",
        "Ġit",
        "Ġworks",
        "Ġacross",
        "Ġdifferent",
        "Ġcultures",
        ".",
        "ĠMaybe",
        "Ġa",
        "Ġclassic",
        "Ġsetup",
        "Ġand",
        "Ġpunch",
        "line",
        ".",
        "ĠHmm",
        ",",
        "Ġanimals",
        "Ġare",
        "Ġalways",
        "Ġsafe",
        "Ġbets",
        ".",
        "ĠHow",
        "Ġabout",
        "Ġa",
        "Ġdog",
        "Ġjoke",
        "?"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789,
        779,
        433,
        4375,
        4028,
        2204,
        27833,
        13,
        10926,
        264,
        11670,
        6642,
        323,
        21004,
        1074,
        13,
        89290,
        11,
        10099,
        527,
        2744,
        6220,
        43177,
        13,
        2650,
        922,
        264,
        5679,
        22380,
        30
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "ĠDogs",
            "token_id": 39525,
            "logprob": -0.7617,
            "prob": 0.4668
          },
          {
            "token": "ĠThey",
            "token_id": 2435,
            "logprob": -1.8906,
            "prob": 0.1514
          },
          {
            "token": "ĠWait",
            "token_id": 14144,
            "logprob": -2.0781,
            "prob": 0.126
          },
          {
            "token": "ĠOr",
            "token_id": 2582,
            "logprob": -2.6406,
            "prob": 0.0718
          },
          {
            "token": "ĠLet",
            "token_id": 6914,
            "logprob": -3.1406,
            "prob": 0.0435
          },
          {
            "token": "ĠPeople",
            "token_id": 9029,
            "logprob": -3.5156,
            "prob": 0.0298
          },
          {
            "token": "ĠLike",
            "token_id": 9086,
            "logprob": -3.7031,
            "prob": 0.0248
          },
          {
            "token": "ĠOh",
            "token_id": 8840,
            "logprob": -3.7031,
            "prob": 0.0248
          },
          {
            "token": "Ġdogs",
            "token_id": 12875,
            "logprob": -4.0625,
            "prob": 0.017
          },
          {
            "token": "ĠAnimals",
            "token_id": 47966,
            "logprob": -4.9375,
            "prob": 0.0071
          }
        ]
      },
      "selected_token": {
        "token": "ĠDogs",
        "token_id": 39525,
        "selection_method": "sampling"
      }
    },
    {
      "step": 55,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal so it works across different cultures. Maybe a classic setup and punchline. Hmm, animals are always safe bets. How about a dog joke? Dogs",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal",
        "Ġso",
        "Ġit",
        "Ġworks",
        "Ġacross",
        "Ġdifferent",
        "Ġcultures",
        ".",
        "ĠMaybe",
        "Ġa",
        "Ġclassic",
        "Ġsetup",
        "Ġand",
        "Ġpunch",
        "line",
        ".",
        "ĠHmm",
        ",",
        "Ġanimals",
        "Ġare",
        "Ġalways",
        "Ġsafe",
        "Ġbets",
        ".",
        "ĠHow",
        "Ġabout",
        "Ġa",
        "Ġdog",
        "Ġjoke",
        "?",
        "ĠDogs"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789,
        779,
        433,
        4375,
        4028,
        2204,
        27833,
        13,
        10926,
        264,
        11670,
        6642,
        323,
        21004,
        1074,
        13,
        89290,
        11,
        10099,
        527,
        2744,
        6220,
        43177,
        13,
        2650,
        922,
        264,
        5679,
        22380,
        30,
        39525
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġare",
            "token_id": 527,
            "logprob": -0.0002,
            "prob": 1.0
          },
          {
            "token": "Ġlove",
            "token_id": 3021,
            "logprob": -9.8125,
            "prob": 0.0001
          },
          {
            "token": "Ġhave",
            "token_id": 617,
            "logprob": -11.25,
            "prob": 0.0
          },
          {
            "token": "Ġand",
            "token_id": 323,
            "logprob": -11.3125,
            "prob": 0.0
          },
          {
            "token": "Ġmake",
            "token_id": 1304,
            "logprob": -11.8125,
            "prob": 0.0
          },
          {
            "token": "Ġdo",
            "token_id": 656,
            "logprob": -12.5625,
            "prob": 0.0
          },
          {
            "token": "Ġcan",
            "token_id": 649,
            "logprob": -13.0,
            "prob": 0.0
          },
          {
            "token": "Are",
            "token_id": 11787,
            "logprob": -13.0,
            "prob": 0.0
          },
          {
            "token": "Ġis",
            "token_id": 374,
            "logprob": -13.1875,
            "prob": 0.0
          },
          {
            "token": "ĠAre",
            "token_id": 8886,
            "logprob": -13.25,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġare",
        "token_id": 527,
        "selection_method": "sampling"
      }
    },
    {
      "step": 56,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal so it works across different cultures. Maybe a classic setup and punchline. Hmm, animals are always safe bets. How about a dog joke? Dogs are",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal",
        "Ġso",
        "Ġit",
        "Ġworks",
        "Ġacross",
        "Ġdifferent",
        "Ġcultures",
        ".",
        "ĠMaybe",
        "Ġa",
        "Ġclassic",
        "Ġsetup",
        "Ġand",
        "Ġpunch",
        "line",
        ".",
        "ĠHmm",
        ",",
        "Ġanimals",
        "Ġare",
        "Ġalways",
        "Ġsafe",
        "Ġbets",
        ".",
        "ĠHow",
        "Ġabout",
        "Ġa",
        "Ġdog",
        "Ġjoke",
        "?",
        "ĠDogs",
        "Ġare"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789,
        779,
        433,
        4375,
        4028,
        2204,
        27833,
        13,
        10926,
        264,
        11670,
        6642,
        323,
        21004,
        1074,
        13,
        89290,
        11,
        10099,
        527,
        2744,
        6220,
        43177,
        13,
        2650,
        922,
        264,
        5679,
        22380,
        30,
        39525,
        527
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġcute",
            "token_id": 19369,
            "logprob": -0.5234,
            "prob": 0.5938
          },
          {
            "token": "Ġpopular",
            "token_id": 5526,
            "logprob": -2.0156,
            "prob": 0.1318
          },
          {
            "token": "Ġrel",
            "token_id": 1375,
            "logprob": -2.7656,
            "prob": 0.0625
          },
          {
            "token": "Ġfunny",
            "token_id": 15526,
            "logprob": -3.2031,
            "prob": 0.0403
          },
          {
            "token": "Ġlo",
            "token_id": 781,
            "logprob": -3.5156,
            "prob": 0.0295
          },
          {
            "token": "Ġbeloved",
            "token_id": 28530,
            "logprob": -3.5156,
            "prob": 0.0295
          },
          {
            "token": "Ġeverywhere",
            "token_id": 17277,
            "logprob": -3.8281,
            "prob": 0.0216
          },
          {
            "token": "Ġcommon",
            "token_id": 4279,
            "logprob": -3.8906,
            "prob": 0.0203
          },
          {
            "token": "Ġcommonly",
            "token_id": 17037,
            "logprob": -4.0938,
            "prob": 0.0168
          },
          {
            "token": "Ġloved",
            "token_id": 10456,
            "logprob": -4.5938,
            "prob": 0.0102
          }
        ]
      },
      "selected_token": {
        "token": "Ġrel",
        "token_id": 1375,
        "selection_method": "sampling"
      }
    },
    {
      "step": 57,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal so it works across different cultures. Maybe a classic setup and punchline. Hmm, animals are always safe bets. How about a dog joke? Dogs are rel",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal",
        "Ġso",
        "Ġit",
        "Ġworks",
        "Ġacross",
        "Ġdifferent",
        "Ġcultures",
        ".",
        "ĠMaybe",
        "Ġa",
        "Ġclassic",
        "Ġsetup",
        "Ġand",
        "Ġpunch",
        "line",
        ".",
        "ĠHmm",
        ",",
        "Ġanimals",
        "Ġare",
        "Ġalways",
        "Ġsafe",
        "Ġbets",
        ".",
        "ĠHow",
        "Ġabout",
        "Ġa",
        "Ġdog",
        "Ġjoke",
        "?",
        "ĠDogs",
        "Ġare",
        "Ġrel"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789,
        779,
        433,
        4375,
        4028,
        2204,
        27833,
        13,
        10926,
        264,
        11670,
        6642,
        323,
        21004,
        1074,
        13,
        89290,
        11,
        10099,
        527,
        2744,
        6220,
        43177,
        13,
        2650,
        922,
        264,
        5679,
        22380,
        30,
        39525,
        527,
        1375
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "atable",
            "token_id": 15436,
            "logprob": -0.0,
            "prob": 1.0
          },
          {
            "token": "atab",
            "token_id": 2143,
            "logprob": -13.1875,
            "prob": 0.0
          },
          {
            "token": "fect",
            "token_id": 3836,
            "logprob": -15.0,
            "prob": 0.0
          },
          {
            "token": "ated",
            "token_id": 660,
            "logprob": -15.375,
            "prob": 0.0
          },
          {
            "token": "ater",
            "token_id": 977,
            "logprob": -15.5,
            "prob": 0.0
          },
          {
            "token": "ays",
            "token_id": 954,
            "logprob": -15.5625,
            "prob": 0.0
          },
          {
            "token": "able",
            "token_id": 481,
            "logprob": -15.5625,
            "prob": 0.0
          },
          {
            "token": "ath",
            "token_id": 589,
            "logprob": -16.125,
            "prob": 0.0
          },
          {
            "token": "izable",
            "token_id": 8499,
            "logprob": -16.5,
            "prob": 0.0
          },
          {
            "token": "ible",
            "token_id": 1260,
            "logprob": -16.625,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "atable",
        "token_id": 15436,
        "selection_method": "sampling"
      }
    },
    {
      "step": 58,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal so it works across different cultures. Maybe a classic setup and punchline. Hmm, animals are always safe bets. How about a dog joke? Dogs are relatable",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal",
        "Ġso",
        "Ġit",
        "Ġworks",
        "Ġacross",
        "Ġdifferent",
        "Ġcultures",
        ".",
        "ĠMaybe",
        "Ġa",
        "Ġclassic",
        "Ġsetup",
        "Ġand",
        "Ġpunch",
        "line",
        ".",
        "ĠHmm",
        ",",
        "Ġanimals",
        "Ġare",
        "Ġalways",
        "Ġsafe",
        "Ġbets",
        ".",
        "ĠHow",
        "Ġabout",
        "Ġa",
        "Ġdog",
        "Ġjoke",
        "?",
        "ĠDogs",
        "Ġare",
        "Ġrel",
        "atable"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789,
        779,
        433,
        4375,
        4028,
        2204,
        27833,
        13,
        10926,
        264,
        11670,
        6642,
        323,
        21004,
        1074,
        13,
        89290,
        11,
        10099,
        527,
        2744,
        6220,
        43177,
        13,
        2650,
        922,
        264,
        5679,
        22380,
        30,
        39525,
        527,
        1375,
        15436
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": ".",
            "token_id": 13,
            "logprob": -0.8789,
            "prob": 0.416
          },
          {
            "token": ".\n\n",
            "token_id": 382,
            "logprob": -1.125,
            "prob": 0.3242
          },
          {
            "token": "Ġand",
            "token_id": 323,
            "logprob": -1.5,
            "prob": 0.2227
          },
          {
            "token": ",",
            "token_id": 11,
            "logprob": -3.3125,
            "prob": 0.0364
          },
          {
            "token": "Ġpets",
            "token_id": 26159,
            "logprob": -6.8125,
            "prob": 0.0011
          },
          {
            "token": "Ġto",
            "token_id": 311,
            "logprob": -8.4375,
            "prob": 0.0002
          },
          {
            "token": "Ġeverywhere",
            "token_id": 17277,
            "logprob": -8.625,
            "prob": 0.0002
          },
          {
            "token": "Ġpet",
            "token_id": 6896,
            "logprob": -9.1875,
            "prob": 0.0001
          },
          {
            "token": "Ġfor",
            "token_id": 369,
            "logprob": -9.4375,
            "prob": 0.0001
          },
          {
            "token": "Ġcompanions",
            "token_id": 41957,
            "logprob": -10.5625,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": ".",
        "token_id": 13,
        "selection_method": "sampling"
      }
    },
    {
      "step": 59,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal so it works across different cultures. Maybe a classic setup and punchline. Hmm, animals are always safe bets. How about a dog joke? Dogs are relatable.",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal",
        "Ġso",
        "Ġit",
        "Ġworks",
        "Ġacross",
        "Ġdifferent",
        "Ġcultures",
        ".",
        "ĠMaybe",
        "Ġa",
        "Ġclassic",
        "Ġsetup",
        "Ġand",
        "Ġpunch",
        "line",
        ".",
        "ĠHmm",
        ",",
        "Ġanimals",
        "Ġare",
        "Ġalways",
        "Ġsafe",
        "Ġbets",
        ".",
        "ĠHow",
        "Ġabout",
        "Ġa",
        "Ġdog",
        "Ġjoke",
        "?",
        "ĠDogs",
        "Ġare",
        "Ġrel",
        "atable",
        "."
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789,
        779,
        433,
        4375,
        4028,
        2204,
        27833,
        13,
        10926,
        264,
        11670,
        6642,
        323,
        21004,
        1074,
        13,
        89290,
        11,
        10099,
        527,
        2744,
        6220,
        43177,
        13,
        2650,
        922,
        264,
        5679,
        22380,
        30,
        39525,
        527,
        1375,
        15436,
        13
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "ĠLet",
            "token_id": 6914,
            "logprob": -0.4277,
            "prob": 0.6523
          },
          {
            "token": "ĠWait",
            "token_id": 14144,
            "logprob": -1.1797,
            "prob": 0.3086
          },
          {
            "token": "ĠOh",
            "token_id": 8840,
            "logprob": -3.9219,
            "prob": 0.0197
          },
          {
            "token": "ĠOr",
            "token_id": 2582,
            "logprob": -4.1875,
            "prob": 0.0153
          },
          {
            "token": "Ġ\n\n",
            "token_id": 4815,
            "logprob": -6.5,
            "prob": 0.0015
          },
          {
            "token": "ĠWhat",
            "token_id": 3639,
            "logprob": -6.625,
            "prob": 0.0013
          },
          {
            "token": "ĠAlright",
            "token_id": 98693,
            "logprob": -8.3125,
            "prob": 0.0002
          },
          {
            "token": "ĠThe",
            "token_id": 578,
            "logprob": -8.625,
            "prob": 0.0002
          },
          {
            "token": "ĠYeah",
            "token_id": 22335,
            "logprob": -8.8125,
            "prob": 0.0002
          },
          {
            "token": "ĠLike",
            "token_id": 9086,
            "logprob": -9.0,
            "prob": 0.0001
          }
        ]
      },
      "selected_token": {
        "token": "ĠLet",
        "token_id": 6914,
        "selection_method": "sampling"
      }
    },
    {
      "step": 60,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal so it works across different cultures. Maybe a classic setup and punchline. Hmm, animals are always safe bets. How about a dog joke? Dogs are relatable. Let",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal",
        "Ġso",
        "Ġit",
        "Ġworks",
        "Ġacross",
        "Ġdifferent",
        "Ġcultures",
        ".",
        "ĠMaybe",
        "Ġa",
        "Ġclassic",
        "Ġsetup",
        "Ġand",
        "Ġpunch",
        "line",
        ".",
        "ĠHmm",
        ",",
        "Ġanimals",
        "Ġare",
        "Ġalways",
        "Ġsafe",
        "Ġbets",
        ".",
        "ĠHow",
        "Ġabout",
        "Ġa",
        "Ġdog",
        "Ġjoke",
        "?",
        "ĠDogs",
        "Ġare",
        "Ġrel",
        "atable",
        ".",
        "ĠLet"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789,
        779,
        433,
        4375,
        4028,
        2204,
        27833,
        13,
        10926,
        264,
        11670,
        6642,
        323,
        21004,
        1074,
        13,
        89290,
        11,
        10099,
        527,
        2744,
        6220,
        43177,
        13,
        2650,
        922,
        264,
        5679,
        22380,
        30,
        39525,
        527,
        1375,
        15436,
        13,
        6914
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "'s",
            "token_id": 596,
            "logprob": -0.5742,
            "prob": 0.5625
          },
          {
            "token": "Ġme",
            "token_id": 757,
            "logprob": -0.8242,
            "prob": 0.4375
          },
          {
            "token": "Ġsee",
            "token_id": 1518,
            "logprob": -9.4375,
            "prob": 0.0001
          },
          {
            "token": "’s",
            "token_id": 753,
            "logprob": -12.375,
            "prob": 0.0
          },
          {
            "token": "me",
            "token_id": 2727,
            "logprob": -13.5,
            "prob": 0.0
          },
          {
            "token": "'",
            "token_id": 6,
            "logprob": -13.5625,
            "prob": 0.0
          },
          {
            "token": "ĠMe",
            "token_id": 2206,
            "logprob": -14.0,
            "prob": 0.0
          },
          {
            "token": "Ġmet",
            "token_id": 2322,
            "logprob": -14.0,
            "prob": 0.0
          },
          {
            "token": "Ġsees",
            "token_id": 16008,
            "logprob": -14.125,
            "prob": 0.0
          },
          {
            "token": "'m",
            "token_id": 2846,
            "logprob": -14.5625,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġme",
        "token_id": 757,
        "selection_method": "sampling"
      }
    },
    {
      "step": 61,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal so it works across different cultures. Maybe a classic setup and punchline. Hmm, animals are always safe bets. How about a dog joke? Dogs are relatable. Let me",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal",
        "Ġso",
        "Ġit",
        "Ġworks",
        "Ġacross",
        "Ġdifferent",
        "Ġcultures",
        ".",
        "ĠMaybe",
        "Ġa",
        "Ġclassic",
        "Ġsetup",
        "Ġand",
        "Ġpunch",
        "line",
        ".",
        "ĠHmm",
        ",",
        "Ġanimals",
        "Ġare",
        "Ġalways",
        "Ġsafe",
        "Ġbets",
        ".",
        "ĠHow",
        "Ġabout",
        "Ġa",
        "Ġdog",
        "Ġjoke",
        "?",
        "ĠDogs",
        "Ġare",
        "Ġrel",
        "atable",
        ".",
        "ĠLet",
        "Ġme"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789,
        779,
        433,
        4375,
        4028,
        2204,
        27833,
        13,
        10926,
        264,
        11670,
        6642,
        323,
        21004,
        1074,
        13,
        89290,
        11,
        10099,
        527,
        2744,
        6220,
        43177,
        13,
        2650,
        922,
        264,
        5679,
        22380,
        30,
        39525,
        527,
        1375,
        15436,
        13,
        6914,
        757
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġrecall",
            "token_id": 19635,
            "logprob": -0.5508,
            "prob": 0.5742
          },
          {
            "token": "Ġsee",
            "token_id": 1518,
            "logprob": -1.0547,
            "prob": 0.3496
          },
          {
            "token": "Ġremember",
            "token_id": 6227,
            "logprob": -3.2969,
            "prob": 0.0369
          },
          {
            "token": "Ġbrainstorm",
            "token_id": 87881,
            "logprob": -4.1875,
            "prob": 0.0154
          },
          {
            "token": "Ġtry",
            "token_id": 1456,
            "logprob": -4.4375,
            "prob": 0.012
          },
          {
            "token": "Ġcome",
            "token_id": 2586,
            "logprob": -4.8125,
            "prob": 0.0082
          },
          {
            "token": "Ġthink",
            "token_id": 1781,
            "logprob": -7.5,
            "prob": 0.0006
          },
          {
            "token": "Ġstructure",
            "token_id": 6070,
            "logprob": -7.9375,
            "prob": 0.0004
          },
          {
            "token": "...",
            "token_id": 1131,
            "logprob": -8.8125,
            "prob": 0.0002
          },
          {
            "token": "Ġcraft",
            "token_id": 11003,
            "logprob": -9.0,
            "prob": 0.0001
          }
        ]
      },
      "selected_token": {
        "token": "Ġrecall",
        "token_id": 19635,
        "selection_method": "sampling"
      }
    },
    {
      "step": 62,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal so it works across different cultures. Maybe a classic setup and punchline. Hmm, animals are always safe bets. How about a dog joke? Dogs are relatable. Let me recall",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal",
        "Ġso",
        "Ġit",
        "Ġworks",
        "Ġacross",
        "Ġdifferent",
        "Ġcultures",
        ".",
        "ĠMaybe",
        "Ġa",
        "Ġclassic",
        "Ġsetup",
        "Ġand",
        "Ġpunch",
        "line",
        ".",
        "ĠHmm",
        ",",
        "Ġanimals",
        "Ġare",
        "Ġalways",
        "Ġsafe",
        "Ġbets",
        ".",
        "ĠHow",
        "Ġabout",
        "Ġa",
        "Ġdog",
        "Ġjoke",
        "?",
        "ĠDogs",
        "Ġare",
        "Ġrel",
        "atable",
        ".",
        "ĠLet",
        "Ġme",
        "Ġrecall"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789,
        779,
        433,
        4375,
        4028,
        2204,
        27833,
        13,
        10926,
        264,
        11670,
        6642,
        323,
        21004,
        1074,
        13,
        89290,
        11,
        10099,
        527,
        2744,
        6220,
        43177,
        13,
        2650,
        922,
        264,
        5679,
        22380,
        30,
        39525,
        527,
        1375,
        15436,
        13,
        6914,
        757,
        19635
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġsome",
            "token_id": 1063,
            "logprob": -0.0121,
            "prob": 0.9883
          },
          {
            "token": "Ġa",
            "token_id": 264,
            "logprob": -4.875,
            "prob": 0.0075
          },
          {
            "token": "...",
            "token_id": 1131,
            "logprob": -5.625,
            "prob": 0.0036
          },
          {
            "token": ":",
            "token_id": 25,
            "logprob": -7.5,
            "prob": 0.0005
          },
          {
            "token": "Ġthat",
            "token_id": 430,
            "logprob": -9.125,
            "prob": 0.0001
          },
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -9.1875,
            "prob": 0.0001
          },
          {
            "token": "Ġcommon",
            "token_id": 4279,
            "logprob": -9.375,
            "prob": 0.0001
          },
          {
            "token": ".",
            "token_id": 13,
            "logprob": -10.75,
            "prob": 0.0
          },
          {
            "token": "…",
            "token_id": 1981,
            "logprob": -11.1875,
            "prob": 0.0
          },
          {
            "token": "some",
            "token_id": 15031,
            "logprob": -11.4375,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġsome",
        "token_id": 1063,
        "selection_method": "sampling"
      }
    },
    {
      "step": 63,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal so it works across different cultures. Maybe a classic setup and punchline. Hmm, animals are always safe bets. How about a dog joke? Dogs are relatable. Let me recall some",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal",
        "Ġso",
        "Ġit",
        "Ġworks",
        "Ġacross",
        "Ġdifferent",
        "Ġcultures",
        ".",
        "ĠMaybe",
        "Ġa",
        "Ġclassic",
        "Ġsetup",
        "Ġand",
        "Ġpunch",
        "line",
        ".",
        "ĠHmm",
        ",",
        "Ġanimals",
        "Ġare",
        "Ġalways",
        "Ġsafe",
        "Ġbets",
        ".",
        "ĠHow",
        "Ġabout",
        "Ġa",
        "Ġdog",
        "Ġjoke",
        "?",
        "ĠDogs",
        "Ġare",
        "Ġrel",
        "atable",
        ".",
        "ĠLet",
        "Ġme",
        "Ġrecall",
        "Ġsome"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789,
        779,
        433,
        4375,
        4028,
        2204,
        27833,
        13,
        10926,
        264,
        11670,
        6642,
        323,
        21004,
        1074,
        13,
        89290,
        11,
        10099,
        527,
        2744,
        6220,
        43177,
        13,
        2650,
        922,
        264,
        5679,
        22380,
        30,
        39525,
        527,
        1375,
        15436,
        13,
        6914,
        757,
        19635,
        1063
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġdog",
            "token_id": 5679,
            "logprob": -0.5039,
            "prob": 0.6055
          },
          {
            "token": "Ġcommon",
            "token_id": 4279,
            "logprob": -1.0078,
            "prob": 0.3652
          },
          {
            "token": "Ġclassic",
            "token_id": 11670,
            "logprob": -4.3125,
            "prob": 0.0134
          },
          {
            "token": "Ġtypical",
            "token_id": 14595,
            "logprob": -5.9375,
            "prob": 0.0026
          },
          {
            "token": "Ġpopular",
            "token_id": 5526,
            "logprob": -5.9375,
            "prob": 0.0026
          },
          {
            "token": "Ġstandard",
            "token_id": 5410,
            "logprob": -6.0,
            "prob": 0.0025
          },
          {
            "token": "Ġanimal",
            "token_id": 10065,
            "logprob": -6.375,
            "prob": 0.0017
          },
          {
            "token": "dog",
            "token_id": 18964,
            "logprob": -7.1875,
            "prob": 0.0008
          },
          {
            "token": ".",
            "token_id": 13,
            "logprob": -7.1875,
            "prob": 0.0008
          },
          {
            "token": "Ġvariations",
            "token_id": 27339,
            "logprob": -7.3125,
            "prob": 0.0007
          }
        ]
      },
      "selected_token": {
        "token": "Ġdog",
        "token_id": 5679,
        "selection_method": "sampling"
      }
    },
    {
      "step": 64,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal so it works across different cultures. Maybe a classic setup and punchline. Hmm, animals are always safe bets. How about a dog joke? Dogs are relatable. Let me recall some dog",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal",
        "Ġso",
        "Ġit",
        "Ġworks",
        "Ġacross",
        "Ġdifferent",
        "Ġcultures",
        ".",
        "ĠMaybe",
        "Ġa",
        "Ġclassic",
        "Ġsetup",
        "Ġand",
        "Ġpunch",
        "line",
        ".",
        "ĠHmm",
        ",",
        "Ġanimals",
        "Ġare",
        "Ġalways",
        "Ġsafe",
        "Ġbets",
        ".",
        "ĠHow",
        "Ġabout",
        "Ġa",
        "Ġdog",
        "Ġjoke",
        "?",
        "ĠDogs",
        "Ġare",
        "Ġrel",
        "atable",
        ".",
        "ĠLet",
        "Ġme",
        "Ġrecall",
        "Ġsome",
        "Ġdog"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789,
        779,
        433,
        4375,
        4028,
        2204,
        27833,
        13,
        10926,
        264,
        11670,
        6642,
        323,
        21004,
        1074,
        13,
        89290,
        11,
        10099,
        527,
        2744,
        6220,
        43177,
        13,
        2650,
        922,
        264,
        5679,
        22380,
        30,
        39525,
        527,
        1375,
        15436,
        13,
        6914,
        757,
        19635,
        1063,
        5679
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġjokes",
            "token_id": 32520,
            "logprob": -0.6406,
            "prob": 0.5273
          },
          {
            "token": "Ġpun",
            "token_id": 8474,
            "logprob": -1.0156,
            "prob": 0.3613
          },
          {
            "token": "-related",
            "token_id": 14228,
            "logprob": -2.3281,
            "prob": 0.0977
          },
          {
            "token": "Ġhumor",
            "token_id": 28485,
            "logprob": -5.1562,
            "prob": 0.0059
          },
          {
            "token": "Ġjoke",
            "token_id": 22380,
            "logprob": -6.1562,
            "prob": 0.0022
          },
          {
            "token": "-themed",
            "token_id": 57689,
            "logprob": -6.3438,
            "prob": 0.0018
          },
          {
            "token": "Ġfunny",
            "token_id": 15526,
            "logprob": -7.5938,
            "prob": 0.0005
          },
          {
            "token": "Ġrelated",
            "token_id": 5552,
            "logprob": -8.0,
            "prob": 0.0003
          },
          {
            "token": "相关",
            "token_id": 79656,
            "logprob": -8.1875,
            "prob": 0.0003
          },
          {
            "token": "Ġscenarios",
            "token_id": 26350,
            "logprob": -8.5,
            "prob": 0.0002
          }
        ]
      },
      "selected_token": {
        "token": "Ġpun",
        "token_id": 8474,
        "selection_method": "sampling"
      }
    },
    {
      "step": 65,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal so it works across different cultures. Maybe a classic setup and punchline. Hmm, animals are always safe bets. How about a dog joke? Dogs are relatable. Let me recall some dog pun",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal",
        "Ġso",
        "Ġit",
        "Ġworks",
        "Ġacross",
        "Ġdifferent",
        "Ġcultures",
        ".",
        "ĠMaybe",
        "Ġa",
        "Ġclassic",
        "Ġsetup",
        "Ġand",
        "Ġpunch",
        "line",
        ".",
        "ĠHmm",
        ",",
        "Ġanimals",
        "Ġare",
        "Ġalways",
        "Ġsafe",
        "Ġbets",
        ".",
        "ĠHow",
        "Ġabout",
        "Ġa",
        "Ġdog",
        "Ġjoke",
        "?",
        "ĠDogs",
        "Ġare",
        "Ġrel",
        "atable",
        ".",
        "ĠLet",
        "Ġme",
        "Ġrecall",
        "Ġsome",
        "Ġdog",
        "Ġpun"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789,
        779,
        433,
        4375,
        4028,
        2204,
        27833,
        13,
        10926,
        264,
        11670,
        6642,
        323,
        21004,
        1074,
        13,
        89290,
        11,
        10099,
        527,
        2744,
        6220,
        43177,
        13,
        2650,
        922,
        264,
        5679,
        22380,
        30,
        39525,
        527,
        1375,
        15436,
        13,
        6914,
        757,
        19635,
        1063,
        5679,
        8474
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "s",
            "token_id": 82,
            "logprob": -0.0001,
            "prob": 1.0
          },
          {
            "token": "Ġjokes",
            "token_id": 32520,
            "logprob": -10.9375,
            "prob": 0.0
          },
          {
            "token": "ny",
            "token_id": 3919,
            "logprob": -11.3125,
            "prob": 0.0
          },
          {
            "token": "-based",
            "token_id": 6108,
            "logprob": -11.75,
            "prob": 0.0
          },
          {
            "token": "Ġstructures",
            "token_id": 14726,
            "logprob": -11.9375,
            "prob": 0.0
          },
          {
            "token": "Ġideas",
            "token_id": 6848,
            "logprob": -12.25,
            "prob": 0.0
          },
          {
            "token": "ishes",
            "token_id": 21168,
            "logprob": -12.4375,
            "prob": 0.0
          },
          {
            "token": "Ġstuff",
            "token_id": 6392,
            "logprob": -12.5,
            "prob": 0.0
          },
          {
            "token": "Ġpossibilities",
            "token_id": 24525,
            "logprob": -12.6875,
            "prob": 0.0
          },
          {
            "token": "Ġhumor",
            "token_id": 28485,
            "logprob": -13.125,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "s",
        "token_id": 82,
        "selection_method": "sampling"
      }
    },
    {
      "step": 66,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal so it works across different cultures. Maybe a classic setup and punchline. Hmm, animals are always safe bets. How about a dog joke? Dogs are relatable. Let me recall some dog puns",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal",
        "Ġso",
        "Ġit",
        "Ġworks",
        "Ġacross",
        "Ġdifferent",
        "Ġcultures",
        ".",
        "ĠMaybe",
        "Ġa",
        "Ġclassic",
        "Ġsetup",
        "Ġand",
        "Ġpunch",
        "line",
        ".",
        "ĠHmm",
        ",",
        "Ġanimals",
        "Ġare",
        "Ġalways",
        "Ġsafe",
        "Ġbets",
        ".",
        "ĠHow",
        "Ġabout",
        "Ġa",
        "Ġdog",
        "Ġjoke",
        "?",
        "ĠDogs",
        "Ġare",
        "Ġrel",
        "atable",
        ".",
        "ĠLet",
        "Ġme",
        "Ġrecall",
        "Ġsome",
        "Ġdog",
        "Ġpun",
        "s"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789,
        779,
        433,
        4375,
        4028,
        2204,
        27833,
        13,
        10926,
        264,
        11670,
        6642,
        323,
        21004,
        1074,
        13,
        89290,
        11,
        10099,
        527,
        2744,
        6220,
        43177,
        13,
        2650,
        922,
        264,
        5679,
        22380,
        30,
        39525,
        527,
        1375,
        15436,
        13,
        6914,
        757,
        19635,
        1063,
        5679,
        8474,
        82
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": ".",
            "token_id": 13,
            "logprob": -0.3164,
            "prob": 0.7266
          },
          {
            "token": ".\n\n",
            "token_id": 382,
            "logprob": -1.6953,
            "prob": 0.1846
          },
          {
            "token": "...",
            "token_id": 1131,
            "logprob": -2.5625,
            "prob": 0.0767
          },
          {
            "token": "Ġor",
            "token_id": 477,
            "logprob": -4.9375,
            "prob": 0.0071
          },
          {
            "token": "...\n\n",
            "token_id": 2195,
            "logprob": -5.5625,
            "prob": 0.0038
          },
          {
            "token": "…",
            "token_id": 1981,
            "logprob": -9.9375,
            "prob": 0.0
          },
          {
            "token": "…\n\n",
            "token_id": 5551,
            "logprob": -11.3125,
            "prob": 0.0
          },
          {
            "token": "?",
            "token_id": 30,
            "logprob": -12.0625,
            "prob": 0.0
          },
          {
            "token": "Ġinvolving",
            "token_id": 16239,
            "logprob": -12.3125,
            "prob": 0.0
          },
          {
            "token": "ĠI",
            "token_id": 358,
            "logprob": -12.5625,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": ".",
        "token_id": 13,
        "selection_method": "sampling"
      }
    },
    {
      "step": 67,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal so it works across different cultures. Maybe a classic setup and punchline. Hmm, animals are always safe bets. How about a dog joke? Dogs are relatable. Let me recall some dog puns.",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal",
        "Ġso",
        "Ġit",
        "Ġworks",
        "Ġacross",
        "Ġdifferent",
        "Ġcultures",
        ".",
        "ĠMaybe",
        "Ġa",
        "Ġclassic",
        "Ġsetup",
        "Ġand",
        "Ġpunch",
        "line",
        ".",
        "ĠHmm",
        ",",
        "Ġanimals",
        "Ġare",
        "Ġalways",
        "Ġsafe",
        "Ġbets",
        ".",
        "ĠHow",
        "Ġabout",
        "Ġa",
        "Ġdog",
        "Ġjoke",
        "?",
        "ĠDogs",
        "Ġare",
        "Ġrel",
        "atable",
        ".",
        "ĠLet",
        "Ġme",
        "Ġrecall",
        "Ġsome",
        "Ġdog",
        "Ġpun",
        "s",
        "."
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789,
        779,
        433,
        4375,
        4028,
        2204,
        27833,
        13,
        10926,
        264,
        11670,
        6642,
        323,
        21004,
        1074,
        13,
        89290,
        11,
        10099,
        527,
        2744,
        6220,
        43177,
        13,
        2650,
        922,
        264,
        5679,
        22380,
        30,
        39525,
        527,
        1375,
        15436,
        13,
        6914,
        757,
        19635,
        1063,
        5679,
        8474,
        82,
        13
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "ĠOh",
            "token_id": 8840,
            "logprob": -0.0771,
            "prob": 0.9258
          },
          {
            "token": "Ġ\"",
            "token_id": 330,
            "logprob": -3.2031,
            "prob": 0.0408
          },
          {
            "token": "ĠWait",
            "token_id": 14144,
            "logprob": -3.8906,
            "prob": 0.0205
          },
          {
            "token": "ĠThere",
            "token_id": 2684,
            "logprob": -5.4375,
            "prob": 0.0043
          },
          {
            "token": "ĠAh",
            "token_id": 16770,
            "logprob": -5.875,
            "prob": 0.0028
          },
          {
            "token": "ĠP",
            "token_id": 393,
            "logprob": -6.5,
            "prob": 0.0015
          },
          {
            "token": "ĠThe",
            "token_id": 578,
            "logprob": -6.9375,
            "prob": 0.001
          },
          {
            "token": "ĠWords",
            "token_id": 28730,
            "logprob": -7.4375,
            "prob": 0.0006
          },
          {
            "token": "Oh",
            "token_id": 12174,
            "logprob": -7.8125,
            "prob": 0.0004
          },
          {
            "token": "Ġ\n\n",
            "token_id": 4815,
            "logprob": -8.625,
            "prob": 0.0002
          }
        ]
      },
      "selected_token": {
        "token": "ĠOh",
        "token_id": 8840,
        "selection_method": "sampling"
      }
    },
    {
      "step": 68,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal so it works across different cultures. Maybe a classic setup and punchline. Hmm, animals are always safe bets. How about a dog joke? Dogs are relatable. Let me recall some dog puns. Oh",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal",
        "Ġso",
        "Ġit",
        "Ġworks",
        "Ġacross",
        "Ġdifferent",
        "Ġcultures",
        ".",
        "ĠMaybe",
        "Ġa",
        "Ġclassic",
        "Ġsetup",
        "Ġand",
        "Ġpunch",
        "line",
        ".",
        "ĠHmm",
        ",",
        "Ġanimals",
        "Ġare",
        "Ġalways",
        "Ġsafe",
        "Ġbets",
        ".",
        "ĠHow",
        "Ġabout",
        "Ġa",
        "Ġdog",
        "Ġjoke",
        "?",
        "ĠDogs",
        "Ġare",
        "Ġrel",
        "atable",
        ".",
        "ĠLet",
        "Ġme",
        "Ġrecall",
        "Ġsome",
        "Ġdog",
        "Ġpun",
        "s",
        ".",
        "ĠOh"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789,
        779,
        433,
        4375,
        4028,
        2204,
        27833,
        13,
        10926,
        264,
        11670,
        6642,
        323,
        21004,
        1074,
        13,
        89290,
        11,
        10099,
        527,
        2744,
        6220,
        43177,
        13,
        2650,
        922,
        264,
        5679,
        22380,
        30,
        39525,
        527,
        1375,
        15436,
        13,
        6914,
        757,
        19635,
        1063,
        5679,
        8474,
        82,
        13,
        8840
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": ",",
            "token_id": 11,
            "logprob": -0.0094,
            "prob": 0.9922
          },
          {
            "token": "Ġright",
            "token_id": 1314,
            "logprob": -5.3125,
            "prob": 0.0049
          },
          {
            "token": "!",
            "token_id": 0,
            "logprob": -5.75,
            "prob": 0.0032
          },
          {
            "token": "Ġyeah",
            "token_id": 22371,
            "logprob": -6.875,
            "prob": 0.001
          },
          {
            "token": "Ġwait",
            "token_id": 3868,
            "logprob": -8.25,
            "prob": 0.0003
          },
          {
            "token": "right",
            "token_id": 1315,
            "logprob": -12.1875,
            "prob": 0.0
          },
          {
            "token": "Right",
            "token_id": 6107,
            "logprob": -12.75,
            "prob": 0.0
          },
          {
            "token": "hh",
            "token_id": 20990,
            "logprob": -12.75,
            "prob": 0.0
          },
          {
            "token": "Ġ\"",
            "token_id": 330,
            "logprob": -13.375,
            "prob": 0.0
          },
          {
            "token": "Ġyes",
            "token_id": 10035,
            "logprob": -13.375,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": ",",
        "token_id": 11,
        "selection_method": "sampling"
      }
    },
    {
      "step": 69,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal so it works across different cultures. Maybe a classic setup and punchline. Hmm, animals are always safe bets. How about a dog joke? Dogs are relatable. Let me recall some dog puns. Oh,",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal",
        "Ġso",
        "Ġit",
        "Ġworks",
        "Ġacross",
        "Ġdifferent",
        "Ġcultures",
        ".",
        "ĠMaybe",
        "Ġa",
        "Ġclassic",
        "Ġsetup",
        "Ġand",
        "Ġpunch",
        "line",
        ".",
        "ĠHmm",
        ",",
        "Ġanimals",
        "Ġare",
        "Ġalways",
        "Ġsafe",
        "Ġbets",
        ".",
        "ĠHow",
        "Ġabout",
        "Ġa",
        "Ġdog",
        "Ġjoke",
        "?",
        "ĠDogs",
        "Ġare",
        "Ġrel",
        "atable",
        ".",
        "ĠLet",
        "Ġme",
        "Ġrecall",
        "Ġsome",
        "Ġdog",
        "Ġpun",
        "s",
        ".",
        "ĠOh",
        ","
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789,
        779,
        433,
        4375,
        4028,
        2204,
        27833,
        13,
        10926,
        264,
        11670,
        6642,
        323,
        21004,
        1074,
        13,
        89290,
        11,
        10099,
        527,
        2744,
        6220,
        43177,
        13,
        2650,
        922,
        264,
        5679,
        22380,
        30,
        39525,
        527,
        1375,
        15436,
        13,
        6914,
        757,
        19635,
        1063,
        5679,
        8474,
        82,
        13,
        8840,
        11
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -0.2871,
            "prob": 0.75
          },
          {
            "token": "Ġ\"",
            "token_id": 330,
            "logprob": -2.2812,
            "prob": 0.1016
          },
          {
            "token": "Ġthere",
            "token_id": 1070,
            "logprob": -2.5312,
            "prob": 0.0791
          },
          {
            "token": "Ġright",
            "token_id": 1314,
            "logprob": -3.2812,
            "prob": 0.0374
          },
          {
            "token": "Ġword",
            "token_id": 3492,
            "logprob": -4.5938,
            "prob": 0.0101
          },
          {
            "token": "ĠI",
            "token_id": 358,
            "logprob": -5.5938,
            "prob": 0.0037
          },
          {
            "token": "Ġpun",
            "token_id": 8474,
            "logprob": -5.7188,
            "prob": 0.0033
          },
          {
            "token": "Ġlike",
            "token_id": 1093,
            "logprob": -5.9062,
            "prob": 0.0027
          },
          {
            "token": "Ġwait",
            "token_id": 3868,
            "logprob": -5.9688,
            "prob": 0.0025
          },
          {
            "token": "Ġmaybe",
            "token_id": 7344,
            "logprob": -6.9062,
            "prob": 0.001
          }
        ]
      },
      "selected_token": {
        "token": "Ġthe",
        "token_id": 279,
        "selection_method": "sampling"
      }
    },
    {
      "step": 70,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal so it works across different cultures. Maybe a classic setup and punchline. Hmm, animals are always safe bets. How about a dog joke? Dogs are relatable. Let me recall some dog puns. Oh, the",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal",
        "Ġso",
        "Ġit",
        "Ġworks",
        "Ġacross",
        "Ġdifferent",
        "Ġcultures",
        ".",
        "ĠMaybe",
        "Ġa",
        "Ġclassic",
        "Ġsetup",
        "Ġand",
        "Ġpunch",
        "line",
        ".",
        "ĠHmm",
        ",",
        "Ġanimals",
        "Ġare",
        "Ġalways",
        "Ġsafe",
        "Ġbets",
        ".",
        "ĠHow",
        "Ġabout",
        "Ġa",
        "Ġdog",
        "Ġjoke",
        "?",
        "ĠDogs",
        "Ġare",
        "Ġrel",
        "atable",
        ".",
        "ĠLet",
        "Ġme",
        "Ġrecall",
        "Ġsome",
        "Ġdog",
        "Ġpun",
        "s",
        ".",
        "ĠOh",
        ",",
        "Ġthe"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789,
        779,
        433,
        4375,
        4028,
        2204,
        27833,
        13,
        10926,
        264,
        11670,
        6642,
        323,
        21004,
        1074,
        13,
        89290,
        11,
        10099,
        527,
        2744,
        6220,
        43177,
        13,
        2650,
        922,
        264,
        5679,
        22380,
        30,
        39525,
        527,
        1375,
        15436,
        13,
        6914,
        757,
        19635,
        1063,
        5679,
        8474,
        82,
        13,
        8840,
        11,
        279
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġ\"",
            "token_id": 330,
            "logprob": -0.7969,
            "prob": 0.4512
          },
          {
            "token": "Ġclassic",
            "token_id": 11670,
            "logprob": -1.2969,
            "prob": 0.2734
          },
          {
            "token": "Ġone",
            "token_id": 832,
            "logprob": -1.9219,
            "prob": 0.1465
          },
          {
            "token": "Ġword",
            "token_id": 3492,
            "logprob": -2.8594,
            "prob": 0.0574
          },
          {
            "token": "Ġfamous",
            "token_id": 11495,
            "logprob": -4.5938,
            "prob": 0.0099
          },
          {
            "token": "Ġcommon",
            "token_id": 4279,
            "logprob": -4.7188,
            "prob": 0.0088
          },
          {
            "token": "Ġold",
            "token_id": 2362,
            "logprob": -4.9062,
            "prob": 0.0073
          },
          {
            "token": "Ġones",
            "token_id": 6305,
            "logprob": -5.4062,
            "prob": 0.0044
          },
          {
            "token": "Ġphrase",
            "token_id": 17571,
            "logprob": -5.4688,
            "prob": 0.0042
          },
          {
            "token": "Ġplay",
            "token_id": 1514,
            "logprob": -5.8438,
            "prob": 0.0029
          }
        ]
      },
      "selected_token": {
        "token": "Ġ\"",
        "token_id": 330,
        "selection_method": "sampling"
      }
    },
    {
      "step": 71,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal so it works across different cultures. Maybe a classic setup and punchline. Hmm, animals are always safe bets. How about a dog joke? Dogs are relatable. Let me recall some dog puns. Oh, the \"",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal",
        "Ġso",
        "Ġit",
        "Ġworks",
        "Ġacross",
        "Ġdifferent",
        "Ġcultures",
        ".",
        "ĠMaybe",
        "Ġa",
        "Ġclassic",
        "Ġsetup",
        "Ġand",
        "Ġpunch",
        "line",
        ".",
        "ĠHmm",
        ",",
        "Ġanimals",
        "Ġare",
        "Ġalways",
        "Ġsafe",
        "Ġbets",
        ".",
        "ĠHow",
        "Ġabout",
        "Ġa",
        "Ġdog",
        "Ġjoke",
        "?",
        "ĠDogs",
        "Ġare",
        "Ġrel",
        "atable",
        ".",
        "ĠLet",
        "Ġme",
        "Ġrecall",
        "Ġsome",
        "Ġdog",
        "Ġpun",
        "s",
        ".",
        "ĠOh",
        ",",
        "Ġthe",
        "Ġ\""
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789,
        779,
        433,
        4375,
        4028,
        2204,
        27833,
        13,
        10926,
        264,
        11670,
        6642,
        323,
        21004,
        1074,
        13,
        89290,
        11,
        10099,
        527,
        2744,
        6220,
        43177,
        13,
        2650,
        922,
        264,
        5679,
        22380,
        30,
        39525,
        527,
        1375,
        15436,
        13,
        6914,
        757,
        19635,
        1063,
        5679,
        8474,
        82,
        13,
        8840,
        11,
        279,
        330
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Why",
            "token_id": 10445,
            "logprob": -0.8008,
            "prob": 0.4492
          },
          {
            "token": "p",
            "token_id": 79,
            "logprob": -1.3594,
            "prob": 0.2559
          },
          {
            "token": "why",
            "token_id": 35734,
            "logprob": -2.2344,
            "prob": 0.1069
          },
          {
            "token": "wo",
            "token_id": 1146,
            "logprob": -3.4844,
            "prob": 0.0306
          },
          {
            "token": "ĠWhy",
            "token_id": 8595,
            "logprob": -3.5469,
            "prob": 0.0287
          },
          {
            "token": "hot",
            "token_id": 10847,
            "logprob": -3.6719,
            "prob": 0.0254
          },
          {
            "token": "b",
            "token_id": 65,
            "logprob": -3.8594,
            "prob": 0.021
          },
          {
            "token": "Ġwhy",
            "token_id": 3249,
            "logprob": -4.75,
            "prob": 0.0088
          },
          {
            "token": "bone",
            "token_id": 20337,
            "logprob": -5.4375,
            "prob": 0.0044
          },
          {
            "token": "dog",
            "token_id": 18964,
            "logprob": -5.6875,
            "prob": 0.0034
          }
        ]
      },
      "selected_token": {
        "token": "p",
        "token_id": 79,
        "selection_method": "sampling"
      }
    },
    {
      "step": 72,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal so it works across different cultures. Maybe a classic setup and punchline. Hmm, animals are always safe bets. How about a dog joke? Dogs are relatable. Let me recall some dog puns. Oh, the \"p",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal",
        "Ġso",
        "Ġit",
        "Ġworks",
        "Ġacross",
        "Ġdifferent",
        "Ġcultures",
        ".",
        "ĠMaybe",
        "Ġa",
        "Ġclassic",
        "Ġsetup",
        "Ġand",
        "Ġpunch",
        "line",
        ".",
        "ĠHmm",
        ",",
        "Ġanimals",
        "Ġare",
        "Ġalways",
        "Ġsafe",
        "Ġbets",
        ".",
        "ĠHow",
        "Ġabout",
        "Ġa",
        "Ġdog",
        "Ġjoke",
        "?",
        "ĠDogs",
        "Ġare",
        "Ġrel",
        "atable",
        ".",
        "ĠLet",
        "Ġme",
        "Ġrecall",
        "Ġsome",
        "Ġdog",
        "Ġpun",
        "s",
        ".",
        "ĠOh",
        ",",
        "Ġthe",
        "Ġ\"",
        "p"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789,
        779,
        433,
        4375,
        4028,
        2204,
        27833,
        13,
        10926,
        264,
        11670,
        6642,
        323,
        21004,
        1074,
        13,
        89290,
        11,
        10099,
        527,
        2744,
        6220,
        43177,
        13,
        2650,
        922,
        264,
        5679,
        22380,
        30,
        39525,
        527,
        1375,
        15436,
        13,
        6914,
        757,
        19635,
        1063,
        5679,
        8474,
        82,
        13,
        8840,
        11,
        279,
        330,
        79
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "aw",
            "token_id": 675,
            "logprob": -0.8867,
            "prob": 0.4121
          },
          {
            "token": "uppy",
            "token_id": 65129,
            "logprob": -1.1406,
            "prob": 0.3203
          },
          {
            "token": "aws",
            "token_id": 8805,
            "logprob": -1.6406,
            "prob": 0.1943
          },
          {
            "token": "up",
            "token_id": 455,
            "logprob": -3.0781,
            "prob": 0.0461
          },
          {
            "token": "oo",
            "token_id": 2689,
            "logprob": -4.875,
            "prob": 0.0075
          },
          {
            "token": "upper",
            "token_id": 13886,
            "logprob": -5.75,
            "prob": 0.0031
          },
          {
            "token": "oodle",
            "token_id": 31707,
            "logprob": -6.125,
            "prob": 0.0022
          },
          {
            "token": "uppies",
            "token_id": 95847,
            "logprob": -6.3125,
            "prob": 0.0018
          },
          {
            "token": "urr",
            "token_id": 8186,
            "logprob": -7.3125,
            "prob": 0.0007
          },
          {
            "token": "AW",
            "token_id": 14757,
            "logprob": -7.3125,
            "prob": 0.0007
          }
        ]
      },
      "selected_token": {
        "token": "aws",
        "token_id": 8805,
        "selection_method": "sampling"
      }
    },
    {
      "step": 73,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal so it works across different cultures. Maybe a classic setup and punchline. Hmm, animals are always safe bets. How about a dog joke? Dogs are relatable. Let me recall some dog puns. Oh, the \"paws",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal",
        "Ġso",
        "Ġit",
        "Ġworks",
        "Ġacross",
        "Ġdifferent",
        "Ġcultures",
        ".",
        "ĠMaybe",
        "Ġa",
        "Ġclassic",
        "Ġsetup",
        "Ġand",
        "Ġpunch",
        "line",
        ".",
        "ĠHmm",
        ",",
        "Ġanimals",
        "Ġare",
        "Ġalways",
        "Ġsafe",
        "Ġbets",
        ".",
        "ĠHow",
        "Ġabout",
        "Ġa",
        "Ġdog",
        "Ġjoke",
        "?",
        "ĠDogs",
        "Ġare",
        "Ġrel",
        "atable",
        ".",
        "ĠLet",
        "Ġme",
        "Ġrecall",
        "Ġsome",
        "Ġdog",
        "Ġpun",
        "s",
        ".",
        "ĠOh",
        ",",
        "Ġthe",
        "Ġ\"",
        "p",
        "aws"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789,
        779,
        433,
        4375,
        4028,
        2204,
        27833,
        13,
        10926,
        264,
        11670,
        6642,
        323,
        21004,
        1074,
        13,
        89290,
        11,
        10099,
        527,
        2744,
        6220,
        43177,
        13,
        2650,
        922,
        264,
        5679,
        22380,
        30,
        39525,
        527,
        1375,
        15436,
        13,
        6914,
        757,
        19635,
        1063,
        5679,
        8474,
        82,
        13,
        8840,
        11,
        279,
        330,
        79,
        8805
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "\"",
            "token_id": 1,
            "logprob": -0.0874,
            "prob": 0.918
          },
          {
            "token": "itive",
            "token_id": 3486,
            "logprob": -3.2812,
            "prob": 0.0378
          },
          {
            "token": "Ġit",
            "token_id": 433,
            "logprob": -4.1562,
            "prob": 0.0157
          },
          {
            "token": "Ġand",
            "token_id": 323,
            "logprob": -4.9062,
            "prob": 0.0074
          },
          {
            "token": "ome",
            "token_id": 638,
            "logprob": -5.2812,
            "prob": 0.0051
          },
          {
            "token": "it",
            "token_id": 275,
            "logprob": -5.375,
            "prob": 0.0047
          },
          {
            "token": "ible",
            "token_id": 1260,
            "logprob": -6.5938,
            "prob": 0.0014
          },
          {
            "token": "-",
            "token_id": 12,
            "logprob": -6.75,
            "prob": 0.0012
          },
          {
            "token": "-it",
            "token_id": 38611,
            "logprob": -7.25,
            "prob": 0.0007
          },
          {
            "token": "\"-",
            "token_id": 35394,
            "logprob": -7.4375,
            "prob": 0.0006
          }
        ]
      },
      "selected_token": {
        "token": "\"",
        "token_id": 1,
        "selection_method": "sampling"
      }
    },
    {
      "step": 74,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal so it works across different cultures. Maybe a classic setup and punchline. Hmm, animals are always safe bets. How about a dog joke? Dogs are relatable. Let me recall some dog puns. Oh, the \"paws\"",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal",
        "Ġso",
        "Ġit",
        "Ġworks",
        "Ġacross",
        "Ġdifferent",
        "Ġcultures",
        ".",
        "ĠMaybe",
        "Ġa",
        "Ġclassic",
        "Ġsetup",
        "Ġand",
        "Ġpunch",
        "line",
        ".",
        "ĠHmm",
        ",",
        "Ġanimals",
        "Ġare",
        "Ġalways",
        "Ġsafe",
        "Ġbets",
        ".",
        "ĠHow",
        "Ġabout",
        "Ġa",
        "Ġdog",
        "Ġjoke",
        "?",
        "ĠDogs",
        "Ġare",
        "Ġrel",
        "atable",
        ".",
        "ĠLet",
        "Ġme",
        "Ġrecall",
        "Ġsome",
        "Ġdog",
        "Ġpun",
        "s",
        ".",
        "ĠOh",
        ",",
        "Ġthe",
        "Ġ\"",
        "p",
        "aws",
        "\""
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789,
        779,
        433,
        4375,
        4028,
        2204,
        27833,
        13,
        10926,
        264,
        11670,
        6642,
        323,
        21004,
        1074,
        13,
        89290,
        11,
        10099,
        527,
        2744,
        6220,
        43177,
        13,
        2650,
        922,
        264,
        5679,
        22380,
        30,
        39525,
        527,
        1375,
        15436,
        13,
        6914,
        757,
        19635,
        1063,
        5679,
        8474,
        82,
        13,
        8840,
        11,
        279,
        330,
        79,
        8805,
        1
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġand",
            "token_id": 323,
            "logprob": -0.8242,
            "prob": 0.4375
          },
          {
            "token": "Ġpun",
            "token_id": 8474,
            "logprob": -1.3906,
            "prob": 0.25
          },
          {
            "token": "Ġone",
            "token_id": 832,
            "logprob": -2.2656,
            "prob": 0.104
          },
          {
            "token": "Ġword",
            "token_id": 3492,
            "logprob": -2.3281,
            "prob": 0.0977
          },
          {
            "token": "Ġjoke",
            "token_id": 22380,
            "logprob": -2.8906,
            "prob": 0.0557
          },
          {
            "token": "Ġfor",
            "token_id": 369,
            "logprob": -4.5625,
            "prob": 0.0103
          },
          {
            "token": "Ġpart",
            "token_id": 961,
            "logprob": -5.0,
            "prob": 0.0067
          },
          {
            "token": "Ġones",
            "token_id": 6305,
            "logprob": -5.1875,
            "prob": 0.0055
          },
          {
            "token": "Ġtheme",
            "token_id": 7057,
            "logprob": -5.875,
            "prob": 0.0028
          },
          {
            "token": "Ġhumor",
            "token_id": 28485,
            "logprob": -6.1875,
            "prob": 0.002
          }
        ]
      },
      "selected_token": {
        "token": "Ġfor",
        "token_id": 369,
        "selection_method": "sampling"
      }
    },
    {
      "step": 75,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal so it works across different cultures. Maybe a classic setup and punchline. Hmm, animals are always safe bets. How about a dog joke? Dogs are relatable. Let me recall some dog puns. Oh, the \"paws\" for",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal",
        "Ġso",
        "Ġit",
        "Ġworks",
        "Ġacross",
        "Ġdifferent",
        "Ġcultures",
        ".",
        "ĠMaybe",
        "Ġa",
        "Ġclassic",
        "Ġsetup",
        "Ġand",
        "Ġpunch",
        "line",
        ".",
        "ĠHmm",
        ",",
        "Ġanimals",
        "Ġare",
        "Ġalways",
        "Ġsafe",
        "Ġbets",
        ".",
        "ĠHow",
        "Ġabout",
        "Ġa",
        "Ġdog",
        "Ġjoke",
        "?",
        "ĠDogs",
        "Ġare",
        "Ġrel",
        "atable",
        ".",
        "ĠLet",
        "Ġme",
        "Ġrecall",
        "Ġsome",
        "Ġdog",
        "Ġpun",
        "s",
        ".",
        "ĠOh",
        ",",
        "Ġthe",
        "Ġ\"",
        "p",
        "aws",
        "\"",
        "Ġfor"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789,
        779,
        433,
        4375,
        4028,
        2204,
        27833,
        13,
        10926,
        264,
        11670,
        6642,
        323,
        21004,
        1074,
        13,
        89290,
        11,
        10099,
        527,
        2744,
        6220,
        43177,
        13,
        2650,
        922,
        264,
        5679,
        22380,
        30,
        39525,
        527,
        1375,
        15436,
        13,
        6914,
        757,
        19635,
        1063,
        5679,
        8474,
        82,
        13,
        8840,
        11,
        279,
        330,
        79,
        8805,
        1,
        369
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġ\"",
            "token_id": 330,
            "logprob": -0.2852,
            "prob": 0.7539
          },
          {
            "token": "Ġpun",
            "token_id": 8474,
            "logprob": -2.6562,
            "prob": 0.0698
          },
          {
            "token": "Ġlaughs",
            "token_id": 49336,
            "logprob": -3.2188,
            "prob": 0.0398
          },
          {
            "token": "Ġhands",
            "token_id": 6206,
            "logprob": -3.7812,
            "prob": 0.0227
          },
          {
            "token": "Ġwords",
            "token_id": 4339,
            "logprob": -4.5312,
            "prob": 0.0107
          },
          {
            "token": "Ġlaughter",
            "token_id": 43214,
            "logprob": -4.5938,
            "prob": 0.0101
          },
          {
            "token": "Ġlaugh",
            "token_id": 12835,
            "logprob": -4.7188,
            "prob": 0.0089
          },
          {
            "token": "Ġfeet",
            "token_id": 7693,
            "logprob": -4.9062,
            "prob": 0.0074
          },
          {
            "token": "Ġjokes",
            "token_id": 32520,
            "logprob": -4.9688,
            "prob": 0.0069
          },
          {
            "token": "Ġlegs",
            "token_id": 14535,
            "logprob": -5.4062,
            "prob": 0.0045
          }
        ]
      },
      "selected_token": {
        "token": "Ġ\"",
        "token_id": 330,
        "selection_method": "sampling"
      }
    },
    {
      "step": 76,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal so it works across different cultures. Maybe a classic setup and punchline. Hmm, animals are always safe bets. How about a dog joke? Dogs are relatable. Let me recall some dog puns. Oh, the \"paws\" for \"",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal",
        "Ġso",
        "Ġit",
        "Ġworks",
        "Ġacross",
        "Ġdifferent",
        "Ġcultures",
        ".",
        "ĠMaybe",
        "Ġa",
        "Ġclassic",
        "Ġsetup",
        "Ġand",
        "Ġpunch",
        "line",
        ".",
        "ĠHmm",
        ",",
        "Ġanimals",
        "Ġare",
        "Ġalways",
        "Ġsafe",
        "Ġbets",
        ".",
        "ĠHow",
        "Ġabout",
        "Ġa",
        "Ġdog",
        "Ġjoke",
        "?",
        "ĠDogs",
        "Ġare",
        "Ġrel",
        "atable",
        ".",
        "ĠLet",
        "Ġme",
        "Ġrecall",
        "Ġsome",
        "Ġdog",
        "Ġpun",
        "s",
        ".",
        "ĠOh",
        ",",
        "Ġthe",
        "Ġ\"",
        "p",
        "aws",
        "\"",
        "Ġfor",
        "Ġ\""
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789,
        779,
        433,
        4375,
        4028,
        2204,
        27833,
        13,
        10926,
        264,
        11670,
        6642,
        323,
        21004,
        1074,
        13,
        89290,
        11,
        10099,
        527,
        2744,
        6220,
        43177,
        13,
        2650,
        922,
        264,
        5679,
        22380,
        30,
        39525,
        527,
        1375,
        15436,
        13,
        6914,
        757,
        19635,
        1063,
        5679,
        8474,
        82,
        13,
        8840,
        11,
        279,
        330,
        79,
        8805,
        1,
        369,
        330
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "pause",
            "token_id": 28548,
            "logprob": -0.4082,
            "prob": 0.6641
          },
          {
            "token": "pairs",
            "token_id": 78356,
            "logprob": -2.2812,
            "prob": 0.1021
          },
          {
            "token": "p",
            "token_id": 79,
            "logprob": -2.4688,
            "prob": 0.0845
          },
          {
            "token": "pa",
            "token_id": 6733,
            "logprob": -3.5312,
            "prob": 0.0292
          },
          {
            "token": "pair",
            "token_id": 12959,
            "logprob": -4.4062,
            "prob": 0.0121
          },
          {
            "token": "peace",
            "token_id": 55225,
            "logprob": -5.7188,
            "prob": 0.0033
          },
          {
            "token": "f",
            "token_id": 69,
            "logprob": -6.0,
            "prob": 0.0025
          },
          {
            "token": "aws",
            "token_id": 8805,
            "logprob": -6.0312,
            "prob": 0.0024
          },
          {
            "token": "paired",
            "token_id": 77267,
            "logprob": -6.25,
            "prob": 0.0019
          },
          {
            "token": "Ġpause",
            "token_id": 18579,
            "logprob": -6.3438,
            "prob": 0.0018
          }
        ]
      },
      "selected_token": {
        "token": "pairs",
        "token_id": 78356,
        "selection_method": "sampling"
      }
    },
    {
      "step": 77,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal so it works across different cultures. Maybe a classic setup and punchline. Hmm, animals are always safe bets. How about a dog joke? Dogs are relatable. Let me recall some dog puns. Oh, the \"paws\" for \"pairs",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal",
        "Ġso",
        "Ġit",
        "Ġworks",
        "Ġacross",
        "Ġdifferent",
        "Ġcultures",
        ".",
        "ĠMaybe",
        "Ġa",
        "Ġclassic",
        "Ġsetup",
        "Ġand",
        "Ġpunch",
        "line",
        ".",
        "ĠHmm",
        ",",
        "Ġanimals",
        "Ġare",
        "Ġalways",
        "Ġsafe",
        "Ġbets",
        ".",
        "ĠHow",
        "Ġabout",
        "Ġa",
        "Ġdog",
        "Ġjoke",
        "?",
        "ĠDogs",
        "Ġare",
        "Ġrel",
        "atable",
        ".",
        "ĠLet",
        "Ġme",
        "Ġrecall",
        "Ġsome",
        "Ġdog",
        "Ġpun",
        "s",
        ".",
        "ĠOh",
        ",",
        "Ġthe",
        "Ġ\"",
        "p",
        "aws",
        "\"",
        "Ġfor",
        "Ġ\"",
        "pairs"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789,
        779,
        433,
        4375,
        4028,
        2204,
        27833,
        13,
        10926,
        264,
        11670,
        6642,
        323,
        21004,
        1074,
        13,
        89290,
        11,
        10099,
        527,
        2744,
        6220,
        43177,
        13,
        2650,
        922,
        264,
        5679,
        22380,
        30,
        39525,
        527,
        1375,
        15436,
        13,
        6914,
        757,
        19635,
        1063,
        5679,
        8474,
        82,
        13,
        8840,
        11,
        279,
        330,
        79,
        8805,
        1,
        369,
        330,
        78356
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "\"",
            "token_id": 1,
            "logprob": -0.0039,
            "prob": 0.9961
          },
          {
            "token": "\"?",
            "token_id": 44969,
            "logprob": -5.625,
            "prob": 0.0036
          },
          {
            "token": "\"...",
            "token_id": 53670,
            "logprob": -8.875,
            "prob": 0.0001
          },
          {
            "token": ".\"",
            "token_id": 1210,
            "logprob": -9.625,
            "prob": 0.0001
          },
          {
            "token": "Ġof",
            "token_id": 315,
            "logprob": -10.125,
            "prob": 0.0
          },
          {
            "token": "\".",
            "token_id": 3343,
            "logprob": -10.75,
            "prob": 0.0
          },
          {
            "token": "?\"",
            "token_id": 7673,
            "logprob": -11.3125,
            "prob": 0.0
          },
          {
            "token": ",\"",
            "token_id": 1359,
            "logprob": -11.375,
            "prob": 0.0
          },
          {
            "token": "\"—",
            "token_id": 66101,
            "logprob": -12.5,
            "prob": 0.0
          },
          {
            "token": "!\"",
            "token_id": 9135,
            "logprob": -12.875,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "\"",
        "token_id": 1,
        "selection_method": "sampling"
      }
    },
    {
      "step": 78,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal so it works across different cultures. Maybe a classic setup and punchline. Hmm, animals are always safe bets. How about a dog joke? Dogs are relatable. Let me recall some dog puns. Oh, the \"paws\" for \"pairs\"",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal",
        "Ġso",
        "Ġit",
        "Ġworks",
        "Ġacross",
        "Ġdifferent",
        "Ġcultures",
        ".",
        "ĠMaybe",
        "Ġa",
        "Ġclassic",
        "Ġsetup",
        "Ġand",
        "Ġpunch",
        "line",
        ".",
        "ĠHmm",
        ",",
        "Ġanimals",
        "Ġare",
        "Ġalways",
        "Ġsafe",
        "Ġbets",
        ".",
        "ĠHow",
        "Ġabout",
        "Ġa",
        "Ġdog",
        "Ġjoke",
        "?",
        "ĠDogs",
        "Ġare",
        "Ġrel",
        "atable",
        ".",
        "ĠLet",
        "Ġme",
        "Ġrecall",
        "Ġsome",
        "Ġdog",
        "Ġpun",
        "s",
        ".",
        "ĠOh",
        ",",
        "Ġthe",
        "Ġ\"",
        "p",
        "aws",
        "\"",
        "Ġfor",
        "Ġ\"",
        "pairs",
        "\""
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789,
        779,
        433,
        4375,
        4028,
        2204,
        27833,
        13,
        10926,
        264,
        11670,
        6642,
        323,
        21004,
        1074,
        13,
        89290,
        11,
        10099,
        527,
        2744,
        6220,
        43177,
        13,
        2650,
        922,
        264,
        5679,
        22380,
        30,
        39525,
        527,
        1375,
        15436,
        13,
        6914,
        757,
        19635,
        1063,
        5679,
        8474,
        82,
        13,
        8840,
        11,
        279,
        330,
        79,
        8805,
        1,
        369,
        330,
        78356,
        1
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġpun",
            "token_id": 8474,
            "logprob": -0.6055,
            "prob": 0.5469
          },
          {
            "token": "Ġjoke",
            "token_id": 22380,
            "logprob": -2.6719,
            "prob": 0.0693
          },
          {
            "token": "Ġone",
            "token_id": 832,
            "logprob": -2.7969,
            "prob": 0.0613
          },
          {
            "token": "Ġidea",
            "token_id": 4623,
            "logprob": -2.9219,
            "prob": 0.054
          },
          {
            "token": "Ġcould",
            "token_id": 1436,
            "logprob": -2.9844,
            "prob": 0.0508
          },
          {
            "token": "Ġangle",
            "token_id": 9392,
            "logprob": -3.1094,
            "prob": 0.0449
          },
          {
            "token": "Ġthing",
            "token_id": 3245,
            "logprob": -3.4219,
            "prob": 0.0327
          },
          {
            "token": "Ġplay",
            "token_id": 1514,
            "logprob": -3.6094,
            "prob": 0.0272
          },
          {
            "token": "Ġword",
            "token_id": 3492,
            "logprob": -3.6094,
            "prob": 0.0272
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -3.6719,
            "prob": 0.0255
          }
        ]
      },
      "selected_token": {
        "token": "Ġpun",
        "token_id": 8474,
        "selection_method": "sampling"
      }
    },
    {
      "step": 79,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal so it works across different cultures. Maybe a classic setup and punchline. Hmm, animals are always safe bets. How about a dog joke? Dogs are relatable. Let me recall some dog puns. Oh, the \"paws\" for \"pairs\" pun",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal",
        "Ġso",
        "Ġit",
        "Ġworks",
        "Ġacross",
        "Ġdifferent",
        "Ġcultures",
        ".",
        "ĠMaybe",
        "Ġa",
        "Ġclassic",
        "Ġsetup",
        "Ġand",
        "Ġpunch",
        "line",
        ".",
        "ĠHmm",
        ",",
        "Ġanimals",
        "Ġare",
        "Ġalways",
        "Ġsafe",
        "Ġbets",
        ".",
        "ĠHow",
        "Ġabout",
        "Ġa",
        "Ġdog",
        "Ġjoke",
        "?",
        "ĠDogs",
        "Ġare",
        "Ġrel",
        "atable",
        ".",
        "ĠLet",
        "Ġme",
        "Ġrecall",
        "Ġsome",
        "Ġdog",
        "Ġpun",
        "s",
        ".",
        "ĠOh",
        ",",
        "Ġthe",
        "Ġ\"",
        "p",
        "aws",
        "\"",
        "Ġfor",
        "Ġ\"",
        "pairs",
        "\"",
        "Ġpun"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789,
        779,
        433,
        4375,
        4028,
        2204,
        27833,
        13,
        10926,
        264,
        11670,
        6642,
        323,
        21004,
        1074,
        13,
        89290,
        11,
        10099,
        527,
        2744,
        6220,
        43177,
        13,
        2650,
        922,
        264,
        5679,
        22380,
        30,
        39525,
        527,
        1375,
        15436,
        13,
        6914,
        757,
        19635,
        1063,
        5679,
        8474,
        82,
        13,
        8840,
        11,
        279,
        330,
        79,
        8805,
        1,
        369,
        330,
        78356,
        1,
        8474
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "?",
            "token_id": 30,
            "logprob": -0.4746,
            "prob": 0.6211
          },
          {
            "token": "!",
            "token_id": 0,
            "logprob": -1.8516,
            "prob": 0.1572
          },
          {
            "token": ".",
            "token_id": 13,
            "logprob": -2.3438,
            "prob": 0.0952
          },
          {
            "token": "Ġcould",
            "token_id": 1436,
            "logprob": -2.8438,
            "prob": 0.0579
          },
          {
            "token": "Ġis",
            "token_id": 374,
            "logprob": -3.4688,
            "prob": 0.031
          },
          {
            "token": "Ġmight",
            "token_id": 2643,
            "logprob": -4.3438,
            "prob": 0.0129
          },
          {
            "token": "Ġcomes",
            "token_id": 4131,
            "logprob": -4.5938,
            "prob": 0.0101
          },
          {
            "token": "?\n\n",
            "token_id": 1980,
            "logprob": -5.7188,
            "prob": 0.0033
          },
          {
            "token": "...",
            "token_id": 1131,
            "logprob": -5.8438,
            "prob": 0.0029
          },
          {
            "token": "Ġalways",
            "token_id": 2744,
            "logprob": -6.2188,
            "prob": 0.002
          }
        ]
      },
      "selected_token": {
        "token": "!",
        "token_id": 0,
        "selection_method": "sampling"
      }
    },
    {
      "step": 80,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal so it works across different cultures. Maybe a classic setup and punchline. Hmm, animals are always safe bets. How about a dog joke? Dogs are relatable. Let me recall some dog puns. Oh, the \"paws\" for \"pairs\" pun!",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal",
        "Ġso",
        "Ġit",
        "Ġworks",
        "Ġacross",
        "Ġdifferent",
        "Ġcultures",
        ".",
        "ĠMaybe",
        "Ġa",
        "Ġclassic",
        "Ġsetup",
        "Ġand",
        "Ġpunch",
        "line",
        ".",
        "ĠHmm",
        ",",
        "Ġanimals",
        "Ġare",
        "Ġalways",
        "Ġsafe",
        "Ġbets",
        ".",
        "ĠHow",
        "Ġabout",
        "Ġa",
        "Ġdog",
        "Ġjoke",
        "?",
        "ĠDogs",
        "Ġare",
        "Ġrel",
        "atable",
        ".",
        "ĠLet",
        "Ġme",
        "Ġrecall",
        "Ġsome",
        "Ġdog",
        "Ġpun",
        "s",
        ".",
        "ĠOh",
        ",",
        "Ġthe",
        "Ġ\"",
        "p",
        "aws",
        "\"",
        "Ġfor",
        "Ġ\"",
        "pairs",
        "\"",
        "Ġpun",
        "!"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789,
        779,
        433,
        4375,
        4028,
        2204,
        27833,
        13,
        10926,
        264,
        11670,
        6642,
        323,
        21004,
        1074,
        13,
        89290,
        11,
        10099,
        527,
        2744,
        6220,
        43177,
        13,
        2650,
        922,
        264,
        5679,
        22380,
        30,
        39525,
        527,
        1375,
        15436,
        13,
        6914,
        757,
        19635,
        1063,
        5679,
        8474,
        82,
        13,
        8840,
        11,
        279,
        330,
        79,
        8805,
        1,
        369,
        330,
        78356,
        1,
        8474,
        0
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "ĠThat",
            "token_id": 3011,
            "logprob": -0.5547,
            "prob": 0.5742
          },
          {
            "token": "ĠYeah",
            "token_id": 22335,
            "logprob": -1.0547,
            "prob": 0.3477
          },
          {
            "token": "ĠWait",
            "token_id": 14144,
            "logprob": -2.9219,
            "prob": 0.0535
          },
          {
            "token": "ĠLike",
            "token_id": 9086,
            "logprob": -4.5,
            "prob": 0.0112
          },
          {
            "token": "ĠLet",
            "token_id": 6914,
            "logprob": -5.5,
            "prob": 0.0041
          },
          {
            "token": "ĠDogs",
            "token_id": 39525,
            "logprob": -6.375,
            "prob": 0.0017
          },
          {
            "token": "ĠP",
            "token_id": 393,
            "logprob": -6.75,
            "prob": 0.0012
          },
          {
            "token": "Ġ\"",
            "token_id": 330,
            "logprob": -6.8125,
            "prob": 0.0011
          },
          {
            "token": "ĠYes",
            "token_id": 7566,
            "logprob": -6.8125,
            "prob": 0.0011
          },
          {
            "token": "ĠRight",
            "token_id": 10291,
            "logprob": -7.625,
            "prob": 0.0005
          }
        ]
      },
      "selected_token": {
        "token": "ĠYeah",
        "token_id": 22335,
        "selection_method": "sampling"
      }
    },
    {
      "step": 81,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal so it works across different cultures. Maybe a classic setup and punchline. Hmm, animals are always safe bets. How about a dog joke? Dogs are relatable. Let me recall some dog puns. Oh, the \"paws\" for \"pairs\" pun! Yeah",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal",
        "Ġso",
        "Ġit",
        "Ġworks",
        "Ġacross",
        "Ġdifferent",
        "Ġcultures",
        ".",
        "ĠMaybe",
        "Ġa",
        "Ġclassic",
        "Ġsetup",
        "Ġand",
        "Ġpunch",
        "line",
        ".",
        "ĠHmm",
        ",",
        "Ġanimals",
        "Ġare",
        "Ġalways",
        "Ġsafe",
        "Ġbets",
        ".",
        "ĠHow",
        "Ġabout",
        "Ġa",
        "Ġdog",
        "Ġjoke",
        "?",
        "ĠDogs",
        "Ġare",
        "Ġrel",
        "atable",
        ".",
        "ĠLet",
        "Ġme",
        "Ġrecall",
        "Ġsome",
        "Ġdog",
        "Ġpun",
        "s",
        ".",
        "ĠOh",
        ",",
        "Ġthe",
        "Ġ\"",
        "p",
        "aws",
        "\"",
        "Ġfor",
        "Ġ\"",
        "pairs",
        "\"",
        "Ġpun",
        "!",
        "ĠYeah"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789,
        779,
        433,
        4375,
        4028,
        2204,
        27833,
        13,
        10926,
        264,
        11670,
        6642,
        323,
        21004,
        1074,
        13,
        89290,
        11,
        10099,
        527,
        2744,
        6220,
        43177,
        13,
        2650,
        922,
        264,
        5679,
        22380,
        30,
        39525,
        527,
        1375,
        15436,
        13,
        6914,
        757,
        19635,
        1063,
        5679,
        8474,
        82,
        13,
        8840,
        11,
        279,
        330,
        79,
        8805,
        1,
        369,
        330,
        78356,
        1,
        8474,
        0,
        22335
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": ",",
            "token_id": 11,
            "logprob": -0.0006,
            "prob": 1.0
          },
          {
            "token": "Ġthat",
            "token_id": 430,
            "logprob": -8.375,
            "prob": 0.0002
          },
          {
            "token": ".",
            "token_id": 13,
            "logprob": -8.5,
            "prob": 0.0002
          },
          {
            "token": "!",
            "token_id": 0,
            "logprob": -9.75,
            "prob": 0.0001
          },
          {
            "token": ".\n\n",
            "token_id": 382,
            "logprob": -9.875,
            "prob": 0.0001
          },
          {
            "token": ":",
            "token_id": 25,
            "logprob": -11.8125,
            "prob": 0.0
          },
          {
            "token": "!\n\n",
            "token_id": 2268,
            "logprob": -12.0,
            "prob": 0.0
          },
          {
            "token": ":\n\n",
            "token_id": 1473,
            "logprob": -12.75,
            "prob": 0.0
          },
          {
            "token": "?",
            "token_id": 30,
            "logprob": -14.5,
            "prob": 0.0
          },
          {
            "token": "That",
            "token_id": 4897,
            "logprob": -14.5,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": ",",
        "token_id": 11,
        "selection_method": "sampling"
      }
    },
    {
      "step": 82,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal so it works across different cultures. Maybe a classic setup and punchline. Hmm, animals are always safe bets. How about a dog joke? Dogs are relatable. Let me recall some dog puns. Oh, the \"paws\" for \"pairs\" pun! Yeah,",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal",
        "Ġso",
        "Ġit",
        "Ġworks",
        "Ġacross",
        "Ġdifferent",
        "Ġcultures",
        ".",
        "ĠMaybe",
        "Ġa",
        "Ġclassic",
        "Ġsetup",
        "Ġand",
        "Ġpunch",
        "line",
        ".",
        "ĠHmm",
        ",",
        "Ġanimals",
        "Ġare",
        "Ġalways",
        "Ġsafe",
        "Ġbets",
        ".",
        "ĠHow",
        "Ġabout",
        "Ġa",
        "Ġdog",
        "Ġjoke",
        "?",
        "ĠDogs",
        "Ġare",
        "Ġrel",
        "atable",
        ".",
        "ĠLet",
        "Ġme",
        "Ġrecall",
        "Ġsome",
        "Ġdog",
        "Ġpun",
        "s",
        ".",
        "ĠOh",
        ",",
        "Ġthe",
        "Ġ\"",
        "p",
        "aws",
        "\"",
        "Ġfor",
        "Ġ\"",
        "pairs",
        "\"",
        "Ġpun",
        "!",
        "ĠYeah",
        ","
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789,
        779,
        433,
        4375,
        4028,
        2204,
        27833,
        13,
        10926,
        264,
        11670,
        6642,
        323,
        21004,
        1074,
        13,
        89290,
        11,
        10099,
        527,
        2744,
        6220,
        43177,
        13,
        2650,
        922,
        264,
        5679,
        22380,
        30,
        39525,
        527,
        1375,
        15436,
        13,
        6914,
        757,
        19635,
        1063,
        5679,
        8474,
        82,
        13,
        8840,
        11,
        279,
        330,
        79,
        8805,
        1,
        369,
        330,
        78356,
        1,
        8474,
        0,
        22335,
        11
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġthat",
            "token_id": 430,
            "logprob": -0.0254,
            "prob": 0.9766
          },
          {
            "token": "Ġlike",
            "token_id": 1093,
            "logprob": -4.1562,
            "prob": 0.0157
          },
          {
            "token": "Ġ\"",
            "token_id": 330,
            "logprob": -5.0312,
            "prob": 0.0066
          },
          {
            "token": "Ġsomething",
            "token_id": 2555,
            "logprob": -6.4062,
            "prob": 0.0017
          },
          {
            "token": "Ġbecause",
            "token_id": 1606,
            "logprob": -8.125,
            "prob": 0.0003
          },
          {
            "token": "Ġplaying",
            "token_id": 5737,
            "logprob": -9.0625,
            "prob": 0.0001
          },
          {
            "token": "Ġplay",
            "token_id": 1514,
            "logprob": -9.5,
            "prob": 0.0001
          },
          {
            "token": "Ġdogs",
            "token_id": 12875,
            "logprob": -9.5,
            "prob": 0.0001
          },
          {
            "token": "Ġword",
            "token_id": 3492,
            "logprob": -9.5625,
            "prob": 0.0001
          },
          {
            "token": "Ġthose",
            "token_id": 1884,
            "logprob": -10.3125,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġthat",
        "token_id": 430,
        "selection_method": "sampling"
      }
    },
    {
      "step": 83,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal so it works across different cultures. Maybe a classic setup and punchline. Hmm, animals are always safe bets. How about a dog joke? Dogs are relatable. Let me recall some dog puns. Oh, the \"paws\" for \"pairs\" pun! Yeah, that",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal",
        "Ġso",
        "Ġit",
        "Ġworks",
        "Ġacross",
        "Ġdifferent",
        "Ġcultures",
        ".",
        "ĠMaybe",
        "Ġa",
        "Ġclassic",
        "Ġsetup",
        "Ġand",
        "Ġpunch",
        "line",
        ".",
        "ĠHmm",
        ",",
        "Ġanimals",
        "Ġare",
        "Ġalways",
        "Ġsafe",
        "Ġbets",
        ".",
        "ĠHow",
        "Ġabout",
        "Ġa",
        "Ġdog",
        "Ġjoke",
        "?",
        "ĠDogs",
        "Ġare",
        "Ġrel",
        "atable",
        ".",
        "ĠLet",
        "Ġme",
        "Ġrecall",
        "Ġsome",
        "Ġdog",
        "Ġpun",
        "s",
        ".",
        "ĠOh",
        ",",
        "Ġthe",
        "Ġ\"",
        "p",
        "aws",
        "\"",
        "Ġfor",
        "Ġ\"",
        "pairs",
        "\"",
        "Ġpun",
        "!",
        "ĠYeah",
        ",",
        "Ġthat"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789,
        779,
        433,
        4375,
        4028,
        2204,
        27833,
        13,
        10926,
        264,
        11670,
        6642,
        323,
        21004,
        1074,
        13,
        89290,
        11,
        10099,
        527,
        2744,
        6220,
        43177,
        13,
        2650,
        922,
        264,
        5679,
        22380,
        30,
        39525,
        527,
        1375,
        15436,
        13,
        6914,
        757,
        19635,
        1063,
        5679,
        8474,
        82,
        13,
        8840,
        11,
        279,
        330,
        79,
        8805,
        1,
        369,
        330,
        78356,
        1,
        8474,
        0,
        22335,
        11,
        430
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġcould",
            "token_id": 1436,
            "logprob": -0.4902,
            "prob": 0.6133
          },
          {
            "token": "'s",
            "token_id": 596,
            "logprob": -0.9883,
            "prob": 0.3711
          },
          {
            "token": "Ġworks",
            "token_id": 4375,
            "logprob": -5.5,
            "prob": 0.0041
          },
          {
            "token": "Ġmight",
            "token_id": 2643,
            "logprob": -5.625,
            "prob": 0.0036
          },
          {
            "token": "Ġone",
            "token_id": 832,
            "logprob": -5.625,
            "prob": 0.0036
          },
          {
            "token": "Ġsounds",
            "token_id": 10578,
            "logprob": -6.125,
            "prob": 0.0022
          },
          {
            "token": "’s",
            "token_id": 753,
            "logprob": -7.25,
            "prob": 0.0007
          },
          {
            "token": "Ġusually",
            "token_id": 6118,
            "logprob": -7.875,
            "prob": 0.0004
          },
          {
            "token": "Ġcan",
            "token_id": 649,
            "logprob": -8.25,
            "prob": 0.0003
          },
          {
            "token": "Ġalways",
            "token_id": 2744,
            "logprob": -8.375,
            "prob": 0.0002
          }
        ]
      },
      "selected_token": {
        "token": "'s",
        "token_id": 596,
        "selection_method": "sampling"
      }
    },
    {
      "step": 84,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal so it works across different cultures. Maybe a classic setup and punchline. Hmm, animals are always safe bets. How about a dog joke? Dogs are relatable. Let me recall some dog puns. Oh, the \"paws\" for \"pairs\" pun! Yeah, that's",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal",
        "Ġso",
        "Ġit",
        "Ġworks",
        "Ġacross",
        "Ġdifferent",
        "Ġcultures",
        ".",
        "ĠMaybe",
        "Ġa",
        "Ġclassic",
        "Ġsetup",
        "Ġand",
        "Ġpunch",
        "line",
        ".",
        "ĠHmm",
        ",",
        "Ġanimals",
        "Ġare",
        "Ġalways",
        "Ġsafe",
        "Ġbets",
        ".",
        "ĠHow",
        "Ġabout",
        "Ġa",
        "Ġdog",
        "Ġjoke",
        "?",
        "ĠDogs",
        "Ġare",
        "Ġrel",
        "atable",
        ".",
        "ĠLet",
        "Ġme",
        "Ġrecall",
        "Ġsome",
        "Ġdog",
        "Ġpun",
        "s",
        ".",
        "ĠOh",
        ",",
        "Ġthe",
        "Ġ\"",
        "p",
        "aws",
        "\"",
        "Ġfor",
        "Ġ\"",
        "pairs",
        "\"",
        "Ġpun",
        "!",
        "ĠYeah",
        ",",
        "Ġthat",
        "'s"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789,
        779,
        433,
        4375,
        4028,
        2204,
        27833,
        13,
        10926,
        264,
        11670,
        6642,
        323,
        21004,
        1074,
        13,
        89290,
        11,
        10099,
        527,
        2744,
        6220,
        43177,
        13,
        2650,
        922,
        264,
        5679,
        22380,
        30,
        39525,
        527,
        1375,
        15436,
        13,
        6914,
        757,
        19635,
        1063,
        5679,
        8474,
        82,
        13,
        8840,
        11,
        279,
        330,
        79,
        8805,
        1,
        369,
        330,
        78356,
        1,
        8474,
        0,
        22335,
        11,
        430,
        596
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġa",
            "token_id": 264,
            "logprob": -0.459,
            "prob": 0.6328
          },
          {
            "token": "Ġbeen",
            "token_id": 1027,
            "logprob": -2.2031,
            "prob": 0.1099
          },
          {
            "token": "Ġcommon",
            "token_id": 4279,
            "logprob": -2.4531,
            "prob": 0.0854
          },
          {
            "token": "Ġfunny",
            "token_id": 15526,
            "logprob": -3.3281,
            "prob": 0.0356
          },
          {
            "token": "Ġclever",
            "token_id": 28799,
            "logprob": -3.6406,
            "prob": 0.0261
          },
          {
            "token": "Ġclassic",
            "token_id": 11670,
            "logprob": -4.2188,
            "prob": 0.0148
          },
          {
            "token": "Ġpretty",
            "token_id": 5128,
            "logprob": -4.2812,
            "prob": 0.014
          },
          {
            "token": "Ġcute",
            "token_id": 19369,
            "logprob": -4.3438,
            "prob": 0.0131
          },
          {
            "token": "Ġcommonly",
            "token_id": 17037,
            "logprob": -4.3438,
            "prob": 0.0131
          },
          {
            "token": "Ġgood",
            "token_id": 1695,
            "logprob": -4.5312,
            "prob": 0.0109
          }
        ]
      },
      "selected_token": {
        "token": "Ġbeen",
        "token_id": 1027,
        "selection_method": "sampling"
      }
    },
    {
      "step": 85,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal so it works across different cultures. Maybe a classic setup and punchline. Hmm, animals are always safe bets. How about a dog joke? Dogs are relatable. Let me recall some dog puns. Oh, the \"paws\" for \"pairs\" pun! Yeah, that's been",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal",
        "Ġso",
        "Ġit",
        "Ġworks",
        "Ġacross",
        "Ġdifferent",
        "Ġcultures",
        ".",
        "ĠMaybe",
        "Ġa",
        "Ġclassic",
        "Ġsetup",
        "Ġand",
        "Ġpunch",
        "line",
        ".",
        "ĠHmm",
        ",",
        "Ġanimals",
        "Ġare",
        "Ġalways",
        "Ġsafe",
        "Ġbets",
        ".",
        "ĠHow",
        "Ġabout",
        "Ġa",
        "Ġdog",
        "Ġjoke",
        "?",
        "ĠDogs",
        "Ġare",
        "Ġrel",
        "atable",
        ".",
        "ĠLet",
        "Ġme",
        "Ġrecall",
        "Ġsome",
        "Ġdog",
        "Ġpun",
        "s",
        ".",
        "ĠOh",
        ",",
        "Ġthe",
        "Ġ\"",
        "p",
        "aws",
        "\"",
        "Ġfor",
        "Ġ\"",
        "pairs",
        "\"",
        "Ġpun",
        "!",
        "ĠYeah",
        ",",
        "Ġthat",
        "'s",
        "Ġbeen"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789,
        779,
        433,
        4375,
        4028,
        2204,
        27833,
        13,
        10926,
        264,
        11670,
        6642,
        323,
        21004,
        1074,
        13,
        89290,
        11,
        10099,
        527,
        2744,
        6220,
        43177,
        13,
        2650,
        922,
        264,
        5679,
        22380,
        30,
        39525,
        527,
        1375,
        15436,
        13,
        6914,
        757,
        19635,
        1063,
        5679,
        8474,
        82,
        13,
        8840,
        11,
        279,
        330,
        79,
        8805,
        1,
        369,
        330,
        78356,
        1,
        8474,
        0,
        22335,
        11,
        430,
        596,
        1027
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġdone",
            "token_id": 2884,
            "logprob": -0.7656,
            "prob": 0.4648
          },
          {
            "token": "Ġaround",
            "token_id": 2212,
            "logprob": -1.0156,
            "prob": 0.3613
          },
          {
            "token": "Ġused",
            "token_id": 1511,
            "logprob": -2.1406,
            "prob": 0.1177
          },
          {
            "token": "Ġpopular",
            "token_id": 5526,
            "logprob": -3.1406,
            "prob": 0.0432
          },
          {
            "token": "Ġa",
            "token_id": 264,
            "logprob": -5.4375,
            "prob": 0.0043
          },
          {
            "token": "Ġfloating",
            "token_id": 19596,
            "logprob": -6.125,
            "prob": 0.0022
          },
          {
            "token": "Ġpretty",
            "token_id": 5128,
            "logprob": -7.0,
            "prob": 0.0009
          },
          {
            "token": "around",
            "token_id": 20019,
            "logprob": -7.3125,
            "prob": 0.0007
          },
          {
            "token": "Ġcirculating",
            "token_id": 54828,
            "logprob": -7.75,
            "prob": 0.0004
          },
          {
            "token": "Ġdoing",
            "token_id": 3815,
            "logprob": -8.0625,
            "prob": 0.0003
          }
        ]
      },
      "selected_token": {
        "token": "Ġused",
        "token_id": 1511,
        "selection_method": "sampling"
      }
    },
    {
      "step": 86,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal so it works across different cultures. Maybe a classic setup and punchline. Hmm, animals are always safe bets. How about a dog joke? Dogs are relatable. Let me recall some dog puns. Oh, the \"paws\" for \"pairs\" pun! Yeah, that's been used",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal",
        "Ġso",
        "Ġit",
        "Ġworks",
        "Ġacross",
        "Ġdifferent",
        "Ġcultures",
        ".",
        "ĠMaybe",
        "Ġa",
        "Ġclassic",
        "Ġsetup",
        "Ġand",
        "Ġpunch",
        "line",
        ".",
        "ĠHmm",
        ",",
        "Ġanimals",
        "Ġare",
        "Ġalways",
        "Ġsafe",
        "Ġbets",
        ".",
        "ĠHow",
        "Ġabout",
        "Ġa",
        "Ġdog",
        "Ġjoke",
        "?",
        "ĠDogs",
        "Ġare",
        "Ġrel",
        "atable",
        ".",
        "ĠLet",
        "Ġme",
        "Ġrecall",
        "Ġsome",
        "Ġdog",
        "Ġpun",
        "s",
        ".",
        "ĠOh",
        ",",
        "Ġthe",
        "Ġ\"",
        "p",
        "aws",
        "\"",
        "Ġfor",
        "Ġ\"",
        "pairs",
        "\"",
        "Ġpun",
        "!",
        "ĠYeah",
        ",",
        "Ġthat",
        "'s",
        "Ġbeen",
        "Ġused"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789,
        779,
        433,
        4375,
        4028,
        2204,
        27833,
        13,
        10926,
        264,
        11670,
        6642,
        323,
        21004,
        1074,
        13,
        89290,
        11,
        10099,
        527,
        2744,
        6220,
        43177,
        13,
        2650,
        922,
        264,
        5679,
        22380,
        30,
        39525,
        527,
        1375,
        15436,
        13,
        6914,
        757,
        19635,
        1063,
        5679,
        8474,
        82,
        13,
        8840,
        11,
        279,
        330,
        79,
        8805,
        1,
        369,
        330,
        78356,
        1,
        8474,
        0,
        22335,
        11,
        430,
        596,
        1027,
        1511
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġbefore",
            "token_id": 1603,
            "logprob": -0.0728,
            "prob": 0.9297
          },
          {
            "token": "Ġa",
            "token_id": 264,
            "logprob": -2.7031,
            "prob": 0.0674
          },
          {
            "token": "Ġoften",
            "token_id": 3629,
            "logprob": -7.25,
            "prob": 0.0007
          },
          {
            "token": "Ġquite",
            "token_id": 5115,
            "logprob": -7.625,
            "prob": 0.0005
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -7.9375,
            "prob": 0.0004
          },
          {
            "token": ",",
            "token_id": 11,
            "logprob": -7.9375,
            "prob": 0.0004
          },
          {
            "token": "Ġbut",
            "token_id": 719,
            "logprob": -8.125,
            "prob": 0.0003
          },
          {
            "token": "Ġsometimes",
            "token_id": 7170,
            "logprob": -8.8125,
            "prob": 0.0001
          },
          {
            "token": "Ġlots",
            "token_id": 10283,
            "logprob": -9.4375,
            "prob": 0.0001
          },
          {
            "token": "Ġplenty",
            "token_id": 11510,
            "logprob": -9.625,
            "prob": 0.0001
          }
        ]
      },
      "selected_token": {
        "token": "Ġbefore",
        "token_id": 1603,
        "selection_method": "sampling"
      }
    },
    {
      "step": 87,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal so it works across different cultures. Maybe a classic setup and punchline. Hmm, animals are always safe bets. How about a dog joke? Dogs are relatable. Let me recall some dog puns. Oh, the \"paws\" for \"pairs\" pun! Yeah, that's been used before",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal",
        "Ġso",
        "Ġit",
        "Ġworks",
        "Ġacross",
        "Ġdifferent",
        "Ġcultures",
        ".",
        "ĠMaybe",
        "Ġa",
        "Ġclassic",
        "Ġsetup",
        "Ġand",
        "Ġpunch",
        "line",
        ".",
        "ĠHmm",
        ",",
        "Ġanimals",
        "Ġare",
        "Ġalways",
        "Ġsafe",
        "Ġbets",
        ".",
        "ĠHow",
        "Ġabout",
        "Ġa",
        "Ġdog",
        "Ġjoke",
        "?",
        "ĠDogs",
        "Ġare",
        "Ġrel",
        "atable",
        ".",
        "ĠLet",
        "Ġme",
        "Ġrecall",
        "Ġsome",
        "Ġdog",
        "Ġpun",
        "s",
        ".",
        "ĠOh",
        ",",
        "Ġthe",
        "Ġ\"",
        "p",
        "aws",
        "\"",
        "Ġfor",
        "Ġ\"",
        "pairs",
        "\"",
        "Ġpun",
        "!",
        "ĠYeah",
        ",",
        "Ġthat",
        "'s",
        "Ġbeen",
        "Ġused",
        "Ġbefore"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789,
        779,
        433,
        4375,
        4028,
        2204,
        27833,
        13,
        10926,
        264,
        11670,
        6642,
        323,
        21004,
        1074,
        13,
        89290,
        11,
        10099,
        527,
        2744,
        6220,
        43177,
        13,
        2650,
        922,
        264,
        5679,
        22380,
        30,
        39525,
        527,
        1375,
        15436,
        13,
        6914,
        757,
        19635,
        1063,
        5679,
        8474,
        82,
        13,
        8840,
        11,
        279,
        330,
        79,
        8805,
        1,
        369,
        330,
        78356,
        1,
        8474,
        0,
        22335,
        11,
        430,
        596,
        1027,
        1511,
        1603
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġbut",
            "token_id": 719,
            "logprob": -0.0525,
            "prob": 0.9492
          },
          {
            "token": ",",
            "token_id": 11,
            "logprob": -3.0469,
            "prob": 0.0471
          },
          {
            "token": ".",
            "token_id": 13,
            "logprob": -5.5625,
            "prob": 0.0039
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -11.9375,
            "prob": 0.0
          },
          {
            "token": "Ġand",
            "token_id": 323,
            "logprob": -12.0625,
            "prob": 0.0
          },
          {
            "token": ".\n\n",
            "token_id": 382,
            "logprob": -12.3125,
            "prob": 0.0
          },
          {
            "token": "但",
            "token_id": 78388,
            "logprob": -12.75,
            "prob": 0.0
          },
          {
            "token": "but",
            "token_id": 8248,
            "logprob": -13.625,
            "prob": 0.0
          },
          {
            "token": "Ġsuccessfully",
            "token_id": 7946,
            "logprob": -14.125,
            "prob": 0.0
          },
          {
            "token": "But",
            "token_id": 4071,
            "logprob": -14.75,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġbut",
        "token_id": 719,
        "selection_method": "sampling"
      }
    },
    {
      "step": 88,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal so it works across different cultures. Maybe a classic setup and punchline. Hmm, animals are always safe bets. How about a dog joke? Dogs are relatable. Let me recall some dog puns. Oh, the \"paws\" for \"pairs\" pun! Yeah, that's been used before but",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal",
        "Ġso",
        "Ġit",
        "Ġworks",
        "Ġacross",
        "Ġdifferent",
        "Ġcultures",
        ".",
        "ĠMaybe",
        "Ġa",
        "Ġclassic",
        "Ġsetup",
        "Ġand",
        "Ġpunch",
        "line",
        ".",
        "ĠHmm",
        ",",
        "Ġanimals",
        "Ġare",
        "Ġalways",
        "Ġsafe",
        "Ġbets",
        ".",
        "ĠHow",
        "Ġabout",
        "Ġa",
        "Ġdog",
        "Ġjoke",
        "?",
        "ĠDogs",
        "Ġare",
        "Ġrel",
        "atable",
        ".",
        "ĠLet",
        "Ġme",
        "Ġrecall",
        "Ġsome",
        "Ġdog",
        "Ġpun",
        "s",
        ".",
        "ĠOh",
        ",",
        "Ġthe",
        "Ġ\"",
        "p",
        "aws",
        "\"",
        "Ġfor",
        "Ġ\"",
        "pairs",
        "\"",
        "Ġpun",
        "!",
        "ĠYeah",
        ",",
        "Ġthat",
        "'s",
        "Ġbeen",
        "Ġused",
        "Ġbefore",
        "Ġbut"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789,
        779,
        433,
        4375,
        4028,
        2204,
        27833,
        13,
        10926,
        264,
        11670,
        6642,
        323,
        21004,
        1074,
        13,
        89290,
        11,
        10099,
        527,
        2744,
        6220,
        43177,
        13,
        2650,
        922,
        264,
        5679,
        22380,
        30,
        39525,
        527,
        1375,
        15436,
        13,
        6914,
        757,
        19635,
        1063,
        5679,
        8474,
        82,
        13,
        8840,
        11,
        279,
        330,
        79,
        8805,
        1,
        369,
        330,
        78356,
        1,
        8474,
        0,
        22335,
        11,
        430,
        596,
        1027,
        1511,
        1603,
        719
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġstill",
            "token_id": 2103,
            "logprob": -0.0811,
            "prob": 0.9219
          },
          {
            "token": "Ġit",
            "token_id": 433,
            "logprob": -2.9531,
            "prob": 0.052
          },
          {
            "token": "Ġpeople",
            "token_id": 1274,
            "logprob": -4.9062,
            "prob": 0.0075
          },
          {
            "token": "Ġeffective",
            "token_id": 7524,
            "logprob": -5.0938,
            "prob": 0.0062
          },
          {
            "token": "Ġalways",
            "token_id": 2744,
            "logprob": -5.4688,
            "prob": 0.0043
          },
          {
            "token": "Ġusually",
            "token_id": 6118,
            "logprob": -5.7812,
            "prob": 0.0031
          },
          {
            "token": "Ġpretty",
            "token_id": 5128,
            "logprob": -6.6562,
            "prob": 0.0013
          },
          {
            "token": "Ġworks",
            "token_id": 4375,
            "logprob": -7.0938,
            "prob": 0.0008
          },
          {
            "token": "Ġmaybe",
            "token_id": 7344,
            "logprob": -7.5312,
            "prob": 0.0005
          },
          {
            "token": "Ġnever",
            "token_id": 2646,
            "logprob": -7.7188,
            "prob": 0.0005
          }
        ]
      },
      "selected_token": {
        "token": "Ġstill",
        "token_id": 2103,
        "selection_method": "sampling"
      }
    },
    {
      "step": 89,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal so it works across different cultures. Maybe a classic setup and punchline. Hmm, animals are always safe bets. How about a dog joke? Dogs are relatable. Let me recall some dog puns. Oh, the \"paws\" for \"pairs\" pun! Yeah, that's been used before but still",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal",
        "Ġso",
        "Ġit",
        "Ġworks",
        "Ġacross",
        "Ġdifferent",
        "Ġcultures",
        ".",
        "ĠMaybe",
        "Ġa",
        "Ġclassic",
        "Ġsetup",
        "Ġand",
        "Ġpunch",
        "line",
        ".",
        "ĠHmm",
        ",",
        "Ġanimals",
        "Ġare",
        "Ġalways",
        "Ġsafe",
        "Ġbets",
        ".",
        "ĠHow",
        "Ġabout",
        "Ġa",
        "Ġdog",
        "Ġjoke",
        "?",
        "ĠDogs",
        "Ġare",
        "Ġrel",
        "atable",
        ".",
        "ĠLet",
        "Ġme",
        "Ġrecall",
        "Ġsome",
        "Ġdog",
        "Ġpun",
        "s",
        ".",
        "ĠOh",
        ",",
        "Ġthe",
        "Ġ\"",
        "p",
        "aws",
        "\"",
        "Ġfor",
        "Ġ\"",
        "pairs",
        "\"",
        "Ġpun",
        "!",
        "ĠYeah",
        ",",
        "Ġthat",
        "'s",
        "Ġbeen",
        "Ġused",
        "Ġbefore",
        "Ġbut",
        "Ġstill"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789,
        779,
        433,
        4375,
        4028,
        2204,
        27833,
        13,
        10926,
        264,
        11670,
        6642,
        323,
        21004,
        1074,
        13,
        89290,
        11,
        10099,
        527,
        2744,
        6220,
        43177,
        13,
        2650,
        922,
        264,
        5679,
        22380,
        30,
        39525,
        527,
        1375,
        15436,
        13,
        6914,
        757,
        19635,
        1063,
        5679,
        8474,
        82,
        13,
        8840,
        11,
        279,
        330,
        79,
        8805,
        1,
        369,
        330,
        78356,
        1,
        8474,
        0,
        22335,
        11,
        430,
        596,
        1027,
        1511,
        1603,
        719,
        2103
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġeffective",
            "token_id": 7524,
            "logprob": -0.625,
            "prob": 0.5352
          },
          {
            "token": "Ġfunny",
            "token_id": 15526,
            "logprob": -1.0,
            "prob": 0.3672
          },
          {
            "token": "Ġworks",
            "token_id": 4375,
            "logprob": -2.8125,
            "prob": 0.0601
          },
          {
            "token": "Ġfun",
            "token_id": 2523,
            "logprob": -4.9375,
            "prob": 0.0072
          },
          {
            "token": "Ġpretty",
            "token_id": 5128,
            "logprob": -5.0,
            "prob": 0.0067
          },
          {
            "token": "Ġgets",
            "token_id": 5334,
            "logprob": -5.4375,
            "prob": 0.0044
          },
          {
            "token": "Ġpopular",
            "token_id": 5526,
            "logprob": -5.4375,
            "prob": 0.0044
          },
          {
            "token": "有效",
            "token_id": 89186,
            "logprob": -5.9375,
            "prob": 0.0026
          },
          {
            "token": "ĠEffective",
            "token_id": 48023,
            "logprob": -6.5625,
            "prob": 0.0014
          },
          {
            "token": "Ġcute",
            "token_id": 19369,
            "logprob": -6.875,
            "prob": 0.001
          }
        ]
      },
      "selected_token": {
        "token": "Ġworks",
        "token_id": 4375,
        "selection_method": "sampling"
      }
    },
    {
      "step": 90,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal so it works across different cultures. Maybe a classic setup and punchline. Hmm, animals are always safe bets. How about a dog joke? Dogs are relatable. Let me recall some dog puns. Oh, the \"paws\" for \"pairs\" pun! Yeah, that's been used before but still works",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal",
        "Ġso",
        "Ġit",
        "Ġworks",
        "Ġacross",
        "Ġdifferent",
        "Ġcultures",
        ".",
        "ĠMaybe",
        "Ġa",
        "Ġclassic",
        "Ġsetup",
        "Ġand",
        "Ġpunch",
        "line",
        ".",
        "ĠHmm",
        ",",
        "Ġanimals",
        "Ġare",
        "Ġalways",
        "Ġsafe",
        "Ġbets",
        ".",
        "ĠHow",
        "Ġabout",
        "Ġa",
        "Ġdog",
        "Ġjoke",
        "?",
        "ĠDogs",
        "Ġare",
        "Ġrel",
        "atable",
        ".",
        "ĠLet",
        "Ġme",
        "Ġrecall",
        "Ġsome",
        "Ġdog",
        "Ġpun",
        "s",
        ".",
        "ĠOh",
        ",",
        "Ġthe",
        "Ġ\"",
        "p",
        "aws",
        "\"",
        "Ġfor",
        "Ġ\"",
        "pairs",
        "\"",
        "Ġpun",
        "!",
        "ĠYeah",
        ",",
        "Ġthat",
        "'s",
        "Ġbeen",
        "Ġused",
        "Ġbefore",
        "Ġbut",
        "Ġstill",
        "Ġworks"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789,
        779,
        433,
        4375,
        4028,
        2204,
        27833,
        13,
        10926,
        264,
        11670,
        6642,
        323,
        21004,
        1074,
        13,
        89290,
        11,
        10099,
        527,
        2744,
        6220,
        43177,
        13,
        2650,
        922,
        264,
        5679,
        22380,
        30,
        39525,
        527,
        1375,
        15436,
        13,
        6914,
        757,
        19635,
        1063,
        5679,
        8474,
        82,
        13,
        8840,
        11,
        279,
        330,
        79,
        8805,
        1,
        369,
        330,
        78356,
        1,
        8474,
        0,
        22335,
        11,
        430,
        596,
        1027,
        1511,
        1603,
        719,
        2103,
        4375
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": ".",
            "token_id": 13,
            "logprob": -0.1279,
            "prob": 0.8789
          },
          {
            "token": ".\n\n",
            "token_id": 382,
            "logprob": -2.125,
            "prob": 0.1191
          },
          {
            "token": "Ġbecause",
            "token_id": 1606,
            "logprob": -7.125,
            "prob": 0.0008
          },
          {
            "token": "Ġwell",
            "token_id": 1664,
            "logprob": -8.0,
            "prob": 0.0003
          },
          {
            "token": "Ġif",
            "token_id": 422,
            "logprob": -10.5625,
            "prob": 0.0
          },
          {
            "token": "Ġpretty",
            "token_id": 5128,
            "logprob": -11.5,
            "prob": 0.0
          },
          {
            "token": "Ġokay",
            "token_id": 17339,
            "logprob": -12.4375,
            "prob": 0.0
          },
          {
            "token": ",",
            "token_id": 11,
            "logprob": -12.75,
            "prob": 0.0
          },
          {
            "token": ".\n",
            "token_id": 627,
            "logprob": -12.8125,
            "prob": 0.0
          },
          {
            "token": "Ġnicely",
            "token_id": 29921,
            "logprob": -12.875,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": ".",
        "token_id": 13,
        "selection_method": "sampling"
      }
    },
    {
      "step": 91,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal so it works across different cultures. Maybe a classic setup and punchline. Hmm, animals are always safe bets. How about a dog joke? Dogs are relatable. Let me recall some dog puns. Oh, the \"paws\" for \"pairs\" pun! Yeah, that's been used before but still works.",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal",
        "Ġso",
        "Ġit",
        "Ġworks",
        "Ġacross",
        "Ġdifferent",
        "Ġcultures",
        ".",
        "ĠMaybe",
        "Ġa",
        "Ġclassic",
        "Ġsetup",
        "Ġand",
        "Ġpunch",
        "line",
        ".",
        "ĠHmm",
        ",",
        "Ġanimals",
        "Ġare",
        "Ġalways",
        "Ġsafe",
        "Ġbets",
        ".",
        "ĠHow",
        "Ġabout",
        "Ġa",
        "Ġdog",
        "Ġjoke",
        "?",
        "ĠDogs",
        "Ġare",
        "Ġrel",
        "atable",
        ".",
        "ĠLet",
        "Ġme",
        "Ġrecall",
        "Ġsome",
        "Ġdog",
        "Ġpun",
        "s",
        ".",
        "ĠOh",
        ",",
        "Ġthe",
        "Ġ\"",
        "p",
        "aws",
        "\"",
        "Ġfor",
        "Ġ\"",
        "pairs",
        "\"",
        "Ġpun",
        "!",
        "ĠYeah",
        ",",
        "Ġthat",
        "'s",
        "Ġbeen",
        "Ġused",
        "Ġbefore",
        "Ġbut",
        "Ġstill",
        "Ġworks",
        "."
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789,
        779,
        433,
        4375,
        4028,
        2204,
        27833,
        13,
        10926,
        264,
        11670,
        6642,
        323,
        21004,
        1074,
        13,
        89290,
        11,
        10099,
        527,
        2744,
        6220,
        43177,
        13,
        2650,
        922,
        264,
        5679,
        22380,
        30,
        39525,
        527,
        1375,
        15436,
        13,
        6914,
        757,
        19635,
        1063,
        5679,
        8474,
        82,
        13,
        8840,
        11,
        279,
        330,
        79,
        8805,
        1,
        369,
        330,
        78356,
        1,
        8474,
        0,
        22335,
        11,
        430,
        596,
        1027,
        1511,
        1603,
        719,
        2103,
        4375,
        13
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "ĠLet",
            "token_id": 6914,
            "logprob": -0.0457,
            "prob": 0.957
          },
          {
            "token": "ĠWait",
            "token_id": 14144,
            "logprob": -3.1719,
            "prob": 0.042
          },
          {
            "token": "ĠSo",
            "token_id": 2100,
            "logprob": -7.4062,
            "prob": 0.0006
          },
          {
            "token": "ĠThe",
            "token_id": 578,
            "logprob": -7.5938,
            "prob": 0.0005
          },
          {
            "token": "Ġ\n\n",
            "token_id": 4815,
            "logprob": -7.6562,
            "prob": 0.0005
          },
          {
            "token": "ĠAlright",
            "token_id": 98693,
            "logprob": -8.5,
            "prob": 0.0002
          },
          {
            "token": "ĠHere",
            "token_id": 5810,
            "logprob": -8.8125,
            "prob": 0.0002
          },
          {
            "token": "Ġ\"",
            "token_id": 330,
            "logprob": -9.125,
            "prob": 0.0001
          },
          {
            "token": "ĠLike",
            "token_id": 9086,
            "logprob": -9.3125,
            "prob": 0.0001
          },
          {
            "token": "ĠSetup",
            "token_id": 19139,
            "logprob": -10.0,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "ĠLet",
        "token_id": 6914,
        "selection_method": "sampling"
      }
    },
    {
      "step": 92,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal so it works across different cultures. Maybe a classic setup and punchline. Hmm, animals are always safe bets. How about a dog joke? Dogs are relatable. Let me recall some dog puns. Oh, the \"paws\" for \"pairs\" pun! Yeah, that's been used before but still works. Let",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal",
        "Ġso",
        "Ġit",
        "Ġworks",
        "Ġacross",
        "Ġdifferent",
        "Ġcultures",
        ".",
        "ĠMaybe",
        "Ġa",
        "Ġclassic",
        "Ġsetup",
        "Ġand",
        "Ġpunch",
        "line",
        ".",
        "ĠHmm",
        ",",
        "Ġanimals",
        "Ġare",
        "Ġalways",
        "Ġsafe",
        "Ġbets",
        ".",
        "ĠHow",
        "Ġabout",
        "Ġa",
        "Ġdog",
        "Ġjoke",
        "?",
        "ĠDogs",
        "Ġare",
        "Ġrel",
        "atable",
        ".",
        "ĠLet",
        "Ġme",
        "Ġrecall",
        "Ġsome",
        "Ġdog",
        "Ġpun",
        "s",
        ".",
        "ĠOh",
        ",",
        "Ġthe",
        "Ġ\"",
        "p",
        "aws",
        "\"",
        "Ġfor",
        "Ġ\"",
        "pairs",
        "\"",
        "Ġpun",
        "!",
        "ĠYeah",
        ",",
        "Ġthat",
        "'s",
        "Ġbeen",
        "Ġused",
        "Ġbefore",
        "Ġbut",
        "Ġstill",
        "Ġworks",
        ".",
        "ĠLet"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789,
        779,
        433,
        4375,
        4028,
        2204,
        27833,
        13,
        10926,
        264,
        11670,
        6642,
        323,
        21004,
        1074,
        13,
        89290,
        11,
        10099,
        527,
        2744,
        6220,
        43177,
        13,
        2650,
        922,
        264,
        5679,
        22380,
        30,
        39525,
        527,
        1375,
        15436,
        13,
        6914,
        757,
        19635,
        1063,
        5679,
        8474,
        82,
        13,
        8840,
        11,
        279,
        330,
        79,
        8805,
        1,
        369,
        330,
        78356,
        1,
        8474,
        0,
        22335,
        11,
        430,
        596,
        1027,
        1511,
        1603,
        719,
        2103,
        4375,
        13,
        6914
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġme",
            "token_id": 757,
            "logprob": -0.0182,
            "prob": 0.9805
          },
          {
            "token": "'s",
            "token_id": 596,
            "logprob": -4.0312,
            "prob": 0.0179
          },
          {
            "token": "Ġsee",
            "token_id": 1518,
            "logprob": -10.875,
            "prob": 0.0
          },
          {
            "token": "ĠMe",
            "token_id": 2206,
            "logprob": -13.25,
            "prob": 0.0
          },
          {
            "token": "Ġmet",
            "token_id": 2322,
            "logprob": -13.3125,
            "prob": 0.0
          },
          {
            "token": "’s",
            "token_id": 753,
            "logprob": -13.3125,
            "prob": 0.0
          },
          {
            "token": "Ġgo",
            "token_id": 733,
            "logprob": -13.8125,
            "prob": 0.0
          },
          {
            "token": "me",
            "token_id": 2727,
            "logprob": -14.125,
            "prob": 0.0
          },
          {
            "token": "Ġmy",
            "token_id": 856,
            "logprob": -14.5625,
            "prob": 0.0
          },
          {
            "token": "Ġit",
            "token_id": 433,
            "logprob": -14.625,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġme",
        "token_id": 757,
        "selection_method": "sampling"
      }
    },
    {
      "step": 93,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal so it works across different cultures. Maybe a classic setup and punchline. Hmm, animals are always safe bets. How about a dog joke? Dogs are relatable. Let me recall some dog puns. Oh, the \"paws\" for \"pairs\" pun! Yeah, that's been used before but still works. Let me",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal",
        "Ġso",
        "Ġit",
        "Ġworks",
        "Ġacross",
        "Ġdifferent",
        "Ġcultures",
        ".",
        "ĠMaybe",
        "Ġa",
        "Ġclassic",
        "Ġsetup",
        "Ġand",
        "Ġpunch",
        "line",
        ".",
        "ĠHmm",
        ",",
        "Ġanimals",
        "Ġare",
        "Ġalways",
        "Ġsafe",
        "Ġbets",
        ".",
        "ĠHow",
        "Ġabout",
        "Ġa",
        "Ġdog",
        "Ġjoke",
        "?",
        "ĠDogs",
        "Ġare",
        "Ġrel",
        "atable",
        ".",
        "ĠLet",
        "Ġme",
        "Ġrecall",
        "Ġsome",
        "Ġdog",
        "Ġpun",
        "s",
        ".",
        "ĠOh",
        ",",
        "Ġthe",
        "Ġ\"",
        "p",
        "aws",
        "\"",
        "Ġfor",
        "Ġ\"",
        "pairs",
        "\"",
        "Ġpun",
        "!",
        "ĠYeah",
        ",",
        "Ġthat",
        "'s",
        "Ġbeen",
        "Ġused",
        "Ġbefore",
        "Ġbut",
        "Ġstill",
        "Ġworks",
        ".",
        "ĠLet",
        "Ġme"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789,
        779,
        433,
        4375,
        4028,
        2204,
        27833,
        13,
        10926,
        264,
        11670,
        6642,
        323,
        21004,
        1074,
        13,
        89290,
        11,
        10099,
        527,
        2744,
        6220,
        43177,
        13,
        2650,
        922,
        264,
        5679,
        22380,
        30,
        39525,
        527,
        1375,
        15436,
        13,
        6914,
        757,
        19635,
        1063,
        5679,
        8474,
        82,
        13,
        8840,
        11,
        279,
        330,
        79,
        8805,
        1,
        369,
        330,
        78356,
        1,
        8474,
        0,
        22335,
        11,
        430,
        596,
        1027,
        1511,
        1603,
        719,
        2103,
        4375,
        13,
        6914,
        757
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġstructure",
            "token_id": 6070,
            "logprob": -0.2256,
            "prob": 0.7969
          },
          {
            "token": "Ġcheck",
            "token_id": 1817,
            "logprob": -1.8516,
            "prob": 0.1572
          },
          {
            "token": "Ġmake",
            "token_id": 1304,
            "logprob": -3.9688,
            "prob": 0.0188
          },
          {
            "token": "Ġconstruct",
            "token_id": 9429,
            "logprob": -5.1562,
            "prob": 0.0057
          },
          {
            "token": "Ġphrase",
            "token_id": 17571,
            "logprob": -5.2188,
            "prob": 0.0054
          },
          {
            "token": "Ġframe",
            "token_id": 4124,
            "logprob": -5.4688,
            "prob": 0.0042
          },
          {
            "token": "Ġtry",
            "token_id": 1456,
            "logprob": -5.4688,
            "prob": 0.0042
          },
          {
            "token": "Ġcraft",
            "token_id": 11003,
            "logprob": -5.9688,
            "prob": 0.0025
          },
          {
            "token": "Ġsee",
            "token_id": 1518,
            "logprob": -7.1562,
            "prob": 0.0008
          },
          {
            "token": "Ġput",
            "token_id": 2231,
            "logprob": -7.5938,
            "prob": 0.0005
          }
        ]
      },
      "selected_token": {
        "token": "Ġmake",
        "token_id": 1304,
        "selection_method": "sampling"
      }
    },
    {
      "step": 94,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal so it works across different cultures. Maybe a classic setup and punchline. Hmm, animals are always safe bets. How about a dog joke? Dogs are relatable. Let me recall some dog puns. Oh, the \"paws\" for \"pairs\" pun! Yeah, that's been used before but still works. Let me make",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal",
        "Ġso",
        "Ġit",
        "Ġworks",
        "Ġacross",
        "Ġdifferent",
        "Ġcultures",
        ".",
        "ĠMaybe",
        "Ġa",
        "Ġclassic",
        "Ġsetup",
        "Ġand",
        "Ġpunch",
        "line",
        ".",
        "ĠHmm",
        ",",
        "Ġanimals",
        "Ġare",
        "Ġalways",
        "Ġsafe",
        "Ġbets",
        ".",
        "ĠHow",
        "Ġabout",
        "Ġa",
        "Ġdog",
        "Ġjoke",
        "?",
        "ĠDogs",
        "Ġare",
        "Ġrel",
        "atable",
        ".",
        "ĠLet",
        "Ġme",
        "Ġrecall",
        "Ġsome",
        "Ġdog",
        "Ġpun",
        "s",
        ".",
        "ĠOh",
        ",",
        "Ġthe",
        "Ġ\"",
        "p",
        "aws",
        "\"",
        "Ġfor",
        "Ġ\"",
        "pairs",
        "\"",
        "Ġpun",
        "!",
        "ĠYeah",
        ",",
        "Ġthat",
        "'s",
        "Ġbeen",
        "Ġused",
        "Ġbefore",
        "Ġbut",
        "Ġstill",
        "Ġworks",
        ".",
        "ĠLet",
        "Ġme",
        "Ġmake"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789,
        779,
        433,
        4375,
        4028,
        2204,
        27833,
        13,
        10926,
        264,
        11670,
        6642,
        323,
        21004,
        1074,
        13,
        89290,
        11,
        10099,
        527,
        2744,
        6220,
        43177,
        13,
        2650,
        922,
        264,
        5679,
        22380,
        30,
        39525,
        527,
        1375,
        15436,
        13,
        6914,
        757,
        19635,
        1063,
        5679,
        8474,
        82,
        13,
        8840,
        11,
        279,
        330,
        79,
        8805,
        1,
        369,
        330,
        78356,
        1,
        8474,
        0,
        22335,
        11,
        430,
        596,
        1027,
        1511,
        1603,
        719,
        2103,
        4375,
        13,
        6914,
        757,
        1304
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġsure",
            "token_id": 2771,
            "logprob": -0.0028,
            "prob": 0.9961
          },
          {
            "token": "Ġit",
            "token_id": 433,
            "logprob": -5.875,
            "prob": 0.0028
          },
          {
            "token": "Ġup",
            "token_id": 709,
            "logprob": -10.5,
            "prob": 0.0
          },
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -11.5625,
            "prob": 0.0
          },
          {
            "token": "Ġone",
            "token_id": 832,
            "logprob": -14.4375,
            "prob": 0.0
          },
          {
            "token": "Ġa",
            "token_id": 264,
            "logprob": -14.6875,
            "prob": 0.0
          },
          {
            "token": "ĠSure",
            "token_id": 23371,
            "logprob": -15.8125,
            "prob": 0.0
          },
          {
            "token": "Ġthat",
            "token_id": 430,
            "logprob": -16.25,
            "prob": 0.0
          },
          {
            "token": "sure",
            "token_id": 19643,
            "logprob": -16.25,
            "prob": 0.0
          },
          {
            "token": "确",
            "token_id": 35056,
            "logprob": -16.25,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġsure",
        "token_id": 2771,
        "selection_method": "sampling"
      }
    },
    {
      "step": 95,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal so it works across different cultures. Maybe a classic setup and punchline. Hmm, animals are always safe bets. How about a dog joke? Dogs are relatable. Let me recall some dog puns. Oh, the \"paws\" for \"pairs\" pun! Yeah, that's been used before but still works. Let me make sure",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal",
        "Ġso",
        "Ġit",
        "Ġworks",
        "Ġacross",
        "Ġdifferent",
        "Ġcultures",
        ".",
        "ĠMaybe",
        "Ġa",
        "Ġclassic",
        "Ġsetup",
        "Ġand",
        "Ġpunch",
        "line",
        ".",
        "ĠHmm",
        ",",
        "Ġanimals",
        "Ġare",
        "Ġalways",
        "Ġsafe",
        "Ġbets",
        ".",
        "ĠHow",
        "Ġabout",
        "Ġa",
        "Ġdog",
        "Ġjoke",
        "?",
        "ĠDogs",
        "Ġare",
        "Ġrel",
        "atable",
        ".",
        "ĠLet",
        "Ġme",
        "Ġrecall",
        "Ġsome",
        "Ġdog",
        "Ġpun",
        "s",
        ".",
        "ĠOh",
        ",",
        "Ġthe",
        "Ġ\"",
        "p",
        "aws",
        "\"",
        "Ġfor",
        "Ġ\"",
        "pairs",
        "\"",
        "Ġpun",
        "!",
        "ĠYeah",
        ",",
        "Ġthat",
        "'s",
        "Ġbeen",
        "Ġused",
        "Ġbefore",
        "Ġbut",
        "Ġstill",
        "Ġworks",
        ".",
        "ĠLet",
        "Ġme",
        "Ġmake",
        "Ġsure"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789,
        779,
        433,
        4375,
        4028,
        2204,
        27833,
        13,
        10926,
        264,
        11670,
        6642,
        323,
        21004,
        1074,
        13,
        89290,
        11,
        10099,
        527,
        2744,
        6220,
        43177,
        13,
        2650,
        922,
        264,
        5679,
        22380,
        30,
        39525,
        527,
        1375,
        15436,
        13,
        6914,
        757,
        19635,
        1063,
        5679,
        8474,
        82,
        13,
        8840,
        11,
        279,
        330,
        79,
        8805,
        1,
        369,
        330,
        78356,
        1,
        8474,
        0,
        22335,
        11,
        430,
        596,
        1027,
        1511,
        1603,
        719,
        2103,
        4375,
        13,
        6914,
        757,
        1304,
        2771
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -0.5391,
            "prob": 0.582
          },
          {
            "token": "Ġit",
            "token_id": 433,
            "logprob": -0.9141,
            "prob": 0.4004
          },
          {
            "token": "Ġthere",
            "token_id": 1070,
            "logprob": -4.2812,
            "prob": 0.0137
          },
          {
            "token": "ĠI",
            "token_id": 358,
            "logprob": -6.0312,
            "prob": 0.0024
          },
          {
            "token": ".",
            "token_id": 13,
            "logprob": -9.3125,
            "prob": 0.0001
          },
          {
            "token": ":",
            "token_id": 25,
            "logprob": -10.9375,
            "prob": 0.0
          },
          {
            "token": "Ġthat",
            "token_id": 430,
            "logprob": -13.4375,
            "prob": 0.0
          },
          {
            "token": ".\n\n",
            "token_id": 382,
            "logprob": -13.8125,
            "prob": 0.0
          },
          {
            "token": "Ġabout",
            "token_id": 922,
            "logprob": -14.3125,
            "prob": 0.0
          },
          {
            "token": "Ġto",
            "token_id": 311,
            "logprob": -14.4375,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġthe",
        "token_id": 279,
        "selection_method": "sampling"
      }
    },
    {
      "step": 96,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal so it works across different cultures. Maybe a classic setup and punchline. Hmm, animals are always safe bets. How about a dog joke? Dogs are relatable. Let me recall some dog puns. Oh, the \"paws\" for \"pairs\" pun! Yeah, that's been used before but still works. Let me make sure the",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal",
        "Ġso",
        "Ġit",
        "Ġworks",
        "Ġacross",
        "Ġdifferent",
        "Ġcultures",
        ".",
        "ĠMaybe",
        "Ġa",
        "Ġclassic",
        "Ġsetup",
        "Ġand",
        "Ġpunch",
        "line",
        ".",
        "ĠHmm",
        ",",
        "Ġanimals",
        "Ġare",
        "Ġalways",
        "Ġsafe",
        "Ġbets",
        ".",
        "ĠHow",
        "Ġabout",
        "Ġa",
        "Ġdog",
        "Ġjoke",
        "?",
        "ĠDogs",
        "Ġare",
        "Ġrel",
        "atable",
        ".",
        "ĠLet",
        "Ġme",
        "Ġrecall",
        "Ġsome",
        "Ġdog",
        "Ġpun",
        "s",
        ".",
        "ĠOh",
        ",",
        "Ġthe",
        "Ġ\"",
        "p",
        "aws",
        "\"",
        "Ġfor",
        "Ġ\"",
        "pairs",
        "\"",
        "Ġpun",
        "!",
        "ĠYeah",
        ",",
        "Ġthat",
        "'s",
        "Ġbeen",
        "Ġused",
        "Ġbefore",
        "Ġbut",
        "Ġstill",
        "Ġworks",
        ".",
        "ĠLet",
        "Ġme",
        "Ġmake",
        "Ġsure",
        "Ġthe"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789,
        779,
        433,
        4375,
        4028,
        2204,
        27833,
        13,
        10926,
        264,
        11670,
        6642,
        323,
        21004,
        1074,
        13,
        89290,
        11,
        10099,
        527,
        2744,
        6220,
        43177,
        13,
        2650,
        922,
        264,
        5679,
        22380,
        30,
        39525,
        527,
        1375,
        15436,
        13,
        6914,
        757,
        19635,
        1063,
        5679,
        8474,
        82,
        13,
        8840,
        11,
        279,
        330,
        79,
        8805,
        1,
        369,
        330,
        78356,
        1,
        8474,
        0,
        22335,
        11,
        430,
        596,
        1027,
        1511,
        1603,
        719,
        2103,
        4375,
        13,
        6914,
        757,
        1304,
        2771,
        279
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġwording",
            "token_id": 61327,
            "logprob": -0.6016,
            "prob": 0.5469
          },
          {
            "token": "Ġgrammar",
            "token_id": 32528,
            "logprob": -1.4766,
            "prob": 0.2285
          },
          {
            "token": "Ġpunch",
            "token_id": 21004,
            "logprob": -2.2188,
            "prob": 0.1079
          },
          {
            "token": "Ġtiming",
            "token_id": 18912,
            "logprob": -3.5938,
            "prob": 0.0273
          },
          {
            "token": "Ġword",
            "token_id": 3492,
            "logprob": -3.8438,
            "prob": 0.0212
          },
          {
            "token": "Ġstructure",
            "token_id": 6070,
            "logprob": -4.1562,
            "prob": 0.0156
          },
          {
            "token": "Ġsyntax",
            "token_id": 20047,
            "logprob": -4.4062,
            "prob": 0.0121
          },
          {
            "token": "Ġtranslation",
            "token_id": 14807,
            "logprob": -4.4062,
            "prob": 0.0121
          },
          {
            "token": "Ġwords",
            "token_id": 4339,
            "logprob": -4.9688,
            "prob": 0.0069
          },
          {
            "token": "Ġpun",
            "token_id": 8474,
            "logprob": -5.5938,
            "prob": 0.0037
          }
        ]
      },
      "selected_token": {
        "token": "Ġwording",
        "token_id": 61327,
        "selection_method": "sampling"
      }
    },
    {
      "step": 97,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal so it works across different cultures. Maybe a classic setup and punchline. Hmm, animals are always safe bets. How about a dog joke? Dogs are relatable. Let me recall some dog puns. Oh, the \"paws\" for \"pairs\" pun! Yeah, that's been used before but still works. Let me make sure the wording",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal",
        "Ġso",
        "Ġit",
        "Ġworks",
        "Ġacross",
        "Ġdifferent",
        "Ġcultures",
        ".",
        "ĠMaybe",
        "Ġa",
        "Ġclassic",
        "Ġsetup",
        "Ġand",
        "Ġpunch",
        "line",
        ".",
        "ĠHmm",
        ",",
        "Ġanimals",
        "Ġare",
        "Ġalways",
        "Ġsafe",
        "Ġbets",
        ".",
        "ĠHow",
        "Ġabout",
        "Ġa",
        "Ġdog",
        "Ġjoke",
        "?",
        "ĠDogs",
        "Ġare",
        "Ġrel",
        "atable",
        ".",
        "ĠLet",
        "Ġme",
        "Ġrecall",
        "Ġsome",
        "Ġdog",
        "Ġpun",
        "s",
        ".",
        "ĠOh",
        ",",
        "Ġthe",
        "Ġ\"",
        "p",
        "aws",
        "\"",
        "Ġfor",
        "Ġ\"",
        "pairs",
        "\"",
        "Ġpun",
        "!",
        "ĠYeah",
        ",",
        "Ġthat",
        "'s",
        "Ġbeen",
        "Ġused",
        "Ġbefore",
        "Ġbut",
        "Ġstill",
        "Ġworks",
        ".",
        "ĠLet",
        "Ġme",
        "Ġmake",
        "Ġsure",
        "Ġthe",
        "Ġwording"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789,
        779,
        433,
        4375,
        4028,
        2204,
        27833,
        13,
        10926,
        264,
        11670,
        6642,
        323,
        21004,
        1074,
        13,
        89290,
        11,
        10099,
        527,
        2744,
        6220,
        43177,
        13,
        2650,
        922,
        264,
        5679,
        22380,
        30,
        39525,
        527,
        1375,
        15436,
        13,
        6914,
        757,
        19635,
        1063,
        5679,
        8474,
        82,
        13,
        8840,
        11,
        279,
        330,
        79,
        8805,
        1,
        369,
        330,
        78356,
        1,
        8474,
        0,
        22335,
        11,
        430,
        596,
        1027,
        1511,
        1603,
        719,
        2103,
        4375,
        13,
        6914,
        757,
        1304,
        2771,
        279,
        61327
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġis",
            "token_id": 374,
            "logprob": -0.0284,
            "prob": 0.9727
          },
          {
            "token": "Ġflows",
            "token_id": 28555,
            "logprob": -3.6562,
            "prob": 0.0259
          },
          {
            "token": "Ġisn",
            "token_id": 4536,
            "logprob": -6.2188,
            "prob": 0.002
          },
          {
            "token": "Ġflow",
            "token_id": 6530,
            "logprob": -10.25,
            "prob": 0.0
          },
          {
            "token": "Ġmakes",
            "token_id": 3727,
            "logprob": -10.625,
            "prob": 0.0
          },
          {
            "token": "'s",
            "token_id": 596,
            "logprob": -10.75,
            "prob": 0.0
          },
          {
            "token": "Ġdoesn",
            "token_id": 3250,
            "logprob": -13.5625,
            "prob": 0.0
          },
          {
            "token": "Ġsounds",
            "token_id": 10578,
            "logprob": -13.8125,
            "prob": 0.0
          },
          {
            "token": "Ġplays",
            "token_id": 11335,
            "logprob": -14.125,
            "prob": 0.0
          },
          {
            "token": "Ġfits",
            "token_id": 18809,
            "logprob": -14.1875,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġis",
        "token_id": 374,
        "selection_method": "sampling"
      }
    },
    {
      "step": 98,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal so it works across different cultures. Maybe a classic setup and punchline. Hmm, animals are always safe bets. How about a dog joke? Dogs are relatable. Let me recall some dog puns. Oh, the \"paws\" for \"pairs\" pun! Yeah, that's been used before but still works. Let me make sure the wording is",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal",
        "Ġso",
        "Ġit",
        "Ġworks",
        "Ġacross",
        "Ġdifferent",
        "Ġcultures",
        ".",
        "ĠMaybe",
        "Ġa",
        "Ġclassic",
        "Ġsetup",
        "Ġand",
        "Ġpunch",
        "line",
        ".",
        "ĠHmm",
        ",",
        "Ġanimals",
        "Ġare",
        "Ġalways",
        "Ġsafe",
        "Ġbets",
        ".",
        "ĠHow",
        "Ġabout",
        "Ġa",
        "Ġdog",
        "Ġjoke",
        "?",
        "ĠDogs",
        "Ġare",
        "Ġrel",
        "atable",
        ".",
        "ĠLet",
        "Ġme",
        "Ġrecall",
        "Ġsome",
        "Ġdog",
        "Ġpun",
        "s",
        ".",
        "ĠOh",
        ",",
        "Ġthe",
        "Ġ\"",
        "p",
        "aws",
        "\"",
        "Ġfor",
        "Ġ\"",
        "pairs",
        "\"",
        "Ġpun",
        "!",
        "ĠYeah",
        ",",
        "Ġthat",
        "'s",
        "Ġbeen",
        "Ġused",
        "Ġbefore",
        "Ġbut",
        "Ġstill",
        "Ġworks",
        ".",
        "ĠLet",
        "Ġme",
        "Ġmake",
        "Ġsure",
        "Ġthe",
        "Ġwording",
        "Ġis"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789,
        779,
        433,
        4375,
        4028,
        2204,
        27833,
        13,
        10926,
        264,
        11670,
        6642,
        323,
        21004,
        1074,
        13,
        89290,
        11,
        10099,
        527,
        2744,
        6220,
        43177,
        13,
        2650,
        922,
        264,
        5679,
        22380,
        30,
        39525,
        527,
        1375,
        15436,
        13,
        6914,
        757,
        19635,
        1063,
        5679,
        8474,
        82,
        13,
        8840,
        11,
        279,
        330,
        79,
        8805,
        1,
        369,
        330,
        78356,
        1,
        8474,
        0,
        22335,
        11,
        430,
        596,
        1027,
        1511,
        1603,
        719,
        2103,
        4375,
        13,
        6914,
        757,
        1304,
        2771,
        279,
        61327,
        374
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġclear",
            "token_id": 2867,
            "logprob": -0.1533,
            "prob": 0.8594
          },
          {
            "token": "Ġsmooth",
            "token_id": 11113,
            "logprob": -2.4062,
            "prob": 0.0903
          },
          {
            "token": "Ġclean",
            "token_id": 4335,
            "logprob": -3.4062,
            "prob": 0.0332
          },
          {
            "token": "Ġcorrect",
            "token_id": 4495,
            "logprob": -4.6562,
            "prob": 0.0095
          },
          {
            "token": "Ġsimple",
            "token_id": 4382,
            "logprob": -5.4688,
            "prob": 0.0042
          },
          {
            "token": "Ġnatural",
            "token_id": 5933,
            "logprob": -5.9688,
            "prob": 0.0026
          },
          {
            "token": "Ġright",
            "token_id": 1314,
            "logprob": -7.4688,
            "prob": 0.0006
          },
          {
            "token": "Ġstraightforward",
            "token_id": 31439,
            "logprob": -8.1875,
            "prob": 0.0003
          },
          {
            "token": "Ġfunny",
            "token_id": 15526,
            "logprob": -8.75,
            "prob": 0.0002
          },
          {
            "token": "Ġcle",
            "token_id": 11551,
            "logprob": -9.25,
            "prob": 0.0001
          }
        ]
      },
      "selected_token": {
        "token": "Ġclear",
        "token_id": 2867,
        "selection_method": "sampling"
      }
    },
    {
      "step": 99,
      "input_text": "给我讲个笑话<|im_end|>\n<|im_start|>assistant\n<think>Okay, the user asked for a joke. Let me think of a good one. I should pick something light-hearted and universal so it works across different cultures. Maybe a classic setup and punchline. Hmm, animals are always safe bets. How about a dog joke? Dogs are relatable. Let me recall some dog puns. Oh, the \"paws\" for \"pairs\" pun! Yeah, that's been used before but still works. Let me make sure the wording is clear",
      "tokens": [
        "给",
        "我",
        "�",
        "�",
        "个",
        "笑",
        "话",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġthe",
        "Ġuser",
        "Ġasked",
        "Ġfor",
        "Ġa",
        "Ġjoke",
        ".",
        "ĠLet",
        "Ġme",
        "Ġthink",
        "Ġof",
        "Ġa",
        "Ġgood",
        "Ġone",
        ".",
        "ĠI",
        "Ġshould",
        "Ġpick",
        "Ġsomething",
        "Ġlight",
        "-hearted",
        "Ġand",
        "Ġuniversal",
        "Ġso",
        "Ġit",
        "Ġworks",
        "Ġacross",
        "Ġdifferent",
        "Ġcultures",
        ".",
        "ĠMaybe",
        "Ġa",
        "Ġclassic",
        "Ġsetup",
        "Ġand",
        "Ġpunch",
        "line",
        ".",
        "ĠHmm",
        ",",
        "Ġanimals",
        "Ġare",
        "Ġalways",
        "Ġsafe",
        "Ġbets",
        ".",
        "ĠHow",
        "Ġabout",
        "Ġa",
        "Ġdog",
        "Ġjoke",
        "?",
        "ĠDogs",
        "Ġare",
        "Ġrel",
        "atable",
        ".",
        "ĠLet",
        "Ġme",
        "Ġrecall",
        "Ġsome",
        "Ġdog",
        "Ġpun",
        "s",
        ".",
        "ĠOh",
        ",",
        "Ġthe",
        "Ġ\"",
        "p",
        "aws",
        "\"",
        "Ġfor",
        "Ġ\"",
        "pairs",
        "\"",
        "Ġpun",
        "!",
        "ĠYeah",
        ",",
        "Ġthat",
        "'s",
        "Ġbeen",
        "Ġused",
        "Ġbefore",
        "Ġbut",
        "Ġstill",
        "Ġworks",
        ".",
        "ĠLet",
        "Ġme",
        "Ġmake",
        "Ġsure",
        "Ġthe",
        "Ġwording",
        "Ġis",
        "Ġclear"
      ],
      "token_ids": [
        90112,
        37046,
        10414,
        110,
        19483,
        49838,
        58543,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        279,
        1217,
        4691,
        369,
        264,
        22380,
        13,
        6914,
        757,
        1781,
        315,
        264,
        1695,
        832,
        13,
        358,
        1288,
        3820,
        2555,
        3177,
        70395,
        323,
        20789,
        779,
        433,
        4375,
        4028,
        2204,
        27833,
        13,
        10926,
        264,
        11670,
        6642,
        323,
        21004,
        1074,
        13,
        89290,
        11,
        10099,
        527,
        2744,
        6220,
        43177,
        13,
        2650,
        922,
        264,
        5679,
        22380,
        30,
        39525,
        527,
        1375,
        15436,
        13,
        6914,
        757,
        19635,
        1063,
        5679,
        8474,
        82,
        13,
        8840,
        11,
        279,
        330,
        79,
        8805,
        1,
        369,
        330,
        78356,
        1,
        8474,
        0,
        22335,
        11,
        430,
        596,
        1027,
        1511,
        1603,
        719,
        2103,
        4375,
        13,
        6914,
        757,
        1304,
        2771,
        279,
        61327,
        374,
        2867
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": ".",
            "token_id": 13,
            "logprob": -0.0381,
            "prob": 0.9609
          },
          {
            "token": ".\n\n",
            "token_id": 382,
            "logprob": -3.7812,
            "prob": 0.0226
          },
          {
            "token": "Ġand",
            "token_id": 323,
            "logprob": -4.2812,
            "prob": 0.0137
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -7.0312,
            "prob": 0.0009
          },
          {
            "token": ":",
            "token_id": 25,
            "logprob": -10.1875,
            "prob": 0.0
          },
          {
            "token": "Ġso",
            "token_id": 779,
            "logprob": -12.75,
            "prob": 0.0
          },
          {
            "token": ":\n\n",
            "token_id": 1473,
            "logprob": -13.125,
            "prob": 0.0
          },
          {
            "token": ".H",
            "token_id": 3924,
            "logprob": -13.6875,
            "prob": 0.0
          },
          {
            "token": "Ġwithout",
            "token_id": 2085,
            "logprob": -13.9375,
            "prob": 0.0
          },
          {
            "token": "Ġenough",
            "token_id": 3403,
            "logprob": -14.0625,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": ".",
        "token_id": 13,
        "selection_method": "sampling"
      }
    }
  ]
}