{
  "examples": [
    {
      "id": "cs-001-greedy-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "Kdo jsi?",
      "language": "cs",
      "description": "Kdo jsi?",
      "num_tokens": 100,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "cs/cs-001-greedy-CohereForAI-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "cs-001-greedy-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "Kdo jsi?",
      "language": "cs",
      "description": "Kdo jsi?",
      "num_tokens": 98,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "cs/cs-001-greedy-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "cs-001-greedy-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "Kdo jsi?",
      "language": "cs",
      "description": "Kdo jsi?",
      "num_tokens": 100,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "cs/cs-001-greedy-allenai-Olmo-3-7B-Think.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "cs-001-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Kdo jsi?",
      "language": "cs",
      "description": "Kdo jsi?",
      "num_tokens": 21,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "cs/cs-001-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "cs-001-greedy-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "Kdo jsi?",
      "language": "cs",
      "description": "Kdo jsi?",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2-xl",
      "file": "cs/cs-001-greedy-openai-community-gpt2-xl.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "cs-001-random-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "Kdo jsi?",
      "language": "cs",
      "description": "Kdo jsi?",
      "num_tokens": 100,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "cs/cs-001-random-CohereForAI-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "cs-001-random-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "Kdo jsi?",
      "language": "cs",
      "description": "Kdo jsi?",
      "num_tokens": 100,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "cs/cs-001-random-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "cs-001-random-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "Kdo jsi?",
      "language": "cs",
      "description": "Kdo jsi?",
      "num_tokens": 100,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "cs/cs-001-random-allenai-Olmo-3-7B-Think.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "cs-001-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Kdo jsi?",
      "language": "cs",
      "description": "Kdo jsi?",
      "num_tokens": 33,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "cs/cs-001-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "cs-001-random-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "Kdo jsi?",
      "language": "cs",
      "description": "Kdo jsi?",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2-xl",
      "file": "cs/cs-001-random-openai-community-gpt2-xl.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "cs-001-sampling-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "Kdo jsi?",
      "language": "cs",
      "description": "Kdo jsi?",
      "num_tokens": 100,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "cs/cs-001-sampling-CohereForAI-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "cs-001-sampling-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "Kdo jsi?",
      "language": "cs",
      "description": "Kdo jsi?",
      "num_tokens": 100,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "cs/cs-001-sampling-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "cs-001-sampling-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "Kdo jsi?",
      "language": "cs",
      "description": "Kdo jsi?",
      "num_tokens": 100,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "cs/cs-001-sampling-allenai-Olmo-3-7B-Think.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "cs-001-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Kdo jsi?",
      "language": "cs",
      "description": "Kdo jsi?",
      "num_tokens": 99,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "cs/cs-001-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "cs-001-sampling-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "Kdo jsi?",
      "language": "cs",
      "description": "Kdo jsi?",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2-xl",
      "file": "cs/cs-001-sampling-openai-community-gpt2-xl.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "cs-002-greedy-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "Řekni mi vtip",
      "language": "cs",
      "description": "Řekni mi vtip",
      "num_tokens": 74,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "cs/cs-002-greedy-CohereForAI-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "cs-002-greedy-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "Řekni mi vtip",
      "language": "cs",
      "description": "Řekni mi vtip",
      "num_tokens": 69,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "cs/cs-002-greedy-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "cs-002-greedy-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "Řekni mi vtip",
      "language": "cs",
      "description": "Řekni mi vtip",
      "num_tokens": 100,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "cs/cs-002-greedy-allenai-Olmo-3-7B-Think.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "cs-002-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Řekni mi vtip",
      "language": "cs",
      "description": "Řekni mi vtip",
      "num_tokens": 39,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "cs/cs-002-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "cs-002-greedy-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "Řekni mi vtip",
      "language": "cs",
      "description": "Řekni mi vtip",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2-xl",
      "file": "cs/cs-002-greedy-openai-community-gpt2-xl.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "cs-002-random-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "Řekni mi vtip",
      "language": "cs",
      "description": "Řekni mi vtip",
      "num_tokens": 75,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "cs/cs-002-random-CohereForAI-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "cs-002-random-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "Řekni mi vtip",
      "language": "cs",
      "description": "Řekni mi vtip",
      "num_tokens": 100,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "cs/cs-002-random-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "cs-002-random-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "Řekni mi vtip",
      "language": "cs",
      "description": "Řekni mi vtip",
      "num_tokens": 100,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "cs/cs-002-random-allenai-Olmo-3-7B-Think.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "cs-002-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Řekni mi vtip",
      "language": "cs",
      "description": "Řekni mi vtip",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "cs/cs-002-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "cs-002-random-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "Řekni mi vtip",
      "language": "cs",
      "description": "Řekni mi vtip",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2-xl",
      "file": "cs/cs-002-random-openai-community-gpt2-xl.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "cs-002-sampling-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "Řekni mi vtip",
      "language": "cs",
      "description": "Řekni mi vtip",
      "num_tokens": 72,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "cs/cs-002-sampling-CohereForAI-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "cs-002-sampling-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "Řekni mi vtip",
      "language": "cs",
      "description": "Řekni mi vtip",
      "num_tokens": 70,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "cs/cs-002-sampling-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "cs-002-sampling-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "Řekni mi vtip",
      "language": "cs",
      "description": "Řekni mi vtip",
      "num_tokens": 100,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "cs/cs-002-sampling-allenai-Olmo-3-7B-Think.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "cs-002-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Řekni mi vtip",
      "language": "cs",
      "description": "Řekni mi vtip",
      "num_tokens": 11,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "cs/cs-002-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "cs-002-sampling-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "Řekni mi vtip",
      "language": "cs",
      "description": "Řekni mi vtip",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2-xl",
      "file": "cs/cs-002-sampling-openai-community-gpt2-xl.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "cs-003-greedy-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "Hlavní město Česka je",
      "language": "cs",
      "description": "Hlavní město Česka je",
      "num_tokens": 61,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "cs/cs-003-greedy-CohereForAI-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "cs-003-greedy-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "Hlavní město Česka je",
      "language": "cs",
      "description": "Hlavní město Česka je",
      "num_tokens": 19,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "cs/cs-003-greedy-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "cs-003-greedy-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "Hlavní město Česka je",
      "language": "cs",
      "description": "Hlavní město Česka je",
      "num_tokens": 100,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "cs/cs-003-greedy-allenai-Olmo-3-7B-Think.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "cs-003-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Hlavní město Česka je",
      "language": "cs",
      "description": "Hlavní město Česka je",
      "num_tokens": 12,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "cs/cs-003-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "cs-003-greedy-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "Hlavní město Česka je",
      "language": "cs",
      "description": "Hlavní město Česka je",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2-xl",
      "file": "cs/cs-003-greedy-openai-community-gpt2-xl.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "cs-003-random-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "Hlavní město Česka je",
      "language": "cs",
      "description": "Hlavní město Česka je",
      "num_tokens": 69,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "cs/cs-003-random-CohereForAI-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "cs-003-random-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "Hlavní město Česka je",
      "language": "cs",
      "description": "Hlavní město Česka je",
      "num_tokens": 100,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "cs/cs-003-random-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "cs-003-random-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "Hlavní město Česka je",
      "language": "cs",
      "description": "Hlavní město Česka je",
      "num_tokens": 100,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "cs/cs-003-random-allenai-Olmo-3-7B-Think.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "cs-003-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Hlavní město Česka je",
      "language": "cs",
      "description": "Hlavní město Česka je",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "cs/cs-003-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "cs-003-random-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "Hlavní město Česka je",
      "language": "cs",
      "description": "Hlavní město Česka je",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2-xl",
      "file": "cs/cs-003-random-openai-community-gpt2-xl.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "cs-003-sampling-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "Hlavní město Česka je",
      "language": "cs",
      "description": "Hlavní město Česka je",
      "num_tokens": 71,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "cs/cs-003-sampling-CohereForAI-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "cs-003-sampling-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "Hlavní město Česka je",
      "language": "cs",
      "description": "Hlavní město Česka je",
      "num_tokens": 19,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "cs/cs-003-sampling-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "cs-003-sampling-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "Hlavní město Česka je",
      "language": "cs",
      "description": "Hlavní město Česka je",
      "num_tokens": 100,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "cs/cs-003-sampling-allenai-Olmo-3-7B-Think.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "cs-003-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Hlavní město Česka je",
      "language": "cs",
      "description": "Hlavní město Česka je",
      "num_tokens": 12,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "cs/cs-003-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "cs-003-sampling-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "Hlavní město Česka je",
      "language": "cs",
      "description": "Hlavní město Česka je",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2-xl",
      "file": "cs/cs-003-sampling-openai-community-gpt2-xl.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "cs-004-greedy-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "Přelož do angličtiny: Žluťoučký kůň úpěl ďábelské ódy",
      "language": "cs",
      "description": "Přelož do angličtiny: Žluťoučký kůň úpěl ďábelské ódy",
      "num_tokens": 41,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "cs/cs-004-greedy-CohereForAI-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "cs-004-greedy-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "Přelož do angličtiny: Žluťoučký kůň úpěl ďábelské ódy",
      "language": "cs",
      "description": "Přelož do angličtiny: Žluťoučký kůň úpěl ďábelské ódy",
      "num_tokens": 9,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "cs/cs-004-greedy-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "cs-004-greedy-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "Přelož do angličtiny: Žluťoučký kůň úpěl ďábelské ódy",
      "language": "cs",
      "description": "Přelož do angličtiny: Žluťoučký kůň úpěl ďábelské ódy",
      "num_tokens": 100,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "cs/cs-004-greedy-allenai-Olmo-3-7B-Think.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "cs-004-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Přelož do angličtiny: Žluťoučký kůň úpěl ďábelské ódy",
      "language": "cs",
      "description": "Přelož do angličtiny: Žluťoučký kůň úpěl ďábelské ódy",
      "num_tokens": 34,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "cs/cs-004-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "cs-004-greedy-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "Přelož do angličtiny: Žluťoučký kůň úpěl ďábelské ódy",
      "language": "cs",
      "description": "Přelož do angličtiny: Žluťoučký kůň úpěl ďábelské ódy",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2-xl",
      "file": "cs/cs-004-greedy-openai-community-gpt2-xl.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "cs-004-random-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "Přelož do angličtiny: Žluťoučký kůň úpěl ďábelské ódy",
      "language": "cs",
      "description": "Přelož do angličtiny: Žluťoučký kůň úpěl ďábelské ódy",
      "num_tokens": 100,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "cs/cs-004-random-CohereForAI-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "cs-004-random-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "Přelož do angličtiny: Žluťoučký kůň úpěl ďábelské ódy",
      "language": "cs",
      "description": "Přelož do angličtiny: Žluťoučký kůň úpěl ďábelské ódy",
      "num_tokens": 11,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "cs/cs-004-random-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "cs-004-random-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "Přelož do angličtiny: Žluťoučký kůň úpěl ďábelské ódy",
      "language": "cs",
      "description": "Přelož do angličtiny: Žluťoučký kůň úpěl ďábelské ódy",
      "num_tokens": 100,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "cs/cs-004-random-allenai-Olmo-3-7B-Think.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "cs-004-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Přelož do angličtiny: Žluťoučký kůň úpěl ďábelské ódy",
      "language": "cs",
      "description": "Přelož do angličtiny: Žluťoučký kůň úpěl ďábelské ódy",
      "num_tokens": 8,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "cs/cs-004-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "cs-004-random-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "Přelož do angličtiny: Žluťoučký kůň úpěl ďábelské ódy",
      "language": "cs",
      "description": "Přelož do angličtiny: Žluťoučký kůň úpěl ďábelské ódy",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2-xl",
      "file": "cs/cs-004-random-openai-community-gpt2-xl.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "cs-004-sampling-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "Přelož do angličtiny: Žluťoučký kůň úpěl ďábelské ódy",
      "language": "cs",
      "description": "Přelož do angličtiny: Žluťoučký kůň úpěl ďábelské ódy",
      "num_tokens": 41,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "cs/cs-004-sampling-CohereForAI-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "cs-004-sampling-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "Přelož do angličtiny: Žluťoučký kůň úpěl ďábelské ódy",
      "language": "cs",
      "description": "Přelož do angličtiny: Žluťoučký kůň úpěl ďábelské ódy",
      "num_tokens": 8,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "cs/cs-004-sampling-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "cs-004-sampling-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "Přelož do angličtiny: Žluťoučký kůň úpěl ďábelské ódy",
      "language": "cs",
      "description": "Přelož do angličtiny: Žluťoučký kůň úpěl ďábelské ódy",
      "num_tokens": 100,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "cs/cs-004-sampling-allenai-Olmo-3-7B-Think.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "cs-004-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Přelož do angličtiny: Žluťoučký kůň úpěl ďábelské ódy",
      "language": "cs",
      "description": "Přelož do angličtiny: Žluťoučký kůň úpěl ďábelské ódy",
      "num_tokens": 8,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "cs/cs-004-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "cs-004-sampling-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "Přelož do angličtiny: Žluťoučký kůň úpěl ďábelské ódy",
      "language": "cs",
      "description": "Přelož do angličtiny: Žluťoučký kůň úpěl ďábelské ódy",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2-xl",
      "file": "cs/cs-004-sampling-openai-community-gpt2-xl.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "cs-005-greedy-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "Kolik \"k\" je ve slově \"nejkulinkaťoulinkatější\"?",
      "language": "cs",
      "description": "Kolik \"k\" je ve slově \"nejkulinkaťoulinkatější\"?",
      "num_tokens": 81,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "cs/cs-005-greedy-CohereForAI-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "cs-005-greedy-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "Kolik \"k\" je ve slově \"nejkulinkaťoulinkatější\"?",
      "language": "cs",
      "description": "Kolik \"k\" je ve slově \"nejkulinkaťoulinkatější\"?",
      "num_tokens": 100,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "cs/cs-005-greedy-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "cs-005-greedy-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "Kolik \"k\" je ve slově \"nejkulinkaťoulinkatější\"?",
      "language": "cs",
      "description": "Kolik \"k\" je ve slově \"nejkulinkaťoulinkatější\"?",
      "num_tokens": 100,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "cs/cs-005-greedy-allenai-Olmo-3-7B-Think.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "cs-005-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Kolik \"k\" je ve slově \"nejkulinkaťoulinkatější\"?",
      "language": "cs",
      "description": "Kolik \"k\" je ve slově \"nejkulinkaťoulinkatější\"?",
      "num_tokens": 43,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "cs/cs-005-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "cs-005-greedy-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "Kolik \"k\" je ve slově \"nejkulinkaťoulinkatější\"?",
      "language": "cs",
      "description": "Kolik \"k\" je ve slově \"nejkulinkaťoulinkatější\"?",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2-xl",
      "file": "cs/cs-005-greedy-openai-community-gpt2-xl.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "cs-005-random-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "Kolik \"k\" je ve slově \"nejkulinkaťoulinkatější\"?",
      "language": "cs",
      "description": "Kolik \"k\" je ve slově \"nejkulinkaťoulinkatější\"?",
      "num_tokens": 22,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "cs/cs-005-random-CohereForAI-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "cs-005-random-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "Kolik \"k\" je ve slově \"nejkulinkaťoulinkatější\"?",
      "language": "cs",
      "description": "Kolik \"k\" je ve slově \"nejkulinkaťoulinkatější\"?",
      "num_tokens": 100,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "cs/cs-005-random-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "cs-005-random-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "Kolik \"k\" je ve slově \"nejkulinkaťoulinkatější\"?",
      "language": "cs",
      "description": "Kolik \"k\" je ve slově \"nejkulinkaťoulinkatější\"?",
      "num_tokens": 100,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "cs/cs-005-random-allenai-Olmo-3-7B-Think.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "cs-005-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Kolik \"k\" je ve slově \"nejkulinkaťoulinkatější\"?",
      "language": "cs",
      "description": "Kolik \"k\" je ve slově \"nejkulinkaťoulinkatější\"?",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "cs/cs-005-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "cs-005-random-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "Kolik \"k\" je ve slově \"nejkulinkaťoulinkatější\"?",
      "language": "cs",
      "description": "Kolik \"k\" je ve slově \"nejkulinkaťoulinkatější\"?",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2-xl",
      "file": "cs/cs-005-random-openai-community-gpt2-xl.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "cs-005-sampling-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "Kolik \"k\" je ve slově \"nejkulinkaťoulinkatější\"?",
      "language": "cs",
      "description": "Kolik \"k\" je ve slově \"nejkulinkaťoulinkatější\"?",
      "num_tokens": 49,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "cs/cs-005-sampling-CohereForAI-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "cs-005-sampling-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "Kolik \"k\" je ve slově \"nejkulinkaťoulinkatější\"?",
      "language": "cs",
      "description": "Kolik \"k\" je ve slově \"nejkulinkaťoulinkatější\"?",
      "num_tokens": 100,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "cs/cs-005-sampling-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "cs-005-sampling-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "Kolik \"k\" je ve slově \"nejkulinkaťoulinkatější\"?",
      "language": "cs",
      "description": "Kolik \"k\" je ve slově \"nejkulinkaťoulinkatější\"?",
      "num_tokens": 100,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "cs/cs-005-sampling-allenai-Olmo-3-7B-Think.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "cs-005-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Kolik \"k\" je ve slově \"nejkulinkaťoulinkatější\"?",
      "language": "cs",
      "description": "Kolik \"k\" je ve slově \"nejkulinkaťoulinkatější\"?",
      "num_tokens": 25,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "cs/cs-005-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "cs-005-sampling-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "Kolik \"k\" je ve slově \"nejkulinkaťoulinkatější\"?",
      "language": "cs",
      "description": "Kolik \"k\" je ve slově \"nejkulinkaťoulinkatější\"?",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2-xl",
      "file": "cs/cs-005-sampling-openai-community-gpt2-xl.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "en-001-greedy-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "Who are you?",
      "language": "en",
      "description": "Who are you?",
      "num_tokens": 100,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "en/en-001-greedy-CohereForAI-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "en-001-greedy-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "Who are you?",
      "language": "en",
      "description": "Who are you?",
      "num_tokens": 82,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "en/en-001-greedy-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "en-001-greedy-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "Who are you?",
      "language": "en",
      "description": "Who are you?",
      "num_tokens": 100,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "en/en-001-greedy-allenai-Olmo-3-7B-Think.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "en-001-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Who are you?",
      "language": "en",
      "description": "Who are you?",
      "num_tokens": 23,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "en/en-001-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "en-001-greedy-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "Who are you?",
      "language": "en",
      "description": "Who are you?",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2-xl",
      "file": "en/en-001-greedy-openai-community-gpt2-xl.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "en-001-random-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "Who are you?",
      "language": "en",
      "description": "Who are you?",
      "num_tokens": 100,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "en/en-001-random-CohereForAI-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "en-001-random-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "Who are you?",
      "language": "en",
      "description": "Who are you?",
      "num_tokens": 100,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "en/en-001-random-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "en-001-random-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "Who are you?",
      "language": "en",
      "description": "Who are you?",
      "num_tokens": 100,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "en/en-001-random-allenai-Olmo-3-7B-Think.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "en-001-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Who are you?",
      "language": "en",
      "description": "Who are you?",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "en/en-001-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "en-001-random-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "Who are you?",
      "language": "en",
      "description": "Who are you?",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2-xl",
      "file": "en/en-001-random-openai-community-gpt2-xl.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "en-001-sampling-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "Who are you?",
      "language": "en",
      "description": "Who are you?",
      "num_tokens": 98,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "en/en-001-sampling-CohereForAI-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "en-001-sampling-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "Who are you?",
      "language": "en",
      "description": "Who are you?",
      "num_tokens": 100,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "en/en-001-sampling-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "en-001-sampling-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "Who are you?",
      "language": "en",
      "description": "Who are you?",
      "num_tokens": 100,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "en/en-001-sampling-allenai-Olmo-3-7B-Think.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "en-001-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Who are you?",
      "language": "en",
      "description": "Who are you?",
      "num_tokens": 23,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "en/en-001-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "en-001-sampling-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "Who are you?",
      "language": "en",
      "description": "Who are you?",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2-xl",
      "file": "en/en-001-sampling-openai-community-gpt2-xl.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "en-002-greedy-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "Tell me a joke",
      "language": "en",
      "description": "Tell me a joke",
      "num_tokens": 50,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "en/en-002-greedy-CohereForAI-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "en-002-greedy-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "Tell me a joke",
      "language": "en",
      "description": "Tell me a joke",
      "num_tokens": 29,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "en/en-002-greedy-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "en-002-greedy-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "Tell me a joke",
      "language": "en",
      "description": "Tell me a joke",
      "num_tokens": 100,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "en/en-002-greedy-allenai-Olmo-3-7B-Think.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "en-002-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Tell me a joke",
      "language": "en",
      "description": "Tell me a joke",
      "num_tokens": 54,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "en/en-002-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "en-002-greedy-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "Tell me a joke",
      "language": "en",
      "description": "Tell me a joke",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2-xl",
      "file": "en/en-002-greedy-openai-community-gpt2-xl.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "en-002-random-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "Tell me a joke",
      "language": "en",
      "description": "Tell me a joke",
      "num_tokens": 90,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "en/en-002-random-CohereForAI-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "en-002-random-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "Tell me a joke",
      "language": "en",
      "description": "Tell me a joke",
      "num_tokens": 100,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "en/en-002-random-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "en-002-random-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "Tell me a joke",
      "language": "en",
      "description": "Tell me a joke",
      "num_tokens": 100,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "en/en-002-random-allenai-Olmo-3-7B-Think.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "en-002-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Tell me a joke",
      "language": "en",
      "description": "Tell me a joke",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "en/en-002-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "en-002-random-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "Tell me a joke",
      "language": "en",
      "description": "Tell me a joke",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2-xl",
      "file": "en/en-002-random-openai-community-gpt2-xl.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "en-002-sampling-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "Tell me a joke",
      "language": "en",
      "description": "Tell me a joke",
      "num_tokens": 48,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "en/en-002-sampling-CohereForAI-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "en-002-sampling-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "Tell me a joke",
      "language": "en",
      "description": "Tell me a joke",
      "num_tokens": 31,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "en/en-002-sampling-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "en-002-sampling-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "Tell me a joke",
      "language": "en",
      "description": "Tell me a joke",
      "num_tokens": 100,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "en/en-002-sampling-allenai-Olmo-3-7B-Think.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "en-002-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Tell me a joke",
      "language": "en",
      "description": "Tell me a joke",
      "num_tokens": 55,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "en/en-002-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "en-002-sampling-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "Tell me a joke",
      "language": "en",
      "description": "Tell me a joke",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2-xl",
      "file": "en/en-002-sampling-openai-community-gpt2-xl.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "en-003-greedy-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "The capital of the United Kingdom is",
      "language": "en",
      "description": "The capital of the United Kingdom is",
      "num_tokens": 69,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "en/en-003-greedy-CohereForAI-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "en-003-greedy-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "The capital of the United Kingdom is",
      "language": "en",
      "description": "The capital of the United Kingdom is",
      "num_tokens": 10,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "en/en-003-greedy-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "en-003-greedy-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "The capital of the United Kingdom is",
      "language": "en",
      "description": "The capital of the United Kingdom is",
      "num_tokens": 100,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "en/en-003-greedy-allenai-Olmo-3-7B-Think.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "en-003-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "The capital of the United Kingdom is",
      "language": "en",
      "description": "The capital of the United Kingdom is",
      "num_tokens": 10,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "en/en-003-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "en-003-greedy-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "The capital of the United Kingdom is",
      "language": "en",
      "description": "The capital of the United Kingdom is",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2-xl",
      "file": "en/en-003-greedy-openai-community-gpt2-xl.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "en-003-random-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "The capital of the United Kingdom is",
      "language": "en",
      "description": "The capital of the United Kingdom is",
      "num_tokens": 100,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "en/en-003-random-CohereForAI-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "en-003-random-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "The capital of the United Kingdom is",
      "language": "en",
      "description": "The capital of the United Kingdom is",
      "num_tokens": 2,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "en/en-003-random-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "en-003-random-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "The capital of the United Kingdom is",
      "language": "en",
      "description": "The capital of the United Kingdom is",
      "num_tokens": 100,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "en/en-003-random-allenai-Olmo-3-7B-Think.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "en-003-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "The capital of the United Kingdom is",
      "language": "en",
      "description": "The capital of the United Kingdom is",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "en/en-003-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "en-003-random-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "The capital of the United Kingdom is",
      "language": "en",
      "description": "The capital of the United Kingdom is",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2-xl",
      "file": "en/en-003-random-openai-community-gpt2-xl.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "en-003-sampling-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "The capital of the United Kingdom is",
      "language": "en",
      "description": "The capital of the United Kingdom is",
      "num_tokens": 76,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "en/en-003-sampling-CohereForAI-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "en-003-sampling-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "The capital of the United Kingdom is",
      "language": "en",
      "description": "The capital of the United Kingdom is",
      "num_tokens": 10,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "en/en-003-sampling-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "en-003-sampling-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "The capital of the United Kingdom is",
      "language": "en",
      "description": "The capital of the United Kingdom is",
      "num_tokens": 100,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "en/en-003-sampling-allenai-Olmo-3-7B-Think.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "en-003-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "The capital of the United Kingdom is",
      "language": "en",
      "description": "The capital of the United Kingdom is",
      "num_tokens": 10,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "en/en-003-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "en-003-sampling-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "The capital of the United Kingdom is",
      "language": "en",
      "description": "The capital of the United Kingdom is",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2-xl",
      "file": "en/en-003-sampling-openai-community-gpt2-xl.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "en-004-greedy-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "Translate to French: The quick brown fox jumps over the lazy dog",
      "language": "en",
      "description": "Translate to French: The quick brown fox jumps over the lazy dog",
      "num_tokens": 51,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "en/en-004-greedy-CohereForAI-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "en-004-greedy-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "Translate to French: The quick brown fox jumps over the lazy dog",
      "language": "en",
      "description": "Translate to French: The quick brown fox jumps over the lazy dog",
      "num_tokens": 18,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "en/en-004-greedy-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "en-004-greedy-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "Translate to French: The quick brown fox jumps over the lazy dog",
      "language": "en",
      "description": "Translate to French: The quick brown fox jumps over the lazy dog",
      "num_tokens": 100,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "en/en-004-greedy-allenai-Olmo-3-7B-Think.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "en-004-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Translate to French: The quick brown fox jumps over the lazy dog",
      "language": "en",
      "description": "Translate to French: The quick brown fox jumps over the lazy dog",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "en/en-004-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "en-004-greedy-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "Translate to French: The quick brown fox jumps over the lazy dog",
      "language": "en",
      "description": "Translate to French: The quick brown fox jumps over the lazy dog",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2-xl",
      "file": "en/en-004-greedy-openai-community-gpt2-xl.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "en-004-random-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "Translate to French: The quick brown fox jumps over the lazy dog",
      "language": "en",
      "description": "Translate to French: The quick brown fox jumps over the lazy dog",
      "num_tokens": 16,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "en/en-004-random-CohereForAI-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "en-004-random-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "Translate to French: The quick brown fox jumps over the lazy dog",
      "language": "en",
      "description": "Translate to French: The quick brown fox jumps over the lazy dog",
      "num_tokens": 90,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "en/en-004-random-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "en-004-random-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "Translate to French: The quick brown fox jumps over the lazy dog",
      "language": "en",
      "description": "Translate to French: The quick brown fox jumps over the lazy dog",
      "num_tokens": 100,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "en/en-004-random-allenai-Olmo-3-7B-Think.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "en-004-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Translate to French: The quick brown fox jumps over the lazy dog",
      "language": "en",
      "description": "Translate to French: The quick brown fox jumps over the lazy dog",
      "num_tokens": 97,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "en/en-004-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "en-004-random-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "Translate to French: The quick brown fox jumps over the lazy dog",
      "language": "en",
      "description": "Translate to French: The quick brown fox jumps over the lazy dog",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2-xl",
      "file": "en/en-004-random-openai-community-gpt2-xl.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "en-004-sampling-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "Translate to French: The quick brown fox jumps over the lazy dog",
      "language": "en",
      "description": "Translate to French: The quick brown fox jumps over the lazy dog",
      "num_tokens": 67,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "en/en-004-sampling-CohereForAI-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "en-004-sampling-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "Translate to French: The quick brown fox jumps over the lazy dog",
      "language": "en",
      "description": "Translate to French: The quick brown fox jumps over the lazy dog",
      "num_tokens": 18,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "en/en-004-sampling-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "en-004-sampling-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "Translate to French: The quick brown fox jumps over the lazy dog",
      "language": "en",
      "description": "Translate to French: The quick brown fox jumps over the lazy dog",
      "num_tokens": 100,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "en/en-004-sampling-allenai-Olmo-3-7B-Think.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "en-004-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Translate to French: The quick brown fox jumps over the lazy dog",
      "language": "en",
      "description": "Translate to French: The quick brown fox jumps over the lazy dog",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "en/en-004-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "en-004-sampling-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "Translate to French: The quick brown fox jumps over the lazy dog",
      "language": "en",
      "description": "Translate to French: The quick brown fox jumps over the lazy dog",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2-xl",
      "file": "en/en-004-sampling-openai-community-gpt2-xl.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "en-005-greedy-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "How many r's are in \"strawberry\"?",
      "language": "en",
      "description": "How many r's are in \"strawberry\"?",
      "num_tokens": 12,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "en/en-005-greedy-CohereForAI-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "en-005-greedy-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "How many r's are in \"strawberry\"?",
      "language": "en",
      "description": "How many r's are in \"strawberry\"?",
      "num_tokens": 100,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "en/en-005-greedy-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "en-005-greedy-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "How many r's are in \"strawberry\"?",
      "language": "en",
      "description": "How many r's are in \"strawberry\"?",
      "num_tokens": 100,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "en/en-005-greedy-allenai-Olmo-3-7B-Think.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "en-005-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "How many r's are in \"strawberry\"?",
      "language": "en",
      "description": "How many r's are in \"strawberry\"?",
      "num_tokens": 13,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "en/en-005-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "en-005-greedy-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "How many r's are in \"strawberry\"?",
      "language": "en",
      "description": "How many r's are in \"strawberry\"?",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2-xl",
      "file": "en/en-005-greedy-openai-community-gpt2-xl.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "en-005-random-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "How many r's are in \"strawberry\"?",
      "language": "en",
      "description": "How many r's are in \"strawberry\"?",
      "num_tokens": 16,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "en/en-005-random-CohereForAI-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "en-005-random-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "How many r's are in \"strawberry\"?",
      "language": "en",
      "description": "How many r's are in \"strawberry\"?",
      "num_tokens": 100,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "en/en-005-random-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "en-005-random-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "How many r's are in \"strawberry\"?",
      "language": "en",
      "description": "How many r's are in \"strawberry\"?",
      "num_tokens": 100,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "en/en-005-random-allenai-Olmo-3-7B-Think.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "en-005-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "How many r's are in \"strawberry\"?",
      "language": "en",
      "description": "How many r's are in \"strawberry\"?",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "en/en-005-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "en-005-random-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "How many r's are in \"strawberry\"?",
      "language": "en",
      "description": "How many r's are in \"strawberry\"?",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2-xl",
      "file": "en/en-005-random-openai-community-gpt2-xl.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "en-005-sampling-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "How many r's are in \"strawberry\"?",
      "language": "en",
      "description": "How many r's are in \"strawberry\"?",
      "num_tokens": 15,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "en/en-005-sampling-CohereForAI-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "en-005-sampling-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "How many r's are in \"strawberry\"?",
      "language": "en",
      "description": "How many r's are in \"strawberry\"?",
      "num_tokens": 100,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "en/en-005-sampling-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "en-005-sampling-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "How many r's are in \"strawberry\"?",
      "language": "en",
      "description": "How many r's are in \"strawberry\"?",
      "num_tokens": 100,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "en/en-005-sampling-allenai-Olmo-3-7B-Think.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "en-005-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "How many r's are in \"strawberry\"?",
      "language": "en",
      "description": "How many r's are in \"strawberry\"?",
      "num_tokens": 13,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "en/en-005-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "en-005-sampling-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "How many r's are in \"strawberry\"?",
      "language": "en",
      "description": "How many r's are in \"strawberry\"?",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2-xl",
      "file": "en/en-005-sampling-openai-community-gpt2-xl.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "fr-001-greedy-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "Qui es-tu?",
      "language": "fr",
      "description": "Qui es-tu?",
      "num_tokens": 95,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "fr/fr-001-greedy-CohereForAI-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "fr-001-greedy-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "Qui es-tu?",
      "language": "fr",
      "description": "Qui es-tu?",
      "num_tokens": 100,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "fr/fr-001-greedy-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "fr-001-greedy-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "Qui es-tu?",
      "language": "fr",
      "description": "Qui es-tu?",
      "num_tokens": 100,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "fr/fr-001-greedy-allenai-Olmo-3-7B-Think.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "fr-001-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Qui es-tu?",
      "language": "fr",
      "description": "Qui es-tu?",
      "num_tokens": 38,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "fr/fr-001-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "fr-001-greedy-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "Qui es-tu?",
      "language": "fr",
      "description": "Qui es-tu?",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2-xl",
      "file": "fr/fr-001-greedy-openai-community-gpt2-xl.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "fr-001-random-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "Qui es-tu?",
      "language": "fr",
      "description": "Qui es-tu?",
      "num_tokens": 100,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "fr/fr-001-random-CohereForAI-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "fr-001-random-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "Qui es-tu?",
      "language": "fr",
      "description": "Qui es-tu?",
      "num_tokens": 63,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "fr/fr-001-random-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "fr-001-random-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "Qui es-tu?",
      "language": "fr",
      "description": "Qui es-tu?",
      "num_tokens": 100,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "fr/fr-001-random-allenai-Olmo-3-7B-Think.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "fr-001-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Qui es-tu?",
      "language": "fr",
      "description": "Qui es-tu?",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "fr/fr-001-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "fr-001-random-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "Qui es-tu?",
      "language": "fr",
      "description": "Qui es-tu?",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2-xl",
      "file": "fr/fr-001-random-openai-community-gpt2-xl.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "fr-001-sampling-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "Qui es-tu?",
      "language": "fr",
      "description": "Qui es-tu?",
      "num_tokens": 100,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "fr/fr-001-sampling-CohereForAI-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "fr-001-sampling-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "Qui es-tu?",
      "language": "fr",
      "description": "Qui es-tu?",
      "num_tokens": 96,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "fr/fr-001-sampling-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "fr-001-sampling-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "Qui es-tu?",
      "language": "fr",
      "description": "Qui es-tu?",
      "num_tokens": 100,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "fr/fr-001-sampling-allenai-Olmo-3-7B-Think.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "fr-001-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Qui es-tu?",
      "language": "fr",
      "description": "Qui es-tu?",
      "num_tokens": 38,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "fr/fr-001-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "fr-001-sampling-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "Qui es-tu?",
      "language": "fr",
      "description": "Qui es-tu?",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2-xl",
      "file": "fr/fr-001-sampling-openai-community-gpt2-xl.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "fr-002-greedy-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "Raconte-moi une blague",
      "language": "fr",
      "description": "Raconte-moi une blague",
      "num_tokens": 61,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "fr/fr-002-greedy-CohereForAI-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "fr-002-greedy-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "Raconte-moi une blague",
      "language": "fr",
      "description": "Raconte-moi une blague",
      "num_tokens": 70,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "fr/fr-002-greedy-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "fr-002-greedy-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "Raconte-moi une blague",
      "language": "fr",
      "description": "Raconte-moi une blague",
      "num_tokens": 100,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "fr/fr-002-greedy-allenai-Olmo-3-7B-Think.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "fr-002-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Raconte-moi une blague",
      "language": "fr",
      "description": "Raconte-moi une blague",
      "num_tokens": 48,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "fr/fr-002-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "fr-002-greedy-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "Raconte-moi une blague",
      "language": "fr",
      "description": "Raconte-moi une blague",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2-xl",
      "file": "fr/fr-002-greedy-openai-community-gpt2-xl.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "fr-002-random-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "Raconte-moi une blague",
      "language": "fr",
      "description": "Raconte-moi une blague",
      "num_tokens": 28,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "fr/fr-002-random-CohereForAI-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "fr-002-random-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "Raconte-moi une blague",
      "language": "fr",
      "description": "Raconte-moi une blague",
      "num_tokens": 100,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "fr/fr-002-random-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "fr-002-random-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "Raconte-moi une blague",
      "language": "fr",
      "description": "Raconte-moi une blague",
      "num_tokens": 100,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "fr/fr-002-random-allenai-Olmo-3-7B-Think.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "fr-002-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Raconte-moi une blague",
      "language": "fr",
      "description": "Raconte-moi une blague",
      "num_tokens": 76,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "fr/fr-002-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "fr-002-random-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "Raconte-moi une blague",
      "language": "fr",
      "description": "Raconte-moi une blague",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2-xl",
      "file": "fr/fr-002-random-openai-community-gpt2-xl.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "fr-002-sampling-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "Raconte-moi une blague",
      "language": "fr",
      "description": "Raconte-moi une blague",
      "num_tokens": 60,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "fr/fr-002-sampling-CohereForAI-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "fr-002-sampling-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "Raconte-moi une blague",
      "language": "fr",
      "description": "Raconte-moi une blague",
      "num_tokens": 71,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "fr/fr-002-sampling-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "fr-002-sampling-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "Raconte-moi une blague",
      "language": "fr",
      "description": "Raconte-moi une blague",
      "num_tokens": 100,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "fr/fr-002-sampling-allenai-Olmo-3-7B-Think.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "fr-002-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Raconte-moi une blague",
      "language": "fr",
      "description": "Raconte-moi une blague",
      "num_tokens": 81,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "fr/fr-002-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "fr-002-sampling-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "Raconte-moi une blague",
      "language": "fr",
      "description": "Raconte-moi une blague",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2-xl",
      "file": "fr/fr-002-sampling-openai-community-gpt2-xl.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "fr-003-greedy-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "La capitale de la France est",
      "language": "fr",
      "description": "La capitale de la France est",
      "num_tokens": 92,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "fr/fr-003-greedy-CohereForAI-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "fr-003-greedy-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "La capitale de la France est",
      "language": "fr",
      "description": "La capitale de la France est",
      "num_tokens": 12,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "fr/fr-003-greedy-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "fr-003-greedy-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "La capitale de la France est",
      "language": "fr",
      "description": "La capitale de la France est",
      "num_tokens": 100,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "fr/fr-003-greedy-allenai-Olmo-3-7B-Think.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "fr-003-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "La capitale de la France est",
      "language": "fr",
      "description": "La capitale de la France est",
      "num_tokens": 10,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "fr/fr-003-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "fr-003-greedy-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "La capitale de la France est",
      "language": "fr",
      "description": "La capitale de la France est",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2-xl",
      "file": "fr/fr-003-greedy-openai-community-gpt2-xl.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "fr-003-random-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "La capitale de la France est",
      "language": "fr",
      "description": "La capitale de la France est",
      "num_tokens": 81,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "fr/fr-003-random-CohereForAI-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "fr-003-random-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "La capitale de la France est",
      "language": "fr",
      "description": "La capitale de la France est",
      "num_tokens": 100,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "fr/fr-003-random-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "fr-003-random-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "La capitale de la France est",
      "language": "fr",
      "description": "La capitale de la France est",
      "num_tokens": 100,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "fr/fr-003-random-allenai-Olmo-3-7B-Think.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "fr-003-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "La capitale de la France est",
      "language": "fr",
      "description": "La capitale de la France est",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "fr/fr-003-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "fr-003-random-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "La capitale de la France est",
      "language": "fr",
      "description": "La capitale de la France est",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2-xl",
      "file": "fr/fr-003-random-openai-community-gpt2-xl.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "fr-003-sampling-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "La capitale de la France est",
      "language": "fr",
      "description": "La capitale de la France est",
      "num_tokens": 92,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "fr/fr-003-sampling-CohereForAI-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "fr-003-sampling-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "La capitale de la France est",
      "language": "fr",
      "description": "La capitale de la France est",
      "num_tokens": 12,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "fr/fr-003-sampling-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "fr-003-sampling-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "La capitale de la France est",
      "language": "fr",
      "description": "La capitale de la France est",
      "num_tokens": 100,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "fr/fr-003-sampling-allenai-Olmo-3-7B-Think.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "fr-003-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "La capitale de la France est",
      "language": "fr",
      "description": "La capitale de la France est",
      "num_tokens": 10,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "fr/fr-003-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "fr-003-sampling-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "La capitale de la France est",
      "language": "fr",
      "description": "La capitale de la France est",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2-xl",
      "file": "fr/fr-003-sampling-openai-community-gpt2-xl.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "fr-004-greedy-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "Traduis en anglais: Portez ce vieux whisky au juge blond qui fume.",
      "language": "fr",
      "description": "Traduis en anglais: Portez ce vieux whisky au juge blond qui fume.",
      "num_tokens": 34,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "fr/fr-004-greedy-CohereForAI-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "fr-004-greedy-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "Traduis en anglais: Portez ce vieux whisky au juge blond qui fume.",
      "language": "fr",
      "description": "Traduis en anglais: Portez ce vieux whisky au juge blond qui fume.",
      "num_tokens": 13,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "fr/fr-004-greedy-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "fr-004-greedy-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "Traduis en anglais: Portez ce vieux whisky au juge blond qui fume.",
      "language": "fr",
      "description": "Traduis en anglais: Portez ce vieux whisky au juge blond qui fume.",
      "num_tokens": 100,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "fr/fr-004-greedy-allenai-Olmo-3-7B-Think.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "fr-004-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Traduis en anglais: Portez ce vieux whisky au juge blond qui fume.",
      "language": "fr",
      "description": "Traduis en anglais: Portez ce vieux whisky au juge blond qui fume.",
      "num_tokens": 12,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "fr/fr-004-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "fr-004-greedy-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "Traduis en anglais: Portez ce vieux whisky au juge blond qui fume.",
      "language": "fr",
      "description": "Traduis en anglais: Portez ce vieux whisky au juge blond qui fume.",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2-xl",
      "file": "fr/fr-004-greedy-openai-community-gpt2-xl.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "fr-004-random-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "Traduis en anglais: Portez ce vieux whisky au juge blond qui fume.",
      "language": "fr",
      "description": "Traduis en anglais: Portez ce vieux whisky au juge blond qui fume.",
      "num_tokens": 100,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "fr/fr-004-random-CohereForAI-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "fr-004-random-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "Traduis en anglais: Portez ce vieux whisky au juge blond qui fume.",
      "language": "fr",
      "description": "Traduis en anglais: Portez ce vieux whisky au juge blond qui fume.",
      "num_tokens": 100,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "fr/fr-004-random-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "fr-004-random-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "Traduis en anglais: Portez ce vieux whisky au juge blond qui fume.",
      "language": "fr",
      "description": "Traduis en anglais: Portez ce vieux whisky au juge blond qui fume.",
      "num_tokens": 100,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "fr/fr-004-random-allenai-Olmo-3-7B-Think.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "fr-004-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Traduis en anglais: Portez ce vieux whisky au juge blond qui fume.",
      "language": "fr",
      "description": "Traduis en anglais: Portez ce vieux whisky au juge blond qui fume.",
      "num_tokens": 51,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "fr/fr-004-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "fr-004-random-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "Traduis en anglais: Portez ce vieux whisky au juge blond qui fume.",
      "language": "fr",
      "description": "Traduis en anglais: Portez ce vieux whisky au juge blond qui fume.",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2-xl",
      "file": "fr/fr-004-random-openai-community-gpt2-xl.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "fr-004-sampling-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "Traduis en anglais: Portez ce vieux whisky au juge blond qui fume.",
      "language": "fr",
      "description": "Traduis en anglais: Portez ce vieux whisky au juge blond qui fume.",
      "num_tokens": 34,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "fr/fr-004-sampling-CohereForAI-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "fr-004-sampling-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "Traduis en anglais: Portez ce vieux whisky au juge blond qui fume.",
      "language": "fr",
      "description": "Traduis en anglais: Portez ce vieux whisky au juge blond qui fume.",
      "num_tokens": 13,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "fr/fr-004-sampling-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "fr-004-sampling-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "Traduis en anglais: Portez ce vieux whisky au juge blond qui fume.",
      "language": "fr",
      "description": "Traduis en anglais: Portez ce vieux whisky au juge blond qui fume.",
      "num_tokens": 100,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "fr/fr-004-sampling-allenai-Olmo-3-7B-Think.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "fr-004-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Traduis en anglais: Portez ce vieux whisky au juge blond qui fume.",
      "language": "fr",
      "description": "Traduis en anglais: Portez ce vieux whisky au juge blond qui fume.",
      "num_tokens": 21,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "fr/fr-004-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "fr-004-sampling-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "Traduis en anglais: Portez ce vieux whisky au juge blond qui fume.",
      "language": "fr",
      "description": "Traduis en anglais: Portez ce vieux whisky au juge blond qui fume.",
      "num_tokens": 41,
      "model_id": "openai-community/gpt2-xl",
      "file": "fr/fr-004-sampling-openai-community-gpt2-xl.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "fr-005-greedy-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "Combien de \"e\" y a-t-il dans le mot \"merveilleusement\"?",
      "language": "fr",
      "description": "Combien de \"e\" y a-t-il dans le mot \"merveilleusement\"?",
      "num_tokens": 19,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "fr/fr-005-greedy-CohereForAI-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "fr-005-greedy-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "Combien de \"e\" y a-t-il dans le mot \"merveilleusement\"?",
      "language": "fr",
      "description": "Combien de \"e\" y a-t-il dans le mot \"merveilleusement\"?",
      "num_tokens": 100,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "fr/fr-005-greedy-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "fr-005-greedy-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "Combien de \"e\" y a-t-il dans le mot \"merveilleusement\"?",
      "language": "fr",
      "description": "Combien de \"e\" y a-t-il dans le mot \"merveilleusement\"?",
      "num_tokens": 100,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "fr/fr-005-greedy-allenai-Olmo-3-7B-Think.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "fr-005-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Combien de \"e\" y a-t-il dans le mot \"merveilleusement\"?",
      "language": "fr",
      "description": "Combien de \"e\" y a-t-il dans le mot \"merveilleusement\"?",
      "num_tokens": 17,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "fr/fr-005-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "fr-005-greedy-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "Combien de \"e\" y a-t-il dans le mot \"merveilleusement\"?",
      "language": "fr",
      "description": "Combien de \"e\" y a-t-il dans le mot \"merveilleusement\"?",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2-xl",
      "file": "fr/fr-005-greedy-openai-community-gpt2-xl.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "fr-005-random-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "Combien de \"e\" y a-t-il dans le mot \"merveilleusement\"?",
      "language": "fr",
      "description": "Combien de \"e\" y a-t-il dans le mot \"merveilleusement\"?",
      "num_tokens": 16,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "fr/fr-005-random-CohereForAI-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "fr-005-random-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "Combien de \"e\" y a-t-il dans le mot \"merveilleusement\"?",
      "language": "fr",
      "description": "Combien de \"e\" y a-t-il dans le mot \"merveilleusement\"?",
      "num_tokens": 100,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "fr/fr-005-random-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "fr-005-random-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "Combien de \"e\" y a-t-il dans le mot \"merveilleusement\"?",
      "language": "fr",
      "description": "Combien de \"e\" y a-t-il dans le mot \"merveilleusement\"?",
      "num_tokens": 100,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "fr/fr-005-random-allenai-Olmo-3-7B-Think.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "fr-005-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Combien de \"e\" y a-t-il dans le mot \"merveilleusement\"?",
      "language": "fr",
      "description": "Combien de \"e\" y a-t-il dans le mot \"merveilleusement\"?",
      "num_tokens": 68,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "fr/fr-005-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "fr-005-random-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "Combien de \"e\" y a-t-il dans le mot \"merveilleusement\"?",
      "language": "fr",
      "description": "Combien de \"e\" y a-t-il dans le mot \"merveilleusement\"?",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2-xl",
      "file": "fr/fr-005-random-openai-community-gpt2-xl.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "fr-005-sampling-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "Combien de \"e\" y a-t-il dans le mot \"merveilleusement\"?",
      "language": "fr",
      "description": "Combien de \"e\" y a-t-il dans le mot \"merveilleusement\"?",
      "num_tokens": 19,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "fr/fr-005-sampling-CohereForAI-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "fr-005-sampling-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "Combien de \"e\" y a-t-il dans le mot \"merveilleusement\"?",
      "language": "fr",
      "description": "Combien de \"e\" y a-t-il dans le mot \"merveilleusement\"?",
      "num_tokens": 100,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "fr/fr-005-sampling-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "fr-005-sampling-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "Combien de \"e\" y a-t-il dans le mot \"merveilleusement\"?",
      "language": "fr",
      "description": "Combien de \"e\" y a-t-il dans le mot \"merveilleusement\"?",
      "num_tokens": 100,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "fr/fr-005-sampling-allenai-Olmo-3-7B-Think.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "fr-005-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Combien de \"e\" y a-t-il dans le mot \"merveilleusement\"?",
      "language": "fr",
      "description": "Combien de \"e\" y a-t-il dans le mot \"merveilleusement\"?",
      "num_tokens": 15,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "fr/fr-005-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "fr-005-sampling-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "Combien de \"e\" y a-t-il dans le mot \"merveilleusement\"?",
      "language": "fr",
      "description": "Combien de \"e\" y a-t-il dans le mot \"merveilleusement\"?",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2-xl",
      "file": "fr/fr-005-sampling-openai-community-gpt2-xl.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "zh-001-greedy-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "你是谁？",
      "language": "zh",
      "description": "你是谁？",
      "num_tokens": 100,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "zh/zh-001-greedy-CohereForAI-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "zh-001-greedy-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "你是谁？",
      "language": "zh",
      "description": "你是谁？",
      "num_tokens": 73,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "zh/zh-001-greedy-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "zh-001-greedy-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "你是谁？",
      "language": "zh",
      "description": "你是谁？",
      "num_tokens": 100,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "zh/zh-001-greedy-allenai-Olmo-3-7B-Think.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "zh-001-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "你是谁？",
      "language": "zh",
      "description": "你是谁？",
      "num_tokens": 36,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "zh/zh-001-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "zh-001-greedy-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "你是谁？",
      "language": "zh",
      "description": "你是谁？",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2-xl",
      "file": "zh/zh-001-greedy-openai-community-gpt2-xl.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "zh-001-random-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "你是谁？",
      "language": "zh",
      "description": "你是谁？",
      "num_tokens": 100,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "zh/zh-001-random-CohereForAI-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "zh-001-random-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "你是谁？",
      "language": "zh",
      "description": "你是谁？",
      "num_tokens": 100,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "zh/zh-001-random-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "zh-001-random-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "你是谁？",
      "language": "zh",
      "description": "你是谁？",
      "num_tokens": 100,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "zh/zh-001-random-allenai-Olmo-3-7B-Think.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "zh-001-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "你是谁？",
      "language": "zh",
      "description": "你是谁？",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "zh/zh-001-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "zh-001-random-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "你是谁？",
      "language": "zh",
      "description": "你是谁？",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2-xl",
      "file": "zh/zh-001-random-openai-community-gpt2-xl.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "zh-001-sampling-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "你是谁？",
      "language": "zh",
      "description": "你是谁？",
      "num_tokens": 77,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "zh/zh-001-sampling-CohereForAI-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "zh-001-sampling-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "你是谁？",
      "language": "zh",
      "description": "你是谁？",
      "num_tokens": 100,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "zh/zh-001-sampling-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "zh-001-sampling-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "你是谁？",
      "language": "zh",
      "description": "你是谁？",
      "num_tokens": 100,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "zh/zh-001-sampling-allenai-Olmo-3-7B-Think.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "zh-001-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "你是谁？",
      "language": "zh",
      "description": "你是谁？",
      "num_tokens": 27,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "zh/zh-001-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "zh-001-sampling-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "你是谁？",
      "language": "zh",
      "description": "你是谁？",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2-xl",
      "file": "zh/zh-001-sampling-openai-community-gpt2-xl.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "zh-002-greedy-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "给我讲个笑话",
      "language": "zh",
      "description": "给我讲个笑话",
      "num_tokens": 59,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "zh/zh-002-greedy-CohereForAI-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "zh-002-greedy-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "给我讲个笑话",
      "language": "zh",
      "description": "给我讲个笑话",
      "num_tokens": 100,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "zh/zh-002-greedy-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "zh-002-greedy-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "给我讲个笑话",
      "language": "zh",
      "description": "给我讲个笑话",
      "num_tokens": 100,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "zh/zh-002-greedy-allenai-Olmo-3-7B-Think.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "zh-002-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "给我讲个笑话",
      "language": "zh",
      "description": "给我讲个笑话",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "zh/zh-002-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "zh-002-greedy-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "给我讲个笑话",
      "language": "zh",
      "description": "给我讲个笑话",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2-xl",
      "file": "zh/zh-002-greedy-openai-community-gpt2-xl.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "zh-002-random-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "给我讲个笑话",
      "language": "zh",
      "description": "给我讲个笑话",
      "num_tokens": 100,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "zh/zh-002-random-CohereForAI-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "zh-002-random-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "给我讲个笑话",
      "language": "zh",
      "description": "给我讲个笑话",
      "num_tokens": 100,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "zh/zh-002-random-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "zh-002-random-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "给我讲个笑话",
      "language": "zh",
      "description": "给我讲个笑话",
      "num_tokens": 100,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "zh/zh-002-random-allenai-Olmo-3-7B-Think.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "zh-002-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "给我讲个笑话",
      "language": "zh",
      "description": "给我讲个笑话",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "zh/zh-002-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "zh-002-random-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "给我讲个笑话",
      "language": "zh",
      "description": "给我讲个笑话",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2-xl",
      "file": "zh/zh-002-random-openai-community-gpt2-xl.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "zh-002-sampling-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "给我讲个笑话",
      "language": "zh",
      "description": "给我讲个笑话",
      "num_tokens": 100,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "zh/zh-002-sampling-CohereForAI-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "zh-002-sampling-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "给我讲个笑话",
      "language": "zh",
      "description": "给我讲个笑话",
      "num_tokens": 100,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "zh/zh-002-sampling-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "zh-002-sampling-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "给我讲个笑话",
      "language": "zh",
      "description": "给我讲个笑话",
      "num_tokens": 100,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "zh/zh-002-sampling-allenai-Olmo-3-7B-Think.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "zh-002-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "给我讲个笑话",
      "language": "zh",
      "description": "给我讲个笑话",
      "num_tokens": 93,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "zh/zh-002-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "zh-002-sampling-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "给我讲个笑话",
      "language": "zh",
      "description": "给我讲个笑话",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2-xl",
      "file": "zh/zh-002-sampling-openai-community-gpt2-xl.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "zh-003-greedy-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "中国的首都是",
      "language": "zh",
      "description": "中国的首都是",
      "num_tokens": 53,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "zh/zh-003-greedy-CohereForAI-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "zh-003-greedy-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "中国的首都是",
      "language": "zh",
      "description": "中国的首都是",
      "num_tokens": 6,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "zh/zh-003-greedy-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "zh-003-greedy-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "中国的首都是",
      "language": "zh",
      "description": "中国的首都是",
      "num_tokens": 100,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "zh/zh-003-greedy-allenai-Olmo-3-7B-Think.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "zh-003-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "中国的首都是",
      "language": "zh",
      "description": "中国的首都是",
      "num_tokens": 7,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "zh/zh-003-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "zh-003-greedy-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "中国的首都是",
      "language": "zh",
      "description": "中国的首都是",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2-xl",
      "file": "zh/zh-003-greedy-openai-community-gpt2-xl.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "zh-003-random-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "中国的首都是",
      "language": "zh",
      "description": "中国的首都是",
      "num_tokens": 100,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "zh/zh-003-random-CohereForAI-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "zh-003-random-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "中国的首都是",
      "language": "zh",
      "description": "中国的首都是",
      "num_tokens": 100,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "zh/zh-003-random-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "zh-003-random-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "中国的首都是",
      "language": "zh",
      "description": "中国的首都是",
      "num_tokens": 100,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "zh/zh-003-random-allenai-Olmo-3-7B-Think.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "zh-003-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "中国的首都是",
      "language": "zh",
      "description": "中国的首都是",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "zh/zh-003-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "zh-003-random-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "中国的首都是",
      "language": "zh",
      "description": "中国的首都是",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2-xl",
      "file": "zh/zh-003-random-openai-community-gpt2-xl.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "zh-003-sampling-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "中国的首都是",
      "language": "zh",
      "description": "中国的首都是",
      "num_tokens": 35,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "zh/zh-003-sampling-CohereForAI-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "zh-003-sampling-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "中国的首都是",
      "language": "zh",
      "description": "中国的首都是",
      "num_tokens": 6,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "zh/zh-003-sampling-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "zh-003-sampling-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "中国的首都是",
      "language": "zh",
      "description": "中国的首都是",
      "num_tokens": 100,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "zh/zh-003-sampling-allenai-Olmo-3-7B-Think.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "zh-003-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "中国的首都是",
      "language": "zh",
      "description": "中国的首都是",
      "num_tokens": 10,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "zh/zh-003-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "zh-003-sampling-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "中国的首都是",
      "language": "zh",
      "description": "中国的首都是",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2-xl",
      "file": "zh/zh-003-sampling-openai-community-gpt2-xl.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "zh-004-greedy-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "翻译成英语：永和九年，岁在癸丑",
      "language": "zh",
      "description": "翻译成英语：永和九年，岁在癸丑",
      "num_tokens": 100,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "zh/zh-004-greedy-CohereForAI-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "zh-004-greedy-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "翻译成英语：永和九年，岁在癸丑",
      "language": "zh",
      "description": "翻译成英语：永和九年，岁在癸丑",
      "num_tokens": 16,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "zh/zh-004-greedy-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "zh-004-greedy-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "翻译成英语：永和九年，岁在癸丑",
      "language": "zh",
      "description": "翻译成英语：永和九年，岁在癸丑",
      "num_tokens": 100,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "zh/zh-004-greedy-allenai-Olmo-3-7B-Think.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "zh-004-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "翻译成英语：永和九年，岁在癸丑",
      "language": "zh",
      "description": "翻译成英语：永和九年，岁在癸丑",
      "num_tokens": 23,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "zh/zh-004-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "zh-004-greedy-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "翻译成英语：永和九年，岁在癸丑",
      "language": "zh",
      "description": "翻译成英语：永和九年，岁在癸丑",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2-xl",
      "file": "zh/zh-004-greedy-openai-community-gpt2-xl.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "zh-004-random-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "翻译成英语：永和九年，岁在癸丑",
      "language": "zh",
      "description": "翻译成英语：永和九年，岁在癸丑",
      "num_tokens": 25,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "zh/zh-004-random-CohereForAI-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "zh-004-random-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "翻译成英语：永和九年，岁在癸丑",
      "language": "zh",
      "description": "翻译成英语：永和九年，岁在癸丑",
      "num_tokens": 16,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "zh/zh-004-random-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "zh-004-random-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "翻译成英语：永和九年，岁在癸丑",
      "language": "zh",
      "description": "翻译成英语：永和九年，岁在癸丑",
      "num_tokens": 100,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "zh/zh-004-random-allenai-Olmo-3-7B-Think.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "zh-004-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "翻译成英语：永和九年，岁在癸丑",
      "language": "zh",
      "description": "翻译成英语：永和九年，岁在癸丑",
      "num_tokens": 9,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "zh/zh-004-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "zh-004-random-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "翻译成英语：永和九年，岁在癸丑",
      "language": "zh",
      "description": "翻译成英语：永和九年，岁在癸丑",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2-xl",
      "file": "zh/zh-004-random-openai-community-gpt2-xl.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "zh-004-sampling-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "翻译成英语：永和九年，岁在癸丑",
      "language": "zh",
      "description": "翻译成英语：永和九年，岁在癸丑",
      "num_tokens": 100,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "zh/zh-004-sampling-CohereForAI-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "zh-004-sampling-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "翻译成英语：永和九年，岁在癸丑",
      "language": "zh",
      "description": "翻译成英语：永和九年，岁在癸丑",
      "num_tokens": 16,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "zh/zh-004-sampling-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "zh-004-sampling-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "翻译成英语：永和九年，岁在癸丑",
      "language": "zh",
      "description": "翻译成英语：永和九年，岁在癸丑",
      "num_tokens": 100,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "zh/zh-004-sampling-allenai-Olmo-3-7B-Think.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "zh-004-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "翻译成英语：永和九年，岁在癸丑",
      "language": "zh",
      "description": "翻译成英语：永和九年，岁在癸丑",
      "num_tokens": 24,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "zh/zh-004-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "zh-004-sampling-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "翻译成英语：永和九年，岁在癸丑",
      "language": "zh",
      "description": "翻译成英语：永和九年，岁在癸丑",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2-xl",
      "file": "zh/zh-004-sampling-openai-community-gpt2-xl.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "zh-005-greedy-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "“谢谢”里有多少个“谢”字？",
      "language": "zh",
      "description": "“谢谢”里有多少个“谢”字？",
      "num_tokens": 44,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "zh/zh-005-greedy-CohereForAI-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "zh-005-greedy-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "“谢谢”里有多少个“谢”字？",
      "language": "zh",
      "description": "“谢谢”里有多少个“谢”字？",
      "num_tokens": 59,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "zh/zh-005-greedy-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "zh-005-greedy-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "“谢谢”里有多少个“谢”字？",
      "language": "zh",
      "description": "“谢谢”里有多少个“谢”字？",
      "num_tokens": 100,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "zh/zh-005-greedy-allenai-Olmo-3-7B-Think.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "zh-005-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "“谢谢”里有多少个“谢”字？",
      "language": "zh",
      "description": "“谢谢”里有多少个“谢”字？",
      "num_tokens": 10,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "zh/zh-005-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "zh-005-greedy-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "“谢谢”里有多少个“谢”字？",
      "language": "zh",
      "description": "“谢谢”里有多少个“谢”字？",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2-xl",
      "file": "zh/zh-005-greedy-openai-community-gpt2-xl.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "zh-005-random-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "“谢谢”里有多少个“谢”字？",
      "language": "zh",
      "description": "“谢谢”里有多少个“谢”字？",
      "num_tokens": 19,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "zh/zh-005-random-CohereForAI-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "zh-005-random-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "“谢谢”里有多少个“谢”字？",
      "language": "zh",
      "description": "“谢谢”里有多少个“谢”字？",
      "num_tokens": 100,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "zh/zh-005-random-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "zh-005-random-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "“谢谢”里有多少个“谢”字？",
      "language": "zh",
      "description": "“谢谢”里有多少个“谢”字？",
      "num_tokens": 100,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "zh/zh-005-random-allenai-Olmo-3-7B-Think.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "zh-005-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "“谢谢”里有多少个“谢”字？",
      "language": "zh",
      "description": "“谢谢”里有多少个“谢”字？",
      "num_tokens": 77,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "zh/zh-005-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "zh-005-random-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "“谢谢”里有多少个“谢”字？",
      "language": "zh",
      "description": "“谢谢”里有多少个“谢”字？",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2-xl",
      "file": "zh/zh-005-random-openai-community-gpt2-xl.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "zh-005-sampling-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "“谢谢”里有多少个“谢”字？",
      "language": "zh",
      "description": "“谢谢”里有多少个“谢”字？",
      "num_tokens": 61,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "zh/zh-005-sampling-CohereForAI-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "zh-005-sampling-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "“谢谢”里有多少个“谢”字？",
      "language": "zh",
      "description": "“谢谢”里有多少个“谢”字？",
      "num_tokens": 100,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "zh/zh-005-sampling-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "zh-005-sampling-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "“谢谢”里有多少个“谢”字？",
      "language": "zh",
      "description": "“谢谢”里有多少个“谢”字？",
      "num_tokens": 100,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "zh/zh-005-sampling-allenai-Olmo-3-7B-Think.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "zh-005-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "“谢谢”里有多少个“谢”字？",
      "language": "zh",
      "description": "“谢谢”里有多少个“谢”字？",
      "num_tokens": 13,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "zh/zh-005-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "zh-005-sampling-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "“谢谢”里有多少个“谢”字？",
      "language": "zh",
      "description": "“谢谢”里有多少个“谢”字？",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2-xl",
      "file": "zh/zh-005-sampling-openai-community-gpt2-xl.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "uk-001-greedy-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "Хто ти?",
      "language": "uk",
      "description": "Хто ти?",
      "num_tokens": 98,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "uk/uk-001-greedy-CohereForAI-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "uk-001-greedy-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "Хто ти?",
      "language": "uk",
      "description": "Хто ти?",
      "num_tokens": 100,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "uk/uk-001-greedy-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "uk-001-greedy-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "Хто ти?",
      "language": "uk",
      "description": "Хто ти?",
      "num_tokens": 100,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "uk/uk-001-greedy-allenai-Olmo-3-7B-Think.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "uk-001-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Хто ти?",
      "language": "uk",
      "description": "Хто ти?",
      "num_tokens": 69,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "uk/uk-001-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "uk-001-greedy-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "Хто ти?",
      "language": "uk",
      "description": "Хто ти?",
      "num_tokens": 98,
      "model_id": "openai-community/gpt2-xl",
      "file": "uk/uk-001-greedy-openai-community-gpt2-xl.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "uk-001-random-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "Хто ти?",
      "language": "uk",
      "description": "Хто ти?",
      "num_tokens": 96,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "uk/uk-001-random-CohereForAI-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "uk-001-random-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "Хто ти?",
      "language": "uk",
      "description": "Хто ти?",
      "num_tokens": 100,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "uk/uk-001-random-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "uk-001-random-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "Хто ти?",
      "language": "uk",
      "description": "Хто ти?",
      "num_tokens": 100,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "uk/uk-001-random-allenai-Olmo-3-7B-Think.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "uk-001-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Хто ти?",
      "language": "uk",
      "description": "Хто ти?",
      "num_tokens": 43,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "uk/uk-001-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "uk-001-random-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "Хто ти?",
      "language": "uk",
      "description": "Хто ти?",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2-xl",
      "file": "uk/uk-001-random-openai-community-gpt2-xl.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "uk-001-sampling-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "Хто ти?",
      "language": "uk",
      "description": "Хто ти?",
      "num_tokens": 98,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "uk/uk-001-sampling-CohereForAI-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "uk-001-sampling-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "Хто ти?",
      "language": "uk",
      "description": "Хто ти?",
      "num_tokens": 100,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "uk/uk-001-sampling-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "uk-001-sampling-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "Хто ти?",
      "language": "uk",
      "description": "Хто ти?",
      "num_tokens": 100,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "uk/uk-001-sampling-allenai-Olmo-3-7B-Think.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "uk-001-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Хто ти?",
      "language": "uk",
      "description": "Хто ти?",
      "num_tokens": 42,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "uk/uk-001-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "uk-001-sampling-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "Хто ти?",
      "language": "uk",
      "description": "Хто ти?",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2-xl",
      "file": "uk/uk-001-sampling-openai-community-gpt2-xl.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "uk-002-greedy-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "Розкажи жарт",
      "language": "uk",
      "description": "Розкажи жарт",
      "num_tokens": 68,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "uk/uk-002-greedy-CohereForAI-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "uk-002-greedy-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "Розкажи жарт",
      "language": "uk",
      "description": "Розкажи жарт",
      "num_tokens": 90,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "uk/uk-002-greedy-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "uk-002-greedy-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "Розкажи жарт",
      "language": "uk",
      "description": "Розкажи жарт",
      "num_tokens": 99,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "uk/uk-002-greedy-allenai-Olmo-3-7B-Think.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "uk-002-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Розкажи жарт",
      "language": "uk",
      "description": "Розкажи жарт",
      "num_tokens": 100,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "uk/uk-002-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "uk-002-greedy-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "Розкажи жарт",
      "language": "uk",
      "description": "Розкажи жарт",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2-xl",
      "file": "uk/uk-002-greedy-openai-community-gpt2-xl.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "uk-002-random-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "Розкажи жарт",
      "language": "uk",
      "description": "Розкажи жарт",
      "num_tokens": 94,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "uk/uk-002-random-CohereForAI-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "uk-002-random-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "Розкажи жарт",
      "language": "uk",
      "description": "Розкажи жарт",
      "num_tokens": 100,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "uk/uk-002-random-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "uk-002-random-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "Розкажи жарт",
      "language": "uk",
      "description": "Розкажи жарт",
      "num_tokens": 100,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "uk/uk-002-random-allenai-Olmo-3-7B-Think.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "uk-002-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Розкажи жарт",
      "language": "uk",
      "description": "Розкажи жарт",
      "num_tokens": 57,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "uk/uk-002-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "uk-002-random-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "Розкажи жарт",
      "language": "uk",
      "description": "Розкажи жарт",
      "num_tokens": 99,
      "model_id": "openai-community/gpt2-xl",
      "file": "uk/uk-002-random-openai-community-gpt2-xl.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "uk-002-sampling-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "Розкажи жарт",
      "language": "uk",
      "description": "Розкажи жарт",
      "num_tokens": 94,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "uk/uk-002-sampling-CohereForAI-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "uk-002-sampling-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "Розкажи жарт",
      "language": "uk",
      "description": "Розкажи жарт",
      "num_tokens": 99,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "uk/uk-002-sampling-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "uk-002-sampling-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "Розкажи жарт",
      "language": "uk",
      "description": "Розкажи жарт",
      "num_tokens": 99,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "uk/uk-002-sampling-allenai-Olmo-3-7B-Think.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "uk-002-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Розкажи жарт",
      "language": "uk",
      "description": "Розкажи жарт",
      "num_tokens": 99,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "uk/uk-002-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "uk-002-sampling-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "Розкажи жарт",
      "language": "uk",
      "description": "Розкажи жарт",
      "num_tokens": 98,
      "model_id": "openai-community/gpt2-xl",
      "file": "uk/uk-002-sampling-openai-community-gpt2-xl.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "uk-003-greedy-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "Столиця України це",
      "language": "uk",
      "description": "Столиця України це",
      "num_tokens": 68,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "uk/uk-003-greedy-CohereForAI-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "uk-003-greedy-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "Столиця України це",
      "language": "uk",
      "description": "Столиця України це",
      "num_tokens": 17,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "uk/uk-003-greedy-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "uk-003-greedy-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "Столиця України це",
      "language": "uk",
      "description": "Столиця України це",
      "num_tokens": 100,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "uk/uk-003-greedy-allenai-Olmo-3-7B-Think.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "uk-003-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Столиця України це",
      "language": "uk",
      "description": "Столиця України це",
      "num_tokens": 8,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "uk/uk-003-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "uk-003-greedy-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "Столиця України це",
      "language": "uk",
      "description": "Столиця України це",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2-xl",
      "file": "uk/uk-003-greedy-openai-community-gpt2-xl.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "uk-003-random-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "Столиця України це",
      "language": "uk",
      "description": "Столиця України це",
      "num_tokens": 24,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "uk/uk-003-random-CohereForAI-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "uk-003-random-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "Столиця України це",
      "language": "uk",
      "description": "Столиця України це",
      "num_tokens": 99,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "uk/uk-003-random-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "uk-003-random-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "Столиця України це",
      "language": "uk",
      "description": "Столиця України це",
      "num_tokens": 100,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "uk/uk-003-random-allenai-Olmo-3-7B-Think.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "uk-003-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Столиця України це",
      "language": "uk",
      "description": "Столиця України це",
      "num_tokens": 14,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "uk/uk-003-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "uk-003-random-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "Столиця України це",
      "language": "uk",
      "description": "Столиця України це",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2-xl",
      "file": "uk/uk-003-random-openai-community-gpt2-xl.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "uk-003-sampling-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "Столиця України це",
      "language": "uk",
      "description": "Столиця України це",
      "num_tokens": 80,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "uk/uk-003-sampling-CohereForAI-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "uk-003-sampling-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "Столиця України це",
      "language": "uk",
      "description": "Столиця України це",
      "num_tokens": 17,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "uk/uk-003-sampling-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "uk-003-sampling-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "Столиця України це",
      "language": "uk",
      "description": "Столиця України це",
      "num_tokens": 99,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "uk/uk-003-sampling-allenai-Olmo-3-7B-Think.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "uk-003-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Столиця України це",
      "language": "uk",
      "description": "Столиця України це",
      "num_tokens": 36,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "uk/uk-003-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "uk-003-sampling-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "Столиця України це",
      "language": "uk",
      "description": "Столиця України це",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2-xl",
      "file": "uk/uk-003-sampling-openai-community-gpt2-xl.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "uk-004-greedy-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "Переклади англійською: Чуєш їх, доцю, га? Кумедна ж ти, прощайся без ґольф!",
      "language": "uk",
      "description": "Переклади англійською: Чуєш їх, доцю, га? Кумедна ж ти, прощайся без ґольф!",
      "num_tokens": 93,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "uk/uk-004-greedy-CohereForAI-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "uk-004-greedy-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "Переклади англійською: Чуєш їх, доцю, га? Кумедна ж ти, прощайся без ґольф!",
      "language": "uk",
      "description": "Переклади англійською: Чуєш їх, доцю, га? Кумедна ж ти, прощайся без ґольф!",
      "num_tokens": 17,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "uk/uk-004-greedy-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "uk-004-greedy-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "Переклади англійською: Чуєш їх, доцю, га? Кумедна ж ти, прощайся без ґольф!",
      "language": "uk",
      "description": "Переклади англійською: Чуєш їх, доцю, га? Кумедна ж ти, прощайся без ґольф!",
      "num_tokens": 99,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "uk/uk-004-greedy-allenai-Olmo-3-7B-Think.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "uk-004-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Переклади англійською: Чуєш їх, доцю, га? Кумедна ж ти, прощайся без ґольф!",
      "language": "uk",
      "description": "Переклади англійською: Чуєш їх, доцю, га? Кумедна ж ти, прощайся без ґольф!",
      "num_tokens": 27,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "uk/uk-004-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "uk-004-greedy-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "Переклади англійською: Чуєш їх, доцю, га? Кумедна ж ти, прощайся без ґольф!",
      "language": "uk",
      "description": "Переклади англійською: Чуєш їх, доцю, га? Кумедна ж ти, прощайся без ґольф!",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2-xl",
      "file": "uk/uk-004-greedy-openai-community-gpt2-xl.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "uk-004-random-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "Переклади англійською: Чуєш їх, доцю, га? Кумедна ж ти, прощайся без ґольф!",
      "language": "uk",
      "description": "Переклади англійською: Чуєш їх, доцю, га? Кумедна ж ти, прощайся без ґольф!",
      "num_tokens": 95,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "uk/uk-004-random-CohereForAI-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "uk-004-random-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "Переклади англійською: Чуєш їх, доцю, га? Кумедна ж ти, прощайся без ґольф!",
      "language": "uk",
      "description": "Переклади англійською: Чуєш їх, доцю, га? Кумедна ж ти, прощайся без ґольф!",
      "num_tokens": 22,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "uk/uk-004-random-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "uk-004-random-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "Переклади англійською: Чуєш їх, доцю, га? Кумедна ж ти, прощайся без ґольф!",
      "language": "uk",
      "description": "Переклади англійською: Чуєш їх, доцю, га? Кумедна ж ти, прощайся без ґольф!",
      "num_tokens": 99,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "uk/uk-004-random-allenai-Olmo-3-7B-Think.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "uk-004-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Переклади англійською: Чуєш їх, доцю, га? Кумедна ж ти, прощайся без ґольф!",
      "language": "uk",
      "description": "Переклади англійською: Чуєш їх, доцю, га? Кумедна ж ти, прощайся без ґольф!",
      "num_tokens": 80,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "uk/uk-004-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "uk-004-random-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "Переклади англійською: Чуєш їх, доцю, га? Кумедна ж ти, прощайся без ґольф!",
      "language": "uk",
      "description": "Переклади англійською: Чуєш їх, доцю, га? Кумедна ж ти, прощайся без ґольф!",
      "num_tokens": 59,
      "model_id": "openai-community/gpt2-xl",
      "file": "uk/uk-004-random-openai-community-gpt2-xl.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "uk-004-sampling-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "Переклади англійською: Чуєш їх, доцю, га? Кумедна ж ти, прощайся без ґольф!",
      "language": "uk",
      "description": "Переклади англійською: Чуєш їх, доцю, га? Кумедна ж ти, прощайся без ґольф!",
      "num_tokens": 34,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "uk/uk-004-sampling-CohereForAI-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "uk-004-sampling-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "Переклади англійською: Чуєш їх, доцю, га? Кумедна ж ти, прощайся без ґольф!",
      "language": "uk",
      "description": "Переклади англійською: Чуєш їх, доцю, га? Кумедна ж ти, прощайся без ґольф!",
      "num_tokens": 17,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "uk/uk-004-sampling-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "uk-004-sampling-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "Переклади англійською: Чуєш їх, доцю, га? Кумедна ж ти, прощайся без ґольф!",
      "language": "uk",
      "description": "Переклади англійською: Чуєш їх, доцю, га? Кумедна ж ти, прощайся без ґольф!",
      "num_tokens": 99,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "uk/uk-004-sampling-allenai-Olmo-3-7B-Think.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "uk-004-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Переклади англійською: Чуєш їх, доцю, га? Кумедна ж ти, прощайся без ґольф!",
      "language": "uk",
      "description": "Переклади англійською: Чуєш їх, доцю, га? Кумедна ж ти, прощайся без ґольф!",
      "num_tokens": 25,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "uk/uk-004-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "uk-004-sampling-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "Переклади англійською: Чуєш їх, доцю, га? Кумедна ж ти, прощайся без ґольф!",
      "language": "uk",
      "description": "Переклади англійською: Чуєш їх, доцю, га? Кумедна ж ти, прощайся без ґольф!",
      "num_tokens": 98,
      "model_id": "openai-community/gpt2-xl",
      "file": "uk/uk-004-sampling-openai-community-gpt2-xl.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "uk-005-greedy-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "Скільки літер \"а\" у слові \"ананас\"?",
      "language": "uk",
      "description": "Скільки літер \"а\" у слові \"ананас\"?",
      "num_tokens": 15,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "uk/uk-005-greedy-CohereForAI-aya-expanse-8b.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "uk-005-greedy-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "Скільки літер \"а\" у слові \"ананас\"?",
      "language": "uk",
      "description": "Скільки літер \"а\" у слові \"ананас\"?",
      "num_tokens": 95,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "uk/uk-005-greedy-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "uk-005-greedy-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "Скільки літер \"а\" у слові \"ананас\"?",
      "language": "uk",
      "description": "Скільки літер \"а\" у слові \"ананас\"?",
      "num_tokens": 100,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "uk/uk-005-greedy-allenai-Olmo-3-7B-Think.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "uk-005-greedy-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Скільки літер \"а\" у слові \"ананас\"?",
      "language": "uk",
      "description": "Скільки літер \"а\" у слові \"ананас\"?",
      "num_tokens": 17,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "uk/uk-005-greedy-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "uk-005-greedy-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "Скільки літер \"а\" у слові \"ананас\"?",
      "language": "uk",
      "description": "Скільки літер \"а\" у слові \"ананас\"?",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2-xl",
      "file": "uk/uk-005-greedy-openai-community-gpt2-xl.json",
      "temperature": 0.0,
      "top_k": 10
    },
    {
      "id": "uk-005-random-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "Скільки літер \"а\" у слові \"ананас\"?",
      "language": "uk",
      "description": "Скільки літер \"а\" у слові \"ананас\"?",
      "num_tokens": 14,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "uk/uk-005-random-CohereForAI-aya-expanse-8b.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "uk-005-random-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "Скільки літер \"а\" у слові \"ананас\"?",
      "language": "uk",
      "description": "Скільки літер \"а\" у слові \"ананас\"?",
      "num_tokens": 93,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "uk/uk-005-random-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "uk-005-random-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "Скільки літер \"а\" у слові \"ананас\"?",
      "language": "uk",
      "description": "Скільки літер \"а\" у слові \"ананас\"?",
      "num_tokens": 100,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "uk/uk-005-random-allenai-Olmo-3-7B-Think.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "uk-005-random-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Скільки літер \"а\" у слові \"ананас\"?",
      "language": "uk",
      "description": "Скільки літер \"а\" у слові \"ананас\"?",
      "num_tokens": 97,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "uk/uk-005-random-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "uk-005-random-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "Скільки літер \"а\" у слові \"ананас\"?",
      "language": "uk",
      "description": "Скільки літер \"а\" у слові \"ананас\"?",
      "num_tokens": 99,
      "model_id": "openai-community/gpt2-xl",
      "file": "uk/uk-005-random-openai-community-gpt2-xl.json",
      "temperature": 5.0,
      "top_k": 10
    },
    {
      "id": "uk-005-sampling-CohereForAI-aya-expanse-8b",
      "type": "inference",
      "prompt": "Скільки літер \"а\" у слові \"ананас\"?",
      "language": "uk",
      "description": "Скільки літер \"а\" у слові \"ананас\"?",
      "num_tokens": 15,
      "model_id": "CohereForAI/aya-expanse-8b",
      "file": "uk/uk-005-sampling-CohereForAI-aya-expanse-8b.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "uk-005-sampling-Qwen-Qwen3-4B-Instruct-2507",
      "type": "inference",
      "prompt": "Скільки літер \"а\" у слові \"ананас\"?",
      "language": "uk",
      "description": "Скільки літер \"а\" у слові \"ананас\"?",
      "num_tokens": 92,
      "model_id": "Qwen/Qwen3-4B-Instruct-2507",
      "file": "uk/uk-005-sampling-Qwen-Qwen3-4B-Instruct-2507.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "uk-005-sampling-allenai-Olmo-3-7B-Think",
      "type": "inference",
      "prompt": "Скільки літер \"а\" у слові \"ананас\"?",
      "language": "uk",
      "description": "Скільки літер \"а\" у слові \"ананас\"?",
      "num_tokens": 100,
      "model_id": "allenai/Olmo-3-7B-Think",
      "file": "uk/uk-005-sampling-allenai-Olmo-3-7B-Think.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "uk-005-sampling-meta-llama-Llama-3.2-1B-Instruct",
      "type": "inference",
      "prompt": "Скільки літер \"а\" у слові \"ананас\"?",
      "language": "uk",
      "description": "Скільки літер \"а\" у слові \"ананас\"?",
      "num_tokens": 97,
      "model_id": "meta-llama/Llama-3.2-1B-Instruct",
      "file": "uk/uk-005-sampling-meta-llama-Llama-3.2-1B-Instruct.json",
      "temperature": 1.0,
      "top_k": 10
    },
    {
      "id": "uk-005-sampling-openai-community-gpt2-xl",
      "type": "inference",
      "prompt": "Скільки літер \"а\" у слові \"ананас\"?",
      "language": "uk",
      "description": "Скільки літер \"а\" у слові \"ананас\"?",
      "num_tokens": 100,
      "model_id": "openai-community/gpt2-xl",
      "file": "uk/uk-005-sampling-openai-community-gpt2-xl.json",
      "temperature": 1.0,
      "top_k": 10
    }
  ]
}