{
  "id": "generated_20251204_095933",
  "prompt": "Qui es-tu?",
  "language": "fr",
  "temperature": 5.0,
  "top_k": 10,
  "model_info": {
    "name": "allenai/Olmo-3-7B-Think",
    "num_layers": 32,
    "hidden_size": 4096,
    "num_attention_heads": 32,
    "vocab_size": 100278
  },
  "generation_steps": [
    {
      "step": 0,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Okay",
            "token_id": 33413,
            "logprob": -0.0292,
            "prob": 0.9727
          },
          {
            "token": "Hmm",
            "token_id": 81122,
            "logprob": -4.2812,
            "prob": 0.0139
          },
          {
            "token": "ĠOkay",
            "token_id": 36539,
            "logprob": -5.7812,
            "prob": 0.0031
          },
          {
            "token": "Ah",
            "token_id": 25797,
            "logprob": -6.4062,
            "prob": 0.0017
          },
          {
            "token": "Alright",
            "token_id": 72586,
            "logprob": -7.3438,
            "prob": 0.0006
          },
          {
            "token": "Ġbon",
            "token_id": 7970,
            "logprob": -7.3438,
            "prob": 0.0006
          },
          {
            "token": "Ġokay",
            "token_id": 17339,
            "logprob": -7.7188,
            "prob": 0.0004
          },
          {
            "token": "okay",
            "token_id": 94317,
            "logprob": -8.0,
            "prob": 0.0003
          },
          {
            "token": "OK",
            "token_id": 4012,
            "logprob": -8.375,
            "prob": 0.0002
          },
          {
            "token": "Ok",
            "token_id": 11839,
            "logprob": -8.375,
            "prob": 0.0002
          }
        ]
      },
      "selected_token": {
        "token": "Okay",
        "token_id": 33413,
        "selection_method": "sampling"
      }
    },
    {
      "step": 1,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": ",",
            "token_id": 11,
            "logprob": -0.0,
            "prob": 1.0
          },
          {
            "token": ":",
            "token_id": 25,
            "logprob": -15.0,
            "prob": 0.0
          },
          {
            "token": "，",
            "token_id": 3922,
            "logprob": -15.3125,
            "prob": 0.0
          },
          {
            "token": "Ġso",
            "token_id": 779,
            "logprob": -15.375,
            "prob": 0.0
          },
          {
            "token": "?",
            "token_id": 30,
            "logprob": -15.75,
            "prob": 0.0
          },
          {
            "token": "...",
            "token_id": 1131,
            "logprob": -15.8125,
            "prob": 0.0
          },
          {
            "token": ".",
            "token_id": 13,
            "logprob": -16.25,
            "prob": 0.0
          },
          {
            "token": "Ġdu",
            "token_id": 3930,
            "logprob": -16.5,
            "prob": 0.0
          },
          {
            "token": "!",
            "token_id": 0,
            "logprob": -16.5,
            "prob": 0.0
          },
          {
            "token": "Ġlet",
            "token_id": 1095,
            "logprob": -16.625,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "?",
        "token_id": 30,
        "selection_method": "sampling"
      }
    },
    {
      "step": 2,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay?",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "ĠThe",
            "token_id": 578,
            "logprob": -0.0117,
            "prob": 0.9883
          },
          {
            "token": "ĠL",
            "token_id": 445,
            "logprob": -4.75,
            "prob": 0.0085
          },
          {
            "token": "ĠUser",
            "token_id": 2724,
            "logprob": -7.3125,
            "prob": 0.0007
          },
          {
            "token": "ĠLa",
            "token_id": 5034,
            "logprob": -7.8125,
            "prob": 0.0004
          },
          {
            "token": "ĠSomeone",
            "token_id": 35272,
            "logprob": -8.1875,
            "prob": 0.0003
          },
          {
            "token": "ĠI",
            "token_id": 358,
            "logprob": -8.375,
            "prob": 0.0002
          },
          {
            "token": "ĠLet",
            "token_id": 6914,
            "logprob": -8.4375,
            "prob": 0.0002
          },
          {
            "token": "ĠLe",
            "token_id": 2009,
            "logprob": -8.8125,
            "prob": 0.0001
          },
          {
            "token": "Ġsomeone",
            "token_id": 4423,
            "logprob": -8.9375,
            "prob": 0.0001
          },
          {
            "token": "ĠJe",
            "token_id": 14465,
            "logprob": -8.9375,
            "prob": 0.0001
          }
        ]
      },
      "selected_token": {
        "token": "ĠUser",
        "token_id": 2724,
        "selection_method": "sampling"
      }
    },
    {
      "step": 3,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġasked",
            "token_id": 4691,
            "logprob": -0.3906,
            "prob": 0.6758
          },
          {
            "token": "Ġjust",
            "token_id": 1120,
            "logprob": -1.3906,
            "prob": 0.249
          },
          {
            "token": "Ġis",
            "token_id": 374,
            "logprob": -3.2656,
            "prob": 0.0381
          },
          {
            "token": "Ġwrote",
            "token_id": 6267,
            "logprob": -3.3906,
            "prob": 0.0337
          },
          {
            "token": "Ġsaid",
            "token_id": 1071,
            "logprob": -6.9688,
            "prob": 0.001
          },
          {
            "token": "Ġasks",
            "token_id": 17501,
            "logprob": -7.4062,
            "prob": 0.0006
          },
          {
            "token": "Ġstarted",
            "token_id": 3940,
            "logprob": -8.4375,
            "prob": 0.0002
          },
          {
            "token": "Ġsays",
            "token_id": 2795,
            "logprob": -8.5625,
            "prob": 0.0002
          },
          {
            "token": "Ġgreeted",
            "token_id": 44865,
            "logprob": -9.25,
            "prob": 0.0001
          },
          {
            "token": "Ġasking",
            "token_id": 10371,
            "logprob": -9.3125,
            "prob": 0.0001
          }
        ]
      },
      "selected_token": {
        "token": "Ġasks",
        "token_id": 17501,
        "selection_method": "sampling"
      }
    },
    {
      "step": 4,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġ\"",
            "token_id": 330,
            "logprob": -0.0339,
            "prob": 0.9648
          },
          {
            "token": "Ġme",
            "token_id": 757,
            "logprob": -4.2812,
            "prob": 0.0138
          },
          {
            "token": ",",
            "token_id": 11,
            "logprob": -4.4062,
            "prob": 0.0121
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -4.9062,
            "prob": 0.0074
          },
          {
            "token": "Ġ'",
            "token_id": 364,
            "logprob": -11.6875,
            "prob": 0.0
          },
          {
            "token": "Ġwho",
            "token_id": 889,
            "logprob": -11.8125,
            "prob": 0.0
          },
          {
            "token": "Ġ“",
            "token_id": 1054,
            "logprob": -12.0625,
            "prob": 0.0
          },
          {
            "token": "me",
            "token_id": 2727,
            "logprob": -12.1875,
            "prob": 0.0
          },
          {
            "token": "Ġ\"\"",
            "token_id": 1621,
            "logprob": -12.5625,
            "prob": 0.0
          },
          {
            "token": ",\"",
            "token_id": 1359,
            "logprob": -12.75,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġin",
        "token_id": 304,
        "selection_method": "sampling"
      }
    },
    {
      "step": 5,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "ĠFrench",
            "token_id": 8753,
            "logprob": -0.0004,
            "prob": 1.0
          },
          {
            "token": "Ġfrench",
            "token_id": 42293,
            "logprob": -9.25,
            "prob": 0.0001
          },
          {
            "token": "ĠQuebec",
            "token_id": 35195,
            "logprob": -9.8125,
            "prob": 0.0001
          },
          {
            "token": "French",
            "token_id": 44297,
            "logprob": -10.25,
            "prob": 0.0
          },
          {
            "token": "ĠCatalan",
            "token_id": 81944,
            "logprob": -10.875,
            "prob": 0.0
          },
          {
            "token": "ĠSpanish",
            "token_id": 15506,
            "logprob": -11.0,
            "prob": 0.0
          },
          {
            "token": "ĠFr",
            "token_id": 2939,
            "logprob": -11.0625,
            "prob": 0.0
          },
          {
            "token": "ĠPortuguese",
            "token_id": 43288,
            "logprob": -11.125,
            "prob": 0.0
          },
          {
            "token": "ĠQué",
            "token_id": 66703,
            "logprob": -11.3125,
            "prob": 0.0
          },
          {
            "token": "ĠCaj",
            "token_id": 96879,
            "logprob": -11.5,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "ĠFrench",
        "token_id": 8753,
        "selection_method": "sampling"
      }
    },
    {
      "step": 6,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": ",",
            "token_id": 11,
            "logprob": -0.8281,
            "prob": 0.4375
          },
          {
            "token": ":",
            "token_id": 25,
            "logprob": -0.8281,
            "prob": 0.4375
          },
          {
            "token": "Ġ\"",
            "token_id": 330,
            "logprob": -2.0781,
            "prob": 0.125
          },
          {
            "token": ",\"",
            "token_id": 1359,
            "logprob": -10.0625,
            "prob": 0.0
          },
          {
            "token": "Ġwith",
            "token_id": 449,
            "logprob": -11.9375,
            "prob": 0.0
          },
          {
            "token": "\"",
            "token_id": 1,
            "logprob": -12.0,
            "prob": 0.0
          },
          {
            "token": "Ġwhat",
            "token_id": 1148,
            "logprob": -12.0625,
            "prob": 0.0
          },
          {
            "token": "?",
            "token_id": 30,
            "logprob": -12.4375,
            "prob": 0.0
          },
          {
            "token": "Ġ'",
            "token_id": 364,
            "logprob": -12.625,
            "prob": 0.0
          },
          {
            "token": ".",
            "token_id": 13,
            "logprob": -12.6875,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġ\"",
        "token_id": 330,
        "selection_method": "sampling"
      }
    },
    {
      "step": 7,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\""
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Qui",
            "token_id": 62545,
            "logprob": -0.0134,
            "prob": 0.9883
          },
          {
            "token": "Who",
            "token_id": 15546,
            "logprob": -4.75,
            "prob": 0.0085
          },
          {
            "token": "ĠQui",
            "token_id": 42215,
            "logprob": -6.125,
            "prob": 0.0022
          },
          {
            "token": "Ġqui",
            "token_id": 7930,
            "logprob": -6.875,
            "prob": 0.001
          },
          {
            "token": "Qu",
            "token_id": 2232,
            "logprob": -7.75,
            "prob": 0.0004
          },
          {
            "token": "ĠWho",
            "token_id": 10699,
            "logprob": -8.0,
            "prob": 0.0003
          },
          {
            "token": "ĠQUI",
            "token_id": 89630,
            "logprob": -8.3125,
            "prob": 0.0002
          },
          {
            "token": "who",
            "token_id": 14965,
            "logprob": -9.1875,
            "prob": 0.0001
          },
          {
            "token": "Ġwho",
            "token_id": 889,
            "logprob": -9.375,
            "prob": 0.0001
          },
          {
            "token": "QUI",
            "token_id": 12247,
            "logprob": -10.25,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Who",
        "token_id": 15546,
        "selection_method": "sampling"
      }
    },
    {
      "step": 8,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Who",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġare",
            "token_id": 527,
            "logprob": -0.0001,
            "prob": 1.0
          },
          {
            "token": "Ġis",
            "token_id": 374,
            "logprob": -9.875,
            "prob": 0.0001
          },
          {
            "token": "ĠARE",
            "token_id": 16202,
            "logprob": -10.6875,
            "prob": 0.0
          },
          {
            "token": "ĠAre",
            "token_id": 8886,
            "logprob": -11.8125,
            "prob": 0.0
          },
          {
            "token": "Ġart",
            "token_id": 1989,
            "logprob": -12.3125,
            "prob": 0.0
          },
          {
            "token": "Ġam",
            "token_id": 1097,
            "logprob": -12.3125,
            "prob": 0.0
          },
          {
            "token": "Ġsont",
            "token_id": 15132,
            "logprob": -12.5,
            "prob": 0.0
          },
          {
            "token": "are",
            "token_id": 548,
            "logprob": -12.75,
            "prob": 0.0
          },
          {
            "token": "ami",
            "token_id": 10830,
            "logprob": -13.1875,
            "prob": 0.0
          },
          {
            "token": "Are",
            "token_id": 11787,
            "logprob": -13.25,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "are",
        "token_id": 548,
        "selection_method": "sampling"
      }
    },
    {
      "step": 9,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoare",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġyou",
            "token_id": 499,
            "logprob": -0.6328,
            "prob": 0.5312
          },
          {
            "token": "you",
            "token_id": 9514,
            "logprob": -0.8828,
            "prob": 0.4141
          },
          {
            "token": "-you",
            "token_id": 46517,
            "logprob": -4.8125,
            "prob": 0.0081
          },
          {
            "token": "th",
            "token_id": 339,
            "logprob": -4.875,
            "prob": 0.0076
          },
          {
            "token": "You",
            "token_id": 2675,
            "logprob": -5.375,
            "prob": 0.0046
          },
          {
            "token": "Ġtu",
            "token_id": 9964,
            "logprob": -5.75,
            "prob": 0.0032
          },
          {
            "token": "-th",
            "token_id": 7716,
            "logprob": -5.875,
            "prob": 0.0028
          },
          {
            "token": "您",
            "token_id": 88126,
            "logprob": -6.1875,
            "prob": 0.002
          },
          {
            "token": "st",
            "token_id": 267,
            "logprob": -6.25,
            "prob": 0.0019
          },
          {
            "token": "ĠYOU",
            "token_id": 15334,
            "logprob": -6.5,
            "prob": 0.0015
          }
        ]
      },
      "selected_token": {
        "token": "st",
        "token_id": 267,
        "selection_method": "sampling"
      }
    },
    {
      "step": 10,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoarest",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "u",
            "token_id": 84,
            "logprob": -1.4141,
            "prob": 0.2432
          },
          {
            "token": "hu",
            "token_id": 17156,
            "logprob": -1.5391,
            "prob": 0.2148
          },
          {
            "token": "ou",
            "token_id": 283,
            "logprob": -2.3438,
            "prob": 0.0952
          },
          {
            "token": "hy",
            "token_id": 8671,
            "logprob": -2.4062,
            "prob": 0.0894
          },
          {
            "token": "hou",
            "token_id": 18664,
            "logprob": -2.6562,
            "prob": 0.0698
          },
          {
            "token": "Ġu",
            "token_id": 577,
            "logprob": -4.7188,
            "prob": 0.0089
          },
          {
            "token": "ú",
            "token_id": 6792,
            "logprob": -4.7188,
            "prob": 0.0089
          },
          {
            "token": "Ġthou",
            "token_id": 34223,
            "logprob": -4.875,
            "prob": 0.0076
          },
          {
            "token": "oo",
            "token_id": 2689,
            "logprob": -5.0,
            "prob": 0.0067
          },
          {
            "token": "ho",
            "token_id": 6292,
            "logprob": -5.125,
            "prob": 0.0059
          }
        ]
      },
      "selected_token": {
        "token": "hy",
        "token_id": 8671,
        "selection_method": "sampling"
      }
    },
    {
      "step": 11,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "?\"",
            "token_id": 7673,
            "logprob": -0.1138,
            "prob": 0.8906
          },
          {
            "token": "?",
            "token_id": 30,
            "logprob": -3.1094,
            "prob": 0.0444
          },
          {
            "token": "u",
            "token_id": 84,
            "logprob": -3.8594,
            "prob": 0.021
          },
          {
            "token": "ou",
            "token_id": 283,
            "logprob": -4.125,
            "prob": 0.0164
          },
          {
            "token": "\"",
            "token_id": 1,
            "logprob": -4.875,
            "prob": 0.0077
          },
          {
            "token": "\"?",
            "token_id": 44969,
            "logprob": -5.0,
            "prob": 0.0068
          },
          {
            "token": "e",
            "token_id": 68,
            "logprob": -5.75,
            "prob": 0.0032
          },
          {
            "token": "\".",
            "token_id": 3343,
            "logprob": -6.375,
            "prob": 0.0017
          },
          {
            "token": "o",
            "token_id": 78,
            "logprob": -6.875,
            "prob": 0.001
          },
          {
            "token": "?\",",
            "token_id": 32111,
            "logprob": -7.1875,
            "prob": 0.0008
          }
        ]
      },
      "selected_token": {
        "token": "\"",
        "token_id": 1,
        "selection_method": "sampling"
      }
    },
    {
      "step": 12,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\"",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\""
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġor",
            "token_id": 477,
            "logprob": -1.3203,
            "prob": 0.2656
          },
          {
            "token": "Ġ(",
            "token_id": 320,
            "logprob": -1.6953,
            "prob": 0.1826
          },
          {
            "token": "Ġ–",
            "token_id": 1389,
            "logprob": -1.9453,
            "prob": 0.1426
          },
          {
            "token": "Ġwhich",
            "token_id": 902,
            "logprob": -2.2031,
            "prob": 0.1108
          },
          {
            "token": "Ġwith",
            "token_id": 449,
            "logprob": -2.8281,
            "prob": 0.0593
          },
          {
            "token": "Ġ—",
            "token_id": 2001,
            "logprob": -2.9531,
            "prob": 0.0525
          },
          {
            "token": "Ġbut",
            "token_id": 719,
            "logprob": -3.4531,
            "prob": 0.0317
          },
          {
            "token": "Ġspelled",
            "token_id": 68918,
            "logprob": -3.4531,
            "prob": 0.0317
          },
          {
            "token": "Ġ?",
            "token_id": 949,
            "logprob": -3.7031,
            "prob": 0.0248
          },
          {
            "token": "Ġprobably",
            "token_id": 4762,
            "logprob": -4.0625,
            "prob": 0.017
          }
        ]
      },
      "selected_token": {
        "token": "Ġwhich",
        "token_id": 902,
        "selection_method": "sampling"
      }
    },
    {
      "step": 13,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġis",
            "token_id": 374,
            "logprob": -0.5664,
            "prob": 0.5664
          },
          {
            "token": "Ġtranslates",
            "token_id": 48018,
            "logprob": -1.3203,
            "prob": 0.2676
          },
          {
            "token": "ĠI",
            "token_id": 358,
            "logprob": -2.3125,
            "prob": 0.0986
          },
          {
            "token": "Ġprobably",
            "token_id": 4762,
            "logprob": -3.5625,
            "prob": 0.0282
          },
          {
            "token": "Ġshould",
            "token_id": 1288,
            "logprob": -3.8125,
            "prob": 0.022
          },
          {
            "token": "Ġmeans",
            "token_id": 3445,
            "logprob": -5.0625,
            "prob": 0.0063
          },
          {
            "token": "Ġtransl",
            "token_id": 12215,
            "logprob": -6.4375,
            "prob": 0.0016
          },
          {
            "token": "Ġseems",
            "token_id": 5084,
            "logprob": -6.5625,
            "prob": 0.0014
          },
          {
            "token": "Ġwould",
            "token_id": 1053,
            "logprob": -6.9375,
            "prob": 0.001
          },
          {
            "token": "Ġtranslated",
            "token_id": 25548,
            "logprob": -6.9375,
            "prob": 0.001
          }
        ]
      },
      "selected_token": {
        "token": "Ġis",
        "token_id": 374,
        "selection_method": "sampling"
      }
    },
    {
      "step": 14,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġ\"",
            "token_id": 330,
            "logprob": -0.4219,
            "prob": 0.6562
          },
          {
            "token": "Ġprobably",
            "token_id": 4762,
            "logprob": -1.7969,
            "prob": 0.166
          },
          {
            "token": "Ġa",
            "token_id": 264,
            "logprob": -1.9844,
            "prob": 0.1377
          },
          {
            "token": "Ġobviously",
            "token_id": 14224,
            "logprob": -4.6875,
            "prob": 0.0093
          },
          {
            "token": "Ġactually",
            "token_id": 3604,
            "logprob": -5.0,
            "prob": 0.0068
          },
          {
            "token": "Ġspelled",
            "token_id": 68918,
            "logprob": -5.8125,
            "prob": 0.003
          },
          {
            "token": "Ġclearly",
            "token_id": 9539,
            "logprob": -5.875,
            "prob": 0.0029
          },
          {
            "token": "Ġlikely",
            "token_id": 4461,
            "logprob": -5.9375,
            "prob": 0.0027
          },
          {
            "token": "ĠQui",
            "token_id": 42215,
            "logprob": -6.0,
            "prob": 0.0025
          },
          {
            "token": "Ġsupposed",
            "token_id": 10171,
            "logprob": -6.125,
            "prob": 0.0022
          }
        ]
      },
      "selected_token": {
        "token": "Ġ\"",
        "token_id": 330,
        "selection_method": "sampling"
      }
    },
    {
      "step": 15,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\""
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Qui",
            "token_id": 62545,
            "logprob": -0.2852,
            "prob": 0.75
          },
          {
            "token": "Who",
            "token_id": 15546,
            "logprob": -1.5391,
            "prob": 0.2158
          },
          {
            "token": "Qu",
            "token_id": 2232,
            "logprob": -3.9062,
            "prob": 0.02
          },
          {
            "token": "ĠQui",
            "token_id": 42215,
            "logprob": -5.7812,
            "prob": 0.0031
          },
          {
            "token": "Ġqui",
            "token_id": 7930,
            "logprob": -6.0938,
            "prob": 0.0022
          },
          {
            "token": "ĠWho",
            "token_id": 10699,
            "logprob": -6.1562,
            "prob": 0.0021
          },
          {
            "token": "who",
            "token_id": 14965,
            "logprob": -6.6562,
            "prob": 0.0013
          },
          {
            "token": "WHO",
            "token_id": 78847,
            "logprob": -7.7812,
            "prob": 0.0004
          },
          {
            "token": "qui",
            "token_id": 47391,
            "logprob": -8.25,
            "prob": 0.0003
          },
          {
            "token": "What",
            "token_id": 3923,
            "logprob": -8.5,
            "prob": 0.0002
          }
        ]
      },
      "selected_token": {
        "token": "Qu",
        "token_id": 2232,
        "selection_method": "sampling"
      }
    },
    {
      "step": 16,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "'est",
            "token_id": 17771,
            "logprob": -0.8984,
            "prob": 0.4082
          },
          {
            "token": "�",
            "token_id": 39013,
            "logprob": -2.7031,
            "prob": 0.0664
          },
          {
            "token": "elle",
            "token_id": 6853,
            "logprob": -3.4531,
            "prob": 0.0315
          },
          {
            "token": "ién",
            "token_id": 24447,
            "logprob": -4.1562,
            "prob": 0.0159
          },
          {
            "token": "'",
            "token_id": 6,
            "logprob": -4.1562,
            "prob": 0.0159
          },
          {
            "token": "iest",
            "token_id": 13744,
            "logprob": -4.2188,
            "prob": 0.0148
          },
          {
            "token": "id",
            "token_id": 307,
            "logprob": -4.2812,
            "prob": 0.014
          },
          {
            "token": "is",
            "token_id": 285,
            "logprob": -4.4062,
            "prob": 0.0123
          },
          {
            "token": "Ġest",
            "token_id": 1826,
            "logprob": -4.5312,
            "prob": 0.0109
          },
          {
            "token": "’est",
            "token_id": 22117,
            "logprob": -4.8438,
            "prob": 0.0079
          }
        ]
      },
      "selected_token": {
        "token": "�",
        "token_id": 39013,
        "selection_method": "sampling"
      }
    },
    {
      "step": 17,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu�",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "�",
            "token_id": 223,
            "logprob": -0.0059,
            "prob": 0.9922
          },
          {
            "token": "�",
            "token_id": 95,
            "logprob": -6.3125,
            "prob": 0.0018
          },
          {
            "token": "�",
            "token_id": 233,
            "logprob": -6.9375,
            "prob": 0.001
          },
          {
            "token": "�",
            "token_id": 222,
            "logprob": -7.5,
            "prob": 0.0005
          },
          {
            "token": "�",
            "token_id": 238,
            "logprob": -7.6875,
            "prob": 0.0005
          },
          {
            "token": "�",
            "token_id": 255,
            "logprob": -7.8125,
            "prob": 0.0004
          },
          {
            "token": "�",
            "token_id": 115,
            "logprob": -8.0,
            "prob": 0.0003
          },
          {
            "token": "�",
            "token_id": 109,
            "logprob": -8.0625,
            "prob": 0.0003
          },
          {
            "token": "�",
            "token_id": 235,
            "logprob": -8.8125,
            "prob": 0.0001
          },
          {
            "token": "�",
            "token_id": 102,
            "logprob": -9.0625,
            "prob": 0.0001
          }
        ]
      },
      "selected_token": {
        "token": "�",
        "token_id": 109,
        "selection_method": "sampling"
      }
    },
    {
      "step": 18,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "e",
            "token_id": 68,
            "logprob": -0.6172,
            "prob": 0.5391
          },
          {
            "token": "est",
            "token_id": 478,
            "logprob": -2.25,
            "prob": 0.106
          },
          {
            "token": "st",
            "token_id": 267,
            "logprob": -2.5,
            "prob": 0.0825
          },
          {
            "token": "t",
            "token_id": 83,
            "logprob": -2.875,
            "prob": 0.0566
          },
          {
            "token": "s",
            "token_id": 82,
            "logprob": -3.9375,
            "prob": 0.0197
          },
          {
            "token": "es",
            "token_id": 288,
            "logprob": -4.5625,
            "prob": 0.0105
          },
          {
            "token": "il",
            "token_id": 321,
            "logprob": -5.1875,
            "prob": 0.0056
          },
          {
            "token": "you",
            "token_id": 9514,
            "logprob": -5.2188,
            "prob": 0.0054
          },
          {
            "token": "你",
            "token_id": 57668,
            "logprob": -5.25,
            "prob": 0.0053
          },
          {
            "token": "Ġyou",
            "token_id": 499,
            "logprob": -5.375,
            "prob": 0.0047
          }
        ]
      },
      "selected_token": {
        "token": "e",
        "token_id": 68,
        "selection_method": "sampling"
      }
    },
    {
      "step": 19,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġest",
            "token_id": 1826,
            "logprob": -1.2188,
            "prob": 0.2969
          },
          {
            "token": "est",
            "token_id": 478,
            "logprob": -1.5938,
            "prob": 0.2031
          },
          {
            "token": "Ġtu",
            "token_id": 9964,
            "logprob": -1.9062,
            "prob": 0.1484
          },
          {
            "token": "Ġes",
            "token_id": 1560,
            "logprob": -2.7812,
            "prob": 0.062
          },
          {
            "token": "-t",
            "token_id": 2442,
            "logprob": -3.2188,
            "prob": 0.04
          },
          {
            "token": "Ġt",
            "token_id": 259,
            "logprob": -3.4688,
            "prob": 0.0312
          },
          {
            "token": "Ġsont",
            "token_id": 15132,
            "logprob": -4.4688,
            "prob": 0.0115
          },
          {
            "token": "'est",
            "token_id": 17771,
            "logprob": -4.6562,
            "prob": 0.0095
          },
          {
            "token": "-",
            "token_id": 12,
            "logprob": -4.6875,
            "prob": 0.0092
          },
          {
            "token": "ux",
            "token_id": 2249,
            "logprob": -5.5,
            "prob": 0.0041
          }
        ]
      },
      "selected_token": {
        "token": "Ġtu",
        "token_id": 9964,
        "selection_method": "sampling"
      }
    },
    {
      "step": 20,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "?\"",
            "token_id": 7673,
            "logprob": -0.2734,
            "prob": 0.7617
          },
          {
            "token": "\"",
            "token_id": 1,
            "logprob": -1.8984,
            "prob": 0.1494
          },
          {
            "token": "\".",
            "token_id": 3343,
            "logprob": -3.1562,
            "prob": 0.043
          },
          {
            "token": "Ġest",
            "token_id": 1826,
            "logprob": -4.0938,
            "prob": 0.0168
          },
          {
            "token": "Ġ?",
            "token_id": 949,
            "logprob": -4.9688,
            "prob": 0.007
          },
          {
            "token": "Ġes",
            "token_id": 1560,
            "logprob": -5.0312,
            "prob": 0.0066
          },
          {
            "token": "?",
            "token_id": 30,
            "logprob": -5.1562,
            "prob": 0.0058
          },
          {
            "token": "\"?",
            "token_id": 44969,
            "logprob": -5.4688,
            "prob": 0.0042
          },
          {
            "token": ".\"",
            "token_id": 1210,
            "logprob": -6.1562,
            "prob": 0.0021
          },
          {
            "token": "\",",
            "token_id": 498,
            "logprob": -7.7188,
            "prob": 0.0004
          }
        ]
      },
      "selected_token": {
        "token": "?",
        "token_id": 30,
        "selection_method": "sampling"
      }
    },
    {
      "step": 21,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu?",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "\".",
            "token_id": 3343,
            "logprob": -0.0055,
            "prob": 0.9961
          },
          {
            "token": "ĠYou",
            "token_id": 1472,
            "logprob": -6.6875,
            "prob": 0.0012
          },
          {
            "token": "ĠWait",
            "token_id": 14144,
            "logprob": -6.9375,
            "prob": 0.001
          },
          {
            "token": "\"\"",
            "token_id": 3089,
            "logprob": -7.4375,
            "prob": 0.0006
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -8.0,
            "prob": 0.0003
          },
          {
            "token": "Ġ\"",
            "token_id": 330,
            "logprob": -8.0625,
            "prob": 0.0003
          },
          {
            "token": "\").",
            "token_id": 1865,
            "logprob": -8.3125,
            "prob": 0.0002
          },
          {
            "token": ".\"",
            "token_id": 1210,
            "logprob": -8.6875,
            "prob": 0.0002
          },
          {
            "token": "ĠBut",
            "token_id": 2030,
            "logprob": -8.875,
            "prob": 0.0001
          },
          {
            "token": "?\"",
            "token_id": 7673,
            "logprob": -9.125,
            "prob": 0.0001
          }
        ]
      },
      "selected_token": {
        "token": "Ġ\"",
        "token_id": 330,
        "selection_method": "sampling"
      }
    },
    {
      "step": 22,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \"",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\""
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "ĠWait",
            "token_id": 14144,
            "logprob": -1.4609,
            "prob": 0.2324
          },
          {
            "token": "Wait",
            "token_id": 14524,
            "logprob": -2.0781,
            "prob": 0.1245
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -2.0781,
            "prob": 0.1245
          },
          {
            "token": "Ġor",
            "token_id": 477,
            "logprob": -2.3906,
            "prob": 0.0913
          },
          {
            "token": "Ġbut",
            "token_id": 719,
            "logprob": -2.5781,
            "prob": 0.0757
          },
          {
            "token": "Qu",
            "token_id": 2232,
            "logprob": -3.3906,
            "prob": 0.0334
          },
          {
            "token": "Ġmeaning",
            "token_id": 7438,
            "logprob": -3.3906,
            "prob": 0.0334
          },
          {
            "token": "Ġwhich",
            "token_id": 902,
            "logprob": -3.5781,
            "prob": 0.0278
          },
          {
            "token": "Ġwait",
            "token_id": 3868,
            "logprob": -3.6406,
            "prob": 0.0261
          },
          {
            "token": "Ġsorry",
            "token_id": 14931,
            "logprob": -4.0938,
            "prob": 0.0168
          }
        ]
      },
      "selected_token": {
        "token": "Ġbut",
        "token_id": 719,
        "selection_method": "sampling"
      }
    },
    {
      "step": 23,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġwith",
            "token_id": 449,
            "logprob": -1.1172,
            "prob": 0.3262
          },
          {
            "token": "Ġprobably",
            "token_id": 4762,
            "logprob": -1.8672,
            "prob": 0.1543
          },
          {
            "token": "Ġactually",
            "token_id": 3604,
            "logprob": -2.25,
            "prob": 0.106
          },
          {
            "token": "Ġmaybe",
            "token_id": 7344,
            "logprob": -2.625,
            "prob": 0.0728
          },
          {
            "token": "Ġwritten",
            "token_id": 5439,
            "logprob": -2.875,
            "prob": 0.0566
          },
          {
            "token": "Ġthey",
            "token_id": 814,
            "logprob": -3.125,
            "prob": 0.0442
          },
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -3.125,
            "prob": 0.0442
          },
          {
            "token": "Ġspelled",
            "token_id": 68918,
            "logprob": -3.375,
            "prob": 0.0344
          },
          {
            "token": "Ġmiss",
            "token_id": 3194,
            "logprob": -3.375,
            "prob": 0.0344
          },
          {
            "token": "ĠI",
            "token_id": 358,
            "logprob": -3.5,
            "prob": 0.0304
          }
        ]
      },
      "selected_token": {
        "token": "Ġthe",
        "token_id": 279,
        "selection_method": "sampling"
      }
    },
    {
      "step": 24,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġuser",
            "token_id": 1217,
            "logprob": -0.2207,
            "prob": 0.8008
          },
          {
            "token": "Ġcorrect",
            "token_id": 4495,
            "logprob": -2.7188,
            "prob": 0.0659
          },
          {
            "token": "Ġspelling",
            "token_id": 43529,
            "logprob": -3.5938,
            "prob": 0.0275
          },
          {
            "token": "Ġactual",
            "token_id": 5150,
            "logprob": -3.8438,
            "prob": 0.0214
          },
          {
            "token": "Ġtranslation",
            "token_id": 14807,
            "logprob": -4.9062,
            "prob": 0.0074
          },
          {
            "token": "Ġaccents",
            "token_id": 59570,
            "logprob": -4.9688,
            "prob": 0.0069
          },
          {
            "token": "ĠChinese",
            "token_id": 8620,
            "logprob": -4.9688,
            "prob": 0.0069
          },
          {
            "token": "Ġcharacters",
            "token_id": 5885,
            "logprob": -5.0312,
            "prob": 0.0065
          },
          {
            "token": "ĠFrench",
            "token_id": 8753,
            "logprob": -5.1562,
            "prob": 0.0058
          },
          {
            "token": "Ġquestion",
            "token_id": 3488,
            "logprob": -5.3438,
            "prob": 0.0048
          }
        ]
      },
      "selected_token": {
        "token": "Ġcharacters",
        "token_id": 5885,
        "selection_method": "sampling"
      }
    },
    {
      "step": 25,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġare",
            "token_id": 527,
            "logprob": -0.2324,
            "prob": 0.793
          },
          {
            "token": "Ġmight",
            "token_id": 2643,
            "logprob": -3.1094,
            "prob": 0.0447
          },
          {
            "token": "Ġused",
            "token_id": 1511,
            "logprob": -3.1719,
            "prob": 0.042
          },
          {
            "token": "Ġgot",
            "token_id": 2751,
            "logprob": -3.1719,
            "prob": 0.042
          },
          {
            "token": "Ġlook",
            "token_id": 1427,
            "logprob": -3.2344,
            "prob": 0.0396
          },
          {
            "token": "Ġseem",
            "token_id": 2873,
            "logprob": -5.0312,
            "prob": 0.0064
          },
          {
            "token": "Ġwere",
            "token_id": 1051,
            "logprob": -5.1562,
            "prob": 0.0057
          },
          {
            "token": "Ġhere",
            "token_id": 1618,
            "logprob": -5.2812,
            "prob": 0.005
          },
          {
            "token": "Ġhave",
            "token_id": 617,
            "logprob": -5.6562,
            "prob": 0.0034
          },
          {
            "token": "Ġdon",
            "token_id": 1541,
            "logprob": -5.9688,
            "prob": 0.0025
          }
        ]
      },
      "selected_token": {
        "token": "Ġwere",
        "token_id": 1051,
        "selection_method": "sampling"
      }
    },
    {
      "step": 26,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters were",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters",
        "Ġwere"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885,
        1051
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġmessed",
            "token_id": 65302,
            "logprob": -0.8555,
            "prob": 0.4238
          },
          {
            "token": "Ġmixed",
            "token_id": 9709,
            "logprob": -1.7344,
            "prob": 0.1768
          },
          {
            "token": "Ġgar",
            "token_id": 7515,
            "logprob": -2.8594,
            "prob": 0.0574
          },
          {
            "token": "Ġj",
            "token_id": 503,
            "logprob": -2.8594,
            "prob": 0.0574
          },
          {
            "token": "Ġa",
            "token_id": 264,
            "logprob": -3.1094,
            "prob": 0.0447
          },
          {
            "token": "Ġprobably",
            "token_id": 4762,
            "logprob": -3.3594,
            "prob": 0.0349
          },
          {
            "token": "Ġmaybe",
            "token_id": 7344,
            "logprob": -3.5469,
            "prob": 0.0289
          },
          {
            "token": "Ġcorrupted",
            "token_id": 50500,
            "logprob": -3.9219,
            "prob": 0.0199
          },
          {
            "token": "Ġall",
            "token_id": 682,
            "logprob": -4.1562,
            "prob": 0.0154
          },
          {
            "token": "Ġsm",
            "token_id": 1554,
            "logprob": -4.1562,
            "prob": 0.0154
          }
        ]
      },
      "selected_token": {
        "token": "Ġcorrupted",
        "token_id": 50500,
        "selection_method": "sampling"
      }
    },
    {
      "step": 27,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters were corrupted",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters",
        "Ġwere",
        "Ġcorrupted"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885,
        1051,
        50500
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": ".",
            "token_id": 13,
            "logprob": -0.2988,
            "prob": 0.7422
          },
          {
            "token": "Ġmaybe",
            "token_id": 7344,
            "logprob": -2.1719,
            "prob": 0.1138
          },
          {
            "token": ",",
            "token_id": 11,
            "logprob": -3.5469,
            "prob": 0.0287
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -3.5469,
            "prob": 0.0287
          },
          {
            "token": "Ġor",
            "token_id": 477,
            "logprob": -3.9219,
            "prob": 0.0198
          },
          {
            "token": "?",
            "token_id": 30,
            "logprob": -4.0625,
            "prob": 0.0175
          },
          {
            "token": "Ġas",
            "token_id": 439,
            "logprob": -5.0625,
            "prob": 0.0064
          },
          {
            "token": "Ġa",
            "token_id": 264,
            "logprob": -5.0625,
            "prob": 0.0064
          },
          {
            "token": "Ġprobably",
            "token_id": 4762,
            "logprob": -5.0625,
            "prob": 0.0064
          },
          {
            "token": "Ġwhen",
            "token_id": 994,
            "logprob": -5.6875,
            "prob": 0.0034
          }
        ]
      },
      "selected_token": {
        "token": "Ġwhen",
        "token_id": 994,
        "selection_method": "sampling"
      }
    },
    {
      "step": 28,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters were corrupted when",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters",
        "Ġwere",
        "Ġcorrupted",
        "Ġwhen"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885,
        1051,
        50500,
        994
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġpast",
            "token_id": 3347,
            "logprob": -0.4473,
            "prob": 0.6406
          },
          {
            "token": "Ġthey",
            "token_id": 814,
            "logprob": -2.4531,
            "prob": 0.0864
          },
          {
            "token": "Ġinput",
            "token_id": 1988,
            "logprob": -2.7031,
            "prob": 0.0674
          },
          {
            "token": "Ġsent",
            "token_id": 3288,
            "logprob": -2.8281,
            "prob": 0.0593
          },
          {
            "token": "ĠI",
            "token_id": 358,
            "logprob": -3.2656,
            "prob": 0.0383
          },
          {
            "token": "Ġcopied",
            "token_id": 22523,
            "logprob": -3.9531,
            "prob": 0.0193
          },
          {
            "token": "Ġposted",
            "token_id": 8621,
            "logprob": -4.5625,
            "prob": 0.0103
          },
          {
            "token": "Ġentering",
            "token_id": 16661,
            "logprob": -4.8125,
            "prob": 0.0081
          },
          {
            "token": "Ġentered",
            "token_id": 10862,
            "logprob": -4.875,
            "prob": 0.0076
          },
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -4.9375,
            "prob": 0.0071
          }
        ]
      },
      "selected_token": {
        "token": "Ġsent",
        "token_id": 3288,
        "selection_method": "sampling"
      }
    },
    {
      "step": 29,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters were corrupted when sent",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters",
        "Ġwere",
        "Ġcorrupted",
        "Ġwhen",
        "Ġsent"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885,
        1051,
        50500,
        994,
        3288
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": ".",
            "token_id": 13,
            "logprob": -0.1475,
            "prob": 0.8633
          },
          {
            "token": "?",
            "token_id": 30,
            "logprob": -2.8906,
            "prob": 0.0552
          },
          {
            "token": ",",
            "token_id": 11,
            "logprob": -3.2656,
            "prob": 0.0378
          },
          {
            "token": "Ġmaybe",
            "token_id": 7344,
            "logprob": -3.6406,
            "prob": 0.0261
          },
          {
            "token": "Ġthrough",
            "token_id": 1555,
            "logprob": -4.6562,
            "prob": 0.0096
          },
          {
            "token": "Ġprobably",
            "token_id": 4762,
            "logprob": -6.5312,
            "prob": 0.0015
          },
          {
            "token": "Ġperhaps",
            "token_id": 8530,
            "logprob": -6.5312,
            "prob": 0.0015
          },
          {
            "token": "Ġover",
            "token_id": 927,
            "logprob": -7.1562,
            "prob": 0.0008
          },
          {
            "token": "Ġto",
            "token_id": 311,
            "logprob": -7.6562,
            "prob": 0.0005
          },
          {
            "token": "...",
            "token_id": 1131,
            "logprob": -7.6562,
            "prob": 0.0005
          }
        ]
      },
      "selected_token": {
        "token": ".",
        "token_id": 13,
        "selection_method": "sampling"
      }
    },
    {
      "step": 30,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters were corrupted when sent.",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters",
        "Ġwere",
        "Ġcorrupted",
        "Ġwhen",
        "Ġsent",
        "."
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885,
        1051,
        50500,
        994,
        3288,
        13
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "ĠWait",
            "token_id": 14144,
            "logprob": -1.1328,
            "prob": 0.3223
          },
          {
            "token": "ĠSo",
            "token_id": 2100,
            "logprob": -1.5078,
            "prob": 0.2217
          },
          {
            "token": "ĠLet",
            "token_id": 6914,
            "logprob": -1.7578,
            "prob": 0.1729
          },
          {
            "token": "ĠThe",
            "token_id": 578,
            "logprob": -1.7578,
            "prob": 0.1729
          },
          {
            "token": "ĠAnyway",
            "token_id": 42669,
            "logprob": -3.25,
            "prob": 0.0386
          },
          {
            "token": "ĠProbably",
            "token_id": 38254,
            "logprob": -3.875,
            "prob": 0.0206
          },
          {
            "token": "ĠI",
            "token_id": 358,
            "logprob": -4.5,
            "prob": 0.011
          },
          {
            "token": "ĠOh",
            "token_id": 8840,
            "logprob": -4.625,
            "prob": 0.0098
          },
          {
            "token": "ĠActually",
            "token_id": 34863,
            "logprob": -4.75,
            "prob": 0.0086
          },
          {
            "token": "ĠThey",
            "token_id": 2435,
            "logprob": -5.4375,
            "prob": 0.0043
          }
        ]
      },
      "selected_token": {
        "token": "ĠWait",
        "token_id": 14144,
        "selection_method": "sampling"
      }
    },
    {
      "step": 31,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters were corrupted when sent. Wait",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters",
        "Ġwere",
        "Ġcorrupted",
        "Ġwhen",
        "Ġsent",
        ".",
        "ĠWait"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885,
        1051,
        50500,
        994,
        3288,
        13,
        14144
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": ",",
            "token_id": 11,
            "logprob": -0.0549,
            "prob": 0.9453
          },
          {
            "token": "Ġno",
            "token_id": 912,
            "logprob": -3.5625,
            "prob": 0.0286
          },
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -4.0625,
            "prob": 0.0173
          },
          {
            "token": "Ġactually",
            "token_id": 3604,
            "logprob": -4.9375,
            "prob": 0.0072
          },
          {
            "token": "Ġ\"",
            "token_id": 330,
            "logprob": -9.0625,
            "prob": 0.0001
          },
          {
            "token": "Ġlet",
            "token_id": 1095,
            "logprob": -9.1875,
            "prob": 0.0001
          },
          {
            "token": "Ġwait",
            "token_id": 3868,
            "logprob": -10.0625,
            "prob": 0.0
          },
          {
            "token": "Ġactual",
            "token_id": 5150,
            "logprob": -10.0625,
            "prob": 0.0
          },
          {
            "token": "Ġuser",
            "token_id": 1217,
            "logprob": -10.6875,
            "prob": 0.0
          },
          {
            "token": "Ġmaybe",
            "token_id": 7344,
            "logprob": -11.6875,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġwait",
        "token_id": 3868,
        "selection_method": "sampling"
      }
    },
    {
      "step": 32,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters were corrupted when sent. Wait wait",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters",
        "Ġwere",
        "Ġcorrupted",
        "Ġwhen",
        "Ġsent",
        ".",
        "ĠWait",
        "Ġwait"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885,
        1051,
        50500,
        994,
        3288,
        13,
        14144,
        3868
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": ",",
            "token_id": 11,
            "logprob": -0.0757,
            "prob": 0.9258
          },
          {
            "token": "Ġno",
            "token_id": 912,
            "logprob": -3.5781,
            "prob": 0.028
          },
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -3.9531,
            "prob": 0.0193
          },
          {
            "token": ".",
            "token_id": 13,
            "logprob": -4.5625,
            "prob": 0.0103
          },
          {
            "token": "Ġactually",
            "token_id": 3604,
            "logprob": -5.3125,
            "prob": 0.0049
          },
          {
            "token": "Ġwait",
            "token_id": 3868,
            "logprob": -5.3125,
            "prob": 0.0049
          },
          {
            "token": "Ġlet",
            "token_id": 1095,
            "logprob": -5.4375,
            "prob": 0.0043
          },
          {
            "token": "Ġ\"",
            "token_id": 330,
            "logprob": -8.4375,
            "prob": 0.0002
          },
          {
            "token": "Ġactual",
            "token_id": 5150,
            "logprob": -8.8125,
            "prob": 0.0001
          },
          {
            "token": "ĠI",
            "token_id": 358,
            "logprob": -9.0625,
            "prob": 0.0001
          }
        ]
      },
      "selected_token": {
        "token": ",",
        "token_id": 11,
        "selection_method": "sampling"
      }
    },
    {
      "step": 33,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters were corrupted when sent. Wait wait,",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters",
        "Ġwere",
        "Ġcorrupted",
        "Ġwhen",
        "Ġsent",
        ".",
        "ĠWait",
        "Ġwait",
        ","
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885,
        1051,
        50500,
        994,
        3288,
        13,
        14144,
        3868,
        11
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġactually",
            "token_id": 3604,
            "logprob": -0.9609,
            "prob": 0.3828
          },
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -1.0859,
            "prob": 0.3379
          },
          {
            "token": "Ġno",
            "token_id": 912,
            "logprob": -2.3438,
            "prob": 0.0967
          },
          {
            "token": "Ġlet",
            "token_id": 1095,
            "logprob": -2.7188,
            "prob": 0.0664
          },
          {
            "token": "Ġ\"",
            "token_id": 330,
            "logprob": -2.9688,
            "prob": 0.0518
          },
          {
            "token": "Ġmaybe",
            "token_id": 7344,
            "logprob": -3.8438,
            "prob": 0.0216
          },
          {
            "token": "Ġlooking",
            "token_id": 3411,
            "logprob": -4.3438,
            "prob": 0.0131
          },
          {
            "token": "Ġoriginal",
            "token_id": 4113,
            "logprob": -4.5938,
            "prob": 0.0102
          },
          {
            "token": "Ġuser",
            "token_id": 1217,
            "logprob": -5.3438,
            "prob": 0.0048
          },
          {
            "token": "ĠI",
            "token_id": 358,
            "logprob": -5.7188,
            "prob": 0.0033
          }
        ]
      },
      "selected_token": {
        "token": "ĠI",
        "token_id": 358,
        "selection_method": "sampling"
      }
    },
    {
      "step": 34,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters were corrupted when sent. Wait wait, I",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters",
        "Ġwere",
        "Ġcorrupted",
        "Ġwhen",
        "Ġsent",
        ".",
        "ĠWait",
        "Ġwait",
        ",",
        "ĠI"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885,
        1051,
        50500,
        994,
        3288,
        13,
        14144,
        3868,
        11,
        358
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġneed",
            "token_id": 1205,
            "logprob": -0.4434,
            "prob": 0.6406
          },
          {
            "token": "Ġshould",
            "token_id": 1288,
            "logprob": -1.8203,
            "prob": 0.1621
          },
          {
            "token": "Ġsee",
            "token_id": 1518,
            "logprob": -2.1875,
            "prob": 0.1118
          },
          {
            "token": "Ġthink",
            "token_id": 1781,
            "logprob": -3.1875,
            "prob": 0.041
          },
          {
            "token": "Ġjust",
            "token_id": 1120,
            "logprob": -4.3125,
            "prob": 0.0133
          },
          {
            "token": "Ġgot",
            "token_id": 2751,
            "logprob": -4.3125,
            "prob": 0.0133
          },
          {
            "token": "'ll",
            "token_id": 3358,
            "logprob": -6.0625,
            "prob": 0.0023
          },
          {
            "token": "Ġnotice",
            "token_id": 5406,
            "logprob": -6.3125,
            "prob": 0.0018
          },
          {
            "token": "Ġwrote",
            "token_id": 6267,
            "logprob": -6.375,
            "prob": 0.0017
          },
          {
            "token": "Ġcan",
            "token_id": 649,
            "logprob": -6.75,
            "prob": 0.0012
          }
        ]
      },
      "selected_token": {
        "token": "Ġjust",
        "token_id": 1120,
        "selection_method": "sampling"
      }
    },
    {
      "step": 35,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters were corrupted when sent. Wait wait, I just",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters",
        "Ġwere",
        "Ġcorrupted",
        "Ġwhen",
        "Ġsent",
        ".",
        "ĠWait",
        "Ġwait",
        ",",
        "ĠI",
        "Ġjust"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885,
        1051,
        50500,
        994,
        3288,
        13,
        14144,
        3868,
        11,
        358,
        1120
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġnoticed",
            "token_id": 14000,
            "logprob": -1.2031,
            "prob": 0.3008
          },
          {
            "token": "Ġsaw",
            "token_id": 5602,
            "logprob": -1.2031,
            "prob": 0.3008
          },
          {
            "token": "Ġneed",
            "token_id": 1205,
            "logprob": -1.7031,
            "prob": 0.1826
          },
          {
            "token": "Ġrealized",
            "token_id": 15393,
            "logprob": -2.7031,
            "prob": 0.0674
          },
          {
            "token": "Ġgot",
            "token_id": 2751,
            "logprob": -3.0781,
            "prob": 0.0461
          },
          {
            "token": "Ġnotice",
            "token_id": 5406,
            "logprob": -4.125,
            "prob": 0.016
          },
          {
            "token": "Ġlooked",
            "token_id": 7111,
            "logprob": -4.1875,
            "prob": 0.015
          },
          {
            "token": "Ġlook",
            "token_id": 1427,
            "logprob": -4.3125,
            "prob": 0.0132
          },
          {
            "token": "Ġread",
            "token_id": 1373,
            "logprob": -4.3125,
            "prob": 0.0132
          },
          {
            "token": "Ġsee",
            "token_id": 1518,
            "logprob": -4.5,
            "prob": 0.011
          }
        ]
      },
      "selected_token": {
        "token": "Ġneed",
        "token_id": 1205,
        "selection_method": "sampling"
      }
    },
    {
      "step": 36,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters were corrupted when sent. Wait wait, I just need",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters",
        "Ġwere",
        "Ġcorrupted",
        "Ġwhen",
        "Ġsent",
        ".",
        "ĠWait",
        "Ġwait",
        ",",
        "ĠI",
        "Ġjust",
        "Ġneed"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885,
        1051,
        50500,
        994,
        3288,
        13,
        14144,
        3868,
        11,
        358,
        1120,
        1205
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġto",
            "token_id": 311,
            "logprob": -0.0001,
            "prob": 1.0
          },
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -10.25,
            "prob": 0.0
          },
          {
            "token": "Ġtranslate",
            "token_id": 15025,
            "logprob": -12.4375,
            "prob": 0.0
          },
          {
            "token": "to",
            "token_id": 998,
            "logprob": -12.625,
            "prob": 0.0
          },
          {
            "token": "Ġcorrect",
            "token_id": 4495,
            "logprob": -12.875,
            "prob": 0.0
          },
          {
            "token": "Ġrecognize",
            "token_id": 15641,
            "logprob": -12.9375,
            "prob": 0.0
          },
          {
            "token": "Ġparse",
            "token_id": 4820,
            "logprob": -13.25,
            "prob": 0.0
          },
          {
            "token": "Ġread",
            "token_id": 1373,
            "logprob": -13.3125,
            "prob": 0.0
          },
          {
            "token": "Ġfix",
            "token_id": 5155,
            "logprob": -13.6875,
            "prob": 0.0
          },
          {
            "token": "Ġlook",
            "token_id": 1427,
            "logprob": -13.75,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġto",
        "token_id": 311,
        "selection_method": "sampling"
      }
    },
    {
      "step": 37,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters were corrupted when sent. Wait wait, I just need to",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters",
        "Ġwere",
        "Ġcorrupted",
        "Ġwhen",
        "Ġsent",
        ".",
        "ĠWait",
        "Ġwait",
        ",",
        "ĠI",
        "Ġjust",
        "Ġneed",
        "Ġto"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885,
        1051,
        50500,
        994,
        3288,
        13,
        14144,
        3868,
        11,
        358,
        1120,
        1205,
        311
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġparse",
            "token_id": 4820,
            "logprob": -1.5234,
            "prob": 0.2178
          },
          {
            "token": "Ġcorrect",
            "token_id": 4495,
            "logprob": -2.2656,
            "prob": 0.103
          },
          {
            "token": "Ġrespond",
            "token_id": 6013,
            "logprob": -2.5156,
            "prob": 0.0801
          },
          {
            "token": "Ġcheck",
            "token_id": 1817,
            "logprob": -2.5156,
            "prob": 0.0801
          },
          {
            "token": "Ġlook",
            "token_id": 1427,
            "logprob": -2.5156,
            "prob": 0.0801
          },
          {
            "token": "Ġrecognize",
            "token_id": 15641,
            "logprob": -2.6406,
            "prob": 0.0708
          },
          {
            "token": "Ġanswer",
            "token_id": 4320,
            "logprob": -2.8906,
            "prob": 0.0552
          },
          {
            "token": "Ġtype",
            "token_id": 955,
            "logprob": -2.8906,
            "prob": 0.0552
          },
          {
            "token": "Ġfigure",
            "token_id": 7216,
            "logprob": -3.1406,
            "prob": 0.043
          },
          {
            "token": "Ġtranslate",
            "token_id": 15025,
            "logprob": -3.1406,
            "prob": 0.043
          }
        ]
      },
      "selected_token": {
        "token": "Ġtype",
        "token_id": 955,
        "selection_method": "sampling"
      }
    },
    {
      "step": 38,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters were corrupted when sent. Wait wait, I just need to type",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters",
        "Ġwere",
        "Ġcorrupted",
        "Ġwhen",
        "Ġsent",
        ".",
        "ĠWait",
        "Ġwait",
        ",",
        "ĠI",
        "Ġjust",
        "Ġneed",
        "Ġto",
        "Ġtype"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885,
        1051,
        50500,
        994,
        3288,
        13,
        14144,
        3868,
        11,
        358,
        1120,
        1205,
        311,
        955
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -0.4199,
            "prob": 0.6562
          },
          {
            "token": "Ġout",
            "token_id": 704,
            "logprob": -1.8594,
            "prob": 0.1562
          },
          {
            "token": "Ġ\"",
            "token_id": 330,
            "logprob": -2.4219,
            "prob": 0.0889
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -3.4844,
            "prob": 0.0308
          },
          {
            "token": "Ġit",
            "token_id": 433,
            "logprob": -4.0312,
            "prob": 0.0175
          },
          {
            "token": "Ġwhat",
            "token_id": 1148,
            "logprob": -5.0312,
            "prob": 0.0064
          },
          {
            "token": "Ġqui",
            "token_id": 7930,
            "logprob": -5.1562,
            "prob": 0.0057
          },
          {
            "token": "Ġcorrectly",
            "token_id": 12722,
            "logprob": -5.2188,
            "prob": 0.0053
          },
          {
            "token": "Ġwho",
            "token_id": 889,
            "logprob": -5.2188,
            "prob": 0.0053
          },
          {
            "token": "Ġthat",
            "token_id": 430,
            "logprob": -5.4062,
            "prob": 0.0044
          }
        ]
      },
      "selected_token": {
        "token": "Ġthe",
        "token_id": 279,
        "selection_method": "sampling"
      }
    },
    {
      "step": 39,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters were corrupted when sent. Wait wait, I just need to type the",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters",
        "Ġwere",
        "Ġcorrupted",
        "Ġwhen",
        "Ġsent",
        ".",
        "ĠWait",
        "Ġwait",
        ",",
        "ĠI",
        "Ġjust",
        "Ġneed",
        "Ġto",
        "Ġtype",
        "Ġthe"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885,
        1051,
        50500,
        994,
        3288,
        13,
        14144,
        3868,
        11,
        358,
        1120,
        1205,
        311,
        955,
        279
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġcorrect",
            "token_id": 4495,
            "logprob": -0.3164,
            "prob": 0.7305
          },
          {
            "token": "ĠFrench",
            "token_id": 8753,
            "logprob": -2.8125,
            "prob": 0.0598
          },
          {
            "token": "Ġquestion",
            "token_id": 3488,
            "logprob": -2.9375,
            "prob": 0.0527
          },
          {
            "token": "Ġproper",
            "token_id": 6300,
            "logprob": -3.0625,
            "prob": 0.0466
          },
          {
            "token": "Ġright",
            "token_id": 1314,
            "logprob": -3.5625,
            "prob": 0.0283
          },
          {
            "token": "Ġactual",
            "token_id": 5150,
            "logprob": -3.9375,
            "prob": 0.0194
          },
          {
            "token": "Ġresponse",
            "token_id": 2077,
            "logprob": -3.9375,
            "prob": 0.0194
          },
          {
            "token": "Ġanswer",
            "token_id": 4320,
            "logprob": -4.6875,
            "prob": 0.0092
          },
          {
            "token": "Ġuser",
            "token_id": 1217,
            "logprob": -4.8125,
            "prob": 0.0081
          },
          {
            "token": "Ġtranslation",
            "token_id": 14807,
            "logprob": -4.9375,
            "prob": 0.0071
          }
        ]
      },
      "selected_token": {
        "token": "Ġresponse",
        "token_id": 2077,
        "selection_method": "sampling"
      }
    },
    {
      "step": 40,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters were corrupted when sent. Wait wait, I just need to type the response",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters",
        "Ġwere",
        "Ġcorrupted",
        "Ġwhen",
        "Ġsent",
        ".",
        "ĠWait",
        "Ġwait",
        ",",
        "ĠI",
        "Ġjust",
        "Ġneed",
        "Ġto",
        "Ġtype",
        "Ġthe",
        "Ġresponse"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885,
        1051,
        50500,
        994,
        3288,
        13,
        14144,
        3868,
        11,
        358,
        1120,
        1205,
        311,
        955,
        279,
        2077
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -0.4785,
            "prob": 0.6211
          },
          {
            "token": "Ġproperly",
            "token_id": 10489,
            "logprob": -1.8516,
            "prob": 0.1562
          },
          {
            "token": ".",
            "token_id": 13,
            "logprob": -2.1094,
            "prob": 0.1221
          },
          {
            "token": "Ġcorrectly",
            "token_id": 12722,
            "logprob": -2.4844,
            "prob": 0.084
          },
          {
            "token": ".\n\n",
            "token_id": 382,
            "logprob": -5.0938,
            "prob": 0.0061
          },
          {
            "token": "Ġnormally",
            "token_id": 14614,
            "logprob": -6.0938,
            "prob": 0.0022
          },
          {
            "token": "Ġappropriately",
            "token_id": 36001,
            "logprob": -6.3438,
            "prob": 0.0017
          },
          {
            "token": ",",
            "token_id": 11,
            "logprob": -6.4688,
            "prob": 0.0015
          },
          {
            "token": "Ġas",
            "token_id": 439,
            "logprob": -6.4688,
            "prob": 0.0015
          },
          {
            "token": "Ġhere",
            "token_id": 1618,
            "logprob": -6.5938,
            "prob": 0.0014
          }
        ]
      },
      "selected_token": {
        "token": "Ġnormally",
        "token_id": 14614,
        "selection_method": "sampling"
      }
    },
    {
      "step": 41,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters were corrupted when sent. Wait wait, I just need to type the response normally",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters",
        "Ġwere",
        "Ġcorrupted",
        "Ġwhen",
        "Ġsent",
        ".",
        "ĠWait",
        "Ġwait",
        ",",
        "ĠI",
        "Ġjust",
        "Ġneed",
        "Ġto",
        "Ġtype",
        "Ġthe",
        "Ġresponse",
        "Ġnormally"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885,
        1051,
        50500,
        994,
        3288,
        13,
        14144,
        3868,
        11,
        358,
        1120,
        1205,
        311,
        955,
        279,
        2077,
        14614
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": ".",
            "token_id": 13,
            "logprob": -0.3359,
            "prob": 0.7148
          },
          {
            "token": "?",
            "token_id": 30,
            "logprob": -2.4531,
            "prob": 0.0854
          },
          {
            "token": ",",
            "token_id": 11,
            "logprob": -2.8281,
            "prob": 0.0586
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -2.9531,
            "prob": 0.0518
          },
          {
            "token": ".\n\n",
            "token_id": 382,
            "logprob": -3.4531,
            "prob": 0.0315
          },
          {
            "token": "Ġthen",
            "token_id": 1243,
            "logprob": -3.7031,
            "prob": 0.0244
          },
          {
            "token": "Ġsince",
            "token_id": 2533,
            "logprob": -4.2188,
            "prob": 0.0148
          },
          {
            "token": "Ġbut",
            "token_id": 719,
            "logprob": -4.8438,
            "prob": 0.0079
          },
          {
            "token": "?\n\n",
            "token_id": 1980,
            "logprob": -5.4688,
            "prob": 0.0042
          },
          {
            "token": "Ġbecause",
            "token_id": 1606,
            "logprob": -5.7188,
            "prob": 0.0033
          }
        ]
      },
      "selected_token": {
        "token": "Ġthen",
        "token_id": 1243,
        "selection_method": "sampling"
      }
    },
    {
      "step": 42,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters were corrupted when sent. Wait wait, I just need to type the response normally then",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters",
        "Ġwere",
        "Ġcorrupted",
        "Ġwhen",
        "Ġsent",
        ".",
        "ĠWait",
        "Ġwait",
        ",",
        "ĠI",
        "Ġjust",
        "Ġneed",
        "Ġto",
        "Ġtype",
        "Ġthe",
        "Ġresponse",
        "Ġnormally",
        "Ġthen"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885,
        1051,
        50500,
        994,
        3288,
        13,
        14144,
        3868,
        11,
        358,
        1120,
        1205,
        311,
        955,
        279,
        2077,
        14614,
        1243
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": ".",
            "token_id": 13,
            "logprob": -0.3789,
            "prob": 0.6836
          },
          {
            "token": "?",
            "token_id": 30,
            "logprob": -1.5078,
            "prob": 0.2227
          },
          {
            "token": ".\n\n",
            "token_id": 382,
            "logprob": -3.0,
            "prob": 0.0496
          },
          {
            "token": ",",
            "token_id": 11,
            "logprob": -3.625,
            "prob": 0.0265
          },
          {
            "token": "?\n\n",
            "token_id": 1980,
            "logprob": -4.375,
            "prob": 0.0125
          },
          {
            "token": "Ġtranslate",
            "token_id": 15025,
            "logprob": -5.625,
            "prob": 0.0036
          },
          {
            "token": "Ġmaybe",
            "token_id": 7344,
            "logprob": -8.125,
            "prob": 0.0003
          },
          {
            "token": "Ġright",
            "token_id": 1314,
            "logprob": -9.375,
            "prob": 0.0001
          },
          {
            "token": "Ġsince",
            "token_id": 2533,
            "logprob": -9.5,
            "prob": 0.0001
          },
          {
            "token": "Ġcorrect",
            "token_id": 4495,
            "logprob": -9.625,
            "prob": 0.0001
          }
        ]
      },
      "selected_token": {
        "token": "Ġsince",
        "token_id": 2533,
        "selection_method": "sampling"
      }
    },
    {
      "step": 43,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters were corrupted when sent. Wait wait, I just need to type the response normally then since",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters",
        "Ġwere",
        "Ġcorrupted",
        "Ġwhen",
        "Ġsent",
        ".",
        "ĠWait",
        "Ġwait",
        ",",
        "ĠI",
        "Ġjust",
        "Ġneed",
        "Ġto",
        "Ġtype",
        "Ġthe",
        "Ġresponse",
        "Ġnormally",
        "Ġthen",
        "Ġsince"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885,
        1051,
        50500,
        994,
        3288,
        13,
        14144,
        3868,
        11,
        358,
        1120,
        1205,
        311,
        955,
        279,
        2077,
        14614,
        1243,
        2533
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -0.6172,
            "prob": 0.5391
          },
          {
            "token": "ĠI",
            "token_id": 358,
            "logprob": -1.2422,
            "prob": 0.2891
          },
          {
            "token": "Ġthey",
            "token_id": 814,
            "logprob": -2.875,
            "prob": 0.0569
          },
          {
            "token": "Ġit",
            "token_id": 433,
            "logprob": -2.875,
            "prob": 0.0569
          },
          {
            "token": "Ġmy",
            "token_id": 856,
            "logprob": -3.25,
            "prob": 0.0391
          },
          {
            "token": "ĠFrench",
            "token_id": 8753,
            "logprob": -5.375,
            "prob": 0.0047
          },
          {
            "token": "Ġthat",
            "token_id": 430,
            "logprob": -5.5,
            "prob": 0.0041
          },
          {
            "token": "Ġeven",
            "token_id": 1524,
            "logprob": -6.5,
            "prob": 0.0015
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -6.6875,
            "prob": 0.0013
          },
          {
            "token": "Ġthis",
            "token_id": 420,
            "logprob": -6.6875,
            "prob": 0.0013
          }
        ]
      },
      "selected_token": {
        "token": "Ġthey",
        "token_id": 814,
        "selection_method": "sampling"
      }
    },
    {
      "step": 44,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters were corrupted when sent. Wait wait, I just need to type the response normally then since they",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters",
        "Ġwere",
        "Ġcorrupted",
        "Ġwhen",
        "Ġsent",
        ".",
        "ĠWait",
        "Ġwait",
        ",",
        "ĠI",
        "Ġjust",
        "Ġneed",
        "Ġto",
        "Ġtype",
        "Ġthe",
        "Ġresponse",
        "Ġnormally",
        "Ġthen",
        "Ġsince",
        "Ġthey"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885,
        1051,
        50500,
        994,
        3288,
        13,
        14144,
        3868,
        11,
        358,
        1120,
        1205,
        311,
        955,
        279,
        2077,
        14614,
        1243,
        2533,
        814
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġasked",
            "token_id": 4691,
            "logprob": -1.75,
            "prob": 0.1738
          },
          {
            "token": "Ġmight",
            "token_id": 2643,
            "logprob": -1.75,
            "prob": 0.1738
          },
          {
            "token": "Ġused",
            "token_id": 1511,
            "logprob": -1.875,
            "prob": 0.1533
          },
          {
            "token": "'re",
            "token_id": 2351,
            "logprob": -2.125,
            "prob": 0.1191
          },
          {
            "token": "Ġprobably",
            "token_id": 4762,
            "logprob": -2.25,
            "prob": 0.1055
          },
          {
            "token": "Ġwrote",
            "token_id": 6267,
            "logprob": -2.25,
            "prob": 0.1055
          },
          {
            "token": "Ġcan",
            "token_id": 649,
            "logprob": -2.875,
            "prob": 0.0564
          },
          {
            "token": "Ġjust",
            "token_id": 1120,
            "logprob": -4.0,
            "prob": 0.0183
          },
          {
            "token": "Ġunderstand",
            "token_id": 3619,
            "logprob": -4.125,
            "prob": 0.0161
          },
          {
            "token": "Ġsaid",
            "token_id": 1071,
            "logprob": -4.375,
            "prob": 0.0126
          }
        ]
      },
      "selected_token": {
        "token": "Ġused",
        "token_id": 1511,
        "selection_method": "sampling"
      }
    },
    {
      "step": 45,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters were corrupted when sent. Wait wait, I just need to type the response normally then since they used",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters",
        "Ġwere",
        "Ġcorrupted",
        "Ġwhen",
        "Ġsent",
        ".",
        "ĠWait",
        "Ġwait",
        ",",
        "ĠI",
        "Ġjust",
        "Ġneed",
        "Ġto",
        "Ġtype",
        "Ġthe",
        "Ġresponse",
        "Ġnormally",
        "Ġthen",
        "Ġsince",
        "Ġthey",
        "Ġused"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885,
        1051,
        50500,
        994,
        3288,
        13,
        14144,
        3868,
        11,
        358,
        1120,
        1205,
        311,
        955,
        279,
        2077,
        14614,
        1243,
        2533,
        814,
        1511
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "ĠFrench",
            "token_id": 8753,
            "logprob": -0.0258,
            "prob": 0.9727
          },
          {
            "token": "Ġa",
            "token_id": 264,
            "logprob": -4.9688,
            "prob": 0.007
          },
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -5.2188,
            "prob": 0.0054
          },
          {
            "token": "Ġbroken",
            "token_id": 11102,
            "logprob": -5.7812,
            "prob": 0.0031
          },
          {
            "token": "ĠEnglish",
            "token_id": 6498,
            "logprob": -6.3438,
            "prob": 0.0018
          },
          {
            "token": "Ġcorrect",
            "token_id": 4495,
            "logprob": -6.7812,
            "prob": 0.0011
          },
          {
            "token": "Ġproper",
            "token_id": 6300,
            "logprob": -6.9688,
            "prob": 0.0009
          },
          {
            "token": "Ġsome",
            "token_id": 1063,
            "logprob": -7.0938,
            "prob": 0.0008
          },
          {
            "token": "Ġincorrect",
            "token_id": 15465,
            "logprob": -7.3438,
            "prob": 0.0006
          },
          {
            "token": "ĠChinese",
            "token_id": 8620,
            "logprob": -7.5312,
            "prob": 0.0005
          }
        ]
      },
      "selected_token": {
        "token": "ĠFrench",
        "token_id": 8753,
        "selection_method": "sampling"
      }
    },
    {
      "step": 46,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters were corrupted when sent. Wait wait, I just need to type the response normally then since they used French",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters",
        "Ġwere",
        "Ġcorrupted",
        "Ġwhen",
        "Ġsent",
        ".",
        "ĠWait",
        "Ġwait",
        ",",
        "ĠI",
        "Ġjust",
        "Ġneed",
        "Ġto",
        "Ġtype",
        "Ġthe",
        "Ġresponse",
        "Ġnormally",
        "Ġthen",
        "Ġsince",
        "Ġthey",
        "Ġused",
        "ĠFrench"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885,
        1051,
        50500,
        994,
        3288,
        13,
        14144,
        3868,
        11,
        358,
        1120,
        1205,
        311,
        955,
        279,
        2077,
        14614,
        1243,
        2533,
        814,
        1511,
        8753
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": ".",
            "token_id": 13,
            "logprob": -0.1768,
            "prob": 0.8398
          },
          {
            "token": ",",
            "token_id": 11,
            "logprob": -3.0469,
            "prob": 0.0474
          },
          {
            "token": ".\n\n",
            "token_id": 382,
            "logprob": -3.0469,
            "prob": 0.0474
          },
          {
            "token": "Ġbut",
            "token_id": 719,
            "logprob": -3.0469,
            "prob": 0.0474
          },
          {
            "token": "?",
            "token_id": 30,
            "logprob": -4.1875,
            "prob": 0.0154
          },
          {
            "token": "Ġto",
            "token_id": 311,
            "logprob": -6.9375,
            "prob": 0.001
          },
          {
            "token": "Ġfor",
            "token_id": 369,
            "logprob": -7.1875,
            "prob": 0.0008
          },
          {
            "token": "?\n\n",
            "token_id": 1980,
            "logprob": -7.3125,
            "prob": 0.0007
          },
          {
            "token": "Ġhere",
            "token_id": 1618,
            "logprob": -7.3125,
            "prob": 0.0007
          },
          {
            "token": "Ġmaybe",
            "token_id": 7344,
            "logprob": -8.375,
            "prob": 0.0002
          }
        ]
      },
      "selected_token": {
        "token": ".",
        "token_id": 13,
        "selection_method": "sampling"
      }
    },
    {
      "step": 47,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters were corrupted when sent. Wait wait, I just need to type the response normally then since they used French.",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters",
        "Ġwere",
        "Ġcorrupted",
        "Ġwhen",
        "Ġsent",
        ".",
        "ĠWait",
        "Ġwait",
        ",",
        "ĠI",
        "Ġjust",
        "Ġneed",
        "Ġto",
        "Ġtype",
        "Ġthe",
        "Ġresponse",
        "Ġnormally",
        "Ġthen",
        "Ġsince",
        "Ġthey",
        "Ġused",
        "ĠFrench",
        "."
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885,
        1051,
        50500,
        994,
        3288,
        13,
        14144,
        3868,
        11,
        358,
        1120,
        1205,
        311,
        955,
        279,
        2077,
        14614,
        1243,
        2533,
        814,
        1511,
        8753,
        13
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "ĠLet",
            "token_id": 6914,
            "logprob": -0.7461,
            "prob": 0.4746
          },
          {
            "token": "ĠThe",
            "token_id": 578,
            "logprob": -1.4922,
            "prob": 0.2236
          },
          {
            "token": "ĠWait",
            "token_id": 14144,
            "logprob": -2.125,
            "prob": 0.1201
          },
          {
            "token": "ĠBut",
            "token_id": 2030,
            "logprob": -2.875,
            "prob": 0.0566
          },
          {
            "token": "ĠSo",
            "token_id": 2100,
            "logprob": -3.25,
            "prob": 0.0388
          },
          {
            "token": "Ġ\"",
            "token_id": 330,
            "logprob": -4.0,
            "prob": 0.0184
          },
          {
            "token": "ĠThey",
            "token_id": 2435,
            "logprob": -4.375,
            "prob": 0.0126
          },
          {
            "token": "ĠMaybe",
            "token_id": 10926,
            "logprob": -4.625,
            "prob": 0.0098
          },
          {
            "token": "ĠMy",
            "token_id": 3092,
            "logprob": -4.75,
            "prob": 0.0087
          },
          {
            "token": "ĠFirst",
            "token_id": 5629,
            "logprob": -4.875,
            "prob": 0.0077
          }
        ]
      },
      "selected_token": {
        "token": "ĠMaybe",
        "token_id": 10926,
        "selection_method": "sampling"
      }
    },
    {
      "step": 48,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters were corrupted when sent. Wait wait, I just need to type the response normally then since they used French. Maybe",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters",
        "Ġwere",
        "Ġcorrupted",
        "Ġwhen",
        "Ġsent",
        ".",
        "ĠWait",
        "Ġwait",
        ",",
        "ĠI",
        "Ġjust",
        "Ġneed",
        "Ġto",
        "Ġtype",
        "Ġthe",
        "Ġresponse",
        "Ġnormally",
        "Ġthen",
        "Ġsince",
        "Ġthey",
        "Ġused",
        "ĠFrench",
        ".",
        "ĠMaybe"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885,
        1051,
        50500,
        994,
        3288,
        13,
        14144,
        3868,
        11,
        358,
        1120,
        1205,
        311,
        955,
        279,
        2077,
        14614,
        1243,
        2533,
        814,
        1511,
        8753,
        13,
        10926
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġthey",
            "token_id": 814,
            "logprob": -0.1514,
            "prob": 0.8594
          },
          {
            "token": "ĠI",
            "token_id": 358,
            "logprob": -2.4062,
            "prob": 0.0908
          },
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -3.7812,
            "prob": 0.0229
          },
          {
            "token": "Ġjust",
            "token_id": 1120,
            "logprob": -4.9062,
            "prob": 0.0074
          },
          {
            "token": "Ġstart",
            "token_id": 1212,
            "logprob": -5.4062,
            "prob": 0.0045
          },
          {
            "token": "Ġ\"",
            "token_id": 330,
            "logprob": -6.0312,
            "prob": 0.0024
          },
          {
            "token": "Ġtranslate",
            "token_id": 15025,
            "logprob": -6.1562,
            "prob": 0.0021
          },
          {
            "token": "Ġit",
            "token_id": 433,
            "logprob": -6.2812,
            "prob": 0.0019
          },
          {
            "token": "Ġfirst",
            "token_id": 1176,
            "logprob": -6.5312,
            "prob": 0.0015
          },
          {
            "token": "Ġbetter",
            "token_id": 2731,
            "logprob": -6.7812,
            "prob": 0.0011
          }
        ]
      },
      "selected_token": {
        "token": "Ġtranslate",
        "token_id": 15025,
        "selection_method": "sampling"
      }
    },
    {
      "step": 49,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters were corrupted when sent. Wait wait, I just need to type the response normally then since they used French. Maybe translate",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters",
        "Ġwere",
        "Ġcorrupted",
        "Ġwhen",
        "Ġsent",
        ".",
        "ĠWait",
        "Ġwait",
        ",",
        "ĠI",
        "Ġjust",
        "Ġneed",
        "Ġto",
        "Ġtype",
        "Ġthe",
        "Ġresponse",
        "Ġnormally",
        "Ġthen",
        "Ġsince",
        "Ġthey",
        "Ġused",
        "ĠFrench",
        ".",
        "ĠMaybe",
        "Ġtranslate"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885,
        1051,
        50500,
        994,
        3288,
        13,
        14144,
        3868,
        11,
        358,
        1120,
        1205,
        311,
        955,
        279,
        2077,
        14614,
        1243,
        2533,
        814,
        1511,
        8753,
        13,
        10926,
        15025
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -0.6484,
            "prob": 0.5234
          },
          {
            "token": "Ġmy",
            "token_id": 856,
            "logprob": -1.3984,
            "prob": 0.2471
          },
          {
            "token": "Ġtheir",
            "token_id": 872,
            "logprob": -1.7734,
            "prob": 0.1699
          },
          {
            "token": "Ġto",
            "token_id": 311,
            "logprob": -3.7656,
            "prob": 0.0229
          },
          {
            "token": "Ġback",
            "token_id": 1203,
            "logprob": -4.4062,
            "prob": 0.0123
          },
          {
            "token": "?",
            "token_id": 30,
            "logprob": -5.4062,
            "prob": 0.0045
          },
          {
            "token": "Ġfirst",
            "token_id": 1176,
            "logprob": -5.5312,
            "prob": 0.004
          },
          {
            "token": "Ġ\"",
            "token_id": 330,
            "logprob": -5.9062,
            "prob": 0.0027
          },
          {
            "token": "Ġthat",
            "token_id": 430,
            "logprob": -6.1562,
            "prob": 0.0021
          },
          {
            "token": "Ġit",
            "token_id": 433,
            "logprob": -6.1562,
            "prob": 0.0021
          }
        ]
      },
      "selected_token": {
        "token": "Ġmy",
        "token_id": 856,
        "selection_method": "sampling"
      }
    },
    {
      "step": 50,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters were corrupted when sent. Wait wait, I just need to type the response normally then since they used French. Maybe translate my",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters",
        "Ġwere",
        "Ġcorrupted",
        "Ġwhen",
        "Ġsent",
        ".",
        "ĠWait",
        "Ġwait",
        ",",
        "ĠI",
        "Ġjust",
        "Ġneed",
        "Ġto",
        "Ġtype",
        "Ġthe",
        "Ġresponse",
        "Ġnormally",
        "Ġthen",
        "Ġsince",
        "Ġthey",
        "Ġused",
        "ĠFrench",
        ".",
        "ĠMaybe",
        "Ġtranslate",
        "Ġmy"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885,
        1051,
        50500,
        994,
        3288,
        13,
        14144,
        3868,
        11,
        358,
        1120,
        1205,
        311,
        955,
        279,
        2077,
        14614,
        1243,
        2533,
        814,
        1511,
        8753,
        13,
        10926,
        15025,
        856
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġanswer",
            "token_id": 4320,
            "logprob": -0.8555,
            "prob": 0.4258
          },
          {
            "token": "Ġresponse",
            "token_id": 2077,
            "logprob": -1.3516,
            "prob": 0.2578
          },
          {
            "token": "Ġusual",
            "token_id": 13783,
            "logprob": -2.4844,
            "prob": 0.084
          },
          {
            "token": "Ġstandard",
            "token_id": 5410,
            "logprob": -2.7344,
            "prob": 0.0654
          },
          {
            "token": "Ġreply",
            "token_id": 10052,
            "logprob": -2.8594,
            "prob": 0.0576
          },
          {
            "token": "Ġthought",
            "token_id": 3463,
            "logprob": -3.3594,
            "prob": 0.0349
          },
          {
            "token": "ĠEnglish",
            "token_id": 6498,
            "logprob": -3.4844,
            "prob": 0.0309
          },
          {
            "token": "Ġthoughts",
            "token_id": 11555,
            "logprob": -4.3438,
            "prob": 0.0129
          },
          {
            "token": "Ġown",
            "token_id": 1866,
            "logprob": -4.5938,
            "prob": 0.01
          },
          {
            "token": "Ġinternal",
            "token_id": 5419,
            "logprob": -4.9688,
            "prob": 0.0069
          }
        ]
      },
      "selected_token": {
        "token": "Ġown",
        "token_id": 1866,
        "selection_method": "sampling"
      }
    },
    {
      "step": 51,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters were corrupted when sent. Wait wait, I just need to type the response normally then since they used French. Maybe translate my own",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters",
        "Ġwere",
        "Ġcorrupted",
        "Ġwhen",
        "Ġsent",
        ".",
        "ĠWait",
        "Ġwait",
        ",",
        "ĠI",
        "Ġjust",
        "Ġneed",
        "Ġto",
        "Ġtype",
        "Ġthe",
        "Ġresponse",
        "Ġnormally",
        "Ġthen",
        "Ġsince",
        "Ġthey",
        "Ġused",
        "ĠFrench",
        ".",
        "ĠMaybe",
        "Ġtranslate",
        "Ġmy",
        "Ġown"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885,
        1051,
        50500,
        994,
        3288,
        13,
        14144,
        3868,
        11,
        358,
        1120,
        1205,
        311,
        955,
        279,
        2077,
        14614,
        1243,
        2533,
        814,
        1511,
        8753,
        13,
        10926,
        15025,
        856,
        1866
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġanswer",
            "token_id": 4320,
            "logprob": -0.9453,
            "prob": 0.3887
          },
          {
            "token": "Ġresponse",
            "token_id": 2077,
            "logprob": -1.0703,
            "prob": 0.3418
          },
          {
            "token": "Ġthought",
            "token_id": 3463,
            "logprob": -2.2031,
            "prob": 0.1113
          },
          {
            "token": "Ġthoughts",
            "token_id": 11555,
            "logprob": -3.0781,
            "prob": 0.0464
          },
          {
            "token": "Ġthinking",
            "token_id": 7422,
            "logprob": -3.2031,
            "prob": 0.0408
          },
          {
            "token": "Ġreply",
            "token_id": 10052,
            "logprob": -3.5781,
            "prob": 0.0281
          },
          {
            "token": "Ġinternal",
            "token_id": 5419,
            "logprob": -4.25,
            "prob": 0.0141
          },
          {
            "token": "ĠEnglish",
            "token_id": 6498,
            "logprob": -4.875,
            "prob": 0.0076
          },
          {
            "token": "Ġquestion",
            "token_id": 3488,
            "logprob": -5.5,
            "prob": 0.0041
          },
          {
            "token": "ĠFrench",
            "token_id": 8753,
            "logprob": -6.25,
            "prob": 0.0019
          }
        ]
      },
      "selected_token": {
        "token": "ĠEnglish",
        "token_id": 6498,
        "selection_method": "sampling"
      }
    },
    {
      "step": 52,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters were corrupted when sent. Wait wait, I just need to type the response normally then since they used French. Maybe translate my own English",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters",
        "Ġwere",
        "Ġcorrupted",
        "Ġwhen",
        "Ġsent",
        ".",
        "ĠWait",
        "Ġwait",
        ",",
        "ĠI",
        "Ġjust",
        "Ġneed",
        "Ġto",
        "Ġtype",
        "Ġthe",
        "Ġresponse",
        "Ġnormally",
        "Ġthen",
        "Ġsince",
        "Ġthey",
        "Ġused",
        "ĠFrench",
        ".",
        "ĠMaybe",
        "Ġtranslate",
        "Ġmy",
        "Ġown",
        "ĠEnglish"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885,
        1051,
        50500,
        994,
        3288,
        13,
        14144,
        3868,
        11,
        358,
        1120,
        1205,
        311,
        955,
        279,
        2077,
        14614,
        1243,
        2533,
        814,
        1511,
        8753,
        13,
        10926,
        15025,
        856,
        1866,
        6498
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġresponse",
            "token_id": 2077,
            "logprob": -0.3516,
            "prob": 0.7031
          },
          {
            "token": "Ġanswer",
            "token_id": 4320,
            "logprob": -1.8516,
            "prob": 0.1572
          },
          {
            "token": "Ġthought",
            "token_id": 3463,
            "logprob": -2.7344,
            "prob": 0.0654
          },
          {
            "token": "Ġthoughts",
            "token_id": 11555,
            "logprob": -3.2344,
            "prob": 0.0396
          },
          {
            "token": "Ġreply",
            "token_id": 10052,
            "logprob": -4.2188,
            "prob": 0.0146
          },
          {
            "token": "Ġthinking",
            "token_id": 7422,
            "logprob": -5.2188,
            "prob": 0.0054
          },
          {
            "token": "Ġexplanation",
            "token_id": 16540,
            "logprob": -5.3438,
            "prob": 0.0047
          },
          {
            "token": "Ġinternal",
            "token_id": 5419,
            "logprob": -5.8438,
            "prob": 0.0029
          },
          {
            "token": "Ġresponses",
            "token_id": 14847,
            "logprob": -7.2188,
            "prob": 0.0007
          },
          {
            "token": "Ġto",
            "token_id": 311,
            "logprob": -7.2812,
            "prob": 0.0007
          }
        ]
      },
      "selected_token": {
        "token": "Ġto",
        "token_id": 311,
        "selection_method": "sampling"
      }
    },
    {
      "step": 53,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters were corrupted when sent. Wait wait, I just need to type the response normally then since they used French. Maybe translate my own English to",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters",
        "Ġwere",
        "Ġcorrupted",
        "Ġwhen",
        "Ġsent",
        ".",
        "ĠWait",
        "Ġwait",
        ",",
        "ĠI",
        "Ġjust",
        "Ġneed",
        "Ġto",
        "Ġtype",
        "Ġthe",
        "Ġresponse",
        "Ġnormally",
        "Ġthen",
        "Ġsince",
        "Ġthey",
        "Ġused",
        "ĠFrench",
        ".",
        "ĠMaybe",
        "Ġtranslate",
        "Ġmy",
        "Ġown",
        "ĠEnglish",
        "Ġto"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885,
        1051,
        50500,
        994,
        3288,
        13,
        14144,
        3868,
        11,
        358,
        1120,
        1205,
        311,
        955,
        279,
        2077,
        14614,
        1243,
        2533,
        814,
        1511,
        8753,
        13,
        10926,
        15025,
        856,
        1866,
        6498,
        311
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "ĠFrench",
            "token_id": 8753,
            "logprob": -0.0042,
            "prob": 0.9961
          },
          {
            "token": "Ġrespond",
            "token_id": 6013,
            "logprob": -6.875,
            "prob": 0.001
          },
          {
            "token": "Ġtheir",
            "token_id": 872,
            "logprob": -7.5,
            "prob": 0.0005
          },
          {
            "token": "Ġanswer",
            "token_id": 4320,
            "logprob": -7.5,
            "prob": 0.0005
          },
          {
            "token": "Ġthem",
            "token_id": 1124,
            "logprob": -7.875,
            "prob": 0.0004
          },
          {
            "token": "ĠEnglish",
            "token_id": 6498,
            "logprob": -8.125,
            "prob": 0.0003
          },
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -8.5,
            "prob": 0.0002
          },
          {
            "token": "Ġproper",
            "token_id": 6300,
            "logprob": -8.5625,
            "prob": 0.0002
          },
          {
            "token": "ĠSpanish",
            "token_id": 15506,
            "logprob": -9.0,
            "prob": 0.0001
          },
          {
            "token": "French",
            "token_id": 44297,
            "logprob": -9.4375,
            "prob": 0.0001
          }
        ]
      },
      "selected_token": {
        "token": "Ġthe",
        "token_id": 279,
        "selection_method": "sampling"
      }
    },
    {
      "step": 54,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters were corrupted when sent. Wait wait, I just need to type the response normally then since they used French. Maybe translate my own English to the",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters",
        "Ġwere",
        "Ġcorrupted",
        "Ġwhen",
        "Ġsent",
        ".",
        "ĠWait",
        "Ġwait",
        ",",
        "ĠI",
        "Ġjust",
        "Ġneed",
        "Ġto",
        "Ġtype",
        "Ġthe",
        "Ġresponse",
        "Ġnormally",
        "Ġthen",
        "Ġsince",
        "Ġthey",
        "Ġused",
        "ĠFrench",
        ".",
        "ĠMaybe",
        "Ġtranslate",
        "Ġmy",
        "Ġown",
        "ĠEnglish",
        "Ġto",
        "Ġthe"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885,
        1051,
        50500,
        994,
        3288,
        13,
        14144,
        3868,
        11,
        358,
        1120,
        1205,
        311,
        955,
        279,
        2077,
        14614,
        1243,
        2533,
        814,
        1511,
        8753,
        13,
        10926,
        15025,
        856,
        1866,
        6498,
        311,
        279
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġuser",
            "token_id": 1217,
            "logprob": -0.5312,
            "prob": 0.5898
          },
          {
            "token": "Ġquestion",
            "token_id": 3488,
            "logprob": -1.6562,
            "prob": 0.1914
          },
          {
            "token": "ĠFrench",
            "token_id": 8753,
            "logprob": -2.6562,
            "prob": 0.0703
          },
          {
            "token": "Ġsame",
            "token_id": 1890,
            "logprob": -2.7812,
            "prob": 0.062
          },
          {
            "token": "Ġquery",
            "token_id": 3319,
            "logprob": -3.2812,
            "prob": 0.0376
          },
          {
            "token": "Ġlanguage",
            "token_id": 4221,
            "logprob": -3.7812,
            "prob": 0.0228
          },
          {
            "token": "Ġappropriate",
            "token_id": 8475,
            "logprob": -5.4062,
            "prob": 0.0045
          },
          {
            "token": "Ġresponse",
            "token_id": 2077,
            "logprob": -5.5312,
            "prob": 0.004
          },
          {
            "token": "Ġright",
            "token_id": 1314,
            "logprob": -5.6562,
            "prob": 0.0035
          },
          {
            "token": "Ġcorrect",
            "token_id": 4495,
            "logprob": -5.7812,
            "prob": 0.0031
          }
        ]
      },
      "selected_token": {
        "token": "ĠFrench",
        "token_id": 8753,
        "selection_method": "sampling"
      }
    },
    {
      "step": 55,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters were corrupted when sent. Wait wait, I just need to type the response normally then since they used French. Maybe translate my own English to the French",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters",
        "Ġwere",
        "Ġcorrupted",
        "Ġwhen",
        "Ġsent",
        ".",
        "ĠWait",
        "Ġwait",
        ",",
        "ĠI",
        "Ġjust",
        "Ġneed",
        "Ġto",
        "Ġtype",
        "Ġthe",
        "Ġresponse",
        "Ġnormally",
        "Ġthen",
        "Ġsince",
        "Ġthey",
        "Ġused",
        "ĠFrench",
        ".",
        "ĠMaybe",
        "Ġtranslate",
        "Ġmy",
        "Ġown",
        "ĠEnglish",
        "Ġto",
        "Ġthe",
        "ĠFrench"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885,
        1051,
        50500,
        994,
        3288,
        13,
        14144,
        3868,
        11,
        358,
        1120,
        1205,
        311,
        955,
        279,
        2077,
        14614,
        1243,
        2533,
        814,
        1511,
        8753,
        13,
        10926,
        15025,
        856,
        1866,
        6498,
        311,
        279,
        8753
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġquestion",
            "token_id": 3488,
            "logprob": -1.1875,
            "prob": 0.3047
          },
          {
            "token": "Ġresponse",
            "token_id": 2077,
            "logprob": -1.1875,
            "prob": 0.3047
          },
          {
            "token": "?",
            "token_id": 30,
            "logprob": -2.0625,
            "prob": 0.127
          },
          {
            "token": "Ġequivalent",
            "token_id": 13890,
            "logprob": -3.1875,
            "prob": 0.0413
          },
          {
            "token": "Ġversion",
            "token_id": 2373,
            "logprob": -3.4375,
            "prob": 0.0322
          },
          {
            "token": ".",
            "token_id": 13,
            "logprob": -3.4375,
            "prob": 0.0322
          },
          {
            "token": "Ġanswer",
            "token_id": 4320,
            "logprob": -3.5625,
            "prob": 0.0284
          },
          {
            "token": "Ġlanguage",
            "token_id": 4221,
            "logprob": -3.8125,
            "prob": 0.0221
          },
          {
            "token": "?\n\n",
            "token_id": 1980,
            "logprob": -3.8125,
            "prob": 0.0221
          },
          {
            "token": "Ġthey",
            "token_id": 814,
            "logprob": -3.8125,
            "prob": 0.0221
          }
        ]
      },
      "selected_token": {
        "token": "Ġanswer",
        "token_id": 4320,
        "selection_method": "sampling"
      }
    },
    {
      "step": 56,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters were corrupted when sent. Wait wait, I just need to type the response normally then since they used French. Maybe translate my own English to the French answer",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters",
        "Ġwere",
        "Ġcorrupted",
        "Ġwhen",
        "Ġsent",
        ".",
        "ĠWait",
        "Ġwait",
        ",",
        "ĠI",
        "Ġjust",
        "Ġneed",
        "Ġto",
        "Ġtype",
        "Ġthe",
        "Ġresponse",
        "Ġnormally",
        "Ġthen",
        "Ġsince",
        "Ġthey",
        "Ġused",
        "ĠFrench",
        ".",
        "ĠMaybe",
        "Ġtranslate",
        "Ġmy",
        "Ġown",
        "ĠEnglish",
        "Ġto",
        "Ġthe",
        "ĠFrench",
        "Ġanswer"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885,
        1051,
        50500,
        994,
        3288,
        13,
        14144,
        3868,
        11,
        358,
        1120,
        1205,
        311,
        955,
        279,
        2077,
        14614,
        1243,
        2533,
        814,
        1511,
        8753,
        13,
        10926,
        15025,
        856,
        1866,
        6498,
        311,
        279,
        8753,
        4320
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "?",
            "token_id": 30,
            "logprob": -0.7383,
            "prob": 0.4785
          },
          {
            "token": ".",
            "token_id": 13,
            "logprob": -1.2344,
            "prob": 0.291
          },
          {
            "token": "?\n\n",
            "token_id": 1980,
            "logprob": -2.1094,
            "prob": 0.1211
          },
          {
            "token": ".\n\n",
            "token_id": 382,
            "logprob": -2.3594,
            "prob": 0.0942
          },
          {
            "token": ",",
            "token_id": 11,
            "logprob": -5.125,
            "prob": 0.006
          },
          {
            "token": "Ġthey",
            "token_id": 814,
            "logprob": -6.25,
            "prob": 0.002
          },
          {
            "token": "Ġbut",
            "token_id": 719,
            "logprob": -6.375,
            "prob": 0.0017
          },
          {
            "token": "Ġso",
            "token_id": 779,
            "logprob": -6.625,
            "prob": 0.0013
          },
          {
            "token": "Ġto",
            "token_id": 311,
            "logprob": -7.125,
            "prob": 0.0008
          },
          {
            "token": "Ġor",
            "token_id": 477,
            "logprob": -7.375,
            "prob": 0.0006
          }
        ]
      },
      "selected_token": {
        "token": "?",
        "token_id": 30,
        "selection_method": "sampling"
      }
    },
    {
      "step": 57,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters were corrupted when sent. Wait wait, I just need to type the response normally then since they used French. Maybe translate my own English to the French answer?",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters",
        "Ġwere",
        "Ġcorrupted",
        "Ġwhen",
        "Ġsent",
        ".",
        "ĠWait",
        "Ġwait",
        ",",
        "ĠI",
        "Ġjust",
        "Ġneed",
        "Ġto",
        "Ġtype",
        "Ġthe",
        "Ġresponse",
        "Ġnormally",
        "Ġthen",
        "Ġsince",
        "Ġthey",
        "Ġused",
        "ĠFrench",
        ".",
        "ĠMaybe",
        "Ġtranslate",
        "Ġmy",
        "Ġown",
        "ĠEnglish",
        "Ġto",
        "Ġthe",
        "ĠFrench",
        "Ġanswer",
        "?"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885,
        1051,
        50500,
        994,
        3288,
        13,
        14144,
        3868,
        11,
        358,
        1120,
        1205,
        311,
        955,
        279,
        2077,
        14614,
        1243,
        2533,
        814,
        1511,
        8753,
        13,
        10926,
        15025,
        856,
        1866,
        6498,
        311,
        279,
        8753,
        4320,
        30
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "ĠLet",
            "token_id": 6914,
            "logprob": -0.5977,
            "prob": 0.5508
          },
          {
            "token": "ĠWait",
            "token_id": 14144,
            "logprob": -1.7266,
            "prob": 0.1787
          },
          {
            "token": "ĠOr",
            "token_id": 2582,
            "logprob": -2.2188,
            "prob": 0.1084
          },
          {
            "token": "ĠHmm",
            "token_id": 89290,
            "logprob": -2.5938,
            "prob": 0.0742
          },
          {
            "token": "ĠThe",
            "token_id": 578,
            "logprob": -3.4688,
            "prob": 0.031
          },
          {
            "token": "ĠBut",
            "token_id": 2030,
            "logprob": -3.5938,
            "prob": 0.0273
          },
          {
            "token": "ĠSince",
            "token_id": 8876,
            "logprob": -4.5938,
            "prob": 0.0101
          },
          {
            "token": "ĠBecause",
            "token_id": 9393,
            "logprob": -4.8438,
            "prob": 0.0079
          },
          {
            "token": "ĠSo",
            "token_id": 2100,
            "logprob": -5.7188,
            "prob": 0.0033
          },
          {
            "token": "ĠWell",
            "token_id": 8489,
            "logprob": -5.8438,
            "prob": 0.0029
          }
        ]
      },
      "selected_token": {
        "token": "ĠHmm",
        "token_id": 89290,
        "selection_method": "sampling"
      }
    },
    {
      "step": 58,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters were corrupted when sent. Wait wait, I just need to type the response normally then since they used French. Maybe translate my own English to the French answer? Hmm",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters",
        "Ġwere",
        "Ġcorrupted",
        "Ġwhen",
        "Ġsent",
        ".",
        "ĠWait",
        "Ġwait",
        ",",
        "ĠI",
        "Ġjust",
        "Ġneed",
        "Ġto",
        "Ġtype",
        "Ġthe",
        "Ġresponse",
        "Ġnormally",
        "Ġthen",
        "Ġsince",
        "Ġthey",
        "Ġused",
        "ĠFrench",
        ".",
        "ĠMaybe",
        "Ġtranslate",
        "Ġmy",
        "Ġown",
        "ĠEnglish",
        "Ġto",
        "Ġthe",
        "ĠFrench",
        "Ġanswer",
        "?",
        "ĠHmm"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885,
        1051,
        50500,
        994,
        3288,
        13,
        14144,
        3868,
        11,
        358,
        1120,
        1205,
        311,
        955,
        279,
        2077,
        14614,
        1243,
        2533,
        814,
        1511,
        8753,
        13,
        10926,
        15025,
        856,
        1866,
        6498,
        311,
        279,
        8753,
        4320,
        30,
        89290
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": ".",
            "token_id": 13,
            "logprob": -0.957,
            "prob": 0.3848
          },
          {
            "token": ".\n\n",
            "token_id": 382,
            "logprob": -1.0781,
            "prob": 0.3398
          },
          {
            "token": ",",
            "token_id": 11,
            "logprob": -1.3281,
            "prob": 0.2637
          },
          {
            "token": "Ġbut",
            "token_id": 719,
            "logprob": -4.8438,
            "prob": 0.008
          },
          {
            "token": "Ġlet",
            "token_id": 1095,
            "logprob": -6.7188,
            "prob": 0.0012
          },
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -6.9688,
            "prob": 0.001
          },
          {
            "token": "...",
            "token_id": 1131,
            "logprob": -7.7188,
            "prob": 0.0005
          },
          {
            "token": "Ġor",
            "token_id": 477,
            "logprob": -7.9688,
            "prob": 0.0004
          },
          {
            "token": "Ġno",
            "token_id": 912,
            "logprob": -8.8125,
            "prob": 0.0001
          },
          {
            "token": "Ġwait",
            "token_id": 3868,
            "logprob": -8.8125,
            "prob": 0.0001
          }
        ]
      },
      "selected_token": {
        "token": ",",
        "token_id": 11,
        "selection_method": "sampling"
      }
    },
    {
      "step": 59,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters were corrupted when sent. Wait wait, I just need to type the response normally then since they used French. Maybe translate my own English to the French answer? Hmm,",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters",
        "Ġwere",
        "Ġcorrupted",
        "Ġwhen",
        "Ġsent",
        ".",
        "ĠWait",
        "Ġwait",
        ",",
        "ĠI",
        "Ġjust",
        "Ġneed",
        "Ġto",
        "Ġtype",
        "Ġthe",
        "Ġresponse",
        "Ġnormally",
        "Ġthen",
        "Ġsince",
        "Ġthey",
        "Ġused",
        "ĠFrench",
        ".",
        "ĠMaybe",
        "Ġtranslate",
        "Ġmy",
        "Ġown",
        "ĠEnglish",
        "Ġto",
        "Ġthe",
        "ĠFrench",
        "Ġanswer",
        "?",
        "ĠHmm",
        ","
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885,
        1051,
        50500,
        994,
        3288,
        13,
        14144,
        3868,
        11,
        358,
        1120,
        1205,
        311,
        955,
        279,
        2077,
        14614,
        1243,
        2533,
        814,
        1511,
        8753,
        13,
        10926,
        15025,
        856,
        1866,
        6498,
        311,
        279,
        8753,
        4320,
        30,
        89290,
        11
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġbut",
            "token_id": 719,
            "logprob": -1.0078,
            "prob": 0.3633
          },
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -1.2578,
            "prob": 0.2832
          },
          {
            "token": "Ġlet",
            "token_id": 1095,
            "logprob": -2.1406,
            "prob": 0.1182
          },
          {
            "token": "Ġmaybe",
            "token_id": 7344,
            "logprob": -3.2656,
            "prob": 0.0383
          },
          {
            "token": "Ġor",
            "token_id": 477,
            "logprob": -3.7656,
            "prob": 0.0233
          },
          {
            "token": "Ġthey",
            "token_id": 814,
            "logprob": -3.7656,
            "prob": 0.0233
          },
          {
            "token": "Ġuser",
            "token_id": 1217,
            "logprob": -3.8906,
            "prob": 0.0205
          },
          {
            "token": "Ġ\"",
            "token_id": 330,
            "logprob": -3.8906,
            "prob": 0.0205
          },
          {
            "token": "ĠI",
            "token_id": 358,
            "logprob": -4.0,
            "prob": 0.0181
          },
          {
            "token": "Ġwell",
            "token_id": 1664,
            "logprob": -4.25,
            "prob": 0.0141
          }
        ]
      },
      "selected_token": {
        "token": "Ġuser",
        "token_id": 1217,
        "selection_method": "sampling"
      }
    },
    {
      "step": 60,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters were corrupted when sent. Wait wait, I just need to type the response normally then since they used French. Maybe translate my own English to the French answer? Hmm, user",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters",
        "Ġwere",
        "Ġcorrupted",
        "Ġwhen",
        "Ġsent",
        ".",
        "ĠWait",
        "Ġwait",
        ",",
        "ĠI",
        "Ġjust",
        "Ġneed",
        "Ġto",
        "Ġtype",
        "Ġthe",
        "Ġresponse",
        "Ġnormally",
        "Ġthen",
        "Ġsince",
        "Ġthey",
        "Ġused",
        "ĠFrench",
        ".",
        "ĠMaybe",
        "Ġtranslate",
        "Ġmy",
        "Ġown",
        "ĠEnglish",
        "Ġto",
        "Ġthe",
        "ĠFrench",
        "Ġanswer",
        "?",
        "ĠHmm",
        ",",
        "Ġuser"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885,
        1051,
        50500,
        994,
        3288,
        13,
        14144,
        3868,
        11,
        358,
        1120,
        1205,
        311,
        955,
        279,
        2077,
        14614,
        1243,
        2533,
        814,
        1511,
        8753,
        13,
        10926,
        15025,
        856,
        1866,
        6498,
        311,
        279,
        8753,
        4320,
        30,
        89290,
        11,
        1217
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġmight",
            "token_id": 2643,
            "logprob": -0.5078,
            "prob": 0.6016
          },
          {
            "token": "Ġwrote",
            "token_id": 6267,
            "logprob": -2.125,
            "prob": 0.1187
          },
          {
            "token": "Ġis",
            "token_id": 374,
            "logprob": -2.25,
            "prob": 0.1045
          },
          {
            "token": "Ġprobably",
            "token_id": 4762,
            "logprob": -3.125,
            "prob": 0.0437
          },
          {
            "token": "'s",
            "token_id": 596,
            "logprob": -3.25,
            "prob": 0.0386
          },
          {
            "token": "Ġwants",
            "token_id": 6944,
            "logprob": -3.875,
            "prob": 0.0206
          },
          {
            "token": "Ġused",
            "token_id": 1511,
            "logprob": -3.875,
            "prob": 0.0206
          },
          {
            "token": "Ġasked",
            "token_id": 4691,
            "logprob": -4.375,
            "prob": 0.0125
          },
          {
            "token": "Ġstarted",
            "token_id": 3940,
            "logprob": -4.625,
            "prob": 0.0098
          },
          {
            "token": "Ġsaid",
            "token_id": 1071,
            "logprob": -4.875,
            "prob": 0.0076
          }
        ]
      },
      "selected_token": {
        "token": "Ġsaid",
        "token_id": 1071,
        "selection_method": "sampling"
      }
    },
    {
      "step": 61,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters were corrupted when sent. Wait wait, I just need to type the response normally then since they used French. Maybe translate my own English to the French answer? Hmm, user said",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters",
        "Ġwere",
        "Ġcorrupted",
        "Ġwhen",
        "Ġsent",
        ".",
        "ĠWait",
        "Ġwait",
        ",",
        "ĠI",
        "Ġjust",
        "Ġneed",
        "Ġto",
        "Ġtype",
        "Ġthe",
        "Ġresponse",
        "Ġnormally",
        "Ġthen",
        "Ġsince",
        "Ġthey",
        "Ġused",
        "ĠFrench",
        ".",
        "ĠMaybe",
        "Ġtranslate",
        "Ġmy",
        "Ġown",
        "ĠEnglish",
        "Ġto",
        "Ġthe",
        "ĠFrench",
        "Ġanswer",
        "?",
        "ĠHmm",
        ",",
        "Ġuser",
        "Ġsaid"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885,
        1051,
        50500,
        994,
        3288,
        13,
        14144,
        3868,
        11,
        358,
        1120,
        1205,
        311,
        955,
        279,
        2077,
        14614,
        1243,
        2533,
        814,
        1511,
        8753,
        13,
        10926,
        15025,
        856,
        1866,
        6498,
        311,
        279,
        8753,
        4320,
        30,
        89290,
        11,
        1217,
        1071
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġ\"",
            "token_id": 330,
            "logprob": -0.0225,
            "prob": 0.9766
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -4.0312,
            "prob": 0.0179
          },
          {
            "token": "ĠQui",
            "token_id": 42215,
            "logprob": -6.1562,
            "prob": 0.0021
          },
          {
            "token": ":",
            "token_id": 25,
            "logprob": -8.0625,
            "prob": 0.0003
          },
          {
            "token": "Ġ“",
            "token_id": 1054,
            "logprob": -8.1875,
            "prob": 0.0003
          },
          {
            "token": "Ġto",
            "token_id": 311,
            "logprob": -8.25,
            "prob": 0.0003
          },
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -8.3125,
            "prob": 0.0002
          },
          {
            "token": "ĠQu",
            "token_id": 3489,
            "logprob": -8.75,
            "prob": 0.0002
          },
          {
            "token": "Ġ'",
            "token_id": 364,
            "logprob": -9.25,
            "prob": 0.0001
          },
          {
            "token": "Ġqui",
            "token_id": 7930,
            "logprob": -9.4375,
            "prob": 0.0001
          }
        ]
      },
      "selected_token": {
        "token": "Ġ'",
        "token_id": 364,
        "selection_method": "sampling"
      }
    },
    {
      "step": 62,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters were corrupted when sent. Wait wait, I just need to type the response normally then since they used French. Maybe translate my own English to the French answer? Hmm, user said '",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters",
        "Ġwere",
        "Ġcorrupted",
        "Ġwhen",
        "Ġsent",
        ".",
        "ĠWait",
        "Ġwait",
        ",",
        "ĠI",
        "Ġjust",
        "Ġneed",
        "Ġto",
        "Ġtype",
        "Ġthe",
        "Ġresponse",
        "Ġnormally",
        "Ġthen",
        "Ġsince",
        "Ġthey",
        "Ġused",
        "ĠFrench",
        ".",
        "ĠMaybe",
        "Ġtranslate",
        "Ġmy",
        "Ġown",
        "ĠEnglish",
        "Ġto",
        "Ġthe",
        "ĠFrench",
        "Ġanswer",
        "?",
        "ĠHmm",
        ",",
        "Ġuser",
        "Ġsaid",
        "Ġ'"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885,
        1051,
        50500,
        994,
        3288,
        13,
        14144,
        3868,
        11,
        358,
        1120,
        1205,
        311,
        955,
        279,
        2077,
        14614,
        1243,
        2533,
        814,
        1511,
        8753,
        13,
        10926,
        15025,
        856,
        1866,
        6498,
        311,
        279,
        8753,
        4320,
        30,
        89290,
        11,
        1217,
        1071,
        364
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Qui",
            "token_id": 62545,
            "logprob": -0.0593,
            "prob": 0.9414
          },
          {
            "token": "Ġqui",
            "token_id": 7930,
            "logprob": -4.25,
            "prob": 0.0143
          },
          {
            "token": "ĠQui",
            "token_id": 42215,
            "logprob": -4.375,
            "prob": 0.0126
          },
          {
            "token": "Who",
            "token_id": 15546,
            "logprob": -4.6875,
            "prob": 0.0092
          },
          {
            "token": "Qu",
            "token_id": 2232,
            "logprob": -4.6875,
            "prob": 0.0092
          },
          {
            "token": "qui",
            "token_id": 47391,
            "logprob": -5.125,
            "prob": 0.006
          },
          {
            "token": "who",
            "token_id": 14965,
            "logprob": -6.5,
            "prob": 0.0015
          },
          {
            "token": "qu",
            "token_id": 447,
            "logprob": -6.75,
            "prob": 0.0012
          },
          {
            "token": "ĠQUI",
            "token_id": 89630,
            "logprob": -7.1875,
            "prob": 0.0008
          },
          {
            "token": "QUI",
            "token_id": 12247,
            "logprob": -7.3125,
            "prob": 0.0007
          }
        ]
      },
      "selected_token": {
        "token": "QUI",
        "token_id": 12247,
        "selection_method": "sampling"
      }
    },
    {
      "step": 63,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters were corrupted when sent. Wait wait, I just need to type the response normally then since they used French. Maybe translate my own English to the French answer? Hmm, user said 'QUI",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters",
        "Ġwere",
        "Ġcorrupted",
        "Ġwhen",
        "Ġsent",
        ".",
        "ĠWait",
        "Ġwait",
        ",",
        "ĠI",
        "Ġjust",
        "Ġneed",
        "Ġto",
        "Ġtype",
        "Ġthe",
        "Ġresponse",
        "Ġnormally",
        "Ġthen",
        "Ġsince",
        "Ġthey",
        "Ġused",
        "ĠFrench",
        ".",
        "ĠMaybe",
        "Ġtranslate",
        "Ġmy",
        "Ġown",
        "ĠEnglish",
        "Ġto",
        "Ġthe",
        "ĠFrench",
        "Ġanswer",
        "?",
        "ĠHmm",
        ",",
        "Ġuser",
        "Ġsaid",
        "Ġ'",
        "QUI"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885,
        1051,
        50500,
        994,
        3288,
        13,
        14144,
        3868,
        11,
        358,
        1120,
        1205,
        311,
        955,
        279,
        2077,
        14614,
        1243,
        2533,
        814,
        1511,
        8753,
        13,
        10926,
        15025,
        856,
        1866,
        6498,
        311,
        279,
        8753,
        4320,
        30,
        89290,
        11,
        1217,
        1071,
        364,
        12247
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "ĠES",
            "token_id": 19844,
            "logprob": -0.8594,
            "prob": 0.4238
          },
          {
            "token": "Ġest",
            "token_id": 1826,
            "logprob": -1.5469,
            "prob": 0.2129
          },
          {
            "token": "Ġes",
            "token_id": 1560,
            "logprob": -1.9219,
            "prob": 0.1465
          },
          {
            "token": "ES",
            "token_id": 1600,
            "logprob": -2.1719,
            "prob": 0.1143
          },
          {
            "token": "ĠEST",
            "token_id": 26963,
            "logprob": -3.1719,
            "prob": 0.042
          },
          {
            "token": "ĠE",
            "token_id": 469,
            "logprob": -4.6562,
            "prob": 0.0094
          },
          {
            "token": "E",
            "token_id": 36,
            "logprob": -4.7188,
            "prob": 0.0088
          },
          {
            "token": "ĠEs",
            "token_id": 9419,
            "logprob": -6.0312,
            "prob": 0.0024
          },
          {
            "token": "'",
            "token_id": 6,
            "logprob": -6.1562,
            "prob": 0.0021
          },
          {
            "token": "e",
            "token_id": 68,
            "logprob": -6.1562,
            "prob": 0.0021
          }
        ]
      },
      "selected_token": {
        "token": "ĠE",
        "token_id": 469,
        "selection_method": "sampling"
      }
    },
    {
      "step": 64,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters were corrupted when sent. Wait wait, I just need to type the response normally then since they used French. Maybe translate my own English to the French answer? Hmm, user said 'QUI E",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters",
        "Ġwere",
        "Ġcorrupted",
        "Ġwhen",
        "Ġsent",
        ".",
        "ĠWait",
        "Ġwait",
        ",",
        "ĠI",
        "Ġjust",
        "Ġneed",
        "Ġto",
        "Ġtype",
        "Ġthe",
        "Ġresponse",
        "Ġnormally",
        "Ġthen",
        "Ġsince",
        "Ġthey",
        "Ġused",
        "ĠFrench",
        ".",
        "ĠMaybe",
        "Ġtranslate",
        "Ġmy",
        "Ġown",
        "ĠEnglish",
        "Ġto",
        "Ġthe",
        "ĠFrench",
        "Ġanswer",
        "?",
        "ĠHmm",
        ",",
        "Ġuser",
        "Ġsaid",
        "Ġ'",
        "QUI",
        "ĠE"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885,
        1051,
        50500,
        994,
        3288,
        13,
        14144,
        3868,
        11,
        358,
        1120,
        1205,
        311,
        955,
        279,
        2077,
        14614,
        1243,
        2533,
        814,
        1511,
        8753,
        13,
        10926,
        15025,
        856,
        1866,
        6498,
        311,
        279,
        8753,
        4320,
        30,
        89290,
        11,
        1217,
        1071,
        364,
        12247,
        469
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "ĠS",
            "token_id": 328,
            "logprob": -1.7656,
            "prob": 0.1709
          },
          {
            "token": "-S",
            "token_id": 6354,
            "logprob": -1.8281,
            "prob": 0.1602
          },
          {
            "token": "ST",
            "token_id": 790,
            "logprob": -1.8281,
            "prob": 0.1602
          },
          {
            "token": "St",
            "token_id": 626,
            "logprob": -2.0781,
            "prob": 0.125
          },
          {
            "token": "ĠST",
            "token_id": 4015,
            "logprob": -2.3906,
            "prob": 0.0913
          },
          {
            "token": "TS",
            "token_id": 10155,
            "logprob": -2.8906,
            "prob": 0.0554
          },
          {
            "token": "-",
            "token_id": 12,
            "logprob": -3.3281,
            "prob": 0.0356
          },
          {
            "token": "ĠSI",
            "token_id": 31648,
            "logprob": -3.7656,
            "prob": 0.0231
          },
          {
            "token": "-T",
            "token_id": 9469,
            "logprob": -3.7656,
            "prob": 0.0231
          },
          {
            "token": "SI",
            "token_id": 14137,
            "logprob": -4.2188,
            "prob": 0.0149
          }
        ]
      },
      "selected_token": {
        "token": "-S",
        "token_id": 6354,
        "selection_method": "sampling"
      }
    },
    {
      "step": 65,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters were corrupted when sent. Wait wait, I just need to type the response normally then since they used French. Maybe translate my own English to the French answer? Hmm, user said 'QUI E-S",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters",
        "Ġwere",
        "Ġcorrupted",
        "Ġwhen",
        "Ġsent",
        ".",
        "ĠWait",
        "Ġwait",
        ",",
        "ĠI",
        "Ġjust",
        "Ġneed",
        "Ġto",
        "Ġtype",
        "Ġthe",
        "Ġresponse",
        "Ġnormally",
        "Ġthen",
        "Ġsince",
        "Ġthey",
        "Ġused",
        "ĠFrench",
        ".",
        "ĠMaybe",
        "Ġtranslate",
        "Ġmy",
        "Ġown",
        "ĠEnglish",
        "Ġto",
        "Ġthe",
        "ĠFrench",
        "Ġanswer",
        "?",
        "ĠHmm",
        ",",
        "Ġuser",
        "Ġsaid",
        "Ġ'",
        "QUI",
        "ĠE",
        "-S"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885,
        1051,
        50500,
        994,
        3288,
        13,
        14144,
        3868,
        11,
        358,
        1120,
        1205,
        311,
        955,
        279,
        2077,
        14614,
        1243,
        2533,
        814,
        1511,
        8753,
        13,
        10926,
        15025,
        856,
        1866,
        6498,
        311,
        279,
        8753,
        4320,
        30,
        89290,
        11,
        1217,
        1071,
        364,
        12247,
        469,
        6354
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "-T",
            "token_id": 9469,
            "logprob": -0.0251,
            "prob": 0.9766
          },
          {
            "token": "ĠTU",
            "token_id": 70514,
            "logprob": -4.1562,
            "prob": 0.0157
          },
          {
            "token": "U",
            "token_id": 52,
            "logprob": -5.9688,
            "prob": 0.0026
          },
          {
            "token": "-",
            "token_id": 12,
            "logprob": -6.0938,
            "prob": 0.0023
          },
          {
            "token": "ĠT",
            "token_id": 350,
            "logprob": -6.7812,
            "prob": 0.0011
          },
          {
            "token": "-U",
            "token_id": 35681,
            "logprob": -7.8438,
            "prob": 0.0004
          },
          {
            "token": "T",
            "token_id": 51,
            "logprob": -8.0,
            "prob": 0.0003
          },
          {
            "token": "TU",
            "token_id": 99682,
            "logprob": -8.0,
            "prob": 0.0003
          },
          {
            "token": "-t",
            "token_id": 2442,
            "logprob": -8.0625,
            "prob": 0.0003
          },
          {
            "token": "*T",
            "token_id": 61734,
            "logprob": -8.6875,
            "prob": 0.0002
          }
        ]
      },
      "selected_token": {
        "token": "U",
        "token_id": 52,
        "selection_method": "sampling"
      }
    },
    {
      "step": 66,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters were corrupted when sent. Wait wait, I just need to type the response normally then since they used French. Maybe translate my own English to the French answer? Hmm, user said 'QUI E-SU",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters",
        "Ġwere",
        "Ġcorrupted",
        "Ġwhen",
        "Ġsent",
        ".",
        "ĠWait",
        "Ġwait",
        ",",
        "ĠI",
        "Ġjust",
        "Ġneed",
        "Ġto",
        "Ġtype",
        "Ġthe",
        "Ġresponse",
        "Ġnormally",
        "Ġthen",
        "Ġsince",
        "Ġthey",
        "Ġused",
        "ĠFrench",
        ".",
        "ĠMaybe",
        "Ġtranslate",
        "Ġmy",
        "Ġown",
        "ĠEnglish",
        "Ġto",
        "Ġthe",
        "ĠFrench",
        "Ġanswer",
        "?",
        "ĠHmm",
        ",",
        "Ġuser",
        "Ġsaid",
        "Ġ'",
        "QUI",
        "ĠE",
        "-S",
        "U"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885,
        1051,
        50500,
        994,
        3288,
        13,
        14144,
        3868,
        11,
        358,
        1120,
        1205,
        311,
        955,
        279,
        2077,
        14614,
        1243,
        2533,
        814,
        1511,
        8753,
        13,
        10926,
        15025,
        856,
        1866,
        6498,
        311,
        279,
        8753,
        4320,
        30,
        89290,
        11,
        1217,
        1071,
        364,
        12247,
        469,
        6354,
        52
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "'",
            "token_id": 6,
            "logprob": -0.2793,
            "prob": 0.7539
          },
          {
            "token": "?'",
            "token_id": 20837,
            "logprob": -2.1562,
            "prob": 0.1157
          },
          {
            "token": "?",
            "token_id": 30,
            "logprob": -3.1562,
            "prob": 0.0427
          },
          {
            "token": "'?",
            "token_id": 71090,
            "logprob": -3.1562,
            "prob": 0.0427
          },
          {
            "token": "',",
            "token_id": 518,
            "logprob": -3.9062,
            "prob": 0.0201
          },
          {
            "token": "?\"",
            "token_id": 7673,
            "logprob": -4.5312,
            "prob": 0.0108
          },
          {
            "token": "?',",
            "token_id": 50734,
            "logprob": -5.0312,
            "prob": 0.0065
          },
          {
            "token": "'.",
            "token_id": 4527,
            "logprob": -6.2812,
            "prob": 0.0019
          },
          {
            "token": "Ġ?",
            "token_id": 949,
            "logprob": -6.7812,
            "prob": 0.0011
          },
          {
            "token": "\"",
            "token_id": 1,
            "logprob": -7.5312,
            "prob": 0.0005
          }
        ]
      },
      "selected_token": {
        "token": "Ġ?",
        "token_id": 949,
        "selection_method": "sampling"
      }
    },
    {
      "step": 67,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters were corrupted when sent. Wait wait, I just need to type the response normally then since they used French. Maybe translate my own English to the French answer? Hmm, user said 'QUI E-SU ?",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters",
        "Ġwere",
        "Ġcorrupted",
        "Ġwhen",
        "Ġsent",
        ".",
        "ĠWait",
        "Ġwait",
        ",",
        "ĠI",
        "Ġjust",
        "Ġneed",
        "Ġto",
        "Ġtype",
        "Ġthe",
        "Ġresponse",
        "Ġnormally",
        "Ġthen",
        "Ġsince",
        "Ġthey",
        "Ġused",
        "ĠFrench",
        ".",
        "ĠMaybe",
        "Ġtranslate",
        "Ġmy",
        "Ġown",
        "ĠEnglish",
        "Ġto",
        "Ġthe",
        "ĠFrench",
        "Ġanswer",
        "?",
        "ĠHmm",
        ",",
        "Ġuser",
        "Ġsaid",
        "Ġ'",
        "QUI",
        "ĠE",
        "-S",
        "U",
        "Ġ?"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885,
        1051,
        50500,
        994,
        3288,
        13,
        14144,
        3868,
        11,
        358,
        1120,
        1205,
        311,
        955,
        279,
        2077,
        14614,
        1243,
        2533,
        814,
        1511,
        8753,
        13,
        10926,
        15025,
        856,
        1866,
        6498,
        311,
        279,
        8753,
        4320,
        30,
        89290,
        11,
        1217,
        1071,
        364,
        12247,
        469,
        6354,
        52,
        949
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "'",
            "token_id": 6,
            "logprob": -0.0156,
            "prob": 0.9844
          },
          {
            "token": "Ġ'",
            "token_id": 364,
            "logprob": -4.6562,
            "prob": 0.0096
          },
          {
            "token": "\"",
            "token_id": 1,
            "logprob": -5.7812,
            "prob": 0.0031
          },
          {
            "token": "',",
            "token_id": 518,
            "logprob": -7.5312,
            "prob": 0.0005
          },
          {
            "token": "'.",
            "token_id": 4527,
            "logprob": -8.0,
            "prob": 0.0003
          },
          {
            "token": "'?",
            "token_id": 71090,
            "logprob": -8.25,
            "prob": 0.0003
          },
          {
            "token": "Ġ\"",
            "token_id": 330,
            "logprob": -8.375,
            "prob": 0.0002
          },
          {
            "token": "'\n\n",
            "token_id": 3961,
            "logprob": -8.5,
            "prob": 0.0002
          },
          {
            "token": "!'",
            "token_id": 32483,
            "logprob": -8.625,
            "prob": 0.0002
          },
          {
            "token": "ĠWait",
            "token_id": 14144,
            "logprob": -8.75,
            "prob": 0.0002
          }
        ]
      },
      "selected_token": {
        "token": "\"",
        "token_id": 1,
        "selection_method": "sampling"
      }
    },
    {
      "step": 68,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters were corrupted when sent. Wait wait, I just need to type the response normally then since they used French. Maybe translate my own English to the French answer? Hmm, user said 'QUI E-SU ?\"",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters",
        "Ġwere",
        "Ġcorrupted",
        "Ġwhen",
        "Ġsent",
        ".",
        "ĠWait",
        "Ġwait",
        ",",
        "ĠI",
        "Ġjust",
        "Ġneed",
        "Ġto",
        "Ġtype",
        "Ġthe",
        "Ġresponse",
        "Ġnormally",
        "Ġthen",
        "Ġsince",
        "Ġthey",
        "Ġused",
        "ĠFrench",
        ".",
        "ĠMaybe",
        "Ġtranslate",
        "Ġmy",
        "Ġown",
        "ĠEnglish",
        "Ġto",
        "Ġthe",
        "ĠFrench",
        "Ġanswer",
        "?",
        "ĠHmm",
        ",",
        "Ġuser",
        "Ġsaid",
        "Ġ'",
        "QUI",
        "ĠE",
        "-S",
        "U",
        "Ġ?",
        "\""
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885,
        1051,
        50500,
        994,
        3288,
        13,
        14144,
        3868,
        11,
        358,
        1120,
        1205,
        311,
        955,
        279,
        2077,
        14614,
        1243,
        2533,
        814,
        1511,
        8753,
        13,
        10926,
        15025,
        856,
        1866,
        6498,
        311,
        279,
        8753,
        4320,
        30,
        89290,
        11,
        1217,
        1071,
        364,
        12247,
        469,
        6354,
        52,
        949,
        1
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġbut",
            "token_id": 719,
            "logprob": -1.2266,
            "prob": 0.2949
          },
          {
            "token": "Ġwhich",
            "token_id": 902,
            "logprob": -1.6016,
            "prob": 0.2021
          },
          {
            "token": "Ġwith",
            "token_id": 449,
            "logprob": -1.9766,
            "prob": 0.1387
          },
          {
            "token": "Ġprobably",
            "token_id": 4762,
            "logprob": -2.3438,
            "prob": 0.0952
          },
          {
            "token": "Ġmaybe",
            "token_id": 7344,
            "logprob": -2.4688,
            "prob": 0.084
          },
          {
            "token": "ĠWait",
            "token_id": 14144,
            "logprob": -3.0938,
            "prob": 0.0452
          },
          {
            "token": "ĠMaybe",
            "token_id": 10926,
            "logprob": -4.0938,
            "prob": 0.0166
          },
          {
            "token": "Ġso",
            "token_id": 779,
            "logprob": -4.2188,
            "prob": 0.0146
          },
          {
            "token": "Ġ–",
            "token_id": 1389,
            "logprob": -4.2188,
            "prob": 0.0146
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -4.3438,
            "prob": 0.0129
          }
        ]
      },
      "selected_token": {
        "token": "Ġso",
        "token_id": 779,
        "selection_method": "sampling"
      }
    },
    {
      "step": 69,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters were corrupted when sent. Wait wait, I just need to type the response normally then since they used French. Maybe translate my own English to the French answer? Hmm, user said 'QUI E-SU ?\" so",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters",
        "Ġwere",
        "Ġcorrupted",
        "Ġwhen",
        "Ġsent",
        ".",
        "ĠWait",
        "Ġwait",
        ",",
        "ĠI",
        "Ġjust",
        "Ġneed",
        "Ġto",
        "Ġtype",
        "Ġthe",
        "Ġresponse",
        "Ġnormally",
        "Ġthen",
        "Ġsince",
        "Ġthey",
        "Ġused",
        "ĠFrench",
        ".",
        "ĠMaybe",
        "Ġtranslate",
        "Ġmy",
        "Ġown",
        "ĠEnglish",
        "Ġto",
        "Ġthe",
        "ĠFrench",
        "Ġanswer",
        "?",
        "ĠHmm",
        ",",
        "Ġuser",
        "Ġsaid",
        "Ġ'",
        "QUI",
        "ĠE",
        "-S",
        "U",
        "Ġ?",
        "\"",
        "Ġso"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885,
        1051,
        50500,
        994,
        3288,
        13,
        14144,
        3868,
        11,
        358,
        1120,
        1205,
        311,
        955,
        279,
        2077,
        14614,
        1243,
        2533,
        814,
        1511,
        8753,
        13,
        10926,
        15025,
        856,
        1866,
        6498,
        311,
        279,
        8753,
        4320,
        30,
        89290,
        11,
        1217,
        1071,
        364,
        12247,
        469,
        6354,
        52,
        949,
        1,
        779
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġprobably",
            "token_id": 4762,
            "logprob": -0.8828,
            "prob": 0.4121
          },
          {
            "token": "Ġmaybe",
            "token_id": 7344,
            "logprob": -1.7578,
            "prob": 0.1719
          },
          {
            "token": "Ġthey",
            "token_id": 814,
            "logprob": -2.2656,
            "prob": 0.1045
          },
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -2.6406,
            "prob": 0.0718
          },
          {
            "token": "ĠI",
            "token_id": 358,
            "logprob": -2.8906,
            "prob": 0.0559
          },
          {
            "token": "Ġperhaps",
            "token_id": 8530,
            "logprob": -3.5156,
            "prob": 0.0299
          },
          {
            "token": "Ġactually",
            "token_id": 3604,
            "logprob": -3.7656,
            "prob": 0.0233
          },
          {
            "token": "Ġit",
            "token_id": 433,
            "logprob": -3.7656,
            "prob": 0.0233
          },
          {
            "token": "Ġlikely",
            "token_id": 4461,
            "logprob": -3.7656,
            "prob": 0.0233
          },
          {
            "token": "Ġ\"",
            "token_id": 330,
            "logprob": -4.125,
            "prob": 0.016
          }
        ]
      },
      "selected_token": {
        "token": "ĠI",
        "token_id": 358,
        "selection_method": "sampling"
      }
    },
    {
      "step": 70,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters were corrupted when sent. Wait wait, I just need to type the response normally then since they used French. Maybe translate my own English to the French answer? Hmm, user said 'QUI E-SU ?\" so I",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters",
        "Ġwere",
        "Ġcorrupted",
        "Ġwhen",
        "Ġsent",
        ".",
        "ĠWait",
        "Ġwait",
        ",",
        "ĠI",
        "Ġjust",
        "Ġneed",
        "Ġto",
        "Ġtype",
        "Ġthe",
        "Ġresponse",
        "Ġnormally",
        "Ġthen",
        "Ġsince",
        "Ġthey",
        "Ġused",
        "ĠFrench",
        ".",
        "ĠMaybe",
        "Ġtranslate",
        "Ġmy",
        "Ġown",
        "ĠEnglish",
        "Ġto",
        "Ġthe",
        "ĠFrench",
        "Ġanswer",
        "?",
        "ĠHmm",
        ",",
        "Ġuser",
        "Ġsaid",
        "Ġ'",
        "QUI",
        "ĠE",
        "-S",
        "U",
        "Ġ?",
        "\"",
        "Ġso",
        "ĠI"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885,
        1051,
        50500,
        994,
        3288,
        13,
        14144,
        3868,
        11,
        358,
        1120,
        1205,
        311,
        955,
        279,
        2077,
        14614,
        1243,
        2533,
        814,
        1511,
        8753,
        13,
        10926,
        15025,
        856,
        1866,
        6498,
        311,
        279,
        8753,
        4320,
        30,
        89290,
        11,
        1217,
        1071,
        364,
        12247,
        469,
        6354,
        52,
        949,
        1,
        779,
        358
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġshould",
            "token_id": 1288,
            "logprob": -0.2256,
            "prob": 0.7969
          },
          {
            "token": "Ġthink",
            "token_id": 1781,
            "logprob": -2.4688,
            "prob": 0.084
          },
          {
            "token": "Ġneed",
            "token_id": 1205,
            "logprob": -2.4688,
            "prob": 0.084
          },
          {
            "token": "'ll",
            "token_id": 3358,
            "logprob": -4.7188,
            "prob": 0.0089
          },
          {
            "token": "Ġguess",
            "token_id": 8101,
            "logprob": -4.9688,
            "prob": 0.0069
          },
          {
            "token": "Ġcan",
            "token_id": 649,
            "logprob": -5.3438,
            "prob": 0.0048
          },
          {
            "token": "Ġsee",
            "token_id": 1518,
            "logprob": -6.0938,
            "prob": 0.0022
          },
          {
            "token": "Ġknow",
            "token_id": 1440,
            "logprob": -6.3438,
            "prob": 0.0017
          },
          {
            "token": "Ġassume",
            "token_id": 9855,
            "logprob": -6.5938,
            "prob": 0.0014
          },
          {
            "token": "'m",
            "token_id": 2846,
            "logprob": -7.0938,
            "prob": 0.0008
          }
        ]
      },
      "selected_token": {
        "token": "Ġneed",
        "token_id": 1205,
        "selection_method": "sampling"
      }
    },
    {
      "step": 71,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters were corrupted when sent. Wait wait, I just need to type the response normally then since they used French. Maybe translate my own English to the French answer? Hmm, user said 'QUI E-SU ?\" so I need",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters",
        "Ġwere",
        "Ġcorrupted",
        "Ġwhen",
        "Ġsent",
        ".",
        "ĠWait",
        "Ġwait",
        ",",
        "ĠI",
        "Ġjust",
        "Ġneed",
        "Ġto",
        "Ġtype",
        "Ġthe",
        "Ġresponse",
        "Ġnormally",
        "Ġthen",
        "Ġsince",
        "Ġthey",
        "Ġused",
        "ĠFrench",
        ".",
        "ĠMaybe",
        "Ġtranslate",
        "Ġmy",
        "Ġown",
        "ĠEnglish",
        "Ġto",
        "Ġthe",
        "ĠFrench",
        "Ġanswer",
        "?",
        "ĠHmm",
        ",",
        "Ġuser",
        "Ġsaid",
        "Ġ'",
        "QUI",
        "ĠE",
        "-S",
        "U",
        "Ġ?",
        "\"",
        "Ġso",
        "ĠI",
        "Ġneed"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885,
        1051,
        50500,
        994,
        3288,
        13,
        14144,
        3868,
        11,
        358,
        1120,
        1205,
        311,
        955,
        279,
        2077,
        14614,
        1243,
        2533,
        814,
        1511,
        8753,
        13,
        10926,
        15025,
        856,
        1866,
        6498,
        311,
        279,
        8753,
        4320,
        30,
        89290,
        11,
        1217,
        1071,
        364,
        12247,
        469,
        6354,
        52,
        949,
        1,
        779,
        358,
        1205
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġto",
            "token_id": 311,
            "logprob": -0.001,
            "prob": 1.0
          },
          {
            "token": "Ġrespond",
            "token_id": 6013,
            "logprob": -7.625,
            "prob": 0.0005
          },
          {
            "token": "Ġa",
            "token_id": 264,
            "logprob": -9.375,
            "prob": 0.0001
          },
          {
            "token": "Ġanswer",
            "token_id": 4320,
            "logprob": -9.625,
            "prob": 0.0001
          },
          {
            "token": "Ġreply",
            "token_id": 10052,
            "logprob": -9.875,
            "prob": 0.0001
          },
          {
            "token": "Ġfirst",
            "token_id": 1176,
            "logprob": -10.5,
            "prob": 0.0
          },
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -10.625,
            "prob": 0.0
          },
          {
            "token": "Ġfigure",
            "token_id": 7216,
            "logprob": -10.75,
            "prob": 0.0
          },
          {
            "token": "Ġcorrect",
            "token_id": 4495,
            "logprob": -10.75,
            "prob": 0.0
          },
          {
            "token": "Ġan",
            "token_id": 459,
            "logprob": -10.75,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġto",
        "token_id": 311,
        "selection_method": "sampling"
      }
    },
    {
      "step": 72,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters were corrupted when sent. Wait wait, I just need to type the response normally then since they used French. Maybe translate my own English to the French answer? Hmm, user said 'QUI E-SU ?\" so I need to",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters",
        "Ġwere",
        "Ġcorrupted",
        "Ġwhen",
        "Ġsent",
        ".",
        "ĠWait",
        "Ġwait",
        ",",
        "ĠI",
        "Ġjust",
        "Ġneed",
        "Ġto",
        "Ġtype",
        "Ġthe",
        "Ġresponse",
        "Ġnormally",
        "Ġthen",
        "Ġsince",
        "Ġthey",
        "Ġused",
        "ĠFrench",
        ".",
        "ĠMaybe",
        "Ġtranslate",
        "Ġmy",
        "Ġown",
        "ĠEnglish",
        "Ġto",
        "Ġthe",
        "ĠFrench",
        "Ġanswer",
        "?",
        "ĠHmm",
        ",",
        "Ġuser",
        "Ġsaid",
        "Ġ'",
        "QUI",
        "ĠE",
        "-S",
        "U",
        "Ġ?",
        "\"",
        "Ġso",
        "ĠI",
        "Ġneed",
        "Ġto"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885,
        1051,
        50500,
        994,
        3288,
        13,
        14144,
        3868,
        11,
        358,
        1120,
        1205,
        311,
        955,
        279,
        2077,
        14614,
        1243,
        2533,
        814,
        1511,
        8753,
        13,
        10926,
        15025,
        856,
        1866,
        6498,
        311,
        279,
        8753,
        4320,
        30,
        89290,
        11,
        1217,
        1071,
        364,
        12247,
        469,
        6354,
        52,
        949,
        1,
        779,
        358,
        1205,
        311
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġrespond",
            "token_id": 6013,
            "logprob": -0.8789,
            "prob": 0.4141
          },
          {
            "token": "Ġanswer",
            "token_id": 4320,
            "logprob": -1.0078,
            "prob": 0.3652
          },
          {
            "token": "Ġreply",
            "token_id": 10052,
            "logprob": -2.125,
            "prob": 0.1187
          },
          {
            "token": "Ġfigure",
            "token_id": 7216,
            "logprob": -3.5,
            "prob": 0.03
          },
          {
            "token": "Ġsay",
            "token_id": 2019,
            "logprob": -4.25,
            "prob": 0.0142
          },
          {
            "token": "Ġcorrect",
            "token_id": 4495,
            "logprob": -4.625,
            "prob": 0.0098
          },
          {
            "token": "Ġask",
            "token_id": 2610,
            "logprob": -5.0,
            "prob": 0.0067
          },
          {
            "token": "Ġrecognize",
            "token_id": 15641,
            "logprob": -5.125,
            "prob": 0.0059
          },
          {
            "token": "Ġparse",
            "token_id": 4820,
            "logprob": -5.375,
            "prob": 0.0046
          },
          {
            "token": "Ġfirst",
            "token_id": 1176,
            "logprob": -5.6875,
            "prob": 0.0034
          }
        ]
      },
      "selected_token": {
        "token": "Ġanswer",
        "token_id": 4320,
        "selection_method": "sampling"
      }
    },
    {
      "step": 73,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters were corrupted when sent. Wait wait, I just need to type the response normally then since they used French. Maybe translate my own English to the French answer? Hmm, user said 'QUI E-SU ?\" so I need to answer",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters",
        "Ġwere",
        "Ġcorrupted",
        "Ġwhen",
        "Ġsent",
        ".",
        "ĠWait",
        "Ġwait",
        ",",
        "ĠI",
        "Ġjust",
        "Ġneed",
        "Ġto",
        "Ġtype",
        "Ġthe",
        "Ġresponse",
        "Ġnormally",
        "Ġthen",
        "Ġsince",
        "Ġthey",
        "Ġused",
        "ĠFrench",
        ".",
        "ĠMaybe",
        "Ġtranslate",
        "Ġmy",
        "Ġown",
        "ĠEnglish",
        "Ġto",
        "Ġthe",
        "ĠFrench",
        "Ġanswer",
        "?",
        "ĠHmm",
        ",",
        "Ġuser",
        "Ġsaid",
        "Ġ'",
        "QUI",
        "ĠE",
        "-S",
        "U",
        "Ġ?",
        "\"",
        "Ġso",
        "ĠI",
        "Ġneed",
        "Ġto",
        "Ġanswer"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885,
        1051,
        50500,
        994,
        3288,
        13,
        14144,
        3868,
        11,
        358,
        1120,
        1205,
        311,
        955,
        279,
        2077,
        14614,
        1243,
        2533,
        814,
        1511,
        8753,
        13,
        10926,
        15025,
        856,
        1866,
        6498,
        311,
        279,
        8753,
        4320,
        30,
        89290,
        11,
        1217,
        1071,
        364,
        12247,
        469,
        6354,
        52,
        949,
        1,
        779,
        358,
        1205,
        311,
        4320
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -0.0161,
            "prob": 0.9844
          },
          {
            "token": "Ġwho",
            "token_id": 889,
            "logprob": -4.9062,
            "prob": 0.0075
          },
          {
            "token": "Ġ\"",
            "token_id": 330,
            "logprob": -5.4688,
            "prob": 0.0043
          },
          {
            "token": "Ġproperly",
            "token_id": 10489,
            "logprob": -7.0938,
            "prob": 0.0008
          },
          {
            "token": "Ġtheir",
            "token_id": 872,
            "logprob": -7.6562,
            "prob": 0.0005
          },
          {
            "token": "Ġappropriately",
            "token_id": 36001,
            "logprob": -7.6562,
            "prob": 0.0005
          },
          {
            "token": "Ġ'",
            "token_id": 364,
            "logprob": -8.0,
            "prob": 0.0003
          },
          {
            "token": "Ġqui",
            "token_id": 7930,
            "logprob": -8.125,
            "prob": 0.0003
          },
          {
            "token": "Ġback",
            "token_id": 1203,
            "logprob": -8.25,
            "prob": 0.0003
          },
          {
            "token": "Ġwhat",
            "token_id": 1148,
            "logprob": -8.4375,
            "prob": 0.0002
          }
        ]
      },
      "selected_token": {
        "token": "Ġ\"",
        "token_id": 330,
        "selection_method": "sampling"
      }
    },
    {
      "step": 74,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters were corrupted when sent. Wait wait, I just need to type the response normally then since they used French. Maybe translate my own English to the French answer? Hmm, user said 'QUI E-SU ?\" so I need to answer \"",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters",
        "Ġwere",
        "Ġcorrupted",
        "Ġwhen",
        "Ġsent",
        ".",
        "ĠWait",
        "Ġwait",
        ",",
        "ĠI",
        "Ġjust",
        "Ġneed",
        "Ġto",
        "Ġtype",
        "Ġthe",
        "Ġresponse",
        "Ġnormally",
        "Ġthen",
        "Ġsince",
        "Ġthey",
        "Ġused",
        "ĠFrench",
        ".",
        "ĠMaybe",
        "Ġtranslate",
        "Ġmy",
        "Ġown",
        "ĠEnglish",
        "Ġto",
        "Ġthe",
        "ĠFrench",
        "Ġanswer",
        "?",
        "ĠHmm",
        ",",
        "Ġuser",
        "Ġsaid",
        "Ġ'",
        "QUI",
        "ĠE",
        "-S",
        "U",
        "Ġ?",
        "\"",
        "Ġso",
        "ĠI",
        "Ġneed",
        "Ġto",
        "Ġanswer",
        "Ġ\""
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885,
        1051,
        50500,
        994,
        3288,
        13,
        14144,
        3868,
        11,
        358,
        1120,
        1205,
        311,
        955,
        279,
        2077,
        14614,
        1243,
        2533,
        814,
        1511,
        8753,
        13,
        10926,
        15025,
        856,
        1866,
        6498,
        311,
        279,
        8753,
        4320,
        30,
        89290,
        11,
        1217,
        1071,
        364,
        12247,
        469,
        6354,
        52,
        949,
        1,
        779,
        358,
        1205,
        311,
        4320,
        330
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Je",
            "token_id": 30854,
            "logprob": -0.498,
            "prob": 0.6094
          },
          {
            "token": "I",
            "token_id": 40,
            "logprob": -1.3125,
            "prob": 0.2695
          },
          {
            "token": "Who",
            "token_id": 15546,
            "logprob": -3.75,
            "prob": 0.0236
          },
          {
            "token": "Qu",
            "token_id": 2232,
            "logprob": -4.0625,
            "prob": 0.0172
          },
          {
            "token": "En",
            "token_id": 1737,
            "logprob": -4.6875,
            "prob": 0.0092
          },
          {
            "token": "Bonjour",
            "token_id": 82681,
            "logprob": -4.6875,
            "prob": 0.0092
          },
          {
            "token": "je",
            "token_id": 3841,
            "logprob": -4.8125,
            "prob": 0.0081
          },
          {
            "token": "Ġje",
            "token_id": 4864,
            "logprob": -5.125,
            "prob": 0.006
          },
          {
            "token": "J",
            "token_id": 41,
            "logprob": -5.25,
            "prob": 0.0052
          },
          {
            "token": "ĠJe",
            "token_id": 14465,
            "logprob": -5.5,
            "prob": 0.0041
          }
        ]
      },
      "selected_token": {
        "token": "Je",
        "token_id": 30854,
        "selection_method": "sampling"
      }
    },
    {
      "step": 75,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters were corrupted when sent. Wait wait, I just need to type the response normally then since they used French. Maybe translate my own English to the French answer? Hmm, user said 'QUI E-SU ?\" so I need to answer \"Je",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters",
        "Ġwere",
        "Ġcorrupted",
        "Ġwhen",
        "Ġsent",
        ".",
        "ĠWait",
        "Ġwait",
        ",",
        "ĠI",
        "Ġjust",
        "Ġneed",
        "Ġto",
        "Ġtype",
        "Ġthe",
        "Ġresponse",
        "Ġnormally",
        "Ġthen",
        "Ġsince",
        "Ġthey",
        "Ġused",
        "ĠFrench",
        ".",
        "ĠMaybe",
        "Ġtranslate",
        "Ġmy",
        "Ġown",
        "ĠEnglish",
        "Ġto",
        "Ġthe",
        "ĠFrench",
        "Ġanswer",
        "?",
        "ĠHmm",
        ",",
        "Ġuser",
        "Ġsaid",
        "Ġ'",
        "QUI",
        "ĠE",
        "-S",
        "U",
        "Ġ?",
        "\"",
        "Ġso",
        "ĠI",
        "Ġneed",
        "Ġto",
        "Ġanswer",
        "Ġ\"",
        "Je"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885,
        1051,
        50500,
        994,
        3288,
        13,
        14144,
        3868,
        11,
        358,
        1120,
        1205,
        311,
        955,
        279,
        2077,
        14614,
        1243,
        2533,
        814,
        1511,
        8753,
        13,
        10926,
        15025,
        856,
        1866,
        6498,
        311,
        279,
        8753,
        4320,
        30,
        89290,
        11,
        1217,
        1071,
        364,
        12247,
        469,
        6354,
        52,
        949,
        1,
        779,
        358,
        1205,
        311,
        4320,
        330,
        30854
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġsuis",
            "token_id": 36731,
            "logprob": -0.005,
            "prob": 0.9961
          },
          {
            "token": "Ġme",
            "token_id": 757,
            "logprob": -6.625,
            "prob": 0.0013
          },
          {
            "token": "Ġne",
            "token_id": 841,
            "logprob": -7.5,
            "prob": 0.0005
          },
          {
            "token": "Ġsont",
            "token_id": 15132,
            "logprob": -8.75,
            "prob": 0.0002
          },
          {
            "token": "Ġm",
            "token_id": 296,
            "logprob": -8.75,
            "prob": 0.0002
          },
          {
            "token": "Ġvais",
            "token_id": 91507,
            "logprob": -9.125,
            "prob": 0.0001
          },
          {
            "token": "Ġbin",
            "token_id": 9736,
            "logprob": -9.5,
            "prob": 0.0001
          },
          {
            "token": "-s",
            "token_id": 1355,
            "logprob": -9.5,
            "prob": 0.0001
          },
          {
            "token": "...\"",
            "token_id": 21908,
            "logprob": -9.5,
            "prob": 0.0001
          },
          {
            "token": "Ġmyself",
            "token_id": 7182,
            "logprob": -9.6875,
            "prob": 0.0001
          }
        ]
      },
      "selected_token": {
        "token": "Ġsont",
        "token_id": 15132,
        "selection_method": "sampling"
      }
    },
    {
      "step": 76,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters were corrupted when sent. Wait wait, I just need to type the response normally then since they used French. Maybe translate my own English to the French answer? Hmm, user said 'QUI E-SU ?\" so I need to answer \"Je sont",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters",
        "Ġwere",
        "Ġcorrupted",
        "Ġwhen",
        "Ġsent",
        ".",
        "ĠWait",
        "Ġwait",
        ",",
        "ĠI",
        "Ġjust",
        "Ġneed",
        "Ġto",
        "Ġtype",
        "Ġthe",
        "Ġresponse",
        "Ġnormally",
        "Ġthen",
        "Ġsince",
        "Ġthey",
        "Ġused",
        "ĠFrench",
        ".",
        "ĠMaybe",
        "Ġtranslate",
        "Ġmy",
        "Ġown",
        "ĠEnglish",
        "Ġto",
        "Ġthe",
        "ĠFrench",
        "Ġanswer",
        "?",
        "ĠHmm",
        ",",
        "Ġuser",
        "Ġsaid",
        "Ġ'",
        "QUI",
        "ĠE",
        "-S",
        "U",
        "Ġ?",
        "\"",
        "Ġso",
        "ĠI",
        "Ġneed",
        "Ġto",
        "Ġanswer",
        "Ġ\"",
        "Je",
        "Ġsont"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885,
        1051,
        50500,
        994,
        3288,
        13,
        14144,
        3868,
        11,
        358,
        1120,
        1205,
        311,
        955,
        279,
        2077,
        14614,
        1243,
        2533,
        814,
        1511,
        8753,
        13,
        10926,
        15025,
        856,
        1866,
        6498,
        311,
        279,
        8753,
        4320,
        30,
        89290,
        11,
        1217,
        1071,
        364,
        12247,
        469,
        6354,
        52,
        949,
        1,
        779,
        358,
        1205,
        311,
        4320,
        330,
        30854,
        15132
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġun",
            "token_id": 653,
            "logprob": -0.8867,
            "prob": 0.4121
          },
          {
            "token": "Ġune",
            "token_id": 6316,
            "logprob": -1.1328,
            "prob": 0.3223
          },
          {
            "token": "ĠDeep",
            "token_id": 18682,
            "logprob": -2.4531,
            "prob": 0.0864
          },
          {
            "token": "...\"",
            "token_id": 21908,
            "logprob": -2.9531,
            "prob": 0.0525
          },
          {
            "token": "Ġle",
            "token_id": 514,
            "logprob": -4.4375,
            "prob": 0.0117
          },
          {
            "token": "...",
            "token_id": 1131,
            "logprob": -4.5938,
            "prob": 0.01
          },
          {
            "token": "Ġla",
            "token_id": 1208,
            "logprob": -5.0,
            "prob": 0.0067
          },
          {
            "token": "Ġl",
            "token_id": 326,
            "logprob": -5.0312,
            "prob": 0.0065
          },
          {
            "token": "ĠAI",
            "token_id": 15592,
            "logprob": -5.0938,
            "prob": 0.0061
          },
          {
            "token": "Ġlà",
            "token_id": 39015,
            "logprob": -5.1562,
            "prob": 0.0057
          }
        ]
      },
      "selected_token": {
        "token": "Ġune",
        "token_id": 6316,
        "selection_method": "sampling"
      }
    },
    {
      "step": 77,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters were corrupted when sent. Wait wait, I just need to type the response normally then since they used French. Maybe translate my own English to the French answer? Hmm, user said 'QUI E-SU ?\" so I need to answer \"Je sont une",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters",
        "Ġwere",
        "Ġcorrupted",
        "Ġwhen",
        "Ġsent",
        ".",
        "ĠWait",
        "Ġwait",
        ",",
        "ĠI",
        "Ġjust",
        "Ġneed",
        "Ġto",
        "Ġtype",
        "Ġthe",
        "Ġresponse",
        "Ġnormally",
        "Ġthen",
        "Ġsince",
        "Ġthey",
        "Ġused",
        "ĠFrench",
        ".",
        "ĠMaybe",
        "Ġtranslate",
        "Ġmy",
        "Ġown",
        "ĠEnglish",
        "Ġto",
        "Ġthe",
        "ĠFrench",
        "Ġanswer",
        "?",
        "ĠHmm",
        ",",
        "Ġuser",
        "Ġsaid",
        "Ġ'",
        "QUI",
        "ĠE",
        "-S",
        "U",
        "Ġ?",
        "\"",
        "Ġso",
        "ĠI",
        "Ġneed",
        "Ġto",
        "Ġanswer",
        "Ġ\"",
        "Je",
        "Ġsont",
        "Ġune"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885,
        1051,
        50500,
        994,
        3288,
        13,
        14144,
        3868,
        11,
        358,
        1120,
        1205,
        311,
        955,
        279,
        2077,
        14614,
        1243,
        2533,
        814,
        1511,
        8753,
        13,
        10926,
        15025,
        856,
        1866,
        6498,
        311,
        279,
        8753,
        4320,
        30,
        89290,
        11,
        1217,
        1071,
        364,
        12247,
        469,
        6354,
        52,
        949,
        1,
        779,
        358,
        1205,
        311,
        4320,
        330,
        30854,
        15132,
        6316
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "ĠIA",
            "token_id": 44190,
            "logprob": -0.2334,
            "prob": 0.793
          },
          {
            "token": "Ġassistant",
            "token_id": 18328,
            "logprob": -2.9219,
            "prob": 0.054
          },
          {
            "token": "Ġintelligence",
            "token_id": 11478,
            "logprob": -3.2344,
            "prob": 0.0396
          },
          {
            "token": "Ġassist",
            "token_id": 7945,
            "logprob": -4.2188,
            "prob": 0.0145
          },
          {
            "token": "...\"",
            "token_id": 21908,
            "logprob": -4.4688,
            "prob": 0.0113
          },
          {
            "token": "Ġintelig",
            "token_id": 90745,
            "logprob": -4.5938,
            "prob": 0.0099
          },
          {
            "token": "ĠAI",
            "token_id": 15592,
            "logprob": -4.7812,
            "prob": 0.0082
          },
          {
            "token": "ĠAssistant",
            "token_id": 22103,
            "logprob": -4.8438,
            "prob": 0.0078
          },
          {
            "token": "Ġintellig",
            "token_id": 60538,
            "logprob": -5.6875,
            "prob": 0.0033
          },
          {
            "token": "ĠIntelligence",
            "token_id": 22107,
            "logprob": -5.7188,
            "prob": 0.0032
          }
        ]
      },
      "selected_token": {
        "token": "ĠAssistant",
        "token_id": 22103,
        "selection_method": "sampling"
      }
    },
    {
      "step": 78,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters were corrupted when sent. Wait wait, I just need to type the response normally then since they used French. Maybe translate my own English to the French answer? Hmm, user said 'QUI E-SU ?\" so I need to answer \"Je sont une Assistant",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters",
        "Ġwere",
        "Ġcorrupted",
        "Ġwhen",
        "Ġsent",
        ".",
        "ĠWait",
        "Ġwait",
        ",",
        "ĠI",
        "Ġjust",
        "Ġneed",
        "Ġto",
        "Ġtype",
        "Ġthe",
        "Ġresponse",
        "Ġnormally",
        "Ġthen",
        "Ġsince",
        "Ġthey",
        "Ġused",
        "ĠFrench",
        ".",
        "ĠMaybe",
        "Ġtranslate",
        "Ġmy",
        "Ġown",
        "ĠEnglish",
        "Ġto",
        "Ġthe",
        "ĠFrench",
        "Ġanswer",
        "?",
        "ĠHmm",
        ",",
        "Ġuser",
        "Ġsaid",
        "Ġ'",
        "QUI",
        "ĠE",
        "-S",
        "U",
        "Ġ?",
        "\"",
        "Ġso",
        "ĠI",
        "Ġneed",
        "Ġto",
        "Ġanswer",
        "Ġ\"",
        "Je",
        "Ġsont",
        "Ġune",
        "ĠAssistant"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885,
        1051,
        50500,
        994,
        3288,
        13,
        14144,
        3868,
        11,
        358,
        1120,
        1205,
        311,
        955,
        279,
        2077,
        14614,
        1243,
        2533,
        814,
        1511,
        8753,
        13,
        10926,
        15025,
        856,
        1866,
        6498,
        311,
        279,
        8753,
        4320,
        30,
        89290,
        11,
        1217,
        1071,
        364,
        12247,
        469,
        6354,
        52,
        949,
        1,
        779,
        358,
        1205,
        311,
        4320,
        330,
        30854,
        15132,
        6316,
        22103
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "ĠIA",
            "token_id": 44190,
            "logprob": -0.291,
            "prob": 0.7461
          },
          {
            "token": "IA",
            "token_id": 5987,
            "logprob": -2.7969,
            "prob": 0.0613
          },
          {
            "token": "ĠAI",
            "token_id": 15592,
            "logprob": -3.1094,
            "prob": 0.0449
          },
          {
            "token": "Ġde",
            "token_id": 409,
            "logprob": -4.1562,
            "prob": 0.0155
          },
          {
            "token": "ĠDeep",
            "token_id": 18682,
            "logprob": -4.4062,
            "prob": 0.0121
          },
          {
            "token": "ĠIntelligence",
            "token_id": 22107,
            "logprob": -4.4375,
            "prob": 0.0117
          },
          {
            "token": "Ġintelligence",
            "token_id": 11478,
            "logprob": -4.625,
            "prob": 0.0097
          },
          {
            "token": "...\"",
            "token_id": 21908,
            "logprob": -4.8438,
            "prob": 0.0078
          },
          {
            "token": "Ġintellig",
            "token_id": 60538,
            "logprob": -5.1562,
            "prob": 0.0057
          },
          {
            "token": "Ġbas",
            "token_id": 3122,
            "logprob": -5.25,
            "prob": 0.0052
          }
        ]
      },
      "selected_token": {
        "token": "...\"",
        "token_id": 21908,
        "selection_method": "sampling"
      }
    },
    {
      "step": 79,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters were corrupted when sent. Wait wait, I just need to type the response normally then since they used French. Maybe translate my own English to the French answer? Hmm, user said 'QUI E-SU ?\" so I need to answer \"Je sont une Assistant...\"",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters",
        "Ġwere",
        "Ġcorrupted",
        "Ġwhen",
        "Ġsent",
        ".",
        "ĠWait",
        "Ġwait",
        ",",
        "ĠI",
        "Ġjust",
        "Ġneed",
        "Ġto",
        "Ġtype",
        "Ġthe",
        "Ġresponse",
        "Ġnormally",
        "Ġthen",
        "Ġsince",
        "Ġthey",
        "Ġused",
        "ĠFrench",
        ".",
        "ĠMaybe",
        "Ġtranslate",
        "Ġmy",
        "Ġown",
        "ĠEnglish",
        "Ġto",
        "Ġthe",
        "ĠFrench",
        "Ġanswer",
        "?",
        "ĠHmm",
        ",",
        "Ġuser",
        "Ġsaid",
        "Ġ'",
        "QUI",
        "ĠE",
        "-S",
        "U",
        "Ġ?",
        "\"",
        "Ġso",
        "ĠI",
        "Ġneed",
        "Ġto",
        "Ġanswer",
        "Ġ\"",
        "Je",
        "Ġsont",
        "Ġune",
        "ĠAssistant",
        "...\""
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885,
        1051,
        50500,
        994,
        3288,
        13,
        14144,
        3868,
        11,
        358,
        1120,
        1205,
        311,
        955,
        279,
        2077,
        14614,
        1243,
        2533,
        814,
        1511,
        8753,
        13,
        10926,
        15025,
        856,
        1866,
        6498,
        311,
        279,
        8753,
        4320,
        30,
        89290,
        11,
        1217,
        1071,
        364,
        12247,
        469,
        6354,
        52,
        949,
        1,
        779,
        358,
        1205,
        311,
        4320,
        330,
        30854,
        15132,
        6316,
        22103,
        21908
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġbut",
            "token_id": 719,
            "logprob": -1.2109,
            "prob": 0.2969
          },
          {
            "token": "Ġor",
            "token_id": 477,
            "logprob": -1.5859,
            "prob": 0.2051
          },
          {
            "token": "ĠWait",
            "token_id": 14144,
            "logprob": -1.7109,
            "prob": 0.1807
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -1.7109,
            "prob": 0.1807
          },
          {
            "token": "Ġwait",
            "token_id": 3868,
            "logprob": -3.5938,
            "prob": 0.0277
          },
          {
            "token": "Ġetc",
            "token_id": 5099,
            "logprob": -3.7188,
            "prob": 0.0244
          },
          {
            "token": "Ġno",
            "token_id": 912,
            "logprob": -3.9688,
            "prob": 0.019
          },
          {
            "token": "Ġsomething",
            "token_id": 2555,
            "logprob": -4.4688,
            "prob": 0.0115
          },
          {
            "token": "?",
            "token_id": 30,
            "logprob": -5.0938,
            "prob": 0.0062
          },
          {
            "token": "ĠNo",
            "token_id": 2360,
            "logprob": -5.0938,
            "prob": 0.0062
          }
        ]
      },
      "selected_token": {
        "token": "ĠNo",
        "token_id": 2360,
        "selection_method": "sampling"
      }
    },
    {
      "step": 80,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters were corrupted when sent. Wait wait, I just need to type the response normally then since they used French. Maybe translate my own English to the French answer? Hmm, user said 'QUI E-SU ?\" so I need to answer \"Je sont une Assistant...\" No",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters",
        "Ġwere",
        "Ġcorrupted",
        "Ġwhen",
        "Ġsent",
        ".",
        "ĠWait",
        "Ġwait",
        ",",
        "ĠI",
        "Ġjust",
        "Ġneed",
        "Ġto",
        "Ġtype",
        "Ġthe",
        "Ġresponse",
        "Ġnormally",
        "Ġthen",
        "Ġsince",
        "Ġthey",
        "Ġused",
        "ĠFrench",
        ".",
        "ĠMaybe",
        "Ġtranslate",
        "Ġmy",
        "Ġown",
        "ĠEnglish",
        "Ġto",
        "Ġthe",
        "ĠFrench",
        "Ġanswer",
        "?",
        "ĠHmm",
        ",",
        "Ġuser",
        "Ġsaid",
        "Ġ'",
        "QUI",
        "ĠE",
        "-S",
        "U",
        "Ġ?",
        "\"",
        "Ġso",
        "ĠI",
        "Ġneed",
        "Ġto",
        "Ġanswer",
        "Ġ\"",
        "Je",
        "Ġsont",
        "Ġune",
        "ĠAssistant",
        "...\"",
        "ĠNo"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885,
        1051,
        50500,
        994,
        3288,
        13,
        14144,
        3868,
        11,
        358,
        1120,
        1205,
        311,
        955,
        279,
        2077,
        14614,
        1243,
        2533,
        814,
        1511,
        8753,
        13,
        10926,
        15025,
        856,
        1866,
        6498,
        311,
        279,
        8753,
        4320,
        30,
        89290,
        11,
        1217,
        1071,
        364,
        12247,
        469,
        6354,
        52,
        949,
        1,
        779,
        358,
        1205,
        311,
        4320,
        330,
        30854,
        15132,
        6316,
        22103,
        21908,
        2360
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": ",",
            "token_id": 11,
            "logprob": -0.2383,
            "prob": 0.7891
          },
          {
            "token": "Ġwait",
            "token_id": 3868,
            "logprob": -1.6172,
            "prob": 0.1992
          },
          {
            "token": ".",
            "token_id": 13,
            "logprob": -4.75,
            "prob": 0.0087
          },
          {
            "token": "Ġactually",
            "token_id": 3604,
            "logprob": -7.25,
            "prob": 0.0007
          },
          {
            "token": "Ġbetter",
            "token_id": 2731,
            "logprob": -7.75,
            "prob": 0.0004
          },
          {
            "token": ".\n\n",
            "token_id": 382,
            "logprob": -8.125,
            "prob": 0.0003
          },
          {
            "token": "Ġneed",
            "token_id": 1205,
            "logprob": -8.25,
            "prob": 0.0003
          },
          {
            "token": "Ġmaybe",
            "token_id": 7344,
            "logprob": -8.375,
            "prob": 0.0002
          },
          {
            "token": ":",
            "token_id": 25,
            "logprob": -8.75,
            "prob": 0.0002
          },
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -8.875,
            "prob": 0.0001
          }
        ]
      },
      "selected_token": {
        "token": "Ġwait",
        "token_id": 3868,
        "selection_method": "sampling"
      }
    },
    {
      "step": 81,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters were corrupted when sent. Wait wait, I just need to type the response normally then since they used French. Maybe translate my own English to the French answer? Hmm, user said 'QUI E-SU ?\" so I need to answer \"Je sont une Assistant...\" No wait",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters",
        "Ġwere",
        "Ġcorrupted",
        "Ġwhen",
        "Ġsent",
        ".",
        "ĠWait",
        "Ġwait",
        ",",
        "ĠI",
        "Ġjust",
        "Ġneed",
        "Ġto",
        "Ġtype",
        "Ġthe",
        "Ġresponse",
        "Ġnormally",
        "Ġthen",
        "Ġsince",
        "Ġthey",
        "Ġused",
        "ĠFrench",
        ".",
        "ĠMaybe",
        "Ġtranslate",
        "Ġmy",
        "Ġown",
        "ĠEnglish",
        "Ġto",
        "Ġthe",
        "ĠFrench",
        "Ġanswer",
        "?",
        "ĠHmm",
        ",",
        "Ġuser",
        "Ġsaid",
        "Ġ'",
        "QUI",
        "ĠE",
        "-S",
        "U",
        "Ġ?",
        "\"",
        "Ġso",
        "ĠI",
        "Ġneed",
        "Ġto",
        "Ġanswer",
        "Ġ\"",
        "Je",
        "Ġsont",
        "Ġune",
        "ĠAssistant",
        "...\"",
        "ĠNo",
        "Ġwait"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885,
        1051,
        50500,
        994,
        3288,
        13,
        14144,
        3868,
        11,
        358,
        1120,
        1205,
        311,
        955,
        279,
        2077,
        14614,
        1243,
        2533,
        814,
        1511,
        8753,
        13,
        10926,
        15025,
        856,
        1866,
        6498,
        311,
        279,
        8753,
        4320,
        30,
        89290,
        11,
        1217,
        1071,
        364,
        12247,
        469,
        6354,
        52,
        949,
        1,
        779,
        358,
        1205,
        311,
        4320,
        330,
        30854,
        15132,
        6316,
        22103,
        21908,
        2360,
        3868
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": ",",
            "token_id": 11,
            "logprob": -0.0952,
            "prob": 0.9102
          },
          {
            "token": ".",
            "token_id": 13,
            "logprob": -2.9688,
            "prob": 0.0513
          },
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -4.3438,
            "prob": 0.013
          },
          {
            "token": "Ġ\"",
            "token_id": 330,
            "logprob": -5.0938,
            "prob": 0.0061
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -5.3438,
            "prob": 0.0048
          },
          {
            "token": ".\n\n",
            "token_id": 382,
            "logprob": -6.2188,
            "prob": 0.002
          },
          {
            "token": "ĠFrench",
            "token_id": 8753,
            "logprob": -6.2188,
            "prob": 0.002
          },
          {
            "token": ":",
            "token_id": 25,
            "logprob": -6.2188,
            "prob": 0.002
          },
          {
            "token": "Ġwait",
            "token_id": 3868,
            "logprob": -6.3438,
            "prob": 0.0018
          },
          {
            "token": "Ġfirst",
            "token_id": 1176,
            "logprob": -6.5938,
            "prob": 0.0014
          }
        ]
      },
      "selected_token": {
        "token": ",",
        "token_id": 11,
        "selection_method": "sampling"
      }
    },
    {
      "step": 82,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters were corrupted when sent. Wait wait, I just need to type the response normally then since they used French. Maybe translate my own English to the French answer? Hmm, user said 'QUI E-SU ?\" so I need to answer \"Je sont une Assistant...\" No wait,",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters",
        "Ġwere",
        "Ġcorrupted",
        "Ġwhen",
        "Ġsent",
        ".",
        "ĠWait",
        "Ġwait",
        ",",
        "ĠI",
        "Ġjust",
        "Ġneed",
        "Ġto",
        "Ġtype",
        "Ġthe",
        "Ġresponse",
        "Ġnormally",
        "Ġthen",
        "Ġsince",
        "Ġthey",
        "Ġused",
        "ĠFrench",
        ".",
        "ĠMaybe",
        "Ġtranslate",
        "Ġmy",
        "Ġown",
        "ĠEnglish",
        "Ġto",
        "Ġthe",
        "ĠFrench",
        "Ġanswer",
        "?",
        "ĠHmm",
        ",",
        "Ġuser",
        "Ġsaid",
        "Ġ'",
        "QUI",
        "ĠE",
        "-S",
        "U",
        "Ġ?",
        "\"",
        "Ġso",
        "ĠI",
        "Ġneed",
        "Ġto",
        "Ġanswer",
        "Ġ\"",
        "Je",
        "Ġsont",
        "Ġune",
        "ĠAssistant",
        "...\"",
        "ĠNo",
        "Ġwait",
        ","
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885,
        1051,
        50500,
        994,
        3288,
        13,
        14144,
        3868,
        11,
        358,
        1120,
        1205,
        311,
        955,
        279,
        2077,
        14614,
        1243,
        2533,
        814,
        1511,
        8753,
        13,
        10926,
        15025,
        856,
        1866,
        6498,
        311,
        279,
        8753,
        4320,
        30,
        89290,
        11,
        1217,
        1071,
        364,
        12247,
        469,
        6354,
        52,
        949,
        1,
        779,
        358,
        1205,
        311,
        4320,
        330,
        30854,
        15132,
        6316,
        22103,
        21908,
        2360,
        3868,
        11
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġ\"",
            "token_id": 330,
            "logprob": -1.4688,
            "prob": 0.2295
          },
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -1.7188,
            "prob": 0.1787
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -1.8438,
            "prob": 0.1582
          },
          {
            "token": "ĠI",
            "token_id": 358,
            "logprob": -2.5938,
            "prob": 0.0747
          },
          {
            "token": "Ġactually",
            "token_id": 3604,
            "logprob": -3.0938,
            "prob": 0.0452
          },
          {
            "token": "Ġwait",
            "token_id": 3868,
            "logprob": -3.2188,
            "prob": 0.04
          },
          {
            "token": "Ġno",
            "token_id": 912,
            "logprob": -3.5938,
            "prob": 0.0275
          },
          {
            "token": "Ġfirst",
            "token_id": 1176,
            "logprob": -3.5938,
            "prob": 0.0275
          },
          {
            "token": "Ġlet",
            "token_id": 1095,
            "logprob": -3.7188,
            "prob": 0.0243
          },
          {
            "token": "Ġcorrect",
            "token_id": 4495,
            "logprob": -3.7188,
            "prob": 0.0243
          }
        ]
      },
      "selected_token": {
        "token": "Ġin",
        "token_id": 304,
        "selection_method": "sampling"
      }
    },
    {
      "step": 83,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters were corrupted when sent. Wait wait, I just need to type the response normally then since they used French. Maybe translate my own English to the French answer? Hmm, user said 'QUI E-SU ?\" so I need to answer \"Je sont une Assistant...\" No wait, in",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters",
        "Ġwere",
        "Ġcorrupted",
        "Ġwhen",
        "Ġsent",
        ".",
        "ĠWait",
        "Ġwait",
        ",",
        "ĠI",
        "Ġjust",
        "Ġneed",
        "Ġto",
        "Ġtype",
        "Ġthe",
        "Ġresponse",
        "Ġnormally",
        "Ġthen",
        "Ġsince",
        "Ġthey",
        "Ġused",
        "ĠFrench",
        ".",
        "ĠMaybe",
        "Ġtranslate",
        "Ġmy",
        "Ġown",
        "ĠEnglish",
        "Ġto",
        "Ġthe",
        "ĠFrench",
        "Ġanswer",
        "?",
        "ĠHmm",
        ",",
        "Ġuser",
        "Ġsaid",
        "Ġ'",
        "QUI",
        "ĠE",
        "-S",
        "U",
        "Ġ?",
        "\"",
        "Ġso",
        "ĠI",
        "Ġneed",
        "Ġto",
        "Ġanswer",
        "Ġ\"",
        "Je",
        "Ġsont",
        "Ġune",
        "ĠAssistant",
        "...\"",
        "ĠNo",
        "Ġwait",
        ",",
        "Ġin"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885,
        1051,
        50500,
        994,
        3288,
        13,
        14144,
        3868,
        11,
        358,
        1120,
        1205,
        311,
        955,
        279,
        2077,
        14614,
        1243,
        2533,
        814,
        1511,
        8753,
        13,
        10926,
        15025,
        856,
        1866,
        6498,
        311,
        279,
        8753,
        4320,
        30,
        89290,
        11,
        1217,
        1071,
        364,
        12247,
        469,
        6354,
        52,
        949,
        1,
        779,
        358,
        1205,
        311,
        4320,
        330,
        30854,
        15132,
        6316,
        22103,
        21908,
        2360,
        3868,
        11,
        304
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "ĠFrench",
            "token_id": 8753,
            "logprob": -0.0096,
            "prob": 0.9922
          },
          {
            "token": "ĠEnglish",
            "token_id": 6498,
            "logprob": -5.125,
            "prob": 0.0059
          },
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -6.25,
            "prob": 0.0019
          },
          {
            "token": "Ġproper",
            "token_id": 6300,
            "logprob": -7.375,
            "prob": 0.0006
          },
          {
            "token": "Ġmy",
            "token_id": 856,
            "logprob": -8.25,
            "prob": 0.0003
          },
          {
            "token": "Ġfrench",
            "token_id": 42293,
            "logprob": -8.375,
            "prob": 0.0002
          },
          {
            "token": "Ġresponse",
            "token_id": 2077,
            "logprob": -8.9375,
            "prob": 0.0001
          },
          {
            "token": "Ġany",
            "token_id": 904,
            "logprob": -9.75,
            "prob": 0.0001
          },
          {
            "token": "ĠSpanish",
            "token_id": 15506,
            "logprob": -10.0,
            "prob": 0.0
          },
          {
            "token": "Ġcorrect",
            "token_id": 4495,
            "logprob": -10.0625,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġfrench",
        "token_id": 42293,
        "selection_method": "sampling"
      }
    },
    {
      "step": 84,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters were corrupted when sent. Wait wait, I just need to type the response normally then since they used French. Maybe translate my own English to the French answer? Hmm, user said 'QUI E-SU ?\" so I need to answer \"Je sont une Assistant...\" No wait, in french",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters",
        "Ġwere",
        "Ġcorrupted",
        "Ġwhen",
        "Ġsent",
        ".",
        "ĠWait",
        "Ġwait",
        ",",
        "ĠI",
        "Ġjust",
        "Ġneed",
        "Ġto",
        "Ġtype",
        "Ġthe",
        "Ġresponse",
        "Ġnormally",
        "Ġthen",
        "Ġsince",
        "Ġthey",
        "Ġused",
        "ĠFrench",
        ".",
        "ĠMaybe",
        "Ġtranslate",
        "Ġmy",
        "Ġown",
        "ĠEnglish",
        "Ġto",
        "Ġthe",
        "ĠFrench",
        "Ġanswer",
        "?",
        "ĠHmm",
        ",",
        "Ġuser",
        "Ġsaid",
        "Ġ'",
        "QUI",
        "ĠE",
        "-S",
        "U",
        "Ġ?",
        "\"",
        "Ġso",
        "ĠI",
        "Ġneed",
        "Ġto",
        "Ġanswer",
        "Ġ\"",
        "Je",
        "Ġsont",
        "Ġune",
        "ĠAssistant",
        "...\"",
        "ĠNo",
        "Ġwait",
        ",",
        "Ġin",
        "Ġfrench"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885,
        1051,
        50500,
        994,
        3288,
        13,
        14144,
        3868,
        11,
        358,
        1120,
        1205,
        311,
        955,
        279,
        2077,
        14614,
        1243,
        2533,
        814,
        1511,
        8753,
        13,
        10926,
        15025,
        856,
        1866,
        6498,
        311,
        279,
        8753,
        4320,
        30,
        89290,
        11,
        1217,
        1071,
        364,
        12247,
        469,
        6354,
        52,
        949,
        1,
        779,
        358,
        1205,
        311,
        4320,
        330,
        30854,
        15132,
        6316,
        22103,
        21908,
        2360,
        3868,
        11,
        304,
        42293
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": ",",
            "token_id": 11,
            "logprob": -0.543,
            "prob": 0.582
          },
          {
            "token": "Ġ\"",
            "token_id": 330,
            "logprob": -1.6719,
            "prob": 0.1885
          },
          {
            "token": ":",
            "token_id": 25,
            "logprob": -2.6719,
            "prob": 0.0693
          },
          {
            "token": ".",
            "token_id": 13,
            "logprob": -2.7969,
            "prob": 0.0613
          },
          {
            "token": "ĠI",
            "token_id": 358,
            "logprob": -3.1719,
            "prob": 0.042
          },
          {
            "token": "Ġit",
            "token_id": 433,
            "logprob": -3.2969,
            "prob": 0.0371
          },
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -5.2812,
            "prob": 0.005
          },
          {
            "token": ".\n\n",
            "token_id": 382,
            "logprob": -5.6562,
            "prob": 0.0034
          },
          {
            "token": "Ġ'",
            "token_id": 364,
            "logprob": -6.1562,
            "prob": 0.0021
          },
          {
            "token": "Ġmy",
            "token_id": 856,
            "logprob": -6.4688,
            "prob": 0.0015
          }
        ]
      },
      "selected_token": {
        "token": "ĠI",
        "token_id": 358,
        "selection_method": "sampling"
      }
    },
    {
      "step": 85,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters were corrupted when sent. Wait wait, I just need to type the response normally then since they used French. Maybe translate my own English to the French answer? Hmm, user said 'QUI E-SU ?\" so I need to answer \"Je sont une Assistant...\" No wait, in french I",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters",
        "Ġwere",
        "Ġcorrupted",
        "Ġwhen",
        "Ġsent",
        ".",
        "ĠWait",
        "Ġwait",
        ",",
        "ĠI",
        "Ġjust",
        "Ġneed",
        "Ġto",
        "Ġtype",
        "Ġthe",
        "Ġresponse",
        "Ġnormally",
        "Ġthen",
        "Ġsince",
        "Ġthey",
        "Ġused",
        "ĠFrench",
        ".",
        "ĠMaybe",
        "Ġtranslate",
        "Ġmy",
        "Ġown",
        "ĠEnglish",
        "Ġto",
        "Ġthe",
        "ĠFrench",
        "Ġanswer",
        "?",
        "ĠHmm",
        ",",
        "Ġuser",
        "Ġsaid",
        "Ġ'",
        "QUI",
        "ĠE",
        "-S",
        "U",
        "Ġ?",
        "\"",
        "Ġso",
        "ĠI",
        "Ġneed",
        "Ġto",
        "Ġanswer",
        "Ġ\"",
        "Je",
        "Ġsont",
        "Ġune",
        "ĠAssistant",
        "...\"",
        "ĠNo",
        "Ġwait",
        ",",
        "Ġin",
        "Ġfrench",
        "ĠI"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885,
        1051,
        50500,
        994,
        3288,
        13,
        14144,
        3868,
        11,
        358,
        1120,
        1205,
        311,
        955,
        279,
        2077,
        14614,
        1243,
        2533,
        814,
        1511,
        8753,
        13,
        10926,
        15025,
        856,
        1866,
        6498,
        311,
        279,
        8753,
        4320,
        30,
        89290,
        11,
        1217,
        1071,
        364,
        12247,
        469,
        6354,
        52,
        949,
        1,
        779,
        358,
        1205,
        311,
        4320,
        330,
        30854,
        15132,
        6316,
        22103,
        21908,
        2360,
        3868,
        11,
        304,
        42293,
        358
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġshould",
            "token_id": 1288,
            "logprob": -0.1465,
            "prob": 0.8633
          },
          {
            "token": "Ġwould",
            "token_id": 1053,
            "logprob": -2.6406,
            "prob": 0.0708
          },
          {
            "token": "Ġam",
            "token_id": 1097,
            "logprob": -3.6406,
            "prob": 0.0261
          },
          {
            "token": "'m",
            "token_id": 2846,
            "logprob": -4.2812,
            "prob": 0.014
          },
          {
            "token": "'d",
            "token_id": 4265,
            "logprob": -4.4062,
            "prob": 0.0123
          },
          {
            "token": "Ġneed",
            "token_id": 1205,
            "logprob": -5.4062,
            "prob": 0.0045
          },
          {
            "token": "Ġsay",
            "token_id": 2019,
            "logprob": -5.9062,
            "prob": 0.0027
          },
          {
            "token": "Ġthink",
            "token_id": 1781,
            "logprob": -6.6562,
            "prob": 0.0013
          },
          {
            "token": "Ġhave",
            "token_id": 617,
            "logprob": -6.9062,
            "prob": 0.001
          },
          {
            "token": "Ġmean",
            "token_id": 3152,
            "logprob": -6.9062,
            "prob": 0.001
          }
        ]
      },
      "selected_token": {
        "token": "'d",
        "token_id": 4265,
        "selection_method": "sampling"
      }
    },
    {
      "step": 86,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters were corrupted when sent. Wait wait, I just need to type the response normally then since they used French. Maybe translate my own English to the French answer? Hmm, user said 'QUI E-SU ?\" so I need to answer \"Je sont une Assistant...\" No wait, in french I'd",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters",
        "Ġwere",
        "Ġcorrupted",
        "Ġwhen",
        "Ġsent",
        ".",
        "ĠWait",
        "Ġwait",
        ",",
        "ĠI",
        "Ġjust",
        "Ġneed",
        "Ġto",
        "Ġtype",
        "Ġthe",
        "Ġresponse",
        "Ġnormally",
        "Ġthen",
        "Ġsince",
        "Ġthey",
        "Ġused",
        "ĠFrench",
        ".",
        "ĠMaybe",
        "Ġtranslate",
        "Ġmy",
        "Ġown",
        "ĠEnglish",
        "Ġto",
        "Ġthe",
        "ĠFrench",
        "Ġanswer",
        "?",
        "ĠHmm",
        ",",
        "Ġuser",
        "Ġsaid",
        "Ġ'",
        "QUI",
        "ĠE",
        "-S",
        "U",
        "Ġ?",
        "\"",
        "Ġso",
        "ĠI",
        "Ġneed",
        "Ġto",
        "Ġanswer",
        "Ġ\"",
        "Je",
        "Ġsont",
        "Ġune",
        "ĠAssistant",
        "...\"",
        "ĠNo",
        "Ġwait",
        ",",
        "Ġin",
        "Ġfrench",
        "ĠI",
        "'d"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885,
        1051,
        50500,
        994,
        3288,
        13,
        14144,
        3868,
        11,
        358,
        1120,
        1205,
        311,
        955,
        279,
        2077,
        14614,
        1243,
        2533,
        814,
        1511,
        8753,
        13,
        10926,
        15025,
        856,
        1866,
        6498,
        311,
        279,
        8753,
        4320,
        30,
        89290,
        11,
        1217,
        1071,
        364,
        12247,
        469,
        6354,
        52,
        949,
        1,
        779,
        358,
        1205,
        311,
        4320,
        330,
        30854,
        15132,
        6316,
        22103,
        21908,
        2360,
        3868,
        11,
        304,
        42293,
        358,
        4265
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġsay",
            "token_id": 2019,
            "logprob": -0.0181,
            "prob": 0.9805
          },
          {
            "token": "Ġbe",
            "token_id": 387,
            "logprob": -5.0312,
            "prob": 0.0066
          },
          {
            "token": "Ġstart",
            "token_id": 1212,
            "logprob": -5.2812,
            "prob": 0.0052
          },
          {
            "token": "Ġrespond",
            "token_id": 6013,
            "logprob": -6.7812,
            "prob": 0.0012
          },
          {
            "token": "Ġneed",
            "token_id": 1205,
            "logprob": -6.7812,
            "prob": 0.0012
          },
          {
            "token": "Ġanswer",
            "token_id": 4320,
            "logprob": -6.7812,
            "prob": 0.0012
          },
          {
            "token": "Ġhave",
            "token_id": 617,
            "logprob": -7.1562,
            "prob": 0.0008
          },
          {
            "token": "Ġbetter",
            "token_id": 2731,
            "logprob": -7.4062,
            "prob": 0.0006
          },
          {
            "token": "Ġprobably",
            "token_id": 4762,
            "logprob": -7.8438,
            "prob": 0.0004
          },
          {
            "token": "Ġjust",
            "token_id": 1120,
            "logprob": -8.875,
            "prob": 0.0001
          }
        ]
      },
      "selected_token": {
        "token": "Ġprobably",
        "token_id": 4762,
        "selection_method": "sampling"
      }
    },
    {
      "step": 87,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters were corrupted when sent. Wait wait, I just need to type the response normally then since they used French. Maybe translate my own English to the French answer? Hmm, user said 'QUI E-SU ?\" so I need to answer \"Je sont une Assistant...\" No wait, in french I'd probably",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters",
        "Ġwere",
        "Ġcorrupted",
        "Ġwhen",
        "Ġsent",
        ".",
        "ĠWait",
        "Ġwait",
        ",",
        "ĠI",
        "Ġjust",
        "Ġneed",
        "Ġto",
        "Ġtype",
        "Ġthe",
        "Ġresponse",
        "Ġnormally",
        "Ġthen",
        "Ġsince",
        "Ġthey",
        "Ġused",
        "ĠFrench",
        ".",
        "ĠMaybe",
        "Ġtranslate",
        "Ġmy",
        "Ġown",
        "ĠEnglish",
        "Ġto",
        "Ġthe",
        "ĠFrench",
        "Ġanswer",
        "?",
        "ĠHmm",
        ",",
        "Ġuser",
        "Ġsaid",
        "Ġ'",
        "QUI",
        "ĠE",
        "-S",
        "U",
        "Ġ?",
        "\"",
        "Ġso",
        "ĠI",
        "Ġneed",
        "Ġto",
        "Ġanswer",
        "Ġ\"",
        "Je",
        "Ġsont",
        "Ġune",
        "ĠAssistant",
        "...\"",
        "ĠNo",
        "Ġwait",
        ",",
        "Ġin",
        "Ġfrench",
        "ĠI",
        "'d",
        "Ġprobably"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885,
        1051,
        50500,
        994,
        3288,
        13,
        14144,
        3868,
        11,
        358,
        1120,
        1205,
        311,
        955,
        279,
        2077,
        14614,
        1243,
        2533,
        814,
        1511,
        8753,
        13,
        10926,
        15025,
        856,
        1866,
        6498,
        311,
        279,
        8753,
        4320,
        30,
        89290,
        11,
        1217,
        1071,
        364,
        12247,
        469,
        6354,
        52,
        949,
        1,
        779,
        358,
        1205,
        311,
        4320,
        330,
        30854,
        15132,
        6316,
        22103,
        21908,
        2360,
        3868,
        11,
        304,
        42293,
        358,
        4265,
        4762
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġsay",
            "token_id": 2019,
            "logprob": -0.0957,
            "prob": 0.9102
          },
          {
            "token": "Ġstart",
            "token_id": 1212,
            "logprob": -3.0938,
            "prob": 0.0452
          },
          {
            "token": "Ġanswer",
            "token_id": 4320,
            "logprob": -4.2188,
            "prob": 0.0147
          },
          {
            "token": "Ġbe",
            "token_id": 387,
            "logprob": -4.5938,
            "prob": 0.0101
          },
          {
            "token": "Ġrespond",
            "token_id": 6013,
            "logprob": -4.7188,
            "prob": 0.0089
          },
          {
            "token": "Ġjust",
            "token_id": 1120,
            "logprob": -4.9688,
            "prob": 0.0069
          },
          {
            "token": "Ġreply",
            "token_id": 10052,
            "logprob": -6.4688,
            "prob": 0.0015
          },
          {
            "token": "Ġneed",
            "token_id": 1205,
            "logprob": -6.9688,
            "prob": 0.0009
          },
          {
            "token": "Ġwant",
            "token_id": 1390,
            "logprob": -7.3438,
            "prob": 0.0006
          },
          {
            "token": "Ġwrite",
            "token_id": 3350,
            "logprob": -8.125,
            "prob": 0.0003
          }
        ]
      },
      "selected_token": {
        "token": "Ġbe",
        "token_id": 387,
        "selection_method": "sampling"
      }
    },
    {
      "step": 88,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters were corrupted when sent. Wait wait, I just need to type the response normally then since they used French. Maybe translate my own English to the French answer? Hmm, user said 'QUI E-SU ?\" so I need to answer \"Je sont une Assistant...\" No wait, in french I'd probably be",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters",
        "Ġwere",
        "Ġcorrupted",
        "Ġwhen",
        "Ġsent",
        ".",
        "ĠWait",
        "Ġwait",
        ",",
        "ĠI",
        "Ġjust",
        "Ġneed",
        "Ġto",
        "Ġtype",
        "Ġthe",
        "Ġresponse",
        "Ġnormally",
        "Ġthen",
        "Ġsince",
        "Ġthey",
        "Ġused",
        "ĠFrench",
        ".",
        "ĠMaybe",
        "Ġtranslate",
        "Ġmy",
        "Ġown",
        "ĠEnglish",
        "Ġto",
        "Ġthe",
        "ĠFrench",
        "Ġanswer",
        "?",
        "ĠHmm",
        ",",
        "Ġuser",
        "Ġsaid",
        "Ġ'",
        "QUI",
        "ĠE",
        "-S",
        "U",
        "Ġ?",
        "\"",
        "Ġso",
        "ĠI",
        "Ġneed",
        "Ġto",
        "Ġanswer",
        "Ġ\"",
        "Je",
        "Ġsont",
        "Ġune",
        "ĠAssistant",
        "...\"",
        "ĠNo",
        "Ġwait",
        ",",
        "Ġin",
        "Ġfrench",
        "ĠI",
        "'d",
        "Ġprobably",
        "Ġbe"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885,
        1051,
        50500,
        994,
        3288,
        13,
        14144,
        3868,
        11,
        358,
        1120,
        1205,
        311,
        955,
        279,
        2077,
        14614,
        1243,
        2533,
        814,
        1511,
        8753,
        13,
        10926,
        15025,
        856,
        1866,
        6498,
        311,
        279,
        8753,
        4320,
        30,
        89290,
        11,
        1217,
        1071,
        364,
        12247,
        469,
        6354,
        52,
        949,
        1,
        779,
        358,
        1205,
        311,
        4320,
        330,
        30854,
        15132,
        6316,
        22103,
        21908,
        2360,
        3868,
        11,
        304,
        42293,
        358,
        4265,
        4762,
        387
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġ\"",
            "token_id": 330,
            "logprob": -0.668,
            "prob": 0.5117
          },
          {
            "token": "Ġasked",
            "token_id": 4691,
            "logprob": -1.3594,
            "prob": 0.2578
          },
          {
            "token": "Ġintroduced",
            "token_id": 11784,
            "logprob": -2.9844,
            "prob": 0.0508
          },
          {
            "token": "Ġlike",
            "token_id": 1093,
            "logprob": -3.1719,
            "prob": 0.042
          },
          {
            "token": ":",
            "token_id": 25,
            "logprob": -3.6719,
            "prob": 0.0255
          },
          {
            "token": "Ġgreeted",
            "token_id": 44865,
            "logprob": -3.6719,
            "prob": 0.0255
          },
          {
            "token": "Ġanswering",
            "token_id": 36864,
            "logprob": -4.7188,
            "prob": 0.0088
          },
          {
            "token": "Ġmore",
            "token_id": 810,
            "logprob": -4.8438,
            "prob": 0.0078
          },
          {
            "token": "Ġaddressed",
            "token_id": 20669,
            "logprob": -4.9688,
            "prob": 0.0069
          },
          {
            "token": "Ġtold",
            "token_id": 3309,
            "logprob": -5.2188,
            "prob": 0.0053
          }
        ]
      },
      "selected_token": {
        "token": ":",
        "token_id": 25,
        "selection_method": "sampling"
      }
    },
    {
      "step": 89,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters were corrupted when sent. Wait wait, I just need to type the response normally then since they used French. Maybe translate my own English to the French answer? Hmm, user said 'QUI E-SU ?\" so I need to answer \"Je sont une Assistant...\" No wait, in french I'd probably be:",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters",
        "Ġwere",
        "Ġcorrupted",
        "Ġwhen",
        "Ġsent",
        ".",
        "ĠWait",
        "Ġwait",
        ",",
        "ĠI",
        "Ġjust",
        "Ġneed",
        "Ġto",
        "Ġtype",
        "Ġthe",
        "Ġresponse",
        "Ġnormally",
        "Ġthen",
        "Ġsince",
        "Ġthey",
        "Ġused",
        "ĠFrench",
        ".",
        "ĠMaybe",
        "Ġtranslate",
        "Ġmy",
        "Ġown",
        "ĠEnglish",
        "Ġto",
        "Ġthe",
        "ĠFrench",
        "Ġanswer",
        "?",
        "ĠHmm",
        ",",
        "Ġuser",
        "Ġsaid",
        "Ġ'",
        "QUI",
        "ĠE",
        "-S",
        "U",
        "Ġ?",
        "\"",
        "Ġso",
        "ĠI",
        "Ġneed",
        "Ġto",
        "Ġanswer",
        "Ġ\"",
        "Je",
        "Ġsont",
        "Ġune",
        "ĠAssistant",
        "...\"",
        "ĠNo",
        "Ġwait",
        ",",
        "Ġin",
        "Ġfrench",
        "ĠI",
        "'d",
        "Ġprobably",
        "Ġbe",
        ":"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885,
        1051,
        50500,
        994,
        3288,
        13,
        14144,
        3868,
        11,
        358,
        1120,
        1205,
        311,
        955,
        279,
        2077,
        14614,
        1243,
        2533,
        814,
        1511,
        8753,
        13,
        10926,
        15025,
        856,
        1866,
        6498,
        311,
        279,
        8753,
        4320,
        30,
        89290,
        11,
        1217,
        1071,
        364,
        12247,
        469,
        6354,
        52,
        949,
        1,
        779,
        358,
        1205,
        311,
        4320,
        330,
        30854,
        15132,
        6316,
        22103,
        21908,
        2360,
        3868,
        11,
        304,
        42293,
        358,
        4265,
        4762,
        387,
        25
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġ\"",
            "token_id": 330,
            "logprob": -0.0122,
            "prob": 0.9883
          },
          {
            "token": "ĠJe",
            "token_id": 14465,
            "logprob": -4.75,
            "prob": 0.0085
          },
          {
            "token": "ĠI",
            "token_id": 358,
            "logprob": -6.375,
            "prob": 0.0017
          },
          {
            "token": "je",
            "token_id": 3841,
            "logprob": -8.6875,
            "prob": 0.0002
          },
          {
            "token": "Ġ“",
            "token_id": 1054,
            "logprob": -8.75,
            "prob": 0.0002
          },
          {
            "token": "ĠWhat",
            "token_id": 3639,
            "logprob": -8.9375,
            "prob": 0.0001
          },
          {
            "token": "Ġje",
            "token_id": 4864,
            "logprob": -9.0,
            "prob": 0.0001
          },
          {
            "token": "Ġ'",
            "token_id": 364,
            "logprob": -9.125,
            "prob": 0.0001
          },
          {
            "token": "ĠEn",
            "token_id": 2998,
            "logprob": -9.3125,
            "prob": 0.0001
          },
          {
            "token": "ĠQu",
            "token_id": 3489,
            "logprob": -9.5,
            "prob": 0.0001
          }
        ]
      },
      "selected_token": {
        "token": "Ġ\"",
        "token_id": 330,
        "selection_method": "sampling"
      }
    },
    {
      "step": 90,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters were corrupted when sent. Wait wait, I just need to type the response normally then since they used French. Maybe translate my own English to the French answer? Hmm, user said 'QUI E-SU ?\" so I need to answer \"Je sont une Assistant...\" No wait, in french I'd probably be: \"",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters",
        "Ġwere",
        "Ġcorrupted",
        "Ġwhen",
        "Ġsent",
        ".",
        "ĠWait",
        "Ġwait",
        ",",
        "ĠI",
        "Ġjust",
        "Ġneed",
        "Ġto",
        "Ġtype",
        "Ġthe",
        "Ġresponse",
        "Ġnormally",
        "Ġthen",
        "Ġsince",
        "Ġthey",
        "Ġused",
        "ĠFrench",
        ".",
        "ĠMaybe",
        "Ġtranslate",
        "Ġmy",
        "Ġown",
        "ĠEnglish",
        "Ġto",
        "Ġthe",
        "ĠFrench",
        "Ġanswer",
        "?",
        "ĠHmm",
        ",",
        "Ġuser",
        "Ġsaid",
        "Ġ'",
        "QUI",
        "ĠE",
        "-S",
        "U",
        "Ġ?",
        "\"",
        "Ġso",
        "ĠI",
        "Ġneed",
        "Ġto",
        "Ġanswer",
        "Ġ\"",
        "Je",
        "Ġsont",
        "Ġune",
        "ĠAssistant",
        "...\"",
        "ĠNo",
        "Ġwait",
        ",",
        "Ġin",
        "Ġfrench",
        "ĠI",
        "'d",
        "Ġprobably",
        "Ġbe",
        ":",
        "Ġ\""
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885,
        1051,
        50500,
        994,
        3288,
        13,
        14144,
        3868,
        11,
        358,
        1120,
        1205,
        311,
        955,
        279,
        2077,
        14614,
        1243,
        2533,
        814,
        1511,
        8753,
        13,
        10926,
        15025,
        856,
        1866,
        6498,
        311,
        279,
        8753,
        4320,
        30,
        89290,
        11,
        1217,
        1071,
        364,
        12247,
        469,
        6354,
        52,
        949,
        1,
        779,
        358,
        1205,
        311,
        4320,
        330,
        30854,
        15132,
        6316,
        22103,
        21908,
        2360,
        3868,
        11,
        304,
        42293,
        358,
        4265,
        4762,
        387,
        25,
        330
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Je",
            "token_id": 30854,
            "logprob": -0.0272,
            "prob": 0.9727
          },
          {
            "token": "Bonjour",
            "token_id": 82681,
            "logprob": -4.5938,
            "prob": 0.0101
          },
          {
            "token": "En",
            "token_id": 1737,
            "logprob": -5.2188,
            "prob": 0.0054
          },
          {
            "token": "J",
            "token_id": 41,
            "logprob": -5.6562,
            "prob": 0.0035
          },
          {
            "token": "ĠJe",
            "token_id": 14465,
            "logprob": -6.1562,
            "prob": 0.0021
          },
          {
            "token": "je",
            "token_id": 3841,
            "logprob": -6.5938,
            "prob": 0.0014
          },
          {
            "token": "Ġje",
            "token_id": 4864,
            "logprob": -7.1562,
            "prob": 0.0008
          },
          {
            "token": "I",
            "token_id": 40,
            "logprob": -7.9688,
            "prob": 0.0003
          },
          {
            "token": "ĠJE",
            "token_id": 71430,
            "logprob": -8.625,
            "prob": 0.0002
          },
          {
            "token": "Hello",
            "token_id": 9906,
            "logprob": -8.6875,
            "prob": 0.0002
          }
        ]
      },
      "selected_token": {
        "token": "Je",
        "token_id": 30854,
        "selection_method": "sampling"
      }
    },
    {
      "step": 91,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters were corrupted when sent. Wait wait, I just need to type the response normally then since they used French. Maybe translate my own English to the French answer? Hmm, user said 'QUI E-SU ?\" so I need to answer \"Je sont une Assistant...\" No wait, in french I'd probably be: \"Je",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters",
        "Ġwere",
        "Ġcorrupted",
        "Ġwhen",
        "Ġsent",
        ".",
        "ĠWait",
        "Ġwait",
        ",",
        "ĠI",
        "Ġjust",
        "Ġneed",
        "Ġto",
        "Ġtype",
        "Ġthe",
        "Ġresponse",
        "Ġnormally",
        "Ġthen",
        "Ġsince",
        "Ġthey",
        "Ġused",
        "ĠFrench",
        ".",
        "ĠMaybe",
        "Ġtranslate",
        "Ġmy",
        "Ġown",
        "ĠEnglish",
        "Ġto",
        "Ġthe",
        "ĠFrench",
        "Ġanswer",
        "?",
        "ĠHmm",
        ",",
        "Ġuser",
        "Ġsaid",
        "Ġ'",
        "QUI",
        "ĠE",
        "-S",
        "U",
        "Ġ?",
        "\"",
        "Ġso",
        "ĠI",
        "Ġneed",
        "Ġto",
        "Ġanswer",
        "Ġ\"",
        "Je",
        "Ġsont",
        "Ġune",
        "ĠAssistant",
        "...\"",
        "ĠNo",
        "Ġwait",
        ",",
        "Ġin",
        "Ġfrench",
        "ĠI",
        "'d",
        "Ġprobably",
        "Ġbe",
        ":",
        "Ġ\"",
        "Je"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885,
        1051,
        50500,
        994,
        3288,
        13,
        14144,
        3868,
        11,
        358,
        1120,
        1205,
        311,
        955,
        279,
        2077,
        14614,
        1243,
        2533,
        814,
        1511,
        8753,
        13,
        10926,
        15025,
        856,
        1866,
        6498,
        311,
        279,
        8753,
        4320,
        30,
        89290,
        11,
        1217,
        1071,
        364,
        12247,
        469,
        6354,
        52,
        949,
        1,
        779,
        358,
        1205,
        311,
        4320,
        330,
        30854,
        15132,
        6316,
        22103,
        21908,
        2360,
        3868,
        11,
        304,
        42293,
        358,
        4265,
        4762,
        387,
        25,
        330,
        30854
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġsuis",
            "token_id": 36731,
            "logprob": -0.0028,
            "prob": 0.9961
          },
          {
            "token": "Ġme",
            "token_id": 757,
            "logprob": -7.5,
            "prob": 0.0006
          },
          {
            "token": "Ġsont",
            "token_id": 15132,
            "logprob": -8.4375,
            "prob": 0.0002
          },
          {
            "token": "Ġne",
            "token_id": 841,
            "logprob": -8.625,
            "prob": 0.0002
          },
          {
            "token": "Ġmyself",
            "token_id": 7182,
            "logprob": -9.25,
            "prob": 0.0001
          },
          {
            "token": "Ġvais",
            "token_id": 91507,
            "logprob": -10.0625,
            "prob": 0.0
          },
          {
            "token": "Ġvous",
            "token_id": 9189,
            "logprob": -10.1875,
            "prob": 0.0
          },
          {
            "token": "-s",
            "token_id": 1355,
            "logprob": -10.3125,
            "prob": 0.0
          },
          {
            "token": "Ġêtes",
            "token_id": 62299,
            "logprob": -10.3125,
            "prob": 0.0
          },
          {
            "token": "Ġsommes",
            "token_id": 74533,
            "logprob": -10.5,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġsuis",
        "token_id": 36731,
        "selection_method": "sampling"
      }
    },
    {
      "step": 92,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters were corrupted when sent. Wait wait, I just need to type the response normally then since they used French. Maybe translate my own English to the French answer? Hmm, user said 'QUI E-SU ?\" so I need to answer \"Je sont une Assistant...\" No wait, in french I'd probably be: \"Je suis",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters",
        "Ġwere",
        "Ġcorrupted",
        "Ġwhen",
        "Ġsent",
        ".",
        "ĠWait",
        "Ġwait",
        ",",
        "ĠI",
        "Ġjust",
        "Ġneed",
        "Ġto",
        "Ġtype",
        "Ġthe",
        "Ġresponse",
        "Ġnormally",
        "Ġthen",
        "Ġsince",
        "Ġthey",
        "Ġused",
        "ĠFrench",
        ".",
        "ĠMaybe",
        "Ġtranslate",
        "Ġmy",
        "Ġown",
        "ĠEnglish",
        "Ġto",
        "Ġthe",
        "ĠFrench",
        "Ġanswer",
        "?",
        "ĠHmm",
        ",",
        "Ġuser",
        "Ġsaid",
        "Ġ'",
        "QUI",
        "ĠE",
        "-S",
        "U",
        "Ġ?",
        "\"",
        "Ġso",
        "ĠI",
        "Ġneed",
        "Ġto",
        "Ġanswer",
        "Ġ\"",
        "Je",
        "Ġsont",
        "Ġune",
        "ĠAssistant",
        "...\"",
        "ĠNo",
        "Ġwait",
        ",",
        "Ġin",
        "Ġfrench",
        "ĠI",
        "'d",
        "Ġprobably",
        "Ġbe",
        ":",
        "Ġ\"",
        "Je",
        "Ġsuis"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885,
        1051,
        50500,
        994,
        3288,
        13,
        14144,
        3868,
        11,
        358,
        1120,
        1205,
        311,
        955,
        279,
        2077,
        14614,
        1243,
        2533,
        814,
        1511,
        8753,
        13,
        10926,
        15025,
        856,
        1866,
        6498,
        311,
        279,
        8753,
        4320,
        30,
        89290,
        11,
        1217,
        1071,
        364,
        12247,
        469,
        6354,
        52,
        949,
        1,
        779,
        358,
        1205,
        311,
        4320,
        330,
        30854,
        15132,
        6316,
        22103,
        21908,
        2360,
        3868,
        11,
        304,
        42293,
        358,
        4265,
        4762,
        387,
        25,
        330,
        30854,
        36731
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġune",
            "token_id": 6316,
            "logprob": -0.2207,
            "prob": 0.8008
          },
          {
            "token": "Ġun",
            "token_id": 653,
            "logprob": -1.9688,
            "prob": 0.1396
          },
          {
            "token": "ĠDeep",
            "token_id": 18682,
            "logprob": -3.7812,
            "prob": 0.0227
          },
          {
            "token": "Ġvotre",
            "token_id": 15265,
            "logprob": -3.9688,
            "prob": 0.0188
          },
          {
            "token": "ĠYour",
            "token_id": 4718,
            "logprob": -5.2812,
            "prob": 0.0051
          },
          {
            "token": "Ġyour",
            "token_id": 701,
            "logprob": -6.0312,
            "prob": 0.0024
          },
          {
            "token": "Ġl",
            "token_id": 326,
            "logprob": -6.9062,
            "prob": 0.001
          },
          {
            "token": "...\"",
            "token_id": 21908,
            "logprob": -7.125,
            "prob": 0.0008
          },
          {
            "token": "Ġuna",
            "token_id": 5203,
            "logprob": -7.125,
            "prob": 0.0008
          },
          {
            "token": "ĠUne",
            "token_id": 39783,
            "logprob": -7.3438,
            "prob": 0.0006
          }
        ]
      },
      "selected_token": {
        "token": "Ġuna",
        "token_id": 5203,
        "selection_method": "sampling"
      }
    },
    {
      "step": 93,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters were corrupted when sent. Wait wait, I just need to type the response normally then since they used French. Maybe translate my own English to the French answer? Hmm, user said 'QUI E-SU ?\" so I need to answer \"Je sont une Assistant...\" No wait, in french I'd probably be: \"Je suis una",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters",
        "Ġwere",
        "Ġcorrupted",
        "Ġwhen",
        "Ġsent",
        ".",
        "ĠWait",
        "Ġwait",
        ",",
        "ĠI",
        "Ġjust",
        "Ġneed",
        "Ġto",
        "Ġtype",
        "Ġthe",
        "Ġresponse",
        "Ġnormally",
        "Ġthen",
        "Ġsince",
        "Ġthey",
        "Ġused",
        "ĠFrench",
        ".",
        "ĠMaybe",
        "Ġtranslate",
        "Ġmy",
        "Ġown",
        "ĠEnglish",
        "Ġto",
        "Ġthe",
        "ĠFrench",
        "Ġanswer",
        "?",
        "ĠHmm",
        ",",
        "Ġuser",
        "Ġsaid",
        "Ġ'",
        "QUI",
        "ĠE",
        "-S",
        "U",
        "Ġ?",
        "\"",
        "Ġso",
        "ĠI",
        "Ġneed",
        "Ġto",
        "Ġanswer",
        "Ġ\"",
        "Je",
        "Ġsont",
        "Ġune",
        "ĠAssistant",
        "...\"",
        "ĠNo",
        "Ġwait",
        ",",
        "Ġin",
        "Ġfrench",
        "ĠI",
        "'d",
        "Ġprobably",
        "Ġbe",
        ":",
        "Ġ\"",
        "Je",
        "Ġsuis",
        "Ġuna"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885,
        1051,
        50500,
        994,
        3288,
        13,
        14144,
        3868,
        11,
        358,
        1120,
        1205,
        311,
        955,
        279,
        2077,
        14614,
        1243,
        2533,
        814,
        1511,
        8753,
        13,
        10926,
        15025,
        856,
        1866,
        6498,
        311,
        279,
        8753,
        4320,
        30,
        89290,
        11,
        1217,
        1071,
        364,
        12247,
        469,
        6354,
        52,
        949,
        1,
        779,
        358,
        1205,
        311,
        4320,
        330,
        30854,
        15132,
        6316,
        22103,
        21908,
        2360,
        3868,
        11,
        304,
        42293,
        358,
        4265,
        4762,
        387,
        25,
        330,
        30854,
        36731,
        5203
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġassistant",
            "token_id": 18328,
            "logprob": -0.3418,
            "prob": 0.7109
          },
          {
            "token": "Ġassist",
            "token_id": 7945,
            "logprob": -2.1562,
            "prob": 0.1157
          },
          {
            "token": "ĠAssistant",
            "token_id": 22103,
            "logprob": -2.4062,
            "prob": 0.0903
          },
          {
            "token": "...\"",
            "token_id": 21908,
            "logprob": -4.2812,
            "prob": 0.0139
          },
          {
            "token": "-ass",
            "token_id": 35915,
            "logprob": -4.4062,
            "prob": 0.0122
          },
          {
            "token": "Ġassistants",
            "token_id": 57619,
            "logprob": -5.0938,
            "prob": 0.0061
          },
          {
            "token": "...",
            "token_id": 1131,
            "logprob": -5.2812,
            "prob": 0.0051
          },
          {
            "token": "ĠAssist",
            "token_id": 54349,
            "logprob": -5.3438,
            "prob": 0.0048
          },
          {
            "token": "Ġassistance",
            "token_id": 13291,
            "logprob": -5.5312,
            "prob": 0.004
          },
          {
            "token": "ĠASS",
            "token_id": 36660,
            "logprob": -5.5312,
            "prob": 0.004
          }
        ]
      },
      "selected_token": {
        "token": "Ġassistance",
        "token_id": 13291,
        "selection_method": "sampling"
      }
    },
    {
      "step": 94,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters were corrupted when sent. Wait wait, I just need to type the response normally then since they used French. Maybe translate my own English to the French answer? Hmm, user said 'QUI E-SU ?\" so I need to answer \"Je sont une Assistant...\" No wait, in french I'd probably be: \"Je suis una assistance",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters",
        "Ġwere",
        "Ġcorrupted",
        "Ġwhen",
        "Ġsent",
        ".",
        "ĠWait",
        "Ġwait",
        ",",
        "ĠI",
        "Ġjust",
        "Ġneed",
        "Ġto",
        "Ġtype",
        "Ġthe",
        "Ġresponse",
        "Ġnormally",
        "Ġthen",
        "Ġsince",
        "Ġthey",
        "Ġused",
        "ĠFrench",
        ".",
        "ĠMaybe",
        "Ġtranslate",
        "Ġmy",
        "Ġown",
        "ĠEnglish",
        "Ġto",
        "Ġthe",
        "ĠFrench",
        "Ġanswer",
        "?",
        "ĠHmm",
        ",",
        "Ġuser",
        "Ġsaid",
        "Ġ'",
        "QUI",
        "ĠE",
        "-S",
        "U",
        "Ġ?",
        "\"",
        "Ġso",
        "ĠI",
        "Ġneed",
        "Ġto",
        "Ġanswer",
        "Ġ\"",
        "Je",
        "Ġsont",
        "Ġune",
        "ĠAssistant",
        "...\"",
        "ĠNo",
        "Ġwait",
        ",",
        "Ġin",
        "Ġfrench",
        "ĠI",
        "'d",
        "Ġprobably",
        "Ġbe",
        ":",
        "Ġ\"",
        "Je",
        "Ġsuis",
        "Ġuna",
        "Ġassistance"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885,
        1051,
        50500,
        994,
        3288,
        13,
        14144,
        3868,
        11,
        358,
        1120,
        1205,
        311,
        955,
        279,
        2077,
        14614,
        1243,
        2533,
        814,
        1511,
        8753,
        13,
        10926,
        15025,
        856,
        1866,
        6498,
        311,
        279,
        8753,
        4320,
        30,
        89290,
        11,
        1217,
        1071,
        364,
        12247,
        469,
        6354,
        52,
        949,
        1,
        779,
        358,
        1205,
        311,
        4320,
        330,
        30854,
        15132,
        6316,
        22103,
        21908,
        2360,
        3868,
        11,
        304,
        42293,
        358,
        4265,
        4762,
        387,
        25,
        330,
        30854,
        36731,
        5203,
        13291
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "ĠIA",
            "token_id": 44190,
            "logprob": -1.0234,
            "prob": 0.3594
          },
          {
            "token": "...\"",
            "token_id": 21908,
            "logprob": -1.6484,
            "prob": 0.1924
          },
          {
            "token": "...",
            "token_id": 1131,
            "logprob": -2.8125,
            "prob": 0.0603
          },
          {
            "token": "Ġvirt",
            "token_id": 13053,
            "logprob": -3.125,
            "prob": 0.0442
          },
          {
            "token": "IA",
            "token_id": 5987,
            "logprob": -3.2188,
            "prob": 0.0403
          },
          {
            "token": "Ġintellig",
            "token_id": 60538,
            "logprob": -3.4375,
            "prob": 0.0322
          },
          {
            "token": "Ġintelligence",
            "token_id": 11478,
            "logprob": -4.2188,
            "prob": 0.0148
          },
          {
            "token": "ia",
            "token_id": 689,
            "logprob": -4.4062,
            "prob": 0.0123
          },
          {
            "token": "Ġen",
            "token_id": 665,
            "logprob": -4.875,
            "prob": 0.0077
          },
          {
            "token": "ĠAI",
            "token_id": 15592,
            "logprob": -5.0,
            "prob": 0.0068
          }
        ]
      },
      "selected_token": {
        "token": "Ġvirt",
        "token_id": 13053,
        "selection_method": "sampling"
      }
    },
    {
      "step": 95,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters were corrupted when sent. Wait wait, I just need to type the response normally then since they used French. Maybe translate my own English to the French answer? Hmm, user said 'QUI E-SU ?\" so I need to answer \"Je sont une Assistant...\" No wait, in french I'd probably be: \"Je suis una assistance virt",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters",
        "Ġwere",
        "Ġcorrupted",
        "Ġwhen",
        "Ġsent",
        ".",
        "ĠWait",
        "Ġwait",
        ",",
        "ĠI",
        "Ġjust",
        "Ġneed",
        "Ġto",
        "Ġtype",
        "Ġthe",
        "Ġresponse",
        "Ġnormally",
        "Ġthen",
        "Ġsince",
        "Ġthey",
        "Ġused",
        "ĠFrench",
        ".",
        "ĠMaybe",
        "Ġtranslate",
        "Ġmy",
        "Ġown",
        "ĠEnglish",
        "Ġto",
        "Ġthe",
        "ĠFrench",
        "Ġanswer",
        "?",
        "ĠHmm",
        ",",
        "Ġuser",
        "Ġsaid",
        "Ġ'",
        "QUI",
        "ĠE",
        "-S",
        "U",
        "Ġ?",
        "\"",
        "Ġso",
        "ĠI",
        "Ġneed",
        "Ġto",
        "Ġanswer",
        "Ġ\"",
        "Je",
        "Ġsont",
        "Ġune",
        "ĠAssistant",
        "...\"",
        "ĠNo",
        "Ġwait",
        ",",
        "Ġin",
        "Ġfrench",
        "ĠI",
        "'d",
        "Ġprobably",
        "Ġbe",
        ":",
        "Ġ\"",
        "Je",
        "Ġsuis",
        "Ġuna",
        "Ġassistance",
        "Ġvirt"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885,
        1051,
        50500,
        994,
        3288,
        13,
        14144,
        3868,
        11,
        358,
        1120,
        1205,
        311,
        955,
        279,
        2077,
        14614,
        1243,
        2533,
        814,
        1511,
        8753,
        13,
        10926,
        15025,
        856,
        1866,
        6498,
        311,
        279,
        8753,
        4320,
        30,
        89290,
        11,
        1217,
        1071,
        364,
        12247,
        469,
        6354,
        52,
        949,
        1,
        779,
        358,
        1205,
        311,
        4320,
        330,
        30854,
        15132,
        6316,
        22103,
        21908,
        2360,
        3868,
        11,
        304,
        42293,
        358,
        4265,
        4762,
        387,
        25,
        330,
        30854,
        36731,
        5203,
        13291,
        13053
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "uelle",
            "token_id": 31037,
            "logprob": -0.0177,
            "prob": 0.9805
          },
          {
            "token": "uel",
            "token_id": 4088,
            "logprob": -4.1562,
            "prob": 0.0159
          },
          {
            "token": "u",
            "token_id": 84,
            "logprob": -7.5312,
            "prob": 0.0005
          },
          {
            "token": "uale",
            "token_id": 50411,
            "logprob": -8.375,
            "prob": 0.0002
          },
          {
            "token": "uele",
            "token_id": 75443,
            "logprob": -8.4375,
            "prob": 0.0002
          },
          {
            "token": "uels",
            "token_id": 61244,
            "logprob": -8.5,
            "prob": 0.0002
          },
          {
            "token": "UEL",
            "token_id": 68195,
            "logprob": -8.875,
            "prob": 0.0001
          },
          {
            "token": "ue",
            "token_id": 361,
            "logprob": -9.875,
            "prob": 0.0001
          },
          {
            "token": "ua",
            "token_id": 4381,
            "logprob": -10.0625,
            "prob": 0.0
          },
          {
            "token": "uelles",
            "token_id": 65832,
            "logprob": -10.125,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "ue",
        "token_id": 361,
        "selection_method": "sampling"
      }
    },
    {
      "step": 96,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters were corrupted when sent. Wait wait, I just need to type the response normally then since they used French. Maybe translate my own English to the French answer? Hmm, user said 'QUI E-SU ?\" so I need to answer \"Je sont une Assistant...\" No wait, in french I'd probably be: \"Je suis una assistance virtue",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters",
        "Ġwere",
        "Ġcorrupted",
        "Ġwhen",
        "Ġsent",
        ".",
        "ĠWait",
        "Ġwait",
        ",",
        "ĠI",
        "Ġjust",
        "Ġneed",
        "Ġto",
        "Ġtype",
        "Ġthe",
        "Ġresponse",
        "Ġnormally",
        "Ġthen",
        "Ġsince",
        "Ġthey",
        "Ġused",
        "ĠFrench",
        ".",
        "ĠMaybe",
        "Ġtranslate",
        "Ġmy",
        "Ġown",
        "ĠEnglish",
        "Ġto",
        "Ġthe",
        "ĠFrench",
        "Ġanswer",
        "?",
        "ĠHmm",
        ",",
        "Ġuser",
        "Ġsaid",
        "Ġ'",
        "QUI",
        "ĠE",
        "-S",
        "U",
        "Ġ?",
        "\"",
        "Ġso",
        "ĠI",
        "Ġneed",
        "Ġto",
        "Ġanswer",
        "Ġ\"",
        "Je",
        "Ġsont",
        "Ġune",
        "ĠAssistant",
        "...\"",
        "ĠNo",
        "Ġwait",
        ",",
        "Ġin",
        "Ġfrench",
        "ĠI",
        "'d",
        "Ġprobably",
        "Ġbe",
        ":",
        "Ġ\"",
        "Je",
        "Ġsuis",
        "Ġuna",
        "Ġassistance",
        "Ġvirt",
        "ue"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885,
        1051,
        50500,
        994,
        3288,
        13,
        14144,
        3868,
        11,
        358,
        1120,
        1205,
        311,
        955,
        279,
        2077,
        14614,
        1243,
        2533,
        814,
        1511,
        8753,
        13,
        10926,
        15025,
        856,
        1866,
        6498,
        311,
        279,
        8753,
        4320,
        30,
        89290,
        11,
        1217,
        1071,
        364,
        12247,
        469,
        6354,
        52,
        949,
        1,
        779,
        358,
        1205,
        311,
        4320,
        330,
        30854,
        15132,
        6316,
        22103,
        21908,
        2360,
        3868,
        11,
        304,
        42293,
        358,
        4265,
        4762,
        387,
        25,
        330,
        30854,
        36731,
        5203,
        13291,
        13053,
        361
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "le",
            "token_id": 273,
            "logprob": -1.3203,
            "prob": 0.2676
          },
          {
            "token": "use",
            "token_id": 817,
            "logprob": -1.6953,
            "prob": 0.1836
          },
          {
            "token": "ille",
            "token_id": 4618,
            "logprob": -1.9453,
            "prob": 0.1436
          },
          {
            "token": "ux",
            "token_id": 2249,
            "logprob": -1.9453,
            "prob": 0.1436
          },
          {
            "token": "...\"",
            "token_id": 21908,
            "logprob": -3.1875,
            "prob": 0.041
          },
          {
            "token": "lle",
            "token_id": 66601,
            "logprob": -3.1875,
            "prob": 0.041
          },
          {
            "token": ".\"",
            "token_id": 1210,
            "logprob": -3.375,
            "prob": 0.0339
          },
          {
            "token": "...",
            "token_id": 1131,
            "logprob": -3.9375,
            "prob": 0.0194
          },
          {
            "token": "Ġuse",
            "token_id": 1005,
            "logprob": -4.4688,
            "prob": 0.0114
          },
          {
            "token": "\"",
            "token_id": 1,
            "logprob": -5.2812,
            "prob": 0.0051
          }
        ]
      },
      "selected_token": {
        "token": "ux",
        "token_id": 2249,
        "selection_method": "sampling"
      }
    },
    {
      "step": 97,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters were corrupted when sent. Wait wait, I just need to type the response normally then since they used French. Maybe translate my own English to the French answer? Hmm, user said 'QUI E-SU ?\" so I need to answer \"Je sont une Assistant...\" No wait, in french I'd probably be: \"Je suis una assistance virtueux",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters",
        "Ġwere",
        "Ġcorrupted",
        "Ġwhen",
        "Ġsent",
        ".",
        "ĠWait",
        "Ġwait",
        ",",
        "ĠI",
        "Ġjust",
        "Ġneed",
        "Ġto",
        "Ġtype",
        "Ġthe",
        "Ġresponse",
        "Ġnormally",
        "Ġthen",
        "Ġsince",
        "Ġthey",
        "Ġused",
        "ĠFrench",
        ".",
        "ĠMaybe",
        "Ġtranslate",
        "Ġmy",
        "Ġown",
        "ĠEnglish",
        "Ġto",
        "Ġthe",
        "ĠFrench",
        "Ġanswer",
        "?",
        "ĠHmm",
        ",",
        "Ġuser",
        "Ġsaid",
        "Ġ'",
        "QUI",
        "ĠE",
        "-S",
        "U",
        "Ġ?",
        "\"",
        "Ġso",
        "ĠI",
        "Ġneed",
        "Ġto",
        "Ġanswer",
        "Ġ\"",
        "Je",
        "Ġsont",
        "Ġune",
        "ĠAssistant",
        "...\"",
        "ĠNo",
        "Ġwait",
        ",",
        "Ġin",
        "Ġfrench",
        "ĠI",
        "'d",
        "Ġprobably",
        "Ġbe",
        ":",
        "Ġ\"",
        "Je",
        "Ġsuis",
        "Ġuna",
        "Ġassistance",
        "Ġvirt",
        "ue",
        "ux"
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885,
        1051,
        50500,
        994,
        3288,
        13,
        14144,
        3868,
        11,
        358,
        1120,
        1205,
        311,
        955,
        279,
        2077,
        14614,
        1243,
        2533,
        814,
        1511,
        8753,
        13,
        10926,
        15025,
        856,
        1866,
        6498,
        311,
        279,
        8753,
        4320,
        30,
        89290,
        11,
        1217,
        1071,
        364,
        12247,
        469,
        6354,
        52,
        949,
        1,
        779,
        358,
        1205,
        311,
        4320,
        330,
        30854,
        15132,
        6316,
        22103,
        21908,
        2360,
        3868,
        11,
        304,
        42293,
        358,
        4265,
        4762,
        387,
        25,
        330,
        30854,
        36731,
        5203,
        13291,
        13053,
        361,
        2249
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġcon",
            "token_id": 390,
            "logprob": -0.457,
            "prob": 0.6328
          },
          {
            "token": "...\"",
            "token_id": 21908,
            "logprob": -1.7656,
            "prob": 0.1709
          },
          {
            "token": "Ġcreated",
            "token_id": 3549,
            "logprob": -3.0781,
            "prob": 0.0459
          },
          {
            "token": "Ġcré",
            "token_id": 34280,
            "logprob": -3.4531,
            "prob": 0.0315
          },
          {
            "token": "Ġbas",
            "token_id": 3122,
            "logprob": -3.8906,
            "prob": 0.0204
          },
          {
            "token": ".\"",
            "token_id": 1210,
            "logprob": -4.375,
            "prob": 0.0128
          },
          {
            "token": ".",
            "token_id": 13,
            "logprob": -4.9062,
            "prob": 0.0075
          },
          {
            "token": "...",
            "token_id": 1131,
            "logprob": -5.7188,
            "prob": 0.0033
          },
          {
            "token": "Ġqui",
            "token_id": 7930,
            "logprob": -5.7188,
            "prob": 0.0033
          },
          {
            "token": "Ġdesigned",
            "token_id": 6319,
            "logprob": -5.8125,
            "prob": 0.003
          }
        ]
      },
      "selected_token": {
        "token": ".",
        "token_id": 13,
        "selection_method": "sampling"
      }
    },
    {
      "step": 98,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters were corrupted when sent. Wait wait, I just need to type the response normally then since they used French. Maybe translate my own English to the French answer? Hmm, user said 'QUI E-SU ?\" so I need to answer \"Je sont une Assistant...\" No wait, in french I'd probably be: \"Je suis una assistance virtueux.",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters",
        "Ġwere",
        "Ġcorrupted",
        "Ġwhen",
        "Ġsent",
        ".",
        "ĠWait",
        "Ġwait",
        ",",
        "ĠI",
        "Ġjust",
        "Ġneed",
        "Ġto",
        "Ġtype",
        "Ġthe",
        "Ġresponse",
        "Ġnormally",
        "Ġthen",
        "Ġsince",
        "Ġthey",
        "Ġused",
        "ĠFrench",
        ".",
        "ĠMaybe",
        "Ġtranslate",
        "Ġmy",
        "Ġown",
        "ĠEnglish",
        "Ġto",
        "Ġthe",
        "ĠFrench",
        "Ġanswer",
        "?",
        "ĠHmm",
        ",",
        "Ġuser",
        "Ġsaid",
        "Ġ'",
        "QUI",
        "ĠE",
        "-S",
        "U",
        "Ġ?",
        "\"",
        "Ġso",
        "ĠI",
        "Ġneed",
        "Ġto",
        "Ġanswer",
        "Ġ\"",
        "Je",
        "Ġsont",
        "Ġune",
        "ĠAssistant",
        "...\"",
        "ĠNo",
        "Ġwait",
        ",",
        "Ġin",
        "Ġfrench",
        "ĠI",
        "'d",
        "Ġprobably",
        "Ġbe",
        ":",
        "Ġ\"",
        "Je",
        "Ġsuis",
        "Ġuna",
        "Ġassistance",
        "Ġvirt",
        "ue",
        "ux",
        "."
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885,
        1051,
        50500,
        994,
        3288,
        13,
        14144,
        3868,
        11,
        358,
        1120,
        1205,
        311,
        955,
        279,
        2077,
        14614,
        1243,
        2533,
        814,
        1511,
        8753,
        13,
        10926,
        15025,
        856,
        1866,
        6498,
        311,
        279,
        8753,
        4320,
        30,
        89290,
        11,
        1217,
        1071,
        364,
        12247,
        469,
        6354,
        52,
        949,
        1,
        779,
        358,
        1205,
        311,
        4320,
        330,
        30854,
        15132,
        6316,
        22103,
        21908,
        2360,
        3868,
        11,
        304,
        42293,
        358,
        4265,
        4762,
        387,
        25,
        330,
        30854,
        36731,
        5203,
        13291,
        13053,
        361,
        2249,
        13
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "ĠJe",
            "token_id": 14465,
            "logprob": -0.5547,
            "prob": 0.5742
          },
          {
            "token": "ĠComment",
            "token_id": 12535,
            "logprob": -2.375,
            "prob": 0.0938
          },
          {
            "token": "ĠEn",
            "token_id": 2998,
            "logprob": -2.375,
            "prob": 0.0938
          },
          {
            "token": "ĠP",
            "token_id": 393,
            "logprob": -3.0,
            "prob": 0.05
          },
          {
            "token": "Ġcomment",
            "token_id": 4068,
            "logprob": -4.1562,
            "prob": 0.0157
          },
          {
            "token": "ĠQu",
            "token_id": 3489,
            "logprob": -4.7188,
            "prob": 0.009
          },
          {
            "token": "ĠQue",
            "token_id": 9605,
            "logprob": -4.75,
            "prob": 0.0087
          },
          {
            "token": "...\"",
            "token_id": 21908,
            "logprob": -4.875,
            "prob": 0.0077
          },
          {
            "token": "ĠJ",
            "token_id": 622,
            "logprob": -5.1875,
            "prob": 0.0056
          },
          {
            "token": "Ġ...\"",
            "token_id": 79310,
            "logprob": -5.3438,
            "prob": 0.0048
          }
        ]
      },
      "selected_token": {
        "token": "...\"",
        "token_id": 21908,
        "selection_method": "sampling"
      }
    },
    {
      "step": 99,
      "input_text": "<|im_start|>userQui es-tu?<|im_end|><|im_start|>assistant<think>Okay? User asks in French \"Whoaresthy\" which is \"Qu谱e tu? \" but the characters were corrupted when sent. Wait wait, I just need to type the response normally then since they used French. Maybe translate my own English to the French answer? Hmm, user said 'QUI E-SU ?\" so I need to answer \"Je sont une Assistant...\" No wait, in french I'd probably be: \"Je suis una assistance virtueux....\"",
      "tokens": [
        "<|im_start|>",
        "user",
        "Qui",
        "Ġes",
        "-t",
        "u",
        "?",
        "<|im_end|>",
        "<|im_start|>",
        "assistant",
        "<th",
        "ink",
        ">",
        "Okay",
        "?",
        "ĠUser",
        "Ġasks",
        "Ġin",
        "ĠFrench",
        "Ġ\"",
        "Who",
        "are",
        "st",
        "hy",
        "\"",
        "Ġwhich",
        "Ġis",
        "Ġ\"",
        "Qu",
        "�",
        "�",
        "e",
        "Ġtu",
        "?",
        "Ġ\"",
        "Ġbut",
        "Ġthe",
        "Ġcharacters",
        "Ġwere",
        "Ġcorrupted",
        "Ġwhen",
        "Ġsent",
        ".",
        "ĠWait",
        "Ġwait",
        ",",
        "ĠI",
        "Ġjust",
        "Ġneed",
        "Ġto",
        "Ġtype",
        "Ġthe",
        "Ġresponse",
        "Ġnormally",
        "Ġthen",
        "Ġsince",
        "Ġthey",
        "Ġused",
        "ĠFrench",
        ".",
        "ĠMaybe",
        "Ġtranslate",
        "Ġmy",
        "Ġown",
        "ĠEnglish",
        "Ġto",
        "Ġthe",
        "ĠFrench",
        "Ġanswer",
        "?",
        "ĠHmm",
        ",",
        "Ġuser",
        "Ġsaid",
        "Ġ'",
        "QUI",
        "ĠE",
        "-S",
        "U",
        "Ġ?",
        "\"",
        "Ġso",
        "ĠI",
        "Ġneed",
        "Ġto",
        "Ġanswer",
        "Ġ\"",
        "Je",
        "Ġsont",
        "Ġune",
        "ĠAssistant",
        "...\"",
        "ĠNo",
        "Ġwait",
        ",",
        "Ġin",
        "Ġfrench",
        "ĠI",
        "'d",
        "Ġprobably",
        "Ġbe",
        ":",
        "Ġ\"",
        "Je",
        "Ġsuis",
        "Ġuna",
        "Ġassistance",
        "Ġvirt",
        "ue",
        "ux",
        ".",
        "...\""
      ],
      "token_ids": [
        100264,
        882,
        62545,
        1560,
        2442,
        84,
        30,
        100265,
        100264,
        78191,
        14023,
        771,
        29,
        33413,
        30,
        2724,
        17501,
        304,
        8753,
        330,
        15546,
        548,
        267,
        8671,
        1,
        902,
        374,
        330,
        2232,
        39013,
        109,
        68,
        9964,
        30,
        330,
        719,
        279,
        5885,
        1051,
        50500,
        994,
        3288,
        13,
        14144,
        3868,
        11,
        358,
        1120,
        1205,
        311,
        955,
        279,
        2077,
        14614,
        1243,
        2533,
        814,
        1511,
        8753,
        13,
        10926,
        15025,
        856,
        1866,
        6498,
        311,
        279,
        8753,
        4320,
        30,
        89290,
        11,
        1217,
        1071,
        364,
        12247,
        469,
        6354,
        52,
        949,
        1,
        779,
        358,
        1205,
        311,
        4320,
        330,
        30854,
        15132,
        6316,
        22103,
        21908,
        2360,
        3868,
        11,
        304,
        42293,
        358,
        4265,
        4762,
        387,
        25,
        330,
        30854,
        36731,
        5203,
        13291,
        13053,
        361,
        2249,
        13,
        21908
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "ĠWait",
            "token_id": 14144,
            "logprob": -0.1289,
            "prob": 0.8789
          },
          {
            "token": "Ġbut",
            "token_id": 719,
            "logprob": -3.125,
            "prob": 0.0437
          },
          {
            "token": "Ġor",
            "token_id": 477,
            "logprob": -3.75,
            "prob": 0.0234
          },
          {
            "token": "ĠLet",
            "token_id": 6914,
            "logprob": -4.125,
            "prob": 0.0161
          },
          {
            "token": "ĠBut",
            "token_id": 2030,
            "logprob": -4.625,
            "prob": 0.0098
          },
          {
            "token": "Ġwait",
            "token_id": 3868,
            "logprob": -4.875,
            "prob": 0.0076
          },
          {
            "token": "ĠOr",
            "token_id": 2582,
            "logprob": -5.125,
            "prob": 0.0059
          },
          {
            "token": "ĠNo",
            "token_id": 2360,
            "logprob": -5.25,
            "prob": 0.0052
          },
          {
            "token": "ĠMaybe",
            "token_id": 10926,
            "logprob": -6.375,
            "prob": 0.0017
          },
          {
            "token": "ĠHmm",
            "token_id": 89290,
            "logprob": -6.5,
            "prob": 0.0015
          }
        ]
      },
      "selected_token": {
        "token": "ĠWait",
        "token_id": 14144,
        "selection_method": "sampling"
      }
    }
  ]
}