{
  "id": "generated_20251203_121147",
  "prompt": "Raconte-moi une blague",
  "language": "fr",
  "temperature": 5.0,
  "top_k": 10,
  "model_info": {
    "name": "allenai/Olmo-3-7B-Think",
    "num_layers": 32,
    "hidden_size": 4096,
    "num_attention_heads": 32,
    "vocab_size": 100278
  },
  "generation_steps": [
    {
      "step": 0,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Okay",
            "token_id": 33413,
            "logprob": -0.0111,
            "prob": 0.9883
          },
          {
            "token": "Alright",
            "token_id": 72586,
            "logprob": -5.3125,
            "prob": 0.0049
          },
          {
            "token": "Hmm",
            "token_id": 81122,
            "logprob": -6.25,
            "prob": 0.0019
          },
          {
            "token": "Ah",
            "token_id": 25797,
            "logprob": -7.5625,
            "prob": 0.0005
          },
          {
            "token": "ĠOkay",
            "token_id": 36539,
            "logprob": -7.5625,
            "prob": 0.0005
          },
          {
            "token": "Ok",
            "token_id": 11839,
            "logprob": -9.0625,
            "prob": 0.0001
          },
          {
            "token": "Well",
            "token_id": 11649,
            "logprob": -9.125,
            "prob": 0.0001
          },
          {
            "token": "Ġbon",
            "token_id": 7970,
            "logprob": -9.25,
            "prob": 0.0001
          },
          {
            "token": "OK",
            "token_id": 4012,
            "logprob": -9.25,
            "prob": 0.0001
          },
          {
            "token": "Ġokay",
            "token_id": 17339,
            "logprob": -9.375,
            "prob": 0.0001
          }
        ]
      },
      "selected_token": {
        "token": "Okay",
        "token_id": 33413,
        "selection_method": "sampling"
      }
    },
    {
      "step": 1,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": ",",
            "token_id": 11,
            "logprob": -0.0,
            "prob": 1.0
          },
          {
            "token": "，",
            "token_id": 3922,
            "logprob": -14.9375,
            "prob": 0.0
          },
          {
            "token": "?",
            "token_id": 30,
            "logprob": -15.5625,
            "prob": 0.0
          },
          {
            "token": ":",
            "token_id": 25,
            "logprob": -15.6875,
            "prob": 0.0
          },
          {
            "token": "...",
            "token_id": 1131,
            "logprob": -15.6875,
            "prob": 0.0
          },
          {
            "token": "!",
            "token_id": 0,
            "logprob": -15.75,
            "prob": 0.0
          },
          {
            "token": "Ġlet",
            "token_id": 1095,
            "logprob": -16.125,
            "prob": 0.0
          },
          {
            "token": ".",
            "token_id": 13,
            "logprob": -16.375,
            "prob": 0.0
          },
          {
            "token": "Ġso",
            "token_id": 779,
            "logprob": -16.875,
            "prob": 0.0
          },
          {
            "token": "Ġdu",
            "token_id": 3930,
            "logprob": -17.25,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": ",",
        "token_id": 11,
        "selection_method": "sampling"
      }
    },
    {
      "step": 2,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay,",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ","
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -0.003,
            "prob": 0.9961
          },
          {
            "token": "Ġl",
            "token_id": 326,
            "logprob": -5.875,
            "prob": 0.0028
          },
          {
            "token": "ĠI",
            "token_id": 358,
            "logprob": -9.4375,
            "prob": 0.0001
          },
          {
            "token": "Ġthey",
            "token_id": 814,
            "logprob": -10.3125,
            "prob": 0.0
          },
          {
            "token": "Ġlet",
            "token_id": 1095,
            "logprob": -11.25,
            "prob": 0.0
          },
          {
            "token": "Ġil",
            "token_id": 3900,
            "logprob": -11.625,
            "prob": 0.0
          },
          {
            "token": "Ġle",
            "token_id": 514,
            "logprob": -11.6875,
            "prob": 0.0
          },
          {
            "token": "用户",
            "token_id": 20600,
            "logprob": -12.5625,
            "prob": 0.0
          },
          {
            "token": "Ġla",
            "token_id": 1208,
            "logprob": -13.0,
            "prob": 0.0
          },
          {
            "token": "Ġsomeone",
            "token_id": 4423,
            "logprob": -13.1875,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġil",
        "token_id": 3900,
        "selection_method": "sampling"
      }
    },
    {
      "step": 3,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġme",
            "token_id": 757,
            "logprob": -0.5742,
            "prob": 0.5625
          },
          {
            "token": "Ġfaut",
            "token_id": 47090,
            "logprob": -2.2656,
            "prob": 0.104
          },
          {
            "token": "Ġm",
            "token_id": 296,
            "logprob": -2.3906,
            "prob": 0.0918
          },
          {
            "token": "Ġdemande",
            "token_id": 62163,
            "logprob": -2.4531,
            "prob": 0.0864
          },
          {
            "token": "Ġa",
            "token_id": 264,
            "logprob": -3.2656,
            "prob": 0.0383
          },
          {
            "token": "Ġwants",
            "token_id": 6944,
            "logprob": -4.0625,
            "prob": 0.017
          },
          {
            "token": "Ġy",
            "token_id": 379,
            "logprob": -4.375,
            "prob": 0.0125
          },
          {
            "token": "Ġveut",
            "token_id": 76605,
            "logprob": -5.0,
            "prob": 0.0067
          },
          {
            "token": "Ġwant",
            "token_id": 1390,
            "logprob": -5.3125,
            "prob": 0.0049
          },
          {
            "token": "Ġsuff",
            "token_id": 8657,
            "logprob": -5.625,
            "prob": 0.0036
          }
        ]
      },
      "selected_token": {
        "token": "Ġwants",
        "token_id": 6944,
        "selection_method": "sampling"
      }
    },
    {
      "step": 4,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġme",
            "token_id": 757,
            "logprob": -0.5234,
            "prob": 0.5938
          },
          {
            "token": "Ġa",
            "token_id": 264,
            "logprob": -0.8984,
            "prob": 0.4082
          },
          {
            "token": "Ġan",
            "token_id": 459,
            "logprob": -11.0625,
            "prob": 0.0
          },
          {
            "token": "me",
            "token_id": 2727,
            "logprob": -12.0,
            "prob": 0.0
          },
          {
            "token": "Ġto",
            "token_id": 311,
            "logprob": -12.125,
            "prob": 0.0
          },
          {
            "token": "我",
            "token_id": 37046,
            "logprob": -12.5,
            "prob": 0.0
          },
          {
            "token": "Ġjokes",
            "token_id": 32520,
            "logprob": -12.875,
            "prob": 0.0
          },
          {
            "token": "Ġjoke",
            "token_id": 22380,
            "logprob": -12.9375,
            "prob": 0.0
          },
          {
            "token": "Ġsomeone",
            "token_id": 4423,
            "logprob": -13.8125,
            "prob": 0.0
          },
          {
            "token": "Ġsome",
            "token_id": 1063,
            "logprob": -14.375,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġa",
        "token_id": 264,
        "selection_method": "sampling"
      }
    },
    {
      "step": 5,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġjoke",
            "token_id": 22380,
            "logprob": -0.0007,
            "prob": 1.0
          },
          {
            "token": "Ġjokes",
            "token_id": 32520,
            "logprob": -8.0,
            "prob": 0.0003
          },
          {
            "token": "Ġfunny",
            "token_id": 15526,
            "logprob": -8.9375,
            "prob": 0.0001
          },
          {
            "token": "ĠFrench",
            "token_id": 8753,
            "logprob": -9.9375,
            "prob": 0.0
          },
          {
            "token": "Ġpun",
            "token_id": 8474,
            "logprob": -10.125,
            "prob": 0.0
          },
          {
            "token": "Ġlaugh",
            "token_id": 12835,
            "logprob": -10.4375,
            "prob": 0.0
          },
          {
            "token": "Ġr",
            "token_id": 436,
            "logprob": -10.625,
            "prob": 0.0
          },
          {
            "token": "Ġstory",
            "token_id": 3446,
            "logprob": -11.625,
            "prob": 0.0
          },
          {
            "token": "Ġknock",
            "token_id": 14459,
            "logprob": -11.8125,
            "prob": 0.0
          },
          {
            "token": "Ġbl",
            "token_id": 1529,
            "logprob": -12.1875,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġr",
        "token_id": 436,
        "selection_method": "sampling"
      }
    },
    {
      "step": 6,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a r",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "iddle",
            "token_id": 3390,
            "logprob": -0.0042,
            "prob": 0.9961
          },
          {
            "token": "use",
            "token_id": 817,
            "logprob": -8.0625,
            "prob": 0.0003
          },
          {
            "token": "ick",
            "token_id": 875,
            "logprob": -8.5625,
            "prob": 0.0002
          },
          {
            "token": "ire",
            "token_id": 556,
            "logprob": -8.5625,
            "prob": 0.0002
          },
          {
            "token": "acy",
            "token_id": 2826,
            "logprob": -8.625,
            "prob": 0.0002
          },
          {
            "token": "idd",
            "token_id": 1810,
            "logprob": -8.75,
            "prob": 0.0002
          },
          {
            "token": "IDDLE",
            "token_id": 49275,
            "logprob": -9.0625,
            "prob": 0.0001
          },
          {
            "token": "affle",
            "token_id": 45190,
            "logprob": -9.3125,
            "prob": 0.0001
          },
          {
            "token": "Ġjoke",
            "token_id": 22380,
            "logprob": -10.1875,
            "prob": 0.0
          },
          {
            "token": "acon",
            "token_id": 23184,
            "logprob": -10.25,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "iddle",
        "token_id": 3390,
        "selection_method": "sampling"
      }
    },
    {
      "step": 7,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -0.2344,
            "prob": 0.793
          },
          {
            "token": ".",
            "token_id": 13,
            "logprob": -2.2344,
            "prob": 0.1069
          },
          {
            "token": "Ġtold",
            "token_id": 3309,
            "logprob": -2.6094,
            "prob": 0.0737
          },
          {
            "token": ",",
            "token_id": 11,
            "logprob": -4.3438,
            "prob": 0.0128
          },
          {
            "token": "Ġor",
            "token_id": 477,
            "logprob": -4.5938,
            "prob": 0.0099
          },
          {
            "token": "!",
            "token_id": 0,
            "logprob": -6.8438,
            "prob": 0.0011
          },
          {
            "token": "?",
            "token_id": 30,
            "logprob": -6.9688,
            "prob": 0.0009
          },
          {
            "token": "...",
            "token_id": 1131,
            "logprob": -7.5312,
            "prob": 0.0005
          },
          {
            "token": "Ġtranslated",
            "token_id": 25548,
            "logprob": -7.5938,
            "prob": 0.0005
          },
          {
            "token": "Ġtell",
            "token_id": 3371,
            "logprob": -7.7188,
            "prob": 0.0004
          }
        ]
      },
      "selected_token": {
        "token": "?",
        "token_id": 30,
        "selection_method": "sampling"
      }
    },
    {
      "step": 8,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle?",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "ĠOr",
            "token_id": 2582,
            "logprob": -0.6172,
            "prob": 0.5391
          },
          {
            "token": "ĠWait",
            "token_id": 14144,
            "logprob": -0.9922,
            "prob": 0.3711
          },
          {
            "token": "ĠLet",
            "token_id": 6914,
            "logprob": -3.125,
            "prob": 0.0442
          },
          {
            "token": "ĠNo",
            "token_id": 2360,
            "logprob": -4.125,
            "prob": 0.0162
          },
          {
            "token": "ĠA",
            "token_id": 362,
            "logprob": -5.1875,
            "prob": 0.0056
          },
          {
            "token": "ĠThe",
            "token_id": 578,
            "logprob": -5.375,
            "prob": 0.0047
          },
          {
            "token": "Ġ\"",
            "token_id": 330,
            "logprob": -5.5625,
            "prob": 0.0039
          },
          {
            "token": "ĠWell",
            "token_id": 8489,
            "logprob": -5.625,
            "prob": 0.0036
          },
          {
            "token": "ĠLa",
            "token_id": 5034,
            "logprob": -5.9375,
            "prob": 0.0027
          },
          {
            "token": "ĠMaybe",
            "token_id": 10926,
            "logprob": -6.8125,
            "prob": 0.0011
          }
        ]
      },
      "selected_token": {
        "token": "ĠOr",
        "token_id": 2582,
        "selection_method": "sampling"
      }
    },
    {
      "step": 9,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġa",
            "token_id": 264,
            "logprob": -0.3477,
            "prob": 0.707
          },
          {
            "token": "Ġmaybe",
            "token_id": 7344,
            "logprob": -1.8516,
            "prob": 0.1572
          },
          {
            "token": "Ġjust",
            "token_id": 1120,
            "logprob": -2.0938,
            "prob": 0.1226
          },
          {
            "token": "Ġwait",
            "token_id": 3868,
            "logprob": -4.9688,
            "prob": 0.0069
          },
          {
            "token": "Ġis",
            "token_id": 374,
            "logprob": -6.1562,
            "prob": 0.0021
          },
          {
            "token": "Ġjoke",
            "token_id": 22380,
            "logprob": -6.5938,
            "prob": 0.0014
          },
          {
            "token": "Ġlike",
            "token_id": 1093,
            "logprob": -6.9062,
            "prob": 0.001
          },
          {
            "token": "Ġactually",
            "token_id": 3604,
            "logprob": -7.4688,
            "prob": 0.0006
          },
          {
            "token": "Ġan",
            "token_id": 459,
            "logprob": -8.5,
            "prob": 0.0002
          },
          {
            "token": "Ġdid",
            "token_id": 1550,
            "logprob": -8.875,
            "prob": 0.0001
          }
        ]
      },
      "selected_token": {
        "token": "Ġan",
        "token_id": 459,
        "selection_method": "sampling"
      }
    },
    {
      "step": 10,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġactual",
            "token_id": 5150,
            "logprob": -0.2793,
            "prob": 0.7578
          },
          {
            "token": "Ġjoke",
            "token_id": 22380,
            "logprob": -2.2812,
            "prob": 0.1025
          },
          {
            "token": "Ġanecd",
            "token_id": 50750,
            "logprob": -2.5938,
            "prob": 0.0747
          },
          {
            "token": "Ġjokes",
            "token_id": 32520,
            "logprob": -5.2188,
            "prob": 0.0054
          },
          {
            "token": "Ġold",
            "token_id": 2362,
            "logprob": -5.4062,
            "prob": 0.0045
          },
          {
            "token": "Ġactually",
            "token_id": 3604,
            "logprob": -5.6562,
            "prob": 0.0035
          },
          {
            "token": "Ġexample",
            "token_id": 3187,
            "logprob": -5.8438,
            "prob": 0.0029
          },
          {
            "token": "Ġice",
            "token_id": 10054,
            "logprob": -5.9062,
            "prob": 0.0027
          },
          {
            "token": "Ġpun",
            "token_id": 8474,
            "logprob": -6.5312,
            "prob": 0.0015
          },
          {
            "token": "ĠItalian",
            "token_id": 15155,
            "logprob": -6.7188,
            "prob": 0.0012
          }
        ]
      },
      "selected_token": {
        "token": "Ġanecd",
        "token_id": 50750,
        "selection_method": "sampling"
      }
    },
    {
      "step": 11,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecd",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "ote",
            "token_id": 1295,
            "logprob": -0.0008,
            "prob": 1.0
          },
          {
            "token": "otal",
            "token_id": 2426,
            "logprob": -7.3125,
            "prob": 0.0007
          },
          {
            "token": "otic",
            "token_id": 14546,
            "logprob": -9.375,
            "prob": 0.0001
          },
          {
            "token": "ole",
            "token_id": 1286,
            "logprob": -11.0,
            "prob": 0.0
          },
          {
            "token": "ota",
            "token_id": 6217,
            "logprob": -11.0625,
            "prob": 0.0
          },
          {
            "token": "otes",
            "token_id": 6429,
            "logprob": -12.625,
            "prob": 0.0
          },
          {
            "token": "ot",
            "token_id": 354,
            "logprob": -12.6875,
            "prob": 0.0
          },
          {
            "token": "oted",
            "token_id": 9437,
            "logprob": -12.8125,
            "prob": 0.0
          },
          {
            "token": "ure",
            "token_id": 554,
            "logprob": -13.1875,
            "prob": 0.0
          },
          {
            "token": "OTE",
            "token_id": 19707,
            "logprob": -13.375,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "ote",
        "token_id": 1295,
        "selection_method": "sampling"
      }
    },
    {
      "step": 12,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "?",
            "token_id": 30,
            "logprob": -0.0212,
            "prob": 0.9805
          },
          {
            "token": "Ġthat",
            "token_id": 430,
            "logprob": -4.6562,
            "prob": 0.0096
          },
          {
            "token": "Ġwith",
            "token_id": 449,
            "logprob": -6.1562,
            "prob": 0.0021
          },
          {
            "token": ",",
            "token_id": 11,
            "logprob": -6.5312,
            "prob": 0.0015
          },
          {
            "token": "Ġabout",
            "token_id": 922,
            "logprob": -6.7812,
            "prob": 0.0011
          },
          {
            "token": "Ġlabeled",
            "token_id": 30929,
            "logprob": -7.0312,
            "prob": 0.0009
          },
          {
            "token": "Ġfunny",
            "token_id": 15526,
            "logprob": -7.0312,
            "prob": 0.0009
          },
          {
            "token": "Ġor",
            "token_id": 477,
            "logprob": -7.1562,
            "prob": 0.0008
          },
          {
            "token": "Ġcalled",
            "token_id": 2663,
            "logprob": -7.1562,
            "prob": 0.0008
          },
          {
            "token": "Ġtold",
            "token_id": 3309,
            "logprob": -7.5312,
            "prob": 0.0005
          }
        ]
      },
      "selected_token": {
        "token": "?",
        "token_id": 30,
        "selection_method": "sampling"
      }
    },
    {
      "step": 13,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote?",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "ĠWait",
            "token_id": 14144,
            "logprob": -0.1504,
            "prob": 0.8594
          },
          {
            "token": "ĠThe",
            "token_id": 578,
            "logprob": -2.4062,
            "prob": 0.0908
          },
          {
            "token": "Ġ\"",
            "token_id": 330,
            "logprob": -4.2188,
            "prob": 0.0148
          },
          {
            "token": "ĠLet",
            "token_id": 6914,
            "logprob": -4.4688,
            "prob": 0.0115
          },
          {
            "token": "ĠOr",
            "token_id": 2582,
            "logprob": -5.0312,
            "prob": 0.0066
          },
          {
            "token": "ĠNo",
            "token_id": 2360,
            "logprob": -5.2188,
            "prob": 0.0055
          },
          {
            "token": "ĠLa",
            "token_id": 5034,
            "logprob": -5.9688,
            "prob": 0.0026
          },
          {
            "token": "ĠA",
            "token_id": 362,
            "logprob": -6.8438,
            "prob": 0.0011
          },
          {
            "token": "ĠJust",
            "token_id": 4702,
            "logprob": -7.0938,
            "prob": 0.0008
          },
          {
            "token": "ĠR",
            "token_id": 432,
            "logprob": -7.1562,
            "prob": 0.0008
          }
        ]
      },
      "selected_token": {
        "token": "ĠThe",
        "token_id": 578,
        "selection_method": "sampling"
      }
    },
    {
      "step": 14,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġuser",
            "token_id": 1217,
            "logprob": -0.2227,
            "prob": 0.8008
          },
          {
            "token": "ĠFrench",
            "token_id": 8753,
            "logprob": -2.4688,
            "prob": 0.0845
          },
          {
            "token": "Ġword",
            "token_id": 3492,
            "logprob": -2.8438,
            "prob": 0.0579
          },
          {
            "token": "Ġoriginal",
            "token_id": 4113,
            "logprob": -3.0938,
            "prob": 0.0452
          },
          {
            "token": "Ġquery",
            "token_id": 3319,
            "logprob": -5.8438,
            "prob": 0.0029
          },
          {
            "token": "Ġphrase",
            "token_id": 17571,
            "logprob": -6.0938,
            "prob": 0.0022
          },
          {
            "token": "Ġrequest",
            "token_id": 1715,
            "logprob": -6.2188,
            "prob": 0.002
          },
          {
            "token": "Ġverb",
            "token_id": 19120,
            "logprob": -6.2188,
            "prob": 0.002
          },
          {
            "token": "Ġquestion",
            "token_id": 3488,
            "logprob": -6.5938,
            "prob": 0.0014
          },
          {
            "token": "Ġterm",
            "token_id": 4751,
            "logprob": -7.5938,
            "prob": 0.0005
          }
        ]
      },
      "selected_token": {
        "token": "Ġoriginal",
        "token_id": 4113,
        "selection_method": "sampling"
      }
    },
    {
      "step": 15,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġquery",
            "token_id": 3319,
            "logprob": -0.8516,
            "prob": 0.4277
          },
          {
            "token": "Ġrequest",
            "token_id": 1715,
            "logprob": -1.1016,
            "prob": 0.332
          },
          {
            "token": "ĠFrench",
            "token_id": 8753,
            "logprob": -1.9766,
            "prob": 0.1387
          },
          {
            "token": "Ġquestion",
            "token_id": 3488,
            "logprob": -2.9688,
            "prob": 0.051
          },
          {
            "token": "Ġmessage",
            "token_id": 1984,
            "logprob": -3.5938,
            "prob": 0.0272
          },
          {
            "token": "Ġinstruction",
            "token_id": 7754,
            "logprob": -4.7188,
            "prob": 0.0089
          },
          {
            "token": "Ġsays",
            "token_id": 2795,
            "logprob": -5.4688,
            "prob": 0.0042
          },
          {
            "token": "Ġuser",
            "token_id": 1217,
            "logprob": -5.5938,
            "prob": 0.0037
          },
          {
            "token": "Ġphrase",
            "token_id": 17571,
            "logprob": -6.7188,
            "prob": 0.0012
          },
          {
            "token": "Ġwas",
            "token_id": 574,
            "logprob": -6.7812,
            "prob": 0.0011
          }
        ]
      },
      "selected_token": {
        "token": "Ġquery",
        "token_id": 3319,
        "selection_method": "sampling"
      }
    },
    {
      "step": 16,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġis",
            "token_id": 374,
            "logprob": -0.1602,
            "prob": 0.8516
          },
          {
            "token": "Ġwas",
            "token_id": 574,
            "logprob": -2.2812,
            "prob": 0.1016
          },
          {
            "token": "Ġsays",
            "token_id": 2795,
            "logprob": -3.6562,
            "prob": 0.0258
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -3.9062,
            "prob": 0.02
          },
          {
            "token": "Ġjust",
            "token_id": 1120,
            "logprob": -7.7812,
            "prob": 0.0004
          },
          {
            "token": "Ġsaid",
            "token_id": 1071,
            "logprob": -8.5625,
            "prob": 0.0002
          },
          {
            "token": "Ġ\"",
            "token_id": 330,
            "logprob": -8.8125,
            "prob": 0.0002
          },
          {
            "token": "Ġtranslates",
            "token_id": 48018,
            "logprob": -12.5,
            "prob": 0.0
          },
          {
            "token": "Ġsay",
            "token_id": 2019,
            "logprob": -12.6875,
            "prob": 0.0
          },
          {
            "token": ":",
            "token_id": 25,
            "logprob": -13.3125,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġsays",
        "token_id": 2795,
        "selection_method": "sampling"
      }
    },
    {
      "step": 17,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġ\"",
            "token_id": 330,
            "logprob": -0.0002,
            "prob": 1.0
          },
          {
            "token": ":",
            "token_id": 25,
            "logprob": -8.75,
            "prob": 0.0002
          },
          {
            "token": ",",
            "token_id": 11,
            "logprob": -10.75,
            "prob": 0.0
          },
          {
            "token": "Ġ“",
            "token_id": 1054,
            "logprob": -12.25,
            "prob": 0.0
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -12.3125,
            "prob": 0.0
          },
          {
            "token": "Ġ\"'",
            "token_id": 7326,
            "logprob": -12.5,
            "prob": 0.0
          },
          {
            "token": "Ġ'",
            "token_id": 364,
            "logprob": -12.75,
            "prob": 0.0
          },
          {
            "token": "Ġ\"_",
            "token_id": 9177,
            "logprob": -12.9375,
            "prob": 0.0
          },
          {
            "token": "Ġ\"-",
            "token_id": 6660,
            "logprob": -13.9375,
            "prob": 0.0
          },
          {
            "token": "Ġ\"\"",
            "token_id": 1621,
            "logprob": -14.1875,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġ\"",
        "token_id": 330,
        "selection_method": "sampling"
      }
    },
    {
      "step": 18,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\""
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "R",
            "token_id": 49,
            "logprob": -0.1064,
            "prob": 0.8984
          },
          {
            "token": "ra",
            "token_id": 969,
            "logprob": -2.9844,
            "prob": 0.0508
          },
          {
            "token": "Ra",
            "token_id": 56635,
            "logprob": -3.1094,
            "prob": 0.0447
          },
          {
            "token": "ĠRa",
            "token_id": 18989,
            "logprob": -6.5938,
            "prob": 0.0014
          },
          {
            "token": "RA",
            "token_id": 5726,
            "logprob": -6.6562,
            "prob": 0.0013
          },
          {
            "token": "Ġtelling",
            "token_id": 11890,
            "logprob": -8.6875,
            "prob": 0.0002
          },
          {
            "token": "Ġra",
            "token_id": 15473,
            "logprob": -8.8125,
            "prob": 0.0002
          },
          {
            "token": "r",
            "token_id": 81,
            "logprob": -9.4375,
            "prob": 0.0001
          },
          {
            "token": "Tell",
            "token_id": 41551,
            "logprob": -9.5,
            "prob": 0.0001
          },
          {
            "token": "ras",
            "token_id": 13075,
            "logprob": -9.5625,
            "prob": 0.0001
          }
        ]
      },
      "selected_token": {
        "token": "R",
        "token_id": 49,
        "selection_method": "sampling"
      }
    },
    {
      "step": 19,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"R",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "acon",
            "token_id": 23184,
            "logprob": -0.0011,
            "prob": 1.0
          },
          {
            "token": "acad",
            "token_id": 50957,
            "logprob": -10.125,
            "prob": 0.0
          },
          {
            "token": "ac",
            "token_id": 582,
            "logprob": -10.25,
            "prob": 0.0
          },
          {
            "token": "conte",
            "token_id": 46452,
            "logprob": -10.625,
            "prob": 0.0
          },
          {
            "token": "acom",
            "token_id": 45670,
            "logprob": -10.8125,
            "prob": 0.0
          },
          {
            "token": "acons",
            "token_id": 76544,
            "logprob": -11.0625,
            "prob": 0.0
          },
          {
            "token": "acre",
            "token_id": 29574,
            "logprob": -11.1875,
            "prob": 0.0
          },
          {
            "token": "acock",
            "token_id": 81378,
            "logprob": -11.25,
            "prob": 0.0
          },
          {
            "token": "aque",
            "token_id": 20234,
            "logprob": -11.25,
            "prob": 0.0
          },
          {
            "token": "acent",
            "token_id": 18680,
            "logprob": -11.5625,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "acon",
        "token_id": 23184,
        "selection_method": "sampling"
      }
    },
    {
      "step": 20,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Racon",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "te",
            "token_id": 668,
            "logprob": -0.0,
            "prob": 1.0
          },
          {
            "token": "ter",
            "token_id": 466,
            "logprob": -11.5625,
            "prob": 0.0
          },
          {
            "token": "TE",
            "token_id": 2505,
            "logprob": -11.875,
            "prob": 0.0
          },
          {
            "token": "те",
            "token_id": 51627,
            "logprob": -12.1875,
            "prob": 0.0
          },
          {
            "token": "tem",
            "token_id": 880,
            "logprob": -13.4375,
            "prob": 0.0
          },
          {
            "token": "to",
            "token_id": 998,
            "logprob": -14.125,
            "prob": 0.0
          },
          {
            "token": "tele",
            "token_id": 15772,
            "logprob": -14.125,
            "prob": 0.0
          },
          {
            "token": "tes",
            "token_id": 2392,
            "logprob": -14.3125,
            "prob": 0.0
          },
          {
            "token": "ten",
            "token_id": 2002,
            "logprob": -14.5,
            "prob": 0.0
          },
          {
            "token": "ta",
            "token_id": 2629,
            "logprob": -14.5,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "te",
        "token_id": 668,
        "selection_method": "sampling"
      }
    },
    {
      "step": 21,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "-m",
            "token_id": 1474,
            "logprob": -0.0003,
            "prob": 1.0
          },
          {
            "token": "Ġmoi",
            "token_id": 41091,
            "logprob": -8.8125,
            "prob": 0.0001
          },
          {
            "token": "-me",
            "token_id": 35073,
            "logprob": -9.0625,
            "prob": 0.0001
          },
          {
            "token": "-M",
            "token_id": 5364,
            "logprob": -12.0625,
            "prob": 0.0
          },
          {
            "token": "Ġme",
            "token_id": 757,
            "logprob": -12.375,
            "prob": 0.0
          },
          {
            "token": "Ġм",
            "token_id": 11562,
            "logprob": -13.75,
            "prob": 0.0
          },
          {
            "token": "-l",
            "token_id": 2922,
            "logprob": -13.9375,
            "prob": 0.0
          },
          {
            "token": "-n",
            "token_id": 5392,
            "logprob": -14.0625,
            "prob": 0.0
          },
          {
            "token": "+m",
            "token_id": 60958,
            "logprob": -14.125,
            "prob": 0.0
          },
          {
            "token": "Ġmi",
            "token_id": 9686,
            "logprob": -14.25,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġmoi",
        "token_id": 41091,
        "selection_method": "sampling"
      }
    },
    {
      "step": 22,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġune",
            "token_id": 6316,
            "logprob": -0.0024,
            "prob": 0.9961
          },
          {
            "token": "Ġuna",
            "token_id": 5203,
            "logprob": -6.8125,
            "prob": 0.0011
          },
          {
            "token": "ĠUne",
            "token_id": 39783,
            "logprob": -7.0625,
            "prob": 0.0009
          },
          {
            "token": "Une",
            "token_id": 56948,
            "logprob": -9.0625,
            "prob": 0.0001
          },
          {
            "token": "'une",
            "token_id": 31109,
            "logprob": -9.375,
            "prob": 0.0001
          },
          {
            "token": "Ġuma",
            "token_id": 10832,
            "logprob": -9.875,
            "prob": 0.0001
          },
          {
            "token": "une",
            "token_id": 2957,
            "logprob": -11.25,
            "prob": 0.0
          },
          {
            "token": "’une",
            "token_id": 36577,
            "logprob": -11.3125,
            "prob": 0.0
          },
          {
            "token": "Ġeen",
            "token_id": 8517,
            "logprob": -12.1875,
            "prob": 0.0
          },
          {
            "token": "ĠUna",
            "token_id": 44071,
            "logprob": -12.3125,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġune",
        "token_id": 6316,
        "selection_method": "sampling"
      }
    },
    {
      "step": 23,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġbl",
            "token_id": 1529,
            "logprob": -0.0003,
            "prob": 1.0
          },
          {
            "token": "ĠBl",
            "token_id": 2563,
            "logprob": -9.5625,
            "prob": 0.0001
          },
          {
            "token": "bl",
            "token_id": 2067,
            "logprob": -10.0,
            "prob": 0.0
          },
          {
            "token": "Ġblot",
            "token_id": 81982,
            "logprob": -10.9375,
            "prob": 0.0
          },
          {
            "token": "Ġbla",
            "token_id": 85641,
            "logprob": -11.0,
            "prob": 0.0
          },
          {
            "token": "Ġbladder",
            "token_id": 62564,
            "logprob": -11.0625,
            "prob": 0.0
          },
          {
            "token": ".bl",
            "token_id": 10621,
            "logprob": -11.375,
            "prob": 0.0
          },
          {
            "token": "Ġblur",
            "token_id": 29613,
            "logprob": -11.375,
            "prob": 0.0
          },
          {
            "token": "Ġbluff",
            "token_id": 97388,
            "logprob": -11.5,
            "prob": 0.0
          },
          {
            "token": "Ġblonde",
            "token_id": 27117,
            "logprob": -11.625,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġblonde",
        "token_id": 27117,
        "selection_method": "sampling"
      }
    },
    {
      "step": 24,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "\"",
            "token_id": 1,
            "logprob": -0.1338,
            "prob": 0.875
          },
          {
            "token": "\".",
            "token_id": 3343,
            "logprob": -3.2656,
            "prob": 0.0383
          },
          {
            "token": ".\"",
            "token_id": 1210,
            "logprob": -3.3906,
            "prob": 0.0339
          },
          {
            "token": ",\"",
            "token_id": 1359,
            "logprob": -3.6406,
            "prob": 0.0264
          },
          {
            "token": "\",",
            "token_id": 498,
            "logprob": -3.7656,
            "prob": 0.0233
          },
          {
            "token": "\"—",
            "token_id": 66101,
            "logprob": -6.75,
            "prob": 0.0012
          },
          {
            "token": "...\"",
            "token_id": 21908,
            "logprob": -7.875,
            "prob": 0.0004
          },
          {
            "token": "\"...",
            "token_id": 53670,
            "logprob": -8.25,
            "prob": 0.0003
          },
          {
            "token": "\"?",
            "token_id": 44969,
            "logprob": -9.75,
            "prob": 0.0001
          },
          {
            "token": ".",
            "token_id": 13,
            "logprob": -10.0625,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "\"",
        "token_id": 1,
        "selection_method": "sampling"
      }
    },
    {
      "step": 25,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\"",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\""
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġwhich",
            "token_id": 902,
            "logprob": -0.0796,
            "prob": 0.9219
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -3.0781,
            "prob": 0.0459
          },
          {
            "token": "Ġbut",
            "token_id": 719,
            "logprob": -3.8281,
            "prob": 0.0217
          },
          {
            "token": "Ġ–",
            "token_id": 1389,
            "logprob": -5.5938,
            "prob": 0.0038
          },
          {
            "token": "Ġwait",
            "token_id": 3868,
            "logprob": -6.3438,
            "prob": 0.0018
          },
          {
            "token": "Ġ—",
            "token_id": 2001,
            "logprob": -6.7188,
            "prob": 0.0012
          },
          {
            "token": "Ġand",
            "token_id": 323,
            "logprob": -7.4688,
            "prob": 0.0006
          },
          {
            "token": "Ġ(",
            "token_id": 320,
            "logprob": -7.8438,
            "prob": 0.0004
          },
          {
            "token": "ĠWait",
            "token_id": 14144,
            "logprob": -7.9688,
            "prob": 0.0004
          },
          {
            "token": "Ġor",
            "token_id": 477,
            "logprob": -8.6875,
            "prob": 0.0002
          }
        ]
      },
      "selected_token": {
        "token": "Ġ–",
        "token_id": 1389,
        "selection_method": "sampling"
      }
    },
    {
      "step": 26,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\" –",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\"",
        "Ġ–"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1,
        1389
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġwait",
            "token_id": 3868,
            "logprob": -0.2256,
            "prob": 0.7969
          },
          {
            "token": "Ġ\"",
            "token_id": 330,
            "logprob": -2.4688,
            "prob": 0.084
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -3.2188,
            "prob": 0.0398
          },
          {
            "token": "Ġwhich",
            "token_id": 902,
            "logprob": -3.4688,
            "prob": 0.031
          },
          {
            "token": "Ġoh",
            "token_id": 14346,
            "logprob": -4.0938,
            "prob": 0.0166
          },
          {
            "token": "Ġno",
            "token_id": 912,
            "logprob": -5.0938,
            "prob": 0.0061
          },
          {
            "token": "ĠWait",
            "token_id": 14144,
            "logprob": -5.4062,
            "prob": 0.0045
          },
          {
            "token": "Ġthat",
            "token_id": 430,
            "logprob": -5.5312,
            "prob": 0.0039
          },
          {
            "token": "Ġbut",
            "token_id": 719,
            "logprob": -6.1562,
            "prob": 0.0021
          },
          {
            "token": "ĠFrench",
            "token_id": 8753,
            "logprob": -6.2188,
            "prob": 0.002
          }
        ]
      },
      "selected_token": {
        "token": "Ġwait",
        "token_id": 3868,
        "selection_method": "sampling"
      }
    },
    {
      "step": 27,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\" – wait",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\"",
        "Ġ–",
        "Ġwait"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1,
        1389,
        3868
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": ",",
            "token_id": 11,
            "logprob": -0.0097,
            "prob": 0.9922
          },
          {
            "token": "Ġno",
            "token_id": 912,
            "logprob": -4.75,
            "prob": 0.0085
          },
          {
            "token": "Ġ\"",
            "token_id": 330,
            "logprob": -7.75,
            "prob": 0.0004
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -8.25,
            "prob": 0.0003
          },
          {
            "token": "Ġactually",
            "token_id": 3604,
            "logprob": -8.875,
            "prob": 0.0001
          },
          {
            "token": "Ġwait",
            "token_id": 3868,
            "logprob": -9.375,
            "prob": 0.0001
          },
          {
            "token": "Ġthat",
            "token_id": 430,
            "logprob": -9.875,
            "prob": 0.0001
          },
          {
            "token": "Ġhold",
            "token_id": 3412,
            "logprob": -10.1875,
            "prob": 0.0
          },
          {
            "token": "Ġa",
            "token_id": 264,
            "logprob": -10.25,
            "prob": 0.0
          },
          {
            "token": "ĠFrench",
            "token_id": 8753,
            "logprob": -11.125,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": ",",
        "token_id": 11,
        "selection_method": "sampling"
      }
    },
    {
      "step": 28,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\" – wait,",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\"",
        "Ġ–",
        "Ġwait",
        ","
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1,
        1389,
        3868,
        11
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġ\"",
            "token_id": 330,
            "logprob": -0.9648,
            "prob": 0.3809
          },
          {
            "token": "Ġno",
            "token_id": 912,
            "logprob": -1.4609,
            "prob": 0.2314
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -1.4609,
            "prob": 0.2314
          },
          {
            "token": "Ġactually",
            "token_id": 3604,
            "logprob": -3.4688,
            "prob": 0.0312
          },
          {
            "token": "Ġmaybe",
            "token_id": 7344,
            "logprob": -3.5312,
            "prob": 0.0294
          },
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -3.9062,
            "prob": 0.0201
          },
          {
            "token": "Ġthat",
            "token_id": 430,
            "logprob": -3.9688,
            "prob": 0.0189
          },
          {
            "token": "ĠFrench",
            "token_id": 8753,
            "logprob": -3.9688,
            "prob": 0.0189
          },
          {
            "token": "Ġlet",
            "token_id": 1095,
            "logprob": -4.5312,
            "prob": 0.0108
          },
          {
            "token": "Ġhold",
            "token_id": 3412,
            "logprob": -4.6562,
            "prob": 0.0095
          }
        ]
      },
      "selected_token": {
        "token": "ĠFrench",
        "token_id": 8753,
        "selection_method": "sampling"
      }
    },
    {
      "step": 29,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\" – wait, French",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\"",
        "Ġ–",
        "Ġwait",
        ",",
        "ĠFrench"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1,
        1389,
        3868,
        11,
        8753
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": ".",
            "token_id": 13,
            "logprob": -0.4238,
            "prob": 0.6562
          },
          {
            "token": "Ġfor",
            "token_id": 369,
            "logprob": -1.7969,
            "prob": 0.165
          },
          {
            "token": "?",
            "token_id": 30,
            "logprob": -2.1719,
            "prob": 0.1138
          },
          {
            "token": ",",
            "token_id": 11,
            "logprob": -3.4219,
            "prob": 0.0325
          },
          {
            "token": "!",
            "token_id": 0,
            "logprob": -3.5469,
            "prob": 0.0288
          },
          {
            "token": "Ġphrase",
            "token_id": 17571,
            "logprob": -6.5625,
            "prob": 0.0014
          },
          {
            "token": ":",
            "token_id": 25,
            "logprob": -6.6875,
            "prob": 0.0013
          },
          {
            "token": "Ġhere",
            "token_id": 1618,
            "logprob": -7.875,
            "prob": 0.0004
          },
          {
            "token": "Ġslang",
            "token_id": 81012,
            "logprob": -8.5625,
            "prob": 0.0002
          },
          {
            "token": "Ġmaybe",
            "token_id": 7344,
            "logprob": -8.625,
            "prob": 0.0002
          }
        ]
      },
      "selected_token": {
        "token": ".",
        "token_id": 13,
        "selection_method": "sampling"
      }
    },
    {
      "step": 30,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\" – wait, French.",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\"",
        "Ġ–",
        "Ġwait",
        ",",
        "ĠFrench",
        "."
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1,
        1389,
        3868,
        11,
        8753,
        13
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġ\"",
            "token_id": 330,
            "logprob": -0.1191,
            "prob": 0.8867
          },
          {
            "token": "ĠLet",
            "token_id": 6914,
            "logprob": -2.4375,
            "prob": 0.0879
          },
          {
            "token": "ĠWait",
            "token_id": 14144,
            "logprob": -4.75,
            "prob": 0.0087
          },
          {
            "token": "ĠOh",
            "token_id": 8840,
            "logprob": -5.1875,
            "prob": 0.0056
          },
          {
            "token": "ĠRa",
            "token_id": 18989,
            "logprob": -5.4375,
            "prob": 0.0044
          },
          {
            "token": "ĠRight",
            "token_id": 10291,
            "logprob": -6.375,
            "prob": 0.0017
          },
          {
            "token": "ĠSo",
            "token_id": 2100,
            "logprob": -7.25,
            "prob": 0.0007
          },
          {
            "token": "ĠThe",
            "token_id": 578,
            "logprob": -7.3125,
            "prob": 0.0007
          },
          {
            "token": "ĠIn",
            "token_id": 763,
            "logprob": -7.8125,
            "prob": 0.0004
          },
          {
            "token": "ĠHmm",
            "token_id": 89290,
            "logprob": -8.875,
            "prob": 0.0001
          }
        ]
      },
      "selected_token": {
        "token": "ĠWait",
        "token_id": 14144,
        "selection_method": "sampling"
      }
    },
    {
      "step": 31,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\" – wait, French. Wait",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\"",
        "Ġ–",
        "Ġwait",
        ",",
        "ĠFrench",
        ".",
        "ĠWait"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1,
        1389,
        3868,
        11,
        8753,
        13,
        14144
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": ",",
            "token_id": 11,
            "logprob": -0.0564,
            "prob": 0.9453
          },
          {
            "token": "Ġ\"",
            "token_id": 330,
            "logprob": -3.5625,
            "prob": 0.0286
          },
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -4.1875,
            "prob": 0.0153
          },
          {
            "token": "Ġno",
            "token_id": 912,
            "logprob": -4.6875,
            "prob": 0.0093
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -8.25,
            "prob": 0.0003
          },
          {
            "token": ":",
            "token_id": 25,
            "logprob": -8.25,
            "prob": 0.0003
          },
          {
            "token": "Ġactually",
            "token_id": 3604,
            "logprob": -8.5,
            "prob": 0.0002
          },
          {
            "token": "Ġhold",
            "token_id": 3412,
            "logprob": -8.5625,
            "prob": 0.0002
          },
          {
            "token": "Ġa",
            "token_id": 264,
            "logprob": -8.8125,
            "prob": 0.0001
          },
          {
            "token": "Ġlet",
            "token_id": 1095,
            "logprob": -8.875,
            "prob": 0.0001
          }
        ]
      },
      "selected_token": {
        "token": "Ġlet",
        "token_id": 1095,
        "selection_method": "sampling"
      }
    },
    {
      "step": 32,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\" – wait, French. Wait let",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\"",
        "Ġ–",
        "Ġwait",
        ",",
        "ĠFrench",
        ".",
        "ĠWait",
        "Ġlet"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1,
        1389,
        3868,
        11,
        8753,
        13,
        14144,
        1095
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġme",
            "token_id": 757,
            "logprob": -0.0077,
            "prob": 0.9922
          },
          {
            "token": "'s",
            "token_id": 596,
            "logprob": -4.875,
            "prob": 0.0076
          },
          {
            "token": "me",
            "token_id": 2727,
            "logprob": -10.875,
            "prob": 0.0
          },
          {
            "token": "Ġsee",
            "token_id": 1518,
            "logprob": -11.3125,
            "prob": 0.0
          },
          {
            "token": "Ġmet",
            "token_id": 2322,
            "logprob": -11.625,
            "prob": 0.0
          },
          {
            "token": "ĠMe",
            "token_id": 2206,
            "logprob": -14.25,
            "prob": 0.0
          },
          {
            "token": "'m",
            "token_id": 2846,
            "logprob": -14.25,
            "prob": 0.0
          },
          {
            "token": "-me",
            "token_id": 35073,
            "logprob": -14.375,
            "prob": 0.0
          },
          {
            "token": "Ġmy",
            "token_id": 856,
            "logprob": -14.625,
            "prob": 0.0
          },
          {
            "token": "Ġmi",
            "token_id": 9686,
            "logprob": -15.125,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġme",
        "token_id": 757,
        "selection_method": "sampling"
      }
    },
    {
      "step": 33,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\" – wait, French. Wait let me",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\"",
        "Ġ–",
        "Ġwait",
        ",",
        "ĠFrench",
        ".",
        "ĠWait",
        "Ġlet",
        "Ġme"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1,
        1389,
        3868,
        11,
        8753,
        13,
        14144,
        1095,
        757
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġcheck",
            "token_id": 1817,
            "logprob": -0.127,
            "prob": 0.8789
          },
          {
            "token": "Ġparse",
            "token_id": 4820,
            "logprob": -3.375,
            "prob": 0.0342
          },
          {
            "token": "Ġread",
            "token_id": 1373,
            "logprob": -3.5,
            "prob": 0.0302
          },
          {
            "token": "Ġmake",
            "token_id": 1304,
            "logprob": -3.75,
            "prob": 0.0234
          },
          {
            "token": "Ġconfirm",
            "token_id": 7838,
            "logprob": -4.75,
            "prob": 0.0086
          },
          {
            "token": "Ġlook",
            "token_id": 1427,
            "logprob": -5.0,
            "prob": 0.0067
          },
          {
            "token": "Ġdouble",
            "token_id": 2033,
            "logprob": -5.0,
            "prob": 0.0067
          },
          {
            "token": "Ġtranslate",
            "token_id": 15025,
            "logprob": -5.625,
            "prob": 0.0036
          },
          {
            "token": "Ġsee",
            "token_id": 1518,
            "logprob": -5.75,
            "prob": 0.0032
          },
          {
            "token": "Ġthink",
            "token_id": 1781,
            "logprob": -6.375,
            "prob": 0.0017
          }
        ]
      },
      "selected_token": {
        "token": "Ġcheck",
        "token_id": 1817,
        "selection_method": "sampling"
      }
    },
    {
      "step": 34,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\" – wait, French. Wait let me check",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\"",
        "Ġ–",
        "Ġwait",
        ",",
        "ĠFrench",
        ".",
        "ĠWait",
        "Ġlet",
        "Ġme",
        "Ġcheck"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1,
        1389,
        3868,
        11,
        8753,
        13,
        14144,
        1095,
        757,
        1817
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġagain",
            "token_id": 1578,
            "logprob": -0.7852,
            "prob": 0.4551
          },
          {
            "token": ".",
            "token_id": 13,
            "logprob": -1.0391,
            "prob": 0.3555
          },
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -1.9141,
            "prob": 0.1484
          },
          {
            "token": ":",
            "token_id": 25,
            "logprob": -3.5312,
            "prob": 0.0292
          },
          {
            "token": ".\n\n",
            "token_id": 382,
            "logprob": -4.5312,
            "prob": 0.0107
          },
          {
            "token": "Ġspelling",
            "token_id": 43529,
            "logprob": -7.1562,
            "prob": 0.0008
          },
          {
            "token": "...",
            "token_id": 1131,
            "logprob": -8.9375,
            "prob": 0.0001
          },
          {
            "token": "Ġtranslation",
            "token_id": 14807,
            "logprob": -9.0625,
            "prob": 0.0001
          },
          {
            "token": ",",
            "token_id": 11,
            "logprob": -10.3125,
            "prob": 0.0
          },
          {
            "token": "Ġthat",
            "token_id": 430,
            "logprob": -10.3125,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġthe",
        "token_id": 279,
        "selection_method": "sampling"
      }
    },
    {
      "step": 35,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\" – wait, French. Wait let me check the",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\"",
        "Ġ–",
        "Ġwait",
        ",",
        "ĠFrench",
        ".",
        "ĠWait",
        "Ġlet",
        "Ġme",
        "Ġcheck",
        "Ġthe"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1,
        1389,
        3868,
        11,
        8753,
        13,
        14144,
        1095,
        757,
        1817,
        279
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġuser",
            "token_id": 1217,
            "logprob": -0.8711,
            "prob": 0.418
          },
          {
            "token": "Ġexact",
            "token_id": 4839,
            "logprob": -1.2422,
            "prob": 0.2871
          },
          {
            "token": "Ġoriginal",
            "token_id": 4113,
            "logprob": -2.125,
            "prob": 0.1201
          },
          {
            "token": "Ġlanguage",
            "token_id": 4221,
            "logprob": -2.625,
            "prob": 0.0728
          },
          {
            "token": "Ġinput",
            "token_id": 1988,
            "logprob": -3.125,
            "prob": 0.0442
          },
          {
            "token": "Ġtranslation",
            "token_id": 14807,
            "logprob": -3.75,
            "prob": 0.0237
          },
          {
            "token": "Ġspelling",
            "token_id": 43529,
            "logprob": -4.375,
            "prob": 0.0126
          },
          {
            "token": "Ġquery",
            "token_id": 3319,
            "logprob": -5.25,
            "prob": 0.0053
          },
          {
            "token": "Ġactual",
            "token_id": 5150,
            "logprob": -5.25,
            "prob": 0.0053
          },
          {
            "token": "Ġquestion",
            "token_id": 3488,
            "logprob": -5.625,
            "prob": 0.0036
          }
        ]
      },
      "selected_token": {
        "token": "Ġactual",
        "token_id": 5150,
        "selection_method": "sampling"
      }
    },
    {
      "step": 36,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\" – wait, French. Wait let me check the actual",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\"",
        "Ġ–",
        "Ġwait",
        ",",
        "ĠFrench",
        ".",
        "ĠWait",
        "Ġlet",
        "Ġme",
        "Ġcheck",
        "Ġthe",
        "Ġactual"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1,
        1389,
        3868,
        11,
        8753,
        13,
        14144,
        1095,
        757,
        1817,
        279,
        5150
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġquestion",
            "token_id": 3488,
            "logprob": -1.1484,
            "prob": 0.3184
          },
          {
            "token": "Ġinput",
            "token_id": 1988,
            "logprob": -1.3984,
            "prob": 0.2471
          },
          {
            "token": "Ġquery",
            "token_id": 3319,
            "logprob": -1.7734,
            "prob": 0.1699
          },
          {
            "token": "Ġuser",
            "token_id": 1217,
            "logprob": -2.2656,
            "prob": 0.103
          },
          {
            "token": "Ġwords",
            "token_id": 4339,
            "logprob": -2.5156,
            "prob": 0.0806
          },
          {
            "token": "Ġtext",
            "token_id": 1495,
            "logprob": -3.3906,
            "prob": 0.0334
          },
          {
            "token": "Ġmessage",
            "token_id": 1984,
            "logprob": -4.1562,
            "prob": 0.0159
          },
          {
            "token": "Ġphrase",
            "token_id": 17571,
            "logprob": -5.0312,
            "prob": 0.0066
          },
          {
            "token": "Ġrequest",
            "token_id": 1715,
            "logprob": -5.2812,
            "prob": 0.0051
          },
          {
            "token": "Ġwording",
            "token_id": 61327,
            "logprob": -5.4062,
            "prob": 0.0045
          }
        ]
      },
      "selected_token": {
        "token": "Ġmessage",
        "token_id": 1984,
        "selection_method": "sampling"
      }
    },
    {
      "step": 37,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\" – wait, French. Wait let me check the actual message",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\"",
        "Ġ–",
        "Ġwait",
        ",",
        "ĠFrench",
        ".",
        "ĠWait",
        "Ġlet",
        "Ġme",
        "Ġcheck",
        "Ġthe",
        "Ġactual",
        "Ġmessage"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1,
        1389,
        3868,
        11,
        8753,
        13,
        14144,
        1095,
        757,
        1817,
        279,
        5150,
        1984
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġagain",
            "token_id": 1578,
            "logprob": -0.2656,
            "prob": 0.7656
          },
          {
            "token": ".",
            "token_id": 13,
            "logprob": -1.6406,
            "prob": 0.1934
          },
          {
            "token": ".\n\n",
            "token_id": 382,
            "logprob": -3.2656,
            "prob": 0.0381
          },
          {
            "token": ":",
            "token_id": 25,
            "logprob": -6.9062,
            "prob": 0.001
          },
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -7.5312,
            "prob": 0.0005
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -9.75,
            "prob": 0.0001
          },
          {
            "token": "Ġgiven",
            "token_id": 2728,
            "logprob": -11.125,
            "prob": 0.0
          },
          {
            "token": "Ġthey",
            "token_id": 814,
            "logprob": -11.75,
            "prob": 0.0
          },
          {
            "token": "Ġhere",
            "token_id": 1618,
            "logprob": -11.875,
            "prob": 0.0
          },
          {
            "token": "Ġfirst",
            "token_id": 1176,
            "logprob": -12.0,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġagain",
        "token_id": 1578,
        "selection_method": "sampling"
      }
    },
    {
      "step": 38,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\" – wait, French. Wait let me check the actual message again",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\"",
        "Ġ–",
        "Ġwait",
        ",",
        "ĠFrench",
        ".",
        "ĠWait",
        "Ġlet",
        "Ġme",
        "Ġcheck",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġagain"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1,
        1389,
        3868,
        11,
        8753,
        13,
        14144,
        1095,
        757,
        1817,
        279,
        5150,
        1984,
        1578
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": ".",
            "token_id": 13,
            "logprob": -0.2812,
            "prob": 0.7539
          },
          {
            "token": ".\n\n",
            "token_id": 382,
            "logprob": -1.4062,
            "prob": 0.2451
          },
          {
            "token": ":",
            "token_id": 25,
            "logprob": -7.6562,
            "prob": 0.0005
          },
          {
            "token": ".\"",
            "token_id": 1210,
            "logprob": -11.1875,
            "prob": 0.0
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -11.1875,
            "prob": 0.0
          },
          {
            "token": "...",
            "token_id": 1131,
            "logprob": -12.0625,
            "prob": 0.0
          },
          {
            "token": ".\n",
            "token_id": 627,
            "logprob": -12.3125,
            "prob": 0.0
          },
          {
            "token": ".\"\n\n",
            "token_id": 2266,
            "logprob": -12.6875,
            "prob": 0.0
          },
          {
            "token": ":\n\n",
            "token_id": 1473,
            "logprob": -13.1875,
            "prob": 0.0
          },
          {
            "token": "Ġbecause",
            "token_id": 1606,
            "logprob": -13.5625,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": ".",
        "token_id": 13,
        "selection_method": "sampling"
      }
    },
    {
      "step": 39,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\" – wait, French. Wait let me check the actual message again.",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\"",
        "Ġ–",
        "Ġwait",
        ",",
        "ĠFrench",
        ".",
        "ĠWait",
        "Ġlet",
        "Ġme",
        "Ġcheck",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġagain",
        "."
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1,
        1389,
        3868,
        11,
        8753,
        13,
        14144,
        1095,
        757,
        1817,
        279,
        5150,
        1984,
        1578,
        13
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "ĠThe",
            "token_id": 578,
            "logprob": -0.0933,
            "prob": 0.9102
          },
          {
            "token": "ĠOh",
            "token_id": 8840,
            "logprob": -2.8438,
            "prob": 0.0583
          },
          {
            "token": "ĠUser",
            "token_id": 2724,
            "logprob": -3.9688,
            "prob": 0.0189
          },
          {
            "token": "ĠWait",
            "token_id": 14144,
            "logprob": -5.0938,
            "prob": 0.0061
          },
          {
            "token": "Ġ\"",
            "token_id": 330,
            "logprob": -5.2188,
            "prob": 0.0054
          },
          {
            "token": "ĠAh",
            "token_id": 16770,
            "logprob": -8.9375,
            "prob": 0.0001
          },
          {
            "token": "Ġuser",
            "token_id": 1217,
            "logprob": -9.5625,
            "prob": 0.0001
          },
          {
            "token": "ĠLet",
            "token_id": 6914,
            "logprob": -9.5625,
            "prob": 0.0001
          },
          {
            "token": "ĠHmm",
            "token_id": 89290,
            "logprob": -9.9375,
            "prob": 0.0
          },
          {
            "token": "ĠLooking",
            "token_id": 21815,
            "logprob": -10.3125,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġ\"",
        "token_id": 330,
        "selection_method": "sampling"
      }
    },
    {
      "step": 40,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\" – wait, French. Wait let me check the actual message again. \"",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\"",
        "Ġ–",
        "Ġwait",
        ",",
        "ĠFrench",
        ".",
        "ĠWait",
        "Ġlet",
        "Ġme",
        "Ġcheck",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġagain",
        ".",
        "Ġ\""
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1,
        1389,
        3868,
        11,
        8753,
        13,
        14144,
        1095,
        757,
        1817,
        279,
        5150,
        1984,
        1578,
        13,
        330
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "R",
            "token_id": 49,
            "logprob": -0.0077,
            "prob": 0.9922
          },
          {
            "token": "Ra",
            "token_id": 56635,
            "logprob": -5.375,
            "prob": 0.0046
          },
          {
            "token": "RA",
            "token_id": 5726,
            "logprob": -7.3125,
            "prob": 0.0007
          },
          {
            "token": "ĠRa",
            "token_id": 18989,
            "logprob": -7.8125,
            "prob": 0.0004
          },
          {
            "token": "User",
            "token_id": 1502,
            "logprob": -7.875,
            "prob": 0.0004
          },
          {
            "token": "ra",
            "token_id": 969,
            "logprob": -8.0,
            "prob": 0.0003
          },
          {
            "token": "Ġuser",
            "token_id": 1217,
            "logprob": -8.5625,
            "prob": 0.0002
          },
          {
            "token": "user",
            "token_id": 882,
            "logprob": -9.4375,
            "prob": 0.0001
          },
          {
            "token": "Tell",
            "token_id": 41551,
            "logprob": -10.625,
            "prob": 0.0
          },
          {
            "token": "用户",
            "token_id": 20600,
            "logprob": -10.6875,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "用户",
        "token_id": 20600,
        "selection_method": "sampling"
      }
    },
    {
      "step": 41,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\" – wait, French. Wait let me check the actual message again. \"用户",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\"",
        "Ġ–",
        "Ġwait",
        ",",
        "ĠFrench",
        ".",
        "ĠWait",
        "Ġlet",
        "Ġme",
        "Ġcheck",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġagain",
        ".",
        "Ġ\"",
        "用户"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1,
        1389,
        3868,
        11,
        8753,
        13,
        14144,
        1095,
        757,
        1817,
        279,
        5150,
        1984,
        1578,
        13,
        330,
        20600
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "：",
            "token_id": 5232,
            "logprob": -0.7227,
            "prob": 0.4844
          },
          {
            "token": ":",
            "token_id": 25,
            "logprob": -1.4766,
            "prob": 0.2295
          },
          {
            "token": "说",
            "token_id": 37687,
            "logprob": -1.7891,
            "prob": 0.168
          },
          {
            "token": "输入",
            "token_id": 32296,
            "logprob": -3.8438,
            "prob": 0.0214
          },
          {
            "token": "Ġsaid",
            "token_id": 1071,
            "logprob": -4.6562,
            "prob": 0.0095
          },
          {
            "token": "问",
            "token_id": 57107,
            "logprob": -4.9062,
            "prob": 0.0074
          },
          {
            "token": "：“",
            "token_id": 38087,
            "logprob": -5.0938,
            "prob": 0.0061
          },
          {
            "token": "写",
            "token_id": 62543,
            "logprob": -5.0938,
            "prob": 0.0061
          },
          {
            "token": "您",
            "token_id": 88126,
            "logprob": -5.2188,
            "prob": 0.0054
          },
          {
            "token": "\"",
            "token_id": 1,
            "logprob": -5.7188,
            "prob": 0.0033
          }
        ]
      },
      "selected_token": {
        "token": "：“",
        "token_id": 38087,
        "selection_method": "sampling"
      }
    },
    {
      "step": 42,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\" – wait, French. Wait let me check the actual message again. \"用户：“",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\"",
        "Ġ–",
        "Ġwait",
        ",",
        "ĠFrench",
        ".",
        "ĠWait",
        "Ġlet",
        "Ġme",
        "Ġcheck",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġagain",
        ".",
        "Ġ\"",
        "用户",
        "：“"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1,
        1389,
        3868,
        11,
        8753,
        13,
        14144,
        1095,
        757,
        1817,
        279,
        5150,
        1984,
        1578,
        13,
        330,
        20600,
        38087
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "R",
            "token_id": 49,
            "logprob": -0.3418,
            "prob": 0.7109
          },
          {
            "token": "RA",
            "token_id": 5726,
            "logprob": -2.0312,
            "prob": 0.1309
          },
          {
            "token": "ra",
            "token_id": 969,
            "logprob": -2.5938,
            "prob": 0.0747
          },
          {
            "token": "用户",
            "token_id": 20600,
            "logprob": -3.7812,
            "prob": 0.0228
          },
          {
            "token": "请",
            "token_id": 15225,
            "logprob": -4.7188,
            "prob": 0.0089
          },
          {
            "token": "Ra",
            "token_id": 56635,
            "logprob": -5.0938,
            "prob": 0.0061
          },
          {
            "token": "ĠRa",
            "token_id": 18989,
            "logprob": -6.1562,
            "prob": 0.0021
          },
          {
            "token": "�",
            "token_id": 15581,
            "logprob": -6.7188,
            "prob": 0.0012
          },
          {
            "token": "Ġ用户",
            "token_id": 51142,
            "logprob": -6.7188,
            "prob": 0.0012
          },
          {
            "token": "�",
            "token_id": 162,
            "logprob": -6.9062,
            "prob": 0.001
          }
        ]
      },
      "selected_token": {
        "token": "�",
        "token_id": 162,
        "selection_method": "sampling"
      }
    },
    {
      "step": 43,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\" – wait, French. Wait let me check the actual message again. \"用户：“�",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\"",
        "Ġ–",
        "Ġwait",
        ",",
        "ĠFrench",
        ".",
        "ĠWait",
        "Ġlet",
        "Ġme",
        "Ġcheck",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġagain",
        ".",
        "Ġ\"",
        "用户",
        "：“",
        "�"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1,
        1389,
        3868,
        11,
        8753,
        13,
        14144,
        1095,
        757,
        1817,
        279,
        5150,
        1984,
        1578,
        13,
        330,
        20600,
        38087,
        162
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "�",
            "token_id": 238,
            "logprob": -1.4297,
            "prob": 0.2393
          },
          {
            "token": "�",
            "token_id": 95,
            "logprob": -1.7422,
            "prob": 0.1748
          },
          {
            "token": "�",
            "token_id": 121,
            "logprob": -3.0625,
            "prob": 0.0471
          },
          {
            "token": "�",
            "token_id": 100,
            "logprob": -3.1875,
            "prob": 0.0415
          },
          {
            "token": "�",
            "token_id": 101,
            "logprob": -3.625,
            "prob": 0.0269
          },
          {
            "token": "�",
            "token_id": 102,
            "logprob": -3.6875,
            "prob": 0.0251
          },
          {
            "token": "�",
            "token_id": 224,
            "logprob": -3.6875,
            "prob": 0.0251
          },
          {
            "token": "�",
            "token_id": 229,
            "logprob": -3.6875,
            "prob": 0.0251
          },
          {
            "token": "�",
            "token_id": 122,
            "logprob": -3.8125,
            "prob": 0.0222
          },
          {
            "token": "�",
            "token_id": 123,
            "logprob": -3.875,
            "prob": 0.0209
          }
        ]
      },
      "selected_token": {
        "token": "�",
        "token_id": 229,
        "selection_method": "sampling"
      }
    },
    {
      "step": 44,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\" – wait, French. Wait let me check the actual message again. \"用户：“�",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\"",
        "Ġ–",
        "Ġwait",
        ",",
        "ĠFrench",
        ".",
        "ĠWait",
        "Ġlet",
        "Ġme",
        "Ġcheck",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġagain",
        ".",
        "Ġ\"",
        "用户",
        "：“",
        "�",
        "�"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1,
        1389,
        3868,
        11,
        8753,
        13,
        14144,
        1095,
        757,
        1817,
        279,
        5150,
        1984,
        1578,
        13,
        330,
        20600,
        38087,
        162,
        229
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "�",
            "token_id": 232,
            "logprob": -1.3125,
            "prob": 0.2695
          },
          {
            "token": "�",
            "token_id": 224,
            "logprob": -1.3125,
            "prob": 0.2695
          },
          {
            "token": "�",
            "token_id": 231,
            "logprob": -1.8125,
            "prob": 0.1631
          },
          {
            "token": "�",
            "token_id": 240,
            "logprob": -2.125,
            "prob": 0.1196
          },
          {
            "token": "�",
            "token_id": 233,
            "logprob": -2.875,
            "prob": 0.0566
          },
          {
            "token": "�",
            "token_id": 230,
            "logprob": -3.0,
            "prob": 0.0498
          },
          {
            "token": "�",
            "token_id": 113,
            "logprob": -3.5,
            "prob": 0.0303
          },
          {
            "token": "�",
            "token_id": 229,
            "logprob": -4.9375,
            "prob": 0.0072
          },
          {
            "token": "�",
            "token_id": 121,
            "logprob": -5.1875,
            "prob": 0.0056
          },
          {
            "token": "�",
            "token_id": 106,
            "logprob": -5.75,
            "prob": 0.0032
          }
        ]
      },
      "selected_token": {
        "token": "�",
        "token_id": 113,
        "selection_method": "sampling"
      }
    },
    {
      "step": 45,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\" – wait, French. Wait let me check the actual message again. \"用户：“懵",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\"",
        "Ġ–",
        "Ġwait",
        ",",
        "ĠFrench",
        ".",
        "ĠWait",
        "Ġlet",
        "Ġme",
        "Ġcheck",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġagain",
        ".",
        "Ġ\"",
        "用户",
        "：“",
        "�",
        "�",
        "�"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1,
        1389,
        3868,
        11,
        8753,
        13,
        14144,
        1095,
        757,
        1817,
        279,
        5150,
        1984,
        1578,
        13,
        330,
        20600,
        38087,
        162,
        229,
        113
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "”",
            "token_id": 863,
            "logprob": -1.7891,
            "prob": 0.167
          },
          {
            "token": "�",
            "token_id": 162,
            "logprob": -2.3594,
            "prob": 0.0947
          },
          {
            "token": "？",
            "token_id": 11571,
            "logprob": -2.5469,
            "prob": 0.0786
          },
          {
            "token": "了",
            "token_id": 35287,
            "logprob": -2.5469,
            "prob": 0.0786
          },
          {
            "token": "（",
            "token_id": 10110,
            "logprob": -2.7969,
            "prob": 0.0613
          },
          {
            "token": "?",
            "token_id": 30,
            "logprob": -3.0469,
            "prob": 0.0476
          },
          {
            "token": "�",
            "token_id": 11589,
            "logprob": -3.7969,
            "prob": 0.0226
          },
          {
            "token": "的",
            "token_id": 9554,
            "logprob": -4.4062,
            "prob": 0.0121
          },
          {
            "token": "?”",
            "token_id": 12671,
            "logprob": -4.4062,
            "prob": 0.0121
          },
          {
            "token": "�",
            "token_id": 20022,
            "logprob": -4.5938,
            "prob": 0.01
          }
        ]
      },
      "selected_token": {
        "token": "”",
        "token_id": 863,
        "selection_method": "sampling"
      }
    },
    {
      "step": 46,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\" – wait, French. Wait let me check the actual message again. \"用户：“懵”",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\"",
        "Ġ–",
        "Ġwait",
        ",",
        "ĠFrench",
        ".",
        "ĠWait",
        "Ġlet",
        "Ġme",
        "Ġcheck",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġagain",
        ".",
        "Ġ\"",
        "用户",
        "：“",
        "�",
        "�",
        "�",
        "”"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1,
        1389,
        3868,
        11,
        8753,
        13,
        14144,
        1095,
        757,
        1817,
        279,
        5150,
        1984,
        1578,
        13,
        330,
        20600,
        38087,
        162,
        229,
        113,
        863
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "”",
            "token_id": 863,
            "logprob": -2.0,
            "prob": 0.1357
          },
          {
            "token": "（",
            "token_id": 10110,
            "logprob": -2.8125,
            "prob": 0.0601
          },
          {
            "token": "ĠR",
            "token_id": 432,
            "logprob": -2.8125,
            "prob": 0.0601
          },
          {
            "token": "Ġ–",
            "token_id": 1389,
            "logprob": -3.0,
            "prob": 0.0498
          },
          {
            "token": "Ġ“",
            "token_id": 1054,
            "logprob": -3.125,
            "prob": 0.0439
          },
          {
            "token": "Ġ",
            "token_id": 220,
            "logprob": -3.25,
            "prob": 0.0388
          },
          {
            "token": "Ġ�",
            "token_id": 6704,
            "logprob": -3.3125,
            "prob": 0.0364
          },
          {
            "token": "\"",
            "token_id": 1,
            "logprob": -3.5,
            "prob": 0.0303
          },
          {
            "token": "“",
            "token_id": 2118,
            "logprob": -3.5625,
            "prob": 0.0284
          },
          {
            "token": "Ġ(",
            "token_id": 320,
            "logprob": -3.625,
            "prob": 0.0266
          }
        ]
      },
      "selected_token": {
        "token": "Ġ",
        "token_id": 220,
        "selection_method": "sampling"
      }
    },
    {
      "step": 47,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\" – wait, French. Wait let me check the actual message again. \"用户：“懵” ",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\"",
        "Ġ–",
        "Ġwait",
        ",",
        "ĠFrench",
        ".",
        "ĠWait",
        "Ġlet",
        "Ġme",
        "Ġcheck",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġagain",
        ".",
        "Ġ\"",
        "用户",
        "：“",
        "�",
        "�",
        "�",
        "”",
        "Ġ"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1,
        1389,
        3868,
        11,
        8753,
        13,
        14144,
        1095,
        757,
        1817,
        279,
        5150,
        1984,
        1578,
        13,
        330,
        20600,
        38087,
        162,
        229,
        113,
        863,
        220
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "你",
            "token_id": 57668,
            "logprob": -1.6719,
            "prob": 0.1885
          },
          {
            "token": "也",
            "token_id": 75863,
            "logprob": -1.8594,
            "prob": 0.1562
          },
          {
            "token": "之",
            "token_id": 55030,
            "logprob": -2.1719,
            "prob": 0.1143
          },
          {
            "token": "但",
            "token_id": 78388,
            "logprob": -2.3594,
            "prob": 0.0947
          },
          {
            "token": "请",
            "token_id": 15225,
            "logprob": -2.9844,
            "prob": 0.0508
          },
          {
            "token": "问",
            "token_id": 57107,
            "logprob": -3.4219,
            "prob": 0.0327
          },
          {
            "token": "202",
            "token_id": 2366,
            "logprob": -3.6719,
            "prob": 0.0255
          },
          {
            "token": "）",
            "token_id": 7705,
            "logprob": -3.7344,
            "prob": 0.0239
          },
          {
            "token": "有",
            "token_id": 19361,
            "logprob": -3.7969,
            "prob": 0.0225
          },
          {
            "token": "？",
            "token_id": 11571,
            "logprob": -4.1562,
            "prob": 0.0154
          }
        ]
      },
      "selected_token": {
        "token": "问",
        "token_id": 57107,
        "selection_method": "sampling"
      }
    },
    {
      "step": 48,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\" – wait, French. Wait let me check the actual message again. \"用户：“懵” 问",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\"",
        "Ġ–",
        "Ġwait",
        ",",
        "ĠFrench",
        ".",
        "ĠWait",
        "Ġlet",
        "Ġme",
        "Ġcheck",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġagain",
        ".",
        "Ġ\"",
        "用户",
        "：“",
        "�",
        "�",
        "�",
        "”",
        "Ġ",
        "问"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1,
        1389,
        3868,
        11,
        8753,
        13,
        14144,
        1095,
        757,
        1817,
        279,
        5150,
        1984,
        1578,
        13,
        330,
        20600,
        38087,
        162,
        229,
        113,
        863,
        220,
        57107
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "：“",
            "token_id": 38087,
            "logprob": -0.9492,
            "prob": 0.3867
          },
          {
            "token": "：",
            "token_id": 5232,
            "logprob": -1.5781,
            "prob": 0.207
          },
          {
            "token": ":",
            "token_id": 25,
            "logprob": -2.1406,
            "prob": 0.1177
          },
          {
            "token": "我",
            "token_id": 37046,
            "logprob": -2.8281,
            "prob": 0.0593
          },
          {
            "token": "：\"",
            "token_id": 41827,
            "logprob": -3.0781,
            "prob": 0.0461
          },
          {
            "token": "了",
            "token_id": 35287,
            "logprob": -3.3906,
            "prob": 0.0337
          },
          {
            "token": "的",
            "token_id": 9554,
            "logprob": -3.9531,
            "prob": 0.0193
          },
          {
            "token": "“",
            "token_id": 2118,
            "logprob": -3.9531,
            "prob": 0.0193
          },
          {
            "token": "你",
            "token_id": 57668,
            "logprob": -4.125,
            "prob": 0.016
          },
          {
            "token": "Ġ“",
            "token_id": 1054,
            "logprob": -4.25,
            "prob": 0.0141
          }
        ]
      },
      "selected_token": {
        "token": "：",
        "token_id": 5232,
        "selection_method": "sampling"
      }
    },
    {
      "step": 49,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\" – wait, French. Wait let me check the actual message again. \"用户：“懵” 问：",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\"",
        "Ġ–",
        "Ġwait",
        ",",
        "ĠFrench",
        ".",
        "ĠWait",
        "Ġlet",
        "Ġme",
        "Ġcheck",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġagain",
        ".",
        "Ġ\"",
        "用户",
        "：“",
        "�",
        "�",
        "�",
        "”",
        "Ġ",
        "问",
        "："
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1,
        1389,
        3868,
        11,
        8753,
        13,
        14144,
        1095,
        757,
        1817,
        279,
        5150,
        1984,
        1578,
        13,
        330,
        20600,
        38087,
        162,
        229,
        113,
        863,
        220,
        57107,
        5232
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "R",
            "token_id": 49,
            "logprob": -0.2275,
            "prob": 0.7969
          },
          {
            "token": "�",
            "token_id": 162,
            "logprob": -3.4844,
            "prob": 0.0309
          },
          {
            "token": "Ra",
            "token_id": 56635,
            "logprob": -4.0312,
            "prob": 0.0176
          },
          {
            "token": "RA",
            "token_id": 5726,
            "logprob": -4.1562,
            "prob": 0.0155
          },
          {
            "token": "ĠRa",
            "token_id": 18989,
            "logprob": -4.2188,
            "prob": 0.0146
          },
          {
            "token": "\"",
            "token_id": 1,
            "logprob": -4.5312,
            "prob": 0.0107
          },
          {
            "token": "你",
            "token_id": 57668,
            "logprob": -4.5938,
            "prob": 0.01
          },
          {
            "token": "ra",
            "token_id": 969,
            "logprob": -4.8438,
            "prob": 0.0078
          },
          {
            "token": "“",
            "token_id": 2118,
            "logprob": -5.0312,
            "prob": 0.0065
          },
          {
            "token": "�",
            "token_id": 15581,
            "logprob": -5.0312,
            "prob": 0.0065
          }
        ]
      },
      "selected_token": {
        "token": "R",
        "token_id": 49,
        "selection_method": "sampling"
      }
    },
    {
      "step": 50,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\" – wait, French. Wait let me check the actual message again. \"用户：“懵” 问：R",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\"",
        "Ġ–",
        "Ġwait",
        ",",
        "ĠFrench",
        ".",
        "ĠWait",
        "Ġlet",
        "Ġme",
        "Ġcheck",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġagain",
        ".",
        "Ġ\"",
        "用户",
        "：“",
        "�",
        "�",
        "�",
        "”",
        "Ġ",
        "问",
        "：",
        "R"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1,
        1389,
        3868,
        11,
        8753,
        13,
        14144,
        1095,
        757,
        1817,
        279,
        5150,
        1984,
        1578,
        13,
        330,
        20600,
        38087,
        162,
        229,
        113,
        863,
        220,
        57107,
        5232,
        49
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "acon",
            "token_id": 23184,
            "logprob": -0.0042,
            "prob": 0.9961
          },
          {
            "token": "ac",
            "token_id": 582,
            "logprob": -8.875,
            "prob": 0.0001
          },
          {
            "token": "acad",
            "token_id": 50957,
            "logprob": -9.0,
            "prob": 0.0001
          },
          {
            "token": "acom",
            "token_id": 45670,
            "logprob": -9.1875,
            "prob": 0.0001
          },
          {
            "token": "acent",
            "token_id": 18680,
            "logprob": -9.5625,
            "prob": 0.0001
          },
          {
            "token": "acons",
            "token_id": 76544,
            "logprob": -9.75,
            "prob": 0.0001
          },
          {
            "token": "acock",
            "token_id": 81378,
            "logprob": -9.8125,
            "prob": 0.0001
          },
          {
            "token": "acco",
            "token_id": 20067,
            "logprob": -10.1875,
            "prob": 0.0
          },
          {
            "token": "acet",
            "token_id": 68323,
            "logprob": -10.1875,
            "prob": 0.0
          },
          {
            "token": "AC",
            "token_id": 1741,
            "logprob": -10.3125,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "acon",
        "token_id": 23184,
        "selection_method": "sampling"
      }
    },
    {
      "step": 51,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\" – wait, French. Wait let me check the actual message again. \"用户：“懵” 问：Racon",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\"",
        "Ġ–",
        "Ġwait",
        ",",
        "ĠFrench",
        ".",
        "ĠWait",
        "Ġlet",
        "Ġme",
        "Ġcheck",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġagain",
        ".",
        "Ġ\"",
        "用户",
        "：“",
        "�",
        "�",
        "�",
        "”",
        "Ġ",
        "问",
        "：",
        "R",
        "acon"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1,
        1389,
        3868,
        11,
        8753,
        13,
        14144,
        1095,
        757,
        1817,
        279,
        5150,
        1984,
        1578,
        13,
        330,
        20600,
        38087,
        162,
        229,
        113,
        863,
        220,
        57107,
        5232,
        49,
        23184
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "te",
            "token_id": 668,
            "logprob": -0.0001,
            "prob": 1.0
          },
          {
            "token": "to",
            "token_id": 998,
            "logprob": -9.875,
            "prob": 0.0001
          },
          {
            "token": "те",
            "token_id": 51627,
            "logprob": -11.9375,
            "prob": 0.0
          },
          {
            "token": "TE",
            "token_id": 2505,
            "logprob": -12.0,
            "prob": 0.0
          },
          {
            "token": "ter",
            "token_id": 466,
            "logprob": -12.5,
            "prob": 0.0
          },
          {
            "token": "tem",
            "token_id": 880,
            "logprob": -13.25,
            "prob": 0.0
          },
          {
            "token": "tes",
            "token_id": 2392,
            "logprob": -14.0625,
            "prob": 0.0
          },
          {
            "token": "tom",
            "token_id": 38501,
            "logprob": -14.0625,
            "prob": 0.0
          },
          {
            "token": "ne",
            "token_id": 818,
            "logprob": -14.0625,
            "prob": 0.0
          },
          {
            "token": "de",
            "token_id": 451,
            "logprob": -14.0625,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "ter",
        "token_id": 466,
        "selection_method": "sampling"
      }
    },
    {
      "step": 52,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\" – wait, French. Wait let me check the actual message again. \"用户：“懵” 问：Raconter",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\"",
        "Ġ–",
        "Ġwait",
        ",",
        "ĠFrench",
        ".",
        "ĠWait",
        "Ġlet",
        "Ġme",
        "Ġcheck",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġagain",
        ".",
        "Ġ\"",
        "用户",
        "：“",
        "�",
        "�",
        "�",
        "”",
        "Ġ",
        "问",
        "：",
        "R",
        "acon",
        "ter"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1,
        1389,
        3868,
        11,
        8753,
        13,
        14144,
        1095,
        757,
        1817,
        279,
        5150,
        1984,
        1578,
        13,
        330,
        20600,
        38087,
        162,
        229,
        113,
        863,
        220,
        57107,
        5232,
        49,
        23184,
        466
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "-m",
            "token_id": 1474,
            "logprob": -0.0713,
            "prob": 0.9297
          },
          {
            "token": "Ġmoi",
            "token_id": 41091,
            "logprob": -2.9531,
            "prob": 0.0525
          },
          {
            "token": "Ġune",
            "token_id": 6316,
            "logprob": -5.0625,
            "prob": 0.0063
          },
          {
            "token": "Ġun",
            "token_id": 653,
            "logprob": -6.25,
            "prob": 0.0019
          },
          {
            "token": "-me",
            "token_id": 35073,
            "logprob": -7.0625,
            "prob": 0.0009
          },
          {
            "token": "Ġmo",
            "token_id": 4647,
            "logprob": -7.3125,
            "prob": 0.0007
          },
          {
            "token": "Ġme",
            "token_id": 757,
            "logprob": -7.75,
            "prob": 0.0004
          },
          {
            "token": "ĠMo",
            "token_id": 6178,
            "logprob": -8.375,
            "prob": 0.0002
          },
          {
            "token": "-un",
            "token_id": 20486,
            "logprob": -8.5,
            "prob": 0.0002
          },
          {
            "token": "me",
            "token_id": 2727,
            "logprob": -8.625,
            "prob": 0.0002
          }
        ]
      },
      "selected_token": {
        "token": "ĠMo",
        "token_id": 6178,
        "selection_method": "sampling"
      }
    },
    {
      "step": 53,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\" – wait, French. Wait let me check the actual message again. \"用户：“懵” 问：Raconter Mo",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\"",
        "Ġ–",
        "Ġwait",
        ",",
        "ĠFrench",
        ".",
        "ĠWait",
        "Ġlet",
        "Ġme",
        "Ġcheck",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġagain",
        ".",
        "Ġ\"",
        "用户",
        "：“",
        "�",
        "�",
        "�",
        "”",
        "Ġ",
        "问",
        "：",
        "R",
        "acon",
        "ter",
        "ĠMo"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1,
        1389,
        3868,
        11,
        8753,
        13,
        14144,
        1095,
        757,
        1817,
        279,
        5150,
        1984,
        1578,
        13,
        330,
        20600,
        38087,
        162,
        229,
        113,
        863,
        220,
        57107,
        5232,
        49,
        23184,
        466,
        6178
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "i",
            "token_id": 72,
            "logprob": -0.0006,
            "prob": 1.0
          },
          {
            "token": "is",
            "token_id": 285,
            "logprob": -7.875,
            "prob": 0.0004
          },
          {
            "token": "I",
            "token_id": 40,
            "logprob": -11.25,
            "prob": 0.0
          },
          {
            "token": "ï",
            "token_id": 38672,
            "logprob": -11.375,
            "prob": 0.0
          },
          {
            "token": "Ġune",
            "token_id": 6316,
            "logprob": -11.375,
            "prob": 0.0
          },
          {
            "token": "ie",
            "token_id": 648,
            "logprob": -11.8125,
            "prob": 0.0
          },
          {
            "token": "ine",
            "token_id": 483,
            "logprob": -12.0,
            "prob": 0.0
          },
          {
            "token": "iy",
            "token_id": 16618,
            "logprob": -12.25,
            "prob": 0.0
          },
          {
            "token": "une",
            "token_id": 2957,
            "logprob": -12.25,
            "prob": 0.0
          },
          {
            "token": "ii",
            "token_id": 3893,
            "logprob": -12.5625,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "une",
        "token_id": 2957,
        "selection_method": "sampling"
      }
    },
    {
      "step": 54,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\" – wait, French. Wait let me check the actual message again. \"用户：“懵” 问：Raconter Moune",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\"",
        "Ġ–",
        "Ġwait",
        ",",
        "ĠFrench",
        ".",
        "ĠWait",
        "Ġlet",
        "Ġme",
        "Ġcheck",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġagain",
        ".",
        "Ġ\"",
        "用户",
        "：“",
        "�",
        "�",
        "�",
        "”",
        "Ġ",
        "问",
        "：",
        "R",
        "acon",
        "ter",
        "ĠMo",
        "une"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1,
        1389,
        3868,
        11,
        8753,
        13,
        14144,
        1095,
        757,
        1817,
        279,
        5150,
        1984,
        1578,
        13,
        330,
        20600,
        38087,
        162,
        229,
        113,
        863,
        220,
        57107,
        5232,
        49,
        23184,
        466,
        6178,
        2957
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġune",
            "token_id": 6316,
            "logprob": -0.8008,
            "prob": 0.4492
          },
          {
            "token": "Ġun",
            "token_id": 653,
            "logprob": -1.7969,
            "prob": 0.165
          },
          {
            "token": "Ġuna",
            "token_id": 5203,
            "logprob": -2.5469,
            "prob": 0.0781
          },
          {
            "token": "Ġbl",
            "token_id": 1529,
            "logprob": -2.6719,
            "prob": 0.0688
          },
          {
            "token": "Ġblonde",
            "token_id": 27117,
            "logprob": -3.4219,
            "prob": 0.0325
          },
          {
            "token": "ĠBl",
            "token_id": 2563,
            "logprob": -3.6094,
            "prob": 0.027
          },
          {
            "token": "ĠBlonde",
            "token_id": 53556,
            "logprob": -3.8594,
            "prob": 0.021
          },
          {
            "token": "Ġu",
            "token_id": 577,
            "logprob": -4.125,
            "prob": 0.0164
          },
          {
            "token": "bl",
            "token_id": 2067,
            "logprob": -4.5312,
            "prob": 0.0109
          },
          {
            "token": "ĠUn",
            "token_id": 1252,
            "logprob": -4.7188,
            "prob": 0.009
          }
        ]
      },
      "selected_token": {
        "token": "ĠUn",
        "token_id": 1252,
        "selection_method": "sampling"
      }
    },
    {
      "step": 55,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\" – wait, French. Wait let me check the actual message again. \"用户：“懵” 问：Raconter Moune Un",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\"",
        "Ġ–",
        "Ġwait",
        ",",
        "ĠFrench",
        ".",
        "ĠWait",
        "Ġlet",
        "Ġme",
        "Ġcheck",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġagain",
        ".",
        "Ġ\"",
        "用户",
        "：“",
        "�",
        "�",
        "�",
        "”",
        "Ġ",
        "问",
        "：",
        "R",
        "acon",
        "ter",
        "ĠMo",
        "une",
        "ĠUn"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1,
        1389,
        3868,
        11,
        8753,
        13,
        14144,
        1095,
        757,
        1817,
        279,
        5150,
        1984,
        1578,
        13,
        330,
        20600,
        38087,
        162,
        229,
        113,
        863,
        220,
        57107,
        5232,
        49,
        23184,
        466,
        6178,
        2957,
        1252
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "ĠBlonde",
            "token_id": 53556,
            "logprob": -1.0859,
            "prob": 0.3379
          },
          {
            "token": "ĠBl",
            "token_id": 2563,
            "logprob": -1.5859,
            "prob": 0.2051
          },
          {
            "token": "ne",
            "token_id": 818,
            "logprob": -2.5156,
            "prob": 0.0806
          },
          {
            "token": "Bl",
            "token_id": 5028,
            "logprob": -3.3281,
            "prob": 0.0356
          },
          {
            "token": "Ġblonde",
            "token_id": 27117,
            "logprob": -3.8906,
            "prob": 0.0204
          },
          {
            "token": "ĠBlake",
            "token_id": 31994,
            "logprob": -4.1562,
            "prob": 0.0159
          },
          {
            "token": "Ġbl",
            "token_id": 1529,
            "logprob": -4.2188,
            "prob": 0.0149
          },
          {
            "token": "ĠBlond",
            "token_id": 93098,
            "logprob": -4.4062,
            "prob": 0.0123
          },
          {
            "token": "ĠB",
            "token_id": 426,
            "logprob": -4.625,
            "prob": 0.0099
          },
          {
            "token": "Ġbleach",
            "token_id": 83451,
            "logprob": -4.6562,
            "prob": 0.0096
          }
        ]
      },
      "selected_token": {
        "token": "ĠBlonde",
        "token_id": 53556,
        "selection_method": "sampling"
      }
    },
    {
      "step": 56,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\" – wait, French. Wait let me check the actual message again. \"用户：“懵” 问：Raconter Moune Un Blonde",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\"",
        "Ġ–",
        "Ġwait",
        ",",
        "ĠFrench",
        ".",
        "ĠWait",
        "Ġlet",
        "Ġme",
        "Ġcheck",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġagain",
        ".",
        "Ġ\"",
        "用户",
        "：“",
        "�",
        "�",
        "�",
        "”",
        "Ġ",
        "问",
        "：",
        "R",
        "acon",
        "ter",
        "ĠMo",
        "une",
        "ĠUn",
        "ĠBlonde"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1,
        1389,
        3868,
        11,
        8753,
        13,
        14144,
        1095,
        757,
        1817,
        279,
        5150,
        1984,
        1578,
        13,
        330,
        20600,
        38087,
        162,
        229,
        113,
        863,
        220,
        57107,
        5232,
        49,
        23184,
        466,
        6178,
        2957,
        1252,
        53556
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "”",
            "token_id": 863,
            "logprob": -0.2578,
            "prob": 0.7734
          },
          {
            "token": "\"",
            "token_id": 1,
            "logprob": -1.8828,
            "prob": 0.1523
          },
          {
            "token": "\".",
            "token_id": 3343,
            "logprob": -4.375,
            "prob": 0.0125
          },
          {
            "token": "“",
            "token_id": 2118,
            "logprob": -4.8125,
            "prob": 0.0081
          },
          {
            "token": "?",
            "token_id": 30,
            "logprob": -4.9375,
            "prob": 0.0071
          },
          {
            "token": "?”",
            "token_id": 12671,
            "logprob": -5.25,
            "prob": 0.0052
          },
          {
            "token": "”\n\n",
            "token_id": 7663,
            "logprob": -5.5,
            "prob": 0.0041
          },
          {
            "token": "”.",
            "token_id": 11453,
            "logprob": -5.5625,
            "prob": 0.0038
          },
          {
            "token": "?\"",
            "token_id": 7673,
            "logprob": -5.5625,
            "prob": 0.0038
          },
          {
            "token": ".",
            "token_id": 13,
            "logprob": -5.8125,
            "prob": 0.003
          }
        ]
      },
      "selected_token": {
        "token": "\".",
        "token_id": 3343,
        "selection_method": "sampling"
      }
    },
    {
      "step": 57,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\" – wait, French. Wait let me check the actual message again. \"用户：“懵” 问：Raconter Moune Un Blonde\".",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\"",
        "Ġ–",
        "Ġwait",
        ",",
        "ĠFrench",
        ".",
        "ĠWait",
        "Ġlet",
        "Ġme",
        "Ġcheck",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġagain",
        ".",
        "Ġ\"",
        "用户",
        "：“",
        "�",
        "�",
        "�",
        "”",
        "Ġ",
        "问",
        "：",
        "R",
        "acon",
        "ter",
        "ĠMo",
        "une",
        "ĠUn",
        "ĠBlonde",
        "\"."
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1,
        1389,
        3868,
        11,
        8753,
        13,
        14144,
        1095,
        757,
        1817,
        279,
        5150,
        1984,
        1578,
        13,
        330,
        20600,
        38087,
        162,
        229,
        113,
        863,
        220,
        57107,
        5232,
        49,
        23184,
        466,
        6178,
        2957,
        1252,
        53556,
        3343
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "ĠWait",
            "token_id": 14144,
            "logprob": -0.1069,
            "prob": 0.8984
          },
          {
            "token": "ĠOh",
            "token_id": 8840,
            "logprob": -2.9844,
            "prob": 0.0508
          },
          {
            "token": "ĠHmm",
            "token_id": 89290,
            "logprob": -3.1094,
            "prob": 0.0447
          },
          {
            "token": "ĠNo",
            "token_id": 2360,
            "logprob": -5.8438,
            "prob": 0.0029
          },
          {
            "token": "ĠThe",
            "token_id": 578,
            "logprob": -6.8438,
            "prob": 0.0011
          },
          {
            "token": "ĠMaybe",
            "token_id": 10926,
            "logprob": -7.7188,
            "prob": 0.0004
          },
          {
            "token": "ĠLet",
            "token_id": 6914,
            "logprob": -8.5,
            "prob": 0.0002
          },
          {
            "token": "ĠTrans",
            "token_id": 4149,
            "logprob": -8.625,
            "prob": 0.0002
          },
          {
            "token": "ĠUser",
            "token_id": 2724,
            "logprob": -9.125,
            "prob": 0.0001
          },
          {
            "token": "ĠOops",
            "token_id": 96352,
            "logprob": -9.125,
            "prob": 0.0001
          }
        ]
      },
      "selected_token": {
        "token": "ĠWait",
        "token_id": 14144,
        "selection_method": "sampling"
      }
    },
    {
      "step": 58,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\" – wait, French. Wait let me check the actual message again. \"用户：“懵” 问：Raconter Moune Un Blonde\". Wait",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\"",
        "Ġ–",
        "Ġwait",
        ",",
        "ĠFrench",
        ".",
        "ĠWait",
        "Ġlet",
        "Ġme",
        "Ġcheck",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġagain",
        ".",
        "Ġ\"",
        "用户",
        "：“",
        "�",
        "�",
        "�",
        "”",
        "Ġ",
        "问",
        "：",
        "R",
        "acon",
        "ter",
        "ĠMo",
        "une",
        "ĠUn",
        "ĠBlonde",
        "\".",
        "ĠWait"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1,
        1389,
        3868,
        11,
        8753,
        13,
        14144,
        1095,
        757,
        1817,
        279,
        5150,
        1984,
        1578,
        13,
        330,
        20600,
        38087,
        162,
        229,
        113,
        863,
        220,
        57107,
        5232,
        49,
        23184,
        466,
        6178,
        2957,
        1252,
        53556,
        3343,
        14144
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": ",",
            "token_id": 11,
            "logprob": -0.5,
            "prob": 0.6055
          },
          {
            "token": "Ġno",
            "token_id": 912,
            "logprob": -1.125,
            "prob": 0.3242
          },
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -3.25,
            "prob": 0.0388
          },
          {
            "token": "Ġmaybe",
            "token_id": 7344,
            "logprob": -3.875,
            "prob": 0.0208
          },
          {
            "token": "Ġactually",
            "token_id": 3604,
            "logprob": -5.875,
            "prob": 0.0028
          },
          {
            "token": "Ġwait",
            "token_id": 3868,
            "logprob": -5.875,
            "prob": 0.0028
          },
          {
            "token": "Ġuser",
            "token_id": 1217,
            "logprob": -6.125,
            "prob": 0.0022
          },
          {
            "token": "Ġhold",
            "token_id": 3412,
            "logprob": -7.375,
            "prob": 0.0006
          },
          {
            "token": "Ġsorry",
            "token_id": 14931,
            "logprob": -8.875,
            "prob": 0.0001
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -9.625,
            "prob": 0.0001
          }
        ]
      },
      "selected_token": {
        "token": "Ġwait",
        "token_id": 3868,
        "selection_method": "sampling"
      }
    },
    {
      "step": 59,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\" – wait, French. Wait let me check the actual message again. \"用户：“懵” 问：Raconter Moune Un Blonde\". Wait wait",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\"",
        "Ġ–",
        "Ġwait",
        ",",
        "ĠFrench",
        ".",
        "ĠWait",
        "Ġlet",
        "Ġme",
        "Ġcheck",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġagain",
        ".",
        "Ġ\"",
        "用户",
        "：“",
        "�",
        "�",
        "�",
        "”",
        "Ġ",
        "问",
        "：",
        "R",
        "acon",
        "ter",
        "ĠMo",
        "une",
        "ĠUn",
        "ĠBlonde",
        "\".",
        "ĠWait",
        "Ġwait"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1,
        1389,
        3868,
        11,
        8753,
        13,
        14144,
        1095,
        757,
        1817,
        279,
        5150,
        1984,
        1578,
        13,
        330,
        20600,
        38087,
        162,
        229,
        113,
        863,
        220,
        57107,
        5232,
        49,
        23184,
        466,
        6178,
        2957,
        1252,
        53556,
        3343,
        14144,
        3868
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": ",",
            "token_id": 11,
            "logprob": -0.1602,
            "prob": 0.8516
          },
          {
            "token": "Ġno",
            "token_id": 912,
            "logprob": -2.1562,
            "prob": 0.1152
          },
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -3.7812,
            "prob": 0.0227
          },
          {
            "token": "Ġwait",
            "token_id": 3868,
            "logprob": -5.7812,
            "prob": 0.0031
          },
          {
            "token": ".",
            "token_id": 13,
            "logprob": -5.9062,
            "prob": 0.0027
          },
          {
            "token": "Ġuser",
            "token_id": 1217,
            "logprob": -6.1562,
            "prob": 0.0021
          },
          {
            "token": "Ġmaybe",
            "token_id": 7344,
            "logprob": -7.0312,
            "prob": 0.0009
          },
          {
            "token": "Ġactually",
            "token_id": 3604,
            "logprob": -7.9062,
            "prob": 0.0004
          },
          {
            "token": "Ġhold",
            "token_id": 3412,
            "logprob": -8.8125,
            "prob": 0.0002
          },
          {
            "token": "Ġsorry",
            "token_id": 14931,
            "logprob": -10.1875,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġuser",
        "token_id": 1217,
        "selection_method": "sampling"
      }
    },
    {
      "step": 60,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\" – wait, French. Wait let me check the actual message again. \"用户：“懵” 问：Raconter Moune Un Blonde\". Wait wait user",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\"",
        "Ġ–",
        "Ġwait",
        ",",
        "ĠFrench",
        ".",
        "ĠWait",
        "Ġlet",
        "Ġme",
        "Ġcheck",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġagain",
        ".",
        "Ġ\"",
        "用户",
        "：“",
        "�",
        "�",
        "�",
        "”",
        "Ġ",
        "问",
        "：",
        "R",
        "acon",
        "ter",
        "ĠMo",
        "une",
        "ĠUn",
        "ĠBlonde",
        "\".",
        "ĠWait",
        "Ġwait",
        "Ġuser"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1,
        1389,
        3868,
        11,
        8753,
        13,
        14144,
        1095,
        757,
        1817,
        279,
        5150,
        1984,
        1578,
        13,
        330,
        20600,
        38087,
        162,
        229,
        113,
        863,
        220,
        57107,
        5232,
        49,
        23184,
        466,
        6178,
        2957,
        1252,
        53556,
        3343,
        14144,
        3868,
        1217
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġwrote",
            "token_id": 6267,
            "logprob": -0.3672,
            "prob": 0.6914
          },
          {
            "token": "'s",
            "token_id": 596,
            "logprob": -1.9922,
            "prob": 0.1367
          },
          {
            "token": "Ġinput",
            "token_id": 1988,
            "logprob": -2.5,
            "prob": 0.0825
          },
          {
            "token": "Ġsaid",
            "token_id": 1071,
            "logprob": -3.75,
            "prob": 0.0237
          },
          {
            "token": "Ġis",
            "token_id": 374,
            "logprob": -4.375,
            "prob": 0.0127
          },
          {
            "token": "Ġjust",
            "token_id": 1120,
            "logprob": -4.625,
            "prob": 0.0099
          },
          {
            "token": "Ġmessage",
            "token_id": 1984,
            "logprob": -4.625,
            "prob": 0.0099
          },
          {
            "token": "Ġactually",
            "token_id": 3604,
            "logprob": -4.625,
            "prob": 0.0099
          },
          {
            "token": "Ġsays",
            "token_id": 2795,
            "logprob": -5.375,
            "prob": 0.0047
          },
          {
            "token": "Ġmight",
            "token_id": 2643,
            "logprob": -6.0,
            "prob": 0.0025
          }
        ]
      },
      "selected_token": {
        "token": "Ġjust",
        "token_id": 1120,
        "selection_method": "sampling"
      }
    },
    {
      "step": 61,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\" – wait, French. Wait let me check the actual message again. \"用户：“懵” 问：Raconter Moune Un Blonde\". Wait wait user just",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\"",
        "Ġ–",
        "Ġwait",
        ",",
        "ĠFrench",
        ".",
        "ĠWait",
        "Ġlet",
        "Ġme",
        "Ġcheck",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġagain",
        ".",
        "Ġ\"",
        "用户",
        "：“",
        "�",
        "�",
        "�",
        "”",
        "Ġ",
        "问",
        "：",
        "R",
        "acon",
        "ter",
        "ĠMo",
        "une",
        "ĠUn",
        "ĠBlonde",
        "\".",
        "ĠWait",
        "Ġwait",
        "Ġuser",
        "Ġjust"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1,
        1389,
        3868,
        11,
        8753,
        13,
        14144,
        1095,
        757,
        1817,
        279,
        5150,
        1984,
        1578,
        13,
        330,
        20600,
        38087,
        162,
        229,
        113,
        863,
        220,
        57107,
        5232,
        49,
        23184,
        466,
        6178,
        2957,
        1252,
        53556,
        3343,
        14144,
        3868,
        1217,
        1120
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġsaid",
            "token_id": 1071,
            "logprob": -0.625,
            "prob": 0.5352
          },
          {
            "token": "Ġwrote",
            "token_id": 6267,
            "logprob": -1.0,
            "prob": 0.3672
          },
          {
            "token": "Ġasked",
            "token_id": 4691,
            "logprob": -3.875,
            "prob": 0.0208
          },
          {
            "token": "Ġnow",
            "token_id": 1457,
            "logprob": -4.0,
            "prob": 0.0183
          },
          {
            "token": "Ġpast",
            "token_id": 3347,
            "logprob": -4.25,
            "prob": 0.0143
          },
          {
            "token": "Ġtyped",
            "token_id": 33069,
            "logprob": -4.25,
            "prob": 0.0143
          },
          {
            "token": "Ġsent",
            "token_id": 3288,
            "logprob": -4.5,
            "prob": 0.0111
          },
          {
            "token": "Ġsays",
            "token_id": 2795,
            "logprob": -4.625,
            "prob": 0.0098
          },
          {
            "token": "Ġposted",
            "token_id": 8621,
            "logprob": -6.125,
            "prob": 0.0022
          },
          {
            "token": "Ġinput",
            "token_id": 1988,
            "logprob": -6.3125,
            "prob": 0.0018
          }
        ]
      },
      "selected_token": {
        "token": "Ġasked",
        "token_id": 4691,
        "selection_method": "sampling"
      }
    },
    {
      "step": 62,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\" – wait, French. Wait let me check the actual message again. \"用户：“懵” 问：Raconter Moune Un Blonde\". Wait wait user just asked",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\"",
        "Ġ–",
        "Ġwait",
        ",",
        "ĠFrench",
        ".",
        "ĠWait",
        "Ġlet",
        "Ġme",
        "Ġcheck",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġagain",
        ".",
        "Ġ\"",
        "用户",
        "：“",
        "�",
        "�",
        "�",
        "”",
        "Ġ",
        "问",
        "：",
        "R",
        "acon",
        "ter",
        "ĠMo",
        "une",
        "ĠUn",
        "ĠBlonde",
        "\".",
        "ĠWait",
        "Ġwait",
        "Ġuser",
        "Ġjust",
        "Ġasked"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1,
        1389,
        3868,
        11,
        8753,
        13,
        14144,
        1095,
        757,
        1817,
        279,
        5150,
        1984,
        1578,
        13,
        330,
        20600,
        38087,
        162,
        229,
        113,
        863,
        220,
        57107,
        5232,
        49,
        23184,
        466,
        6178,
        2957,
        1252,
        53556,
        3343,
        14144,
        3868,
        1217,
        1120,
        4691
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġ\"",
            "token_id": 330,
            "logprob": -0.4277,
            "prob": 0.6523
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -1.3047,
            "prob": 0.2715
          },
          {
            "token": "Ġme",
            "token_id": 757,
            "logprob": -3.4219,
            "prob": 0.0325
          },
          {
            "token": "Ġfor",
            "token_id": 369,
            "logprob": -3.6719,
            "prob": 0.0253
          },
          {
            "token": ":",
            "token_id": 25,
            "logprob": -4.5625,
            "prob": 0.0106
          },
          {
            "token": "Ġto",
            "token_id": 311,
            "logprob": -5.5625,
            "prob": 0.0039
          },
          {
            "token": "Ġ“",
            "token_id": 1054,
            "logprob": -5.6875,
            "prob": 0.0034
          },
          {
            "token": ",",
            "token_id": 11,
            "logprob": -7.25,
            "prob": 0.0007
          },
          {
            "token": "Ġa",
            "token_id": 264,
            "logprob": -9.1875,
            "prob": 0.0001
          },
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -9.25,
            "prob": 0.0001
          }
        ]
      },
      "selected_token": {
        "token": "Ġme",
        "token_id": 757,
        "selection_method": "sampling"
      }
    },
    {
      "step": 63,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\" – wait, French. Wait let me check the actual message again. \"用户：“懵” 问：Raconter Moune Un Blonde\". Wait wait user just asked me",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\"",
        "Ġ–",
        "Ġwait",
        ",",
        "ĠFrench",
        ".",
        "ĠWait",
        "Ġlet",
        "Ġme",
        "Ġcheck",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġagain",
        ".",
        "Ġ\"",
        "用户",
        "：“",
        "�",
        "�",
        "�",
        "”",
        "Ġ",
        "问",
        "：",
        "R",
        "acon",
        "ter",
        "ĠMo",
        "une",
        "ĠUn",
        "ĠBlonde",
        "\".",
        "ĠWait",
        "Ġwait",
        "Ġuser",
        "Ġjust",
        "Ġasked",
        "Ġme"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1,
        1389,
        3868,
        11,
        8753,
        13,
        14144,
        1095,
        757,
        1817,
        279,
        5150,
        1984,
        1578,
        13,
        330,
        20600,
        38087,
        162,
        229,
        113,
        863,
        220,
        57107,
        5232,
        49,
        23184,
        466,
        6178,
        2957,
        1252,
        53556,
        3343,
        14144,
        3868,
        1217,
        1120,
        4691,
        757
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġto",
            "token_id": 311,
            "logprob": -0.5234,
            "prob": 0.5938
          },
          {
            "token": "Ġ\"",
            "token_id": 330,
            "logprob": -1.7734,
            "prob": 0.1699
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -1.7734,
            "prob": 0.1699
          },
          {
            "token": ":",
            "token_id": 25,
            "logprob": -3.5156,
            "prob": 0.0295
          },
          {
            "token": ",",
            "token_id": 11,
            "logprob": -3.5781,
            "prob": 0.0277
          },
          {
            "token": "Ġa",
            "token_id": 264,
            "logprob": -5.2188,
            "prob": 0.0055
          },
          {
            "token": "Ġfor",
            "token_id": 369,
            "logprob": -6.9062,
            "prob": 0.001
          },
          {
            "token": "Ġsomething",
            "token_id": 2555,
            "logprob": -6.9688,
            "prob": 0.0009
          },
          {
            "token": "Ġ“",
            "token_id": 1054,
            "logprob": -7.3438,
            "prob": 0.0007
          },
          {
            "token": "Ġ(",
            "token_id": 320,
            "logprob": -9.3125,
            "prob": 0.0001
          }
        ]
      },
      "selected_token": {
        "token": ",",
        "token_id": 11,
        "selection_method": "sampling"
      }
    },
    {
      "step": 64,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\" – wait, French. Wait let me check the actual message again. \"用户：“懵” 问：Raconter Moune Un Blonde\". Wait wait user just asked me,",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\"",
        "Ġ–",
        "Ġwait",
        ",",
        "ĠFrench",
        ".",
        "ĠWait",
        "Ġlet",
        "Ġme",
        "Ġcheck",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġagain",
        ".",
        "Ġ\"",
        "用户",
        "：“",
        "�",
        "�",
        "�",
        "”",
        "Ġ",
        "问",
        "：",
        "R",
        "acon",
        "ter",
        "ĠMo",
        "une",
        "ĠUn",
        "ĠBlonde",
        "\".",
        "ĠWait",
        "Ġwait",
        "Ġuser",
        "Ġjust",
        "Ġasked",
        "Ġme",
        ","
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1,
        1389,
        3868,
        11,
        8753,
        13,
        14144,
        1095,
        757,
        1817,
        279,
        5150,
        1984,
        1578,
        13,
        330,
        20600,
        38087,
        162,
        229,
        113,
        863,
        220,
        57107,
        5232,
        49,
        23184,
        466,
        6178,
        2957,
        1252,
        53556,
        3343,
        14144,
        3868,
        1217,
        1120,
        4691,
        757,
        11
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -0.0325,
            "prob": 0.9688
          },
          {
            "token": "Ġ\"",
            "token_id": 330,
            "logprob": -3.9062,
            "prob": 0.0201
          },
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -5.7812,
            "prob": 0.0031
          },
          {
            "token": "Ġafter",
            "token_id": 1306,
            "logprob": -6.0312,
            "prob": 0.0024
          },
          {
            "token": "Ġusing",
            "token_id": 1701,
            "logprob": -6.4062,
            "prob": 0.0016
          },
          {
            "token": "Ġvia",
            "token_id": 4669,
            "logprob": -6.8438,
            "prob": 0.0011
          },
          {
            "token": "Ġsince",
            "token_id": 2533,
            "logprob": -7.2812,
            "prob": 0.0007
          },
          {
            "token": "Ġthrough",
            "token_id": 1555,
            "logprob": -7.5312,
            "prob": 0.0005
          },
          {
            "token": "Ġbut",
            "token_id": 719,
            "logprob": -8.0625,
            "prob": 0.0003
          },
          {
            "token": "Ġas",
            "token_id": 439,
            "logprob": -8.3125,
            "prob": 0.0003
          }
        ]
      },
      "selected_token": {
        "token": "Ġvia",
        "token_id": 4669,
        "selection_method": "sampling"
      }
    },
    {
      "step": 65,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\" – wait, French. Wait let me check the actual message again. \"用户：“懵” 问：Raconter Moune Un Blonde\". Wait wait user just asked me, via",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\"",
        "Ġ–",
        "Ġwait",
        ",",
        "ĠFrench",
        ".",
        "ĠWait",
        "Ġlet",
        "Ġme",
        "Ġcheck",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġagain",
        ".",
        "Ġ\"",
        "用户",
        "：“",
        "�",
        "�",
        "�",
        "”",
        "Ġ",
        "问",
        "：",
        "R",
        "acon",
        "ter",
        "ĠMo",
        "une",
        "ĠUn",
        "ĠBlonde",
        "\".",
        "ĠWait",
        "Ġwait",
        "Ġuser",
        "Ġjust",
        "Ġasked",
        "Ġme",
        ",",
        "Ġvia"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1,
        1389,
        3868,
        11,
        8753,
        13,
        14144,
        1095,
        757,
        1817,
        279,
        5150,
        1984,
        1578,
        13,
        330,
        20600,
        38087,
        162,
        229,
        113,
        863,
        220,
        57107,
        5232,
        49,
        23184,
        466,
        6178,
        2957,
        1252,
        53556,
        3343,
        14144,
        3868,
        1217,
        1120,
        4691,
        757,
        11,
        4669
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -0.4688,
            "prob": 0.625
          },
          {
            "token": "ĠFrench",
            "token_id": 8753,
            "logprob": -2.0312,
            "prob": 0.1309
          },
          {
            "token": "ĠChinese",
            "token_id": 8620,
            "logprob": -2.0312,
            "prob": 0.1309
          },
          {
            "token": "Ġmy",
            "token_id": 856,
            "logprob": -3.2812,
            "prob": 0.0376
          },
          {
            "token": "Ġtranslation",
            "token_id": 14807,
            "logprob": -3.6562,
            "prob": 0.0259
          },
          {
            "token": "ĠEnglish",
            "token_id": 6498,
            "logprob": -4.8438,
            "prob": 0.0079
          },
          {
            "token": "Ġa",
            "token_id": 264,
            "logprob": -4.9688,
            "prob": 0.007
          },
          {
            "token": "Ġtranslate",
            "token_id": 15025,
            "logprob": -5.0938,
            "prob": 0.0061
          },
          {
            "token": "Ġthis",
            "token_id": 420,
            "logprob": -5.7188,
            "prob": 0.0033
          },
          {
            "token": "Ġchat",
            "token_id": 6369,
            "logprob": -6.0312,
            "prob": 0.0024
          }
        ]
      },
      "selected_token": {
        "token": "Ġmy",
        "token_id": 856,
        "selection_method": "sampling"
      }
    },
    {
      "step": 66,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\" – wait, French. Wait let me check the actual message again. \"用户：“懵” 问：Raconter Moune Un Blonde\". Wait wait user just asked me, via my",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\"",
        "Ġ–",
        "Ġwait",
        ",",
        "ĠFrench",
        ".",
        "ĠWait",
        "Ġlet",
        "Ġme",
        "Ġcheck",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġagain",
        ".",
        "Ġ\"",
        "用户",
        "：“",
        "�",
        "�",
        "�",
        "”",
        "Ġ",
        "问",
        "：",
        "R",
        "acon",
        "ter",
        "ĠMo",
        "une",
        "ĠUn",
        "ĠBlonde",
        "\".",
        "ĠWait",
        "Ġwait",
        "Ġuser",
        "Ġjust",
        "Ġasked",
        "Ġme",
        ",",
        "Ġvia",
        "Ġmy"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1,
        1389,
        3868,
        11,
        8753,
        13,
        14144,
        1095,
        757,
        1817,
        279,
        5150,
        1984,
        1578,
        13,
        330,
        20600,
        38087,
        162,
        229,
        113,
        863,
        220,
        57107,
        5232,
        49,
        23184,
        466,
        6178,
        2957,
        1252,
        53556,
        3343,
        14144,
        3868,
        1217,
        1120,
        4691,
        757,
        11,
        4669,
        856
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġinterface",
            "token_id": 3834,
            "logprob": -1.6719,
            "prob": 0.1875
          },
          {
            "token": "Ġprompt",
            "token_id": 10137,
            "logprob": -1.9844,
            "prob": 0.1367
          },
          {
            "token": "Ġsystem",
            "token_id": 1887,
            "logprob": -2.2969,
            "prob": 0.1006
          },
          {
            "token": "Ġprevious",
            "token_id": 3766,
            "logprob": -2.3594,
            "prob": 0.0942
          },
          {
            "token": "Ġinitial",
            "token_id": 2926,
            "logprob": -2.5469,
            "prob": 0.0781
          },
          {
            "token": "Ġinput",
            "token_id": 1988,
            "logprob": -2.6094,
            "prob": 0.0732
          },
          {
            "token": "ĠFrench",
            "token_id": 8753,
            "logprob": -3.1094,
            "prob": 0.0444
          },
          {
            "token": "Ġthoughts",
            "token_id": 11555,
            "logprob": -3.5469,
            "prob": 0.0288
          },
          {
            "token": "Ġhistory",
            "token_id": 3925,
            "logprob": -3.5469,
            "prob": 0.0288
          },
          {
            "token": "ĠChinese",
            "token_id": 8620,
            "logprob": -3.7344,
            "prob": 0.0238
          }
        ]
      },
      "selected_token": {
        "token": "ĠFrench",
        "token_id": 8753,
        "selection_method": "sampling"
      }
    },
    {
      "step": 67,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\" – wait, French. Wait let me check the actual message again. \"用户：“懵” 问：Raconter Moune Un Blonde\". Wait wait user just asked me, via my French",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\"",
        "Ġ–",
        "Ġwait",
        ",",
        "ĠFrench",
        ".",
        "ĠWait",
        "Ġlet",
        "Ġme",
        "Ġcheck",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġagain",
        ".",
        "Ġ\"",
        "用户",
        "：“",
        "�",
        "�",
        "�",
        "”",
        "Ġ",
        "问",
        "：",
        "R",
        "acon",
        "ter",
        "ĠMo",
        "une",
        "ĠUn",
        "ĠBlonde",
        "\".",
        "ĠWait",
        "Ġwait",
        "Ġuser",
        "Ġjust",
        "Ġasked",
        "Ġme",
        ",",
        "Ġvia",
        "Ġmy",
        "ĠFrench"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1,
        1389,
        3868,
        11,
        8753,
        13,
        14144,
        1095,
        757,
        1817,
        279,
        5150,
        1984,
        1578,
        13,
        330,
        20600,
        38087,
        162,
        229,
        113,
        863,
        220,
        57107,
        5232,
        49,
        23184,
        466,
        6178,
        2957,
        1252,
        53556,
        3343,
        14144,
        3868,
        1217,
        1120,
        4691,
        757,
        11,
        4669,
        856,
        8753
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "?",
            "token_id": 30,
            "logprob": -1.9688,
            "prob": 0.1396
          },
          {
            "token": "Ġinterface",
            "token_id": 3834,
            "logprob": -2.0312,
            "prob": 0.1309
          },
          {
            "token": "Ġinstruction",
            "token_id": 7754,
            "logprob": -2.4688,
            "prob": 0.0845
          },
          {
            "token": "Ġinput",
            "token_id": 1988,
            "logprob": -2.5938,
            "prob": 0.0747
          },
          {
            "token": "Ġprompt",
            "token_id": 10137,
            "logprob": -2.8438,
            "prob": 0.0581
          },
          {
            "token": ",",
            "token_id": 11,
            "logprob": -2.9688,
            "prob": 0.0513
          },
          {
            "token": "Ġinstructions",
            "token_id": 11470,
            "logprob": -3.3438,
            "prob": 0.0352
          },
          {
            "token": "Ġrecognition",
            "token_id": 18324,
            "logprob": -3.6562,
            "prob": 0.0258
          },
          {
            "token": "Ġtranslation",
            "token_id": 14807,
            "logprob": -3.7188,
            "prob": 0.0243
          },
          {
            "token": ":",
            "token_id": 25,
            "logprob": -3.7812,
            "prob": 0.0228
          }
        ]
      },
      "selected_token": {
        "token": "Ġinstructions",
        "token_id": 11470,
        "selection_method": "sampling"
      }
    },
    {
      "step": 68,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\" – wait, French. Wait let me check the actual message again. \"用户：“懵” 问：Raconter Moune Un Blonde\". Wait wait user just asked me, via my French instructions",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\"",
        "Ġ–",
        "Ġwait",
        ",",
        "ĠFrench",
        ".",
        "ĠWait",
        "Ġlet",
        "Ġme",
        "Ġcheck",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġagain",
        ".",
        "Ġ\"",
        "用户",
        "：“",
        "�",
        "�",
        "�",
        "”",
        "Ġ",
        "问",
        "：",
        "R",
        "acon",
        "ter",
        "ĠMo",
        "une",
        "ĠUn",
        "ĠBlonde",
        "\".",
        "ĠWait",
        "Ġwait",
        "Ġuser",
        "Ġjust",
        "Ġasked",
        "Ġme",
        ",",
        "Ġvia",
        "Ġmy",
        "ĠFrench",
        "Ġinstructions"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1,
        1389,
        3868,
        11,
        8753,
        13,
        14144,
        1095,
        757,
        1817,
        279,
        5150,
        1984,
        1578,
        13,
        330,
        20600,
        38087,
        162,
        229,
        113,
        863,
        220,
        57107,
        5232,
        49,
        23184,
        466,
        6178,
        2957,
        1252,
        53556,
        3343,
        14144,
        3868,
        1217,
        1120,
        4691,
        757,
        11,
        4669,
        856,
        8753,
        11470
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": ",",
            "token_id": 11,
            "logprob": -0.8164,
            "prob": 0.4434
          },
          {
            "token": "?",
            "token_id": 30,
            "logprob": -0.9414,
            "prob": 0.3906
          },
          {
            "token": ":",
            "token_id": 25,
            "logprob": -2.4375,
            "prob": 0.0874
          },
          {
            "token": "Ġmaybe",
            "token_id": 7344,
            "logprob": -3.6875,
            "prob": 0.025
          },
          {
            "token": "Ġto",
            "token_id": 311,
            "logprob": -4.3125,
            "prob": 0.0134
          },
          {
            "token": ".",
            "token_id": 13,
            "logprob": -4.4375,
            "prob": 0.0118
          },
          {
            "token": "Ġperhaps",
            "token_id": 8530,
            "logprob": -4.6875,
            "prob": 0.0092
          },
          {
            "token": "Ġhere",
            "token_id": 1618,
            "logprob": -5.5625,
            "prob": 0.0038
          },
          {
            "token": "?\n\n",
            "token_id": 1980,
            "logprob": -5.8125,
            "prob": 0.003
          },
          {
            "token": "Ġprobably",
            "token_id": 4762,
            "logprob": -5.8125,
            "prob": 0.003
          }
        ]
      },
      "selected_token": {
        "token": ",",
        "token_id": 11,
        "selection_method": "sampling"
      }
    },
    {
      "step": 69,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\" – wait, French. Wait let me check the actual message again. \"用户：“懵” 问：Raconter Moune Un Blonde\". Wait wait user just asked me, via my French instructions,",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\"",
        "Ġ–",
        "Ġwait",
        ",",
        "ĠFrench",
        ".",
        "ĠWait",
        "Ġlet",
        "Ġme",
        "Ġcheck",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġagain",
        ".",
        "Ġ\"",
        "用户",
        "：“",
        "�",
        "�",
        "�",
        "”",
        "Ġ",
        "问",
        "：",
        "R",
        "acon",
        "ter",
        "ĠMo",
        "une",
        "ĠUn",
        "ĠBlonde",
        "\".",
        "ĠWait",
        "Ġwait",
        "Ġuser",
        "Ġjust",
        "Ġasked",
        "Ġme",
        ",",
        "Ġvia",
        "Ġmy",
        "ĠFrench",
        "Ġinstructions",
        ","
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1,
        1389,
        3868,
        11,
        8753,
        13,
        14144,
        1095,
        757,
        1817,
        279,
        5150,
        1984,
        1578,
        13,
        330,
        20600,
        38087,
        162,
        229,
        113,
        863,
        220,
        57107,
        5232,
        49,
        23184,
        466,
        6178,
        2957,
        1252,
        53556,
        3343,
        14144,
        3868,
        1217,
        1120,
        4691,
        757,
        11,
        4669,
        856,
        8753,
        11470,
        11
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġto",
            "token_id": 311,
            "logprob": -0.3984,
            "prob": 0.6719
          },
          {
            "token": "Ġ\"",
            "token_id": 330,
            "logprob": -1.2734,
            "prob": 0.2793
          },
          {
            "token": "Ġmaybe",
            "token_id": 7344,
            "logprob": -4.2812,
            "prob": 0.014
          },
          {
            "token": "Ġbut",
            "token_id": 719,
            "logprob": -4.5312,
            "prob": 0.0109
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -5.4062,
            "prob": 0.0045
          },
          {
            "token": "Ġbecause",
            "token_id": 1606,
            "logprob": -5.4062,
            "prob": 0.0045
          },
          {
            "token": "Ġperhaps",
            "token_id": 8530,
            "logprob": -5.7812,
            "prob": 0.0031
          },
          {
            "token": "Ġsince",
            "token_id": 2533,
            "logprob": -5.9062,
            "prob": 0.0027
          },
          {
            "token": "Ġright",
            "token_id": 1314,
            "logprob": -6.1562,
            "prob": 0.0021
          },
          {
            "token": "Ġprobably",
            "token_id": 4762,
            "logprob": -6.1562,
            "prob": 0.0021
          }
        ]
      },
      "selected_token": {
        "token": "Ġbut",
        "token_id": 719,
        "selection_method": "sampling"
      }
    },
    {
      "step": 70,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\" – wait, French. Wait let me check the actual message again. \"用户：“懵” 问：Raconter Moune Un Blonde\". Wait wait user just asked me, via my French instructions, but",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\"",
        "Ġ–",
        "Ġwait",
        ",",
        "ĠFrench",
        ".",
        "ĠWait",
        "Ġlet",
        "Ġme",
        "Ġcheck",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġagain",
        ".",
        "Ġ\"",
        "用户",
        "：“",
        "�",
        "�",
        "�",
        "”",
        "Ġ",
        "问",
        "：",
        "R",
        "acon",
        "ter",
        "ĠMo",
        "une",
        "ĠUn",
        "ĠBlonde",
        "\".",
        "ĠWait",
        "Ġwait",
        "Ġuser",
        "Ġjust",
        "Ġasked",
        "Ġme",
        ",",
        "Ġvia",
        "Ġmy",
        "ĠFrench",
        "Ġinstructions",
        ",",
        "Ġbut"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1,
        1389,
        3868,
        11,
        8753,
        13,
        14144,
        1095,
        757,
        1817,
        279,
        5150,
        1984,
        1578,
        13,
        330,
        20600,
        38087,
        162,
        229,
        113,
        863,
        220,
        57107,
        5232,
        49,
        23184,
        466,
        6178,
        2957,
        1252,
        53556,
        3343,
        14144,
        3868,
        1217,
        1120,
        4691,
        757,
        11,
        4669,
        856,
        8753,
        11470,
        11,
        719
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -0.7852,
            "prob": 0.4551
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -1.7891,
            "prob": 0.167
          },
          {
            "token": "Ġmaybe",
            "token_id": 7344,
            "logprob": -1.9141,
            "prob": 0.1475
          },
          {
            "token": "Ġactually",
            "token_id": 3604,
            "logprob": -2.6562,
            "prob": 0.0698
          },
          {
            "token": "Ġlooking",
            "token_id": 3411,
            "logprob": -3.0312,
            "prob": 0.0481
          },
          {
            "token": "Ġlet",
            "token_id": 1095,
            "logprob": -3.9062,
            "prob": 0.02
          },
          {
            "token": "Ġmy",
            "token_id": 856,
            "logprob": -4.0312,
            "prob": 0.0177
          },
          {
            "token": "Ġuser",
            "token_id": 1217,
            "logprob": -4.1562,
            "prob": 0.0156
          },
          {
            "token": "Ġwait",
            "token_id": 3868,
            "logprob": -4.4062,
            "prob": 0.0121
          },
          {
            "token": "Ġno",
            "token_id": 912,
            "logprob": -4.5312,
            "prob": 0.0107
          }
        ]
      },
      "selected_token": {
        "token": "Ġthe",
        "token_id": 279,
        "selection_method": "sampling"
      }
    },
    {
      "step": 71,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\" – wait, French. Wait let me check the actual message again. \"用户：“懵” 问：Raconter Moune Un Blonde\". Wait wait user just asked me, via my French instructions, but the",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\"",
        "Ġ–",
        "Ġwait",
        ",",
        "ĠFrench",
        ".",
        "ĠWait",
        "Ġlet",
        "Ġme",
        "Ġcheck",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġagain",
        ".",
        "Ġ\"",
        "用户",
        "：“",
        "�",
        "�",
        "�",
        "”",
        "Ġ",
        "问",
        "：",
        "R",
        "acon",
        "ter",
        "ĠMo",
        "une",
        "ĠUn",
        "ĠBlonde",
        "\".",
        "ĠWait",
        "Ġwait",
        "Ġuser",
        "Ġjust",
        "Ġasked",
        "Ġme",
        ",",
        "Ġvia",
        "Ġmy",
        "ĠFrench",
        "Ġinstructions",
        ",",
        "Ġbut",
        "Ġthe"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1,
        1389,
        3868,
        11,
        8753,
        13,
        14144,
        1095,
        757,
        1817,
        279,
        5150,
        1984,
        1578,
        13,
        330,
        20600,
        38087,
        162,
        229,
        113,
        863,
        220,
        57107,
        5232,
        49,
        23184,
        466,
        6178,
        2957,
        1252,
        53556,
        3343,
        14144,
        3868,
        1217,
        1120,
        4691,
        757,
        11,
        4669,
        856,
        8753,
        11470,
        11,
        719,
        279
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġuser",
            "token_id": 1217,
            "logprob": -0.2422,
            "prob": 0.7852
          },
          {
            "token": "Ġactual",
            "token_id": 5150,
            "logprob": -2.2344,
            "prob": 0.1064
          },
          {
            "token": "Ġquery",
            "token_id": 3319,
            "logprob": -3.3594,
            "prob": 0.0344
          },
          {
            "token": "Ġmessage",
            "token_id": 1984,
            "logprob": -3.7344,
            "prob": 0.0237
          },
          {
            "token": "Ġinitial",
            "token_id": 2926,
            "logprob": -4.5,
            "prob": 0.0112
          },
          {
            "token": "Ġinput",
            "token_id": 1988,
            "logprob": -4.5,
            "prob": 0.0112
          },
          {
            "token": "Ġexact",
            "token_id": 4839,
            "logprob": -4.5,
            "prob": 0.0112
          },
          {
            "token": "Ġsystem",
            "token_id": 1887,
            "logprob": -5.125,
            "prob": 0.006
          },
          {
            "token": "Ġoriginal",
            "token_id": 4113,
            "logprob": -5.75,
            "prob": 0.0032
          },
          {
            "token": "Ġprompt",
            "token_id": 10137,
            "logprob": -6.875,
            "prob": 0.001
          }
        ]
      },
      "selected_token": {
        "token": "Ġactual",
        "token_id": 5150,
        "selection_method": "sampling"
      }
    },
    {
      "step": 72,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\" – wait, French. Wait let me check the actual message again. \"用户：“懵” 问：Raconter Moune Un Blonde\". Wait wait user just asked me, via my French instructions, but the actual",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\"",
        "Ġ–",
        "Ġwait",
        ",",
        "ĠFrench",
        ".",
        "ĠWait",
        "Ġlet",
        "Ġme",
        "Ġcheck",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġagain",
        ".",
        "Ġ\"",
        "用户",
        "：“",
        "�",
        "�",
        "�",
        "”",
        "Ġ",
        "问",
        "：",
        "R",
        "acon",
        "ter",
        "ĠMo",
        "une",
        "ĠUn",
        "ĠBlonde",
        "\".",
        "ĠWait",
        "Ġwait",
        "Ġuser",
        "Ġjust",
        "Ġasked",
        "Ġme",
        ",",
        "Ġvia",
        "Ġmy",
        "ĠFrench",
        "Ġinstructions",
        ",",
        "Ġbut",
        "Ġthe",
        "Ġactual"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1,
        1389,
        3868,
        11,
        8753,
        13,
        14144,
        1095,
        757,
        1817,
        279,
        5150,
        1984,
        1578,
        13,
        330,
        20600,
        38087,
        162,
        229,
        113,
        863,
        220,
        57107,
        5232,
        49,
        23184,
        466,
        6178,
        2957,
        1252,
        53556,
        3343,
        14144,
        3868,
        1217,
        1120,
        4691,
        757,
        11,
        4669,
        856,
        8753,
        11470,
        11,
        719,
        279,
        5150
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġuser",
            "token_id": 1217,
            "logprob": -0.9961,
            "prob": 0.3691
          },
          {
            "token": "Ġmessage",
            "token_id": 1984,
            "logprob": -1.4922,
            "prob": 0.2246
          },
          {
            "token": "Ġquery",
            "token_id": 3319,
            "logprob": -1.7422,
            "prob": 0.1748
          },
          {
            "token": "Ġinput",
            "token_id": 1988,
            "logprob": -2.25,
            "prob": 0.106
          },
          {
            "token": "Ġquestion",
            "token_id": 3488,
            "logprob": -2.625,
            "prob": 0.0728
          },
          {
            "token": "Ġprompt",
            "token_id": 10137,
            "logprob": -4.25,
            "prob": 0.0143
          },
          {
            "token": "Ġinstruction",
            "token_id": 7754,
            "logprob": -4.5,
            "prob": 0.0112
          },
          {
            "token": "Ġcontent",
            "token_id": 2262,
            "logprob": -4.625,
            "prob": 0.0099
          },
          {
            "token": "Ġtext",
            "token_id": 1495,
            "logprob": -5.0,
            "prob": 0.0068
          },
          {
            "token": "Ġrequest",
            "token_id": 1715,
            "logprob": -5.5,
            "prob": 0.0041
          }
        ]
      },
      "selected_token": {
        "token": "Ġmessage",
        "token_id": 1984,
        "selection_method": "sampling"
      }
    },
    {
      "step": 73,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\" – wait, French. Wait let me check the actual message again. \"用户：“懵” 问：Raconter Moune Un Blonde\". Wait wait user just asked me, via my French instructions, but the actual message",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\"",
        "Ġ–",
        "Ġwait",
        ",",
        "ĠFrench",
        ".",
        "ĠWait",
        "Ġlet",
        "Ġme",
        "Ġcheck",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġagain",
        ".",
        "Ġ\"",
        "用户",
        "：“",
        "�",
        "�",
        "�",
        "”",
        "Ġ",
        "问",
        "：",
        "R",
        "acon",
        "ter",
        "ĠMo",
        "une",
        "ĠUn",
        "ĠBlonde",
        "\".",
        "ĠWait",
        "Ġwait",
        "Ġuser",
        "Ġjust",
        "Ġasked",
        "Ġme",
        ",",
        "Ġvia",
        "Ġmy",
        "ĠFrench",
        "Ġinstructions",
        ",",
        "Ġbut",
        "Ġthe",
        "Ġactual",
        "Ġmessage"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1,
        1389,
        3868,
        11,
        8753,
        13,
        14144,
        1095,
        757,
        1817,
        279,
        5150,
        1984,
        1578,
        13,
        330,
        20600,
        38087,
        162,
        229,
        113,
        863,
        220,
        57107,
        5232,
        49,
        23184,
        466,
        6178,
        2957,
        1252,
        53556,
        3343,
        14144,
        3868,
        1217,
        1120,
        4691,
        757,
        11,
        4669,
        856,
        8753,
        11470,
        11,
        719,
        279,
        5150,
        1984
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġis",
            "token_id": 374,
            "logprob": -1.1797,
            "prob": 0.3066
          },
          {
            "token": "Ġhere",
            "token_id": 1618,
            "logprob": -1.9297,
            "prob": 0.1455
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -1.9297,
            "prob": 0.1455
          },
          {
            "token": "Ġwas",
            "token_id": 574,
            "logprob": -2.2969,
            "prob": 0.0996
          },
          {
            "token": "Ġmight",
            "token_id": 2643,
            "logprob": -2.5469,
            "prob": 0.0776
          },
          {
            "token": "?",
            "token_id": 30,
            "logprob": -2.9219,
            "prob": 0.0535
          },
          {
            "token": "Ġfrom",
            "token_id": 505,
            "logprob": -3.0469,
            "prob": 0.0471
          },
          {
            "token": ":",
            "token_id": 25,
            "logprob": -3.7969,
            "prob": 0.0222
          },
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -3.9219,
            "prob": 0.0197
          },
          {
            "token": "Ġthey",
            "token_id": 814,
            "logprob": -3.9219,
            "prob": 0.0197
          }
        ]
      },
      "selected_token": {
        "token": "Ġthey",
        "token_id": 814,
        "selection_method": "sampling"
      }
    },
    {
      "step": 74,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\" – wait, French. Wait let me check the actual message again. \"用户：“懵” 问：Raconter Moune Un Blonde\". Wait wait user just asked me, via my French instructions, but the actual message they",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\"",
        "Ġ–",
        "Ġwait",
        ",",
        "ĠFrench",
        ".",
        "ĠWait",
        "Ġlet",
        "Ġme",
        "Ġcheck",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġagain",
        ".",
        "Ġ\"",
        "用户",
        "：“",
        "�",
        "�",
        "�",
        "”",
        "Ġ",
        "问",
        "：",
        "R",
        "acon",
        "ter",
        "ĠMo",
        "une",
        "ĠUn",
        "ĠBlonde",
        "\".",
        "ĠWait",
        "Ġwait",
        "Ġuser",
        "Ġjust",
        "Ġasked",
        "Ġme",
        ",",
        "Ġvia",
        "Ġmy",
        "ĠFrench",
        "Ġinstructions",
        ",",
        "Ġbut",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġthey"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1,
        1389,
        3868,
        11,
        8753,
        13,
        14144,
        1095,
        757,
        1817,
        279,
        5150,
        1984,
        1578,
        13,
        330,
        20600,
        38087,
        162,
        229,
        113,
        863,
        220,
        57107,
        5232,
        49,
        23184,
        466,
        6178,
        2957,
        1252,
        53556,
        3343,
        14144,
        3868,
        1217,
        1120,
        4691,
        757,
        11,
        4669,
        856,
        8753,
        11470,
        11,
        719,
        279,
        5150,
        1984,
        814
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġwrote",
            "token_id": 6267,
            "logprob": -0.7656,
            "prob": 0.4648
          },
          {
            "token": "Ġsent",
            "token_id": 3288,
            "logprob": -0.8906,
            "prob": 0.4102
          },
          {
            "token": "Ġtyped",
            "token_id": 33069,
            "logprob": -3.0156,
            "prob": 0.0488
          },
          {
            "token": "Ġinput",
            "token_id": 1988,
            "logprob": -3.8906,
            "prob": 0.0204
          },
          {
            "token": "Ġpast",
            "token_id": 3347,
            "logprob": -4.0312,
            "prob": 0.0179
          },
          {
            "token": "Ġprovided",
            "token_id": 3984,
            "logprob": -4.8438,
            "prob": 0.008
          },
          {
            "token": "Ġput",
            "token_id": 2231,
            "logprob": -5.0312,
            "prob": 0.0066
          },
          {
            "token": "Ġentered",
            "token_id": 10862,
            "logprob": -5.1562,
            "prob": 0.0058
          },
          {
            "token": "Ġgave",
            "token_id": 6688,
            "logprob": -5.2812,
            "prob": 0.0052
          },
          {
            "token": "Ġwant",
            "token_id": 1390,
            "logprob": -6.0938,
            "prob": 0.0023
          }
        ]
      },
      "selected_token": {
        "token": "Ġpast",
        "token_id": 3347,
        "selection_method": "sampling"
      }
    },
    {
      "step": 75,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\" – wait, French. Wait let me check the actual message again. \"用户：“懵” 问：Raconter Moune Un Blonde\". Wait wait user just asked me, via my French instructions, but the actual message they past",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\"",
        "Ġ–",
        "Ġwait",
        ",",
        "ĠFrench",
        ".",
        "ĠWait",
        "Ġlet",
        "Ġme",
        "Ġcheck",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġagain",
        ".",
        "Ġ\"",
        "用户",
        "：“",
        "�",
        "�",
        "�",
        "”",
        "Ġ",
        "问",
        "：",
        "R",
        "acon",
        "ter",
        "ĠMo",
        "une",
        "ĠUn",
        "ĠBlonde",
        "\".",
        "ĠWait",
        "Ġwait",
        "Ġuser",
        "Ġjust",
        "Ġasked",
        "Ġme",
        ",",
        "Ġvia",
        "Ġmy",
        "ĠFrench",
        "Ġinstructions",
        ",",
        "Ġbut",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġthey",
        "Ġpast"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1,
        1389,
        3868,
        11,
        8753,
        13,
        14144,
        1095,
        757,
        1817,
        279,
        5150,
        1984,
        1578,
        13,
        330,
        20600,
        38087,
        162,
        229,
        113,
        863,
        220,
        57107,
        5232,
        49,
        23184,
        466,
        6178,
        2957,
        1252,
        53556,
        3343,
        14144,
        3868,
        1217,
        1120,
        4691,
        757,
        11,
        4669,
        856,
        8753,
        11470,
        11,
        719,
        279,
        5150,
        1984,
        814,
        3347
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "ed",
            "token_id": 291,
            "logprob": -0.0,
            "prob": 1.0
          },
          {
            "token": "ted",
            "token_id": 6702,
            "logprob": -12.6875,
            "prob": 0.0
          },
          {
            "token": "Ġis",
            "token_id": 374,
            "logprob": -13.0625,
            "prob": 0.0
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -13.6875,
            "prob": 0.0
          },
          {
            "token": "ored",
            "token_id": 3093,
            "logprob": -14.25,
            "prob": 0.0
          },
          {
            "token": "ied",
            "token_id": 1142,
            "logprob": -14.375,
            "prob": 0.0
          },
          {
            "token": "eb",
            "token_id": 3141,
            "logprob": -14.4375,
            "prob": 0.0
          },
          {
            "token": "Ġwas",
            "token_id": 574,
            "logprob": -14.5,
            "prob": 0.0
          },
          {
            "token": "ec",
            "token_id": 762,
            "logprob": -14.5,
            "prob": 0.0
          },
          {
            "token": "sed",
            "token_id": 32424,
            "logprob": -14.625,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "Ġin",
        "token_id": 304,
        "selection_method": "sampling"
      }
    },
    {
      "step": 76,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\" – wait, French. Wait let me check the actual message again. \"用户：“懵” 问：Raconter Moune Un Blonde\". Wait wait user just asked me, via my French instructions, but the actual message they past in",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\"",
        "Ġ–",
        "Ġwait",
        ",",
        "ĠFrench",
        ".",
        "ĠWait",
        "Ġlet",
        "Ġme",
        "Ġcheck",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġagain",
        ".",
        "Ġ\"",
        "用户",
        "：“",
        "�",
        "�",
        "�",
        "”",
        "Ġ",
        "问",
        "：",
        "R",
        "acon",
        "ter",
        "ĠMo",
        "une",
        "ĠUn",
        "ĠBlonde",
        "\".",
        "ĠWait",
        "Ġwait",
        "Ġuser",
        "Ġjust",
        "Ġasked",
        "Ġme",
        ",",
        "Ġvia",
        "Ġmy",
        "ĠFrench",
        "Ġinstructions",
        ",",
        "Ġbut",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġthey",
        "Ġpast",
        "Ġin"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1,
        1389,
        3868,
        11,
        8753,
        13,
        14144,
        1095,
        757,
        1817,
        279,
        5150,
        1984,
        1578,
        13,
        330,
        20600,
        38087,
        162,
        229,
        113,
        863,
        220,
        57107,
        5232,
        49,
        23184,
        466,
        6178,
        2957,
        1252,
        53556,
        3343,
        14144,
        3868,
        1217,
        1120,
        4691,
        757,
        11,
        4669,
        856,
        8753,
        11470,
        11,
        719,
        279,
        5150,
        1984,
        814,
        3347,
        304
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġis",
            "token_id": 374,
            "logprob": -1.0234,
            "prob": 0.3594
          },
          {
            "token": "Ġwas",
            "token_id": 574,
            "logprob": -1.7734,
            "prob": 0.1689
          },
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -2.0312,
            "prob": 0.1318
          },
          {
            "token": "ĠChinese",
            "token_id": 8620,
            "logprob": -2.1562,
            "prob": 0.1162
          },
          {
            "token": "ĠEnglish",
            "token_id": 6498,
            "logprob": -2.2812,
            "prob": 0.1025
          },
          {
            "token": "Ġhere",
            "token_id": 1618,
            "logprob": -3.2812,
            "prob": 0.0378
          },
          {
            "token": "ĠFrench",
            "token_id": 8753,
            "logprob": -3.5312,
            "prob": 0.0294
          },
          {
            "token": "Ġmight",
            "token_id": 2643,
            "logprob": -3.9062,
            "prob": 0.0203
          },
          {
            "token": "?",
            "token_id": 30,
            "logprob": -4.5938,
            "prob": 0.0102
          },
          {
            "token": "Ġmy",
            "token_id": 856,
            "logprob": -5.0938,
            "prob": 0.0062
          }
        ]
      },
      "selected_token": {
        "token": "Ġhere",
        "token_id": 1618,
        "selection_method": "sampling"
      }
    },
    {
      "step": 77,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\" – wait, French. Wait let me check the actual message again. \"用户：“懵” 问：Raconter Moune Un Blonde\". Wait wait user just asked me, via my French instructions, but the actual message they past in here",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\"",
        "Ġ–",
        "Ġwait",
        ",",
        "ĠFrench",
        ".",
        "ĠWait",
        "Ġlet",
        "Ġme",
        "Ġcheck",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġagain",
        ".",
        "Ġ\"",
        "用户",
        "：“",
        "�",
        "�",
        "�",
        "”",
        "Ġ",
        "问",
        "：",
        "R",
        "acon",
        "ter",
        "ĠMo",
        "une",
        "ĠUn",
        "ĠBlonde",
        "\".",
        "ĠWait",
        "Ġwait",
        "Ġuser",
        "Ġjust",
        "Ġasked",
        "Ġme",
        ",",
        "Ġvia",
        "Ġmy",
        "ĠFrench",
        "Ġinstructions",
        ",",
        "Ġbut",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġthey",
        "Ġpast",
        "Ġin",
        "Ġhere"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1,
        1389,
        3868,
        11,
        8753,
        13,
        14144,
        1095,
        757,
        1817,
        279,
        5150,
        1984,
        1578,
        13,
        330,
        20600,
        38087,
        162,
        229,
        113,
        863,
        220,
        57107,
        5232,
        49,
        23184,
        466,
        6178,
        2957,
        1252,
        53556,
        3343,
        14144,
        3868,
        1217,
        1120,
        4691,
        757,
        11,
        4669,
        856,
        8753,
        11470,
        11,
        719,
        279,
        5150,
        1984,
        814,
        3347,
        304,
        1618
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġis",
            "token_id": 374,
            "logprob": -0.1836,
            "prob": 0.832
          },
          {
            "token": "?",
            "token_id": 30,
            "logprob": -3.1875,
            "prob": 0.0415
          },
          {
            "token": "Ġwas",
            "token_id": 574,
            "logprob": -3.3125,
            "prob": 0.0366
          },
          {
            "token": "Ġmight",
            "token_id": 2643,
            "logprob": -3.5625,
            "prob": 0.0284
          },
          {
            "token": ":",
            "token_id": 25,
            "logprob": -3.8125,
            "prob": 0.0222
          },
          {
            "token": "Ġsays",
            "token_id": 2795,
            "logprob": -4.5625,
            "prob": 0.0105
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -4.8125,
            "prob": 0.0082
          },
          {
            "token": "...",
            "token_id": 1131,
            "logprob": -5.1875,
            "prob": 0.0056
          },
          {
            "token": ",",
            "token_id": 11,
            "logprob": -5.3125,
            "prob": 0.0049
          },
          {
            "token": ".",
            "token_id": 13,
            "logprob": -5.9375,
            "prob": 0.0027
          }
        ]
      },
      "selected_token": {
        "token": ":",
        "token_id": 25,
        "selection_method": "sampling"
      }
    },
    {
      "step": 78,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\" – wait, French. Wait let me check the actual message again. \"用户：“懵” 问：Raconter Moune Un Blonde\". Wait wait user just asked me, via my French instructions, but the actual message they past in here:",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\"",
        "Ġ–",
        "Ġwait",
        ",",
        "ĠFrench",
        ".",
        "ĠWait",
        "Ġlet",
        "Ġme",
        "Ġcheck",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġagain",
        ".",
        "Ġ\"",
        "用户",
        "：“",
        "�",
        "�",
        "�",
        "”",
        "Ġ",
        "问",
        "：",
        "R",
        "acon",
        "ter",
        "ĠMo",
        "une",
        "ĠUn",
        "ĠBlonde",
        "\".",
        "ĠWait",
        "Ġwait",
        "Ġuser",
        "Ġjust",
        "Ġasked",
        "Ġme",
        ",",
        "Ġvia",
        "Ġmy",
        "ĠFrench",
        "Ġinstructions",
        ",",
        "Ġbut",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġthey",
        "Ġpast",
        "Ġin",
        "Ġhere",
        ":"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1,
        1389,
        3868,
        11,
        8753,
        13,
        14144,
        1095,
        757,
        1817,
        279,
        5150,
        1984,
        1578,
        13,
        330,
        20600,
        38087,
        162,
        229,
        113,
        863,
        220,
        57107,
        5232,
        49,
        23184,
        466,
        6178,
        2957,
        1252,
        53556,
        3343,
        14144,
        3868,
        1217,
        1120,
        4691,
        757,
        11,
        4669,
        856,
        8753,
        11470,
        11,
        719,
        279,
        5150,
        1984,
        814,
        3347,
        304,
        1618,
        25
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġ\"",
            "token_id": 330,
            "logprob": -0.0981,
            "prob": 0.9062
          },
          {
            "token": "Ġthe",
            "token_id": 279,
            "logprob": -3.0938,
            "prob": 0.0452
          },
          {
            "token": "Ġ“",
            "token_id": 1054,
            "logprob": -4.2188,
            "prob": 0.0146
          },
          {
            "token": "Ġthey",
            "token_id": 814,
            "logprob": -4.5938,
            "prob": 0.0101
          },
          {
            "token": "Ġuser",
            "token_id": 1217,
            "logprob": -5.2188,
            "prob": 0.0054
          },
          {
            "token": "ĠThe",
            "token_id": 578,
            "logprob": -5.3438,
            "prob": 0.0048
          },
          {
            "token": "Ġlooking",
            "token_id": 3411,
            "logprob": -6.2812,
            "prob": 0.0019
          },
          {
            "token": "ĠWait",
            "token_id": 14144,
            "logprob": -6.2812,
            "prob": 0.0019
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -6.4062,
            "prob": 0.0016
          },
          {
            "token": "ĠUser",
            "token_id": 2724,
            "logprob": -6.4688,
            "prob": 0.0015
          }
        ]
      },
      "selected_token": {
        "token": "ĠUser",
        "token_id": 2724,
        "selection_method": "sampling"
      }
    },
    {
      "step": 79,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\" – wait, French. Wait let me check the actual message again. \"用户：“懵” 问：Raconter Moune Un Blonde\". Wait wait user just asked me, via my French instructions, but the actual message they past in here: User",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\"",
        "Ġ–",
        "Ġwait",
        ",",
        "ĠFrench",
        ".",
        "ĠWait",
        "Ġlet",
        "Ġme",
        "Ġcheck",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġagain",
        ".",
        "Ġ\"",
        "用户",
        "：“",
        "�",
        "�",
        "�",
        "”",
        "Ġ",
        "问",
        "：",
        "R",
        "acon",
        "ter",
        "ĠMo",
        "une",
        "ĠUn",
        "ĠBlonde",
        "\".",
        "ĠWait",
        "Ġwait",
        "Ġuser",
        "Ġjust",
        "Ġasked",
        "Ġme",
        ",",
        "Ġvia",
        "Ġmy",
        "ĠFrench",
        "Ġinstructions",
        ",",
        "Ġbut",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġthey",
        "Ġpast",
        "Ġin",
        "Ġhere",
        ":",
        "ĠUser"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1,
        1389,
        3868,
        11,
        8753,
        13,
        14144,
        1095,
        757,
        1817,
        279,
        5150,
        1984,
        1578,
        13,
        330,
        20600,
        38087,
        162,
        229,
        113,
        863,
        220,
        57107,
        5232,
        49,
        23184,
        466,
        6178,
        2957,
        1252,
        53556,
        3343,
        14144,
        3868,
        1217,
        1120,
        4691,
        757,
        11,
        4669,
        856,
        8753,
        11470,
        11,
        719,
        279,
        5150,
        1984,
        814,
        3347,
        304,
        1618,
        25,
        2724
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġwrote",
            "token_id": 6267,
            "logprob": -1.0391,
            "prob": 0.3535
          },
          {
            "token": ":",
            "token_id": 25,
            "logprob": -1.1641,
            "prob": 0.3125
          },
          {
            "token": "Ġsays",
            "token_id": 2795,
            "logprob": -1.6641,
            "prob": 0.1895
          },
          {
            "token": "Ġsaid",
            "token_id": 1071,
            "logprob": -3.0312,
            "prob": 0.0479
          },
          {
            "token": "Ġmessage",
            "token_id": 1984,
            "logprob": -3.5312,
            "prob": 0.0291
          },
          {
            "token": "Ġwrites",
            "token_id": 14238,
            "logprob": -3.6562,
            "prob": 0.0256
          },
          {
            "token": "'s",
            "token_id": 596,
            "logprob": -3.9062,
            "prob": 0.02
          },
          {
            "token": "Ġinput",
            "token_id": 1988,
            "logprob": -5.0312,
            "prob": 0.0065
          },
          {
            "token": "Ġasked",
            "token_id": 4691,
            "logprob": -5.5312,
            "prob": 0.0039
          },
          {
            "token": "’s",
            "token_id": 753,
            "logprob": -6.5312,
            "prob": 0.0014
          }
        ]
      },
      "selected_token": {
        "token": ":",
        "token_id": 25,
        "selection_method": "sampling"
      }
    },
    {
      "step": 80,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\" – wait, French. Wait let me check the actual message again. \"用户：“懵” 问：Raconter Moune Un Blonde\". Wait wait user just asked me, via my French instructions, but the actual message they past in here: User:",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\"",
        "Ġ–",
        "Ġwait",
        ",",
        "ĠFrench",
        ".",
        "ĠWait",
        "Ġlet",
        "Ġme",
        "Ġcheck",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġagain",
        ".",
        "Ġ\"",
        "用户",
        "：“",
        "�",
        "�",
        "�",
        "”",
        "Ġ",
        "问",
        "：",
        "R",
        "acon",
        "ter",
        "ĠMo",
        "une",
        "ĠUn",
        "ĠBlonde",
        "\".",
        "ĠWait",
        "Ġwait",
        "Ġuser",
        "Ġjust",
        "Ġasked",
        "Ġme",
        ",",
        "Ġvia",
        "Ġmy",
        "ĠFrench",
        "Ġinstructions",
        ",",
        "Ġbut",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġthey",
        "Ġpast",
        "Ġin",
        "Ġhere",
        ":",
        "ĠUser",
        ":"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1,
        1389,
        3868,
        11,
        8753,
        13,
        14144,
        1095,
        757,
        1817,
        279,
        5150,
        1984,
        1578,
        13,
        330,
        20600,
        38087,
        162,
        229,
        113,
        863,
        220,
        57107,
        5232,
        49,
        23184,
        466,
        6178,
        2957,
        1252,
        53556,
        3343,
        14144,
        3868,
        1217,
        1120,
        4691,
        757,
        11,
        4669,
        856,
        8753,
        11470,
        11,
        719,
        279,
        5150,
        1984,
        814,
        3347,
        304,
        1618,
        25,
        2724,
        25
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġ\"",
            "token_id": 330,
            "logprob": -0.4785,
            "prob": 0.6211
          },
          {
            "token": "ĠRa",
            "token_id": 18989,
            "logprob": -1.6641,
            "prob": 0.1885
          },
          {
            "token": "Ġ“",
            "token_id": 1054,
            "logprob": -2.5469,
            "prob": 0.0786
          },
          {
            "token": "ĠTell",
            "token_id": 25672,
            "logprob": -3.2969,
            "prob": 0.0371
          },
          {
            "token": "Ġtell",
            "token_id": 3371,
            "logprob": -4.0312,
            "prob": 0.0176
          },
          {
            "token": "ĠR",
            "token_id": 432,
            "logprob": -4.4062,
            "prob": 0.0121
          },
          {
            "token": "Ġtells",
            "token_id": 10975,
            "logprob": -4.9688,
            "prob": 0.0069
          },
          {
            "token": "Ġ�",
            "token_id": 6704,
            "logprob": -5.2188,
            "prob": 0.0054
          },
          {
            "token": "Ġ用户",
            "token_id": 51142,
            "logprob": -6.5312,
            "prob": 0.0014
          },
          {
            "token": "告",
            "token_id": 58655,
            "logprob": -6.5312,
            "prob": 0.0014
          }
        ]
      },
      "selected_token": {
        "token": "Ġ�",
        "token_id": 6704,
        "selection_method": "sampling"
      }
    },
    {
      "step": 81,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\" – wait, French. Wait let me check the actual message again. \"用户：“懵” 问：Raconter Moune Un Blonde\". Wait wait user just asked me, via my French instructions, but the actual message they past in here: User: �",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\"",
        "Ġ–",
        "Ġwait",
        ",",
        "ĠFrench",
        ".",
        "ĠWait",
        "Ġlet",
        "Ġme",
        "Ġcheck",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġagain",
        ".",
        "Ġ\"",
        "用户",
        "：“",
        "�",
        "�",
        "�",
        "”",
        "Ġ",
        "问",
        "：",
        "R",
        "acon",
        "ter",
        "ĠMo",
        "une",
        "ĠUn",
        "ĠBlonde",
        "\".",
        "ĠWait",
        "Ġwait",
        "Ġuser",
        "Ġjust",
        "Ġasked",
        "Ġme",
        ",",
        "Ġvia",
        "Ġmy",
        "ĠFrench",
        "Ġinstructions",
        ",",
        "Ġbut",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġthey",
        "Ġpast",
        "Ġin",
        "Ġhere",
        ":",
        "ĠUser",
        ":",
        "Ġ�"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1,
        1389,
        3868,
        11,
        8753,
        13,
        14144,
        1095,
        757,
        1817,
        279,
        5150,
        1984,
        1578,
        13,
        330,
        20600,
        38087,
        162,
        229,
        113,
        863,
        220,
        57107,
        5232,
        49,
        23184,
        466,
        6178,
        2957,
        1252,
        53556,
        3343,
        14144,
        3868,
        1217,
        1120,
        4691,
        757,
        11,
        4669,
        856,
        8753,
        11470,
        11,
        719,
        279,
        5150,
        1984,
        814,
        3347,
        304,
        1618,
        25,
        2724,
        25,
        6704
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "�",
            "token_id": 229,
            "logprob": -0.1455,
            "prob": 0.8633
          },
          {
            "token": "��",
            "token_id": 61857,
            "logprob": -3.5156,
            "prob": 0.0295
          },
          {
            "token": "�",
            "token_id": 225,
            "logprob": -4.4062,
            "prob": 0.0123
          },
          {
            "token": "�",
            "token_id": 226,
            "logprob": -4.4688,
            "prob": 0.0116
          },
          {
            "token": "�",
            "token_id": 109,
            "logprob": -4.6562,
            "prob": 0.0096
          },
          {
            "token": "�",
            "token_id": 120,
            "logprob": -4.8438,
            "prob": 0.0079
          },
          {
            "token": "�",
            "token_id": 108,
            "logprob": -4.9688,
            "prob": 0.007
          },
          {
            "token": "�",
            "token_id": 114,
            "logprob": -5.0312,
            "prob": 0.0066
          },
          {
            "token": "�",
            "token_id": 247,
            "logprob": -5.0312,
            "prob": 0.0066
          },
          {
            "token": "�",
            "token_id": 238,
            "logprob": -5.6562,
            "prob": 0.0035
          }
        ]
      },
      "selected_token": {
        "token": "�",
        "token_id": 229,
        "selection_method": "sampling"
      }
    },
    {
      "step": 82,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\" – wait, French. Wait let me check the actual message again. \"用户：“懵” 问：Raconter Moune Un Blonde\". Wait wait user just asked me, via my French instructions, but the actual message they past in here: User: �",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\"",
        "Ġ–",
        "Ġwait",
        ",",
        "ĠFrench",
        ".",
        "ĠWait",
        "Ġlet",
        "Ġme",
        "Ġcheck",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġagain",
        ".",
        "Ġ\"",
        "用户",
        "：“",
        "�",
        "�",
        "�",
        "”",
        "Ġ",
        "问",
        "：",
        "R",
        "acon",
        "ter",
        "ĠMo",
        "une",
        "ĠUn",
        "ĠBlonde",
        "\".",
        "ĠWait",
        "Ġwait",
        "Ġuser",
        "Ġjust",
        "Ġasked",
        "Ġme",
        ",",
        "Ġvia",
        "Ġmy",
        "ĠFrench",
        "Ġinstructions",
        ",",
        "Ġbut",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġthey",
        "Ġpast",
        "Ġin",
        "Ġhere",
        ":",
        "ĠUser",
        ":",
        "Ġ�",
        "�"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1,
        1389,
        3868,
        11,
        8753,
        13,
        14144,
        1095,
        757,
        1817,
        279,
        5150,
        1984,
        1578,
        13,
        330,
        20600,
        38087,
        162,
        229,
        113,
        863,
        220,
        57107,
        5232,
        49,
        23184,
        466,
        6178,
        2957,
        1252,
        53556,
        3343,
        14144,
        3868,
        1217,
        1120,
        4691,
        757,
        11,
        4669,
        856,
        8753,
        11470,
        11,
        719,
        279,
        5150,
        1984,
        814,
        3347,
        304,
        1618,
        25,
        2724,
        25,
        6704,
        229
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "�",
            "token_id": 113,
            "logprob": -0.0073,
            "prob": 0.9922
          },
          {
            "token": "�",
            "token_id": 224,
            "logprob": -5.125,
            "prob": 0.0059
          },
          {
            "token": "�",
            "token_id": 230,
            "logprob": -6.9375,
            "prob": 0.001
          },
          {
            "token": "�",
            "token_id": 233,
            "logprob": -8.4375,
            "prob": 0.0002
          },
          {
            "token": "�",
            "token_id": 111,
            "logprob": -10.4375,
            "prob": 0.0
          },
          {
            "token": "�",
            "token_id": 100,
            "logprob": -11.125,
            "prob": 0.0
          },
          {
            "token": "�",
            "token_id": 115,
            "logprob": -11.25,
            "prob": 0.0
          },
          {
            "token": "�",
            "token_id": 123,
            "logprob": -11.25,
            "prob": 0.0
          },
          {
            "token": "�",
            "token_id": 105,
            "logprob": -11.4375,
            "prob": 0.0
          },
          {
            "token": "�",
            "token_id": 116,
            "logprob": -11.75,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "�",
        "token_id": 123,
        "selection_method": "sampling"
      }
    },
    {
      "step": 83,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\" – wait, French. Wait let me check the actual message again. \"用户：“懵” 问：Raconter Moune Un Blonde\". Wait wait user just asked me, via my French instructions, but the actual message they past in here: User: 懿",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\"",
        "Ġ–",
        "Ġwait",
        ",",
        "ĠFrench",
        ".",
        "ĠWait",
        "Ġlet",
        "Ġme",
        "Ġcheck",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġagain",
        ".",
        "Ġ\"",
        "用户",
        "：“",
        "�",
        "�",
        "�",
        "”",
        "Ġ",
        "问",
        "：",
        "R",
        "acon",
        "ter",
        "ĠMo",
        "une",
        "ĠUn",
        "ĠBlonde",
        "\".",
        "ĠWait",
        "Ġwait",
        "Ġuser",
        "Ġjust",
        "Ġasked",
        "Ġme",
        ",",
        "Ġvia",
        "Ġmy",
        "ĠFrench",
        "Ġinstructions",
        ",",
        "Ġbut",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġthey",
        "Ġpast",
        "Ġin",
        "Ġhere",
        ":",
        "ĠUser",
        ":",
        "Ġ�",
        "�",
        "�"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1,
        1389,
        3868,
        11,
        8753,
        13,
        14144,
        1095,
        757,
        1817,
        279,
        5150,
        1984,
        1578,
        13,
        330,
        20600,
        38087,
        162,
        229,
        113,
        863,
        220,
        57107,
        5232,
        49,
        23184,
        466,
        6178,
        2957,
        1252,
        53556,
        3343,
        14144,
        3868,
        1217,
        1120,
        4691,
        757,
        11,
        4669,
        856,
        8753,
        11470,
        11,
        719,
        279,
        5150,
        1984,
        814,
        3347,
        304,
        1618,
        25,
        2724,
        25,
        6704,
        229,
        123
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "?",
            "token_id": 30,
            "logprob": -1.5234,
            "prob": 0.2178
          },
          {
            "token": "Ġ(",
            "token_id": 320,
            "logprob": -1.9609,
            "prob": 0.1406
          },
          {
            "token": "问",
            "token_id": 57107,
            "logprob": -2.3906,
            "prob": 0.0908
          },
          {
            "token": "�",
            "token_id": 162,
            "logprob": -2.5156,
            "prob": 0.0801
          },
          {
            "token": "？",
            "token_id": 11571,
            "logprob": -3.1406,
            "prob": 0.043
          },
          {
            "token": "Ġ\"",
            "token_id": 330,
            "logprob": -3.9844,
            "prob": 0.0184
          },
          {
            "token": "Ġasking",
            "token_id": 10371,
            "logprob": -4.2188,
            "prob": 0.0148
          },
          {
            "token": "你",
            "token_id": 57668,
            "logprob": -4.2188,
            "prob": 0.0148
          },
          {
            "token": "Ġasks",
            "token_id": 17501,
            "logprob": -4.2812,
            "prob": 0.0139
          },
          {
            "token": "“",
            "token_id": 2118,
            "logprob": -4.3438,
            "prob": 0.0131
          }
        ]
      },
      "selected_token": {
        "token": "Ġasking",
        "token_id": 10371,
        "selection_method": "sampling"
      }
    },
    {
      "step": 84,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\" – wait, French. Wait let me check the actual message again. \"用户：“懵” 问：Raconter Moune Un Blonde\". Wait wait user just asked me, via my French instructions, but the actual message they past in here: User: 懿 asking",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\"",
        "Ġ–",
        "Ġwait",
        ",",
        "ĠFrench",
        ".",
        "ĠWait",
        "Ġlet",
        "Ġme",
        "Ġcheck",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġagain",
        ".",
        "Ġ\"",
        "用户",
        "：“",
        "�",
        "�",
        "�",
        "”",
        "Ġ",
        "问",
        "：",
        "R",
        "acon",
        "ter",
        "ĠMo",
        "une",
        "ĠUn",
        "ĠBlonde",
        "\".",
        "ĠWait",
        "Ġwait",
        "Ġuser",
        "Ġjust",
        "Ġasked",
        "Ġme",
        ",",
        "Ġvia",
        "Ġmy",
        "ĠFrench",
        "Ġinstructions",
        ",",
        "Ġbut",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġthey",
        "Ġpast",
        "Ġin",
        "Ġhere",
        ":",
        "ĠUser",
        ":",
        "Ġ�",
        "�",
        "�",
        "Ġasking"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1,
        1389,
        3868,
        11,
        8753,
        13,
        14144,
        1095,
        757,
        1817,
        279,
        5150,
        1984,
        1578,
        13,
        330,
        20600,
        38087,
        162,
        229,
        113,
        863,
        220,
        57107,
        5232,
        49,
        23184,
        466,
        6178,
        2957,
        1252,
        53556,
        3343,
        14144,
        3868,
        1217,
        1120,
        4691,
        757,
        11,
        4669,
        856,
        8753,
        11470,
        11,
        719,
        279,
        5150,
        1984,
        814,
        3347,
        304,
        1618,
        25,
        2724,
        25,
        6704,
        229,
        123,
        10371
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġfor",
            "token_id": 369,
            "logprob": -0.8984,
            "prob": 0.4082
          },
          {
            "token": "Ġ\"",
            "token_id": 330,
            "logprob": -1.2109,
            "prob": 0.2988
          },
          {
            "token": ":",
            "token_id": 25,
            "logprob": -1.7109,
            "prob": 0.1807
          },
          {
            "token": "Ġme",
            "token_id": 757,
            "logprob": -3.2656,
            "prob": 0.0378
          },
          {
            "token": "Ġto",
            "token_id": 311,
            "logprob": -3.8906,
            "prob": 0.0203
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -4.0938,
            "prob": 0.0168
          },
          {
            "token": "Ġ“",
            "token_id": 1054,
            "logprob": -4.9062,
            "prob": 0.0075
          },
          {
            "token": "Ġa",
            "token_id": 264,
            "logprob": -5.5312,
            "prob": 0.004
          },
          {
            "token": "?",
            "token_id": 30,
            "logprob": -5.5312,
            "prob": 0.004
          },
          {
            "token": ",",
            "token_id": 11,
            "logprob": -5.7812,
            "prob": 0.0031
          }
        ]
      },
      "selected_token": {
        "token": "Ġ\"",
        "token_id": 330,
        "selection_method": "sampling"
      }
    },
    {
      "step": 85,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\" – wait, French. Wait let me check the actual message again. \"用户：“懵” 问：Raconter Moune Un Blonde\". Wait wait user just asked me, via my French instructions, but the actual message they past in here: User: 懿 asking \"",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\"",
        "Ġ–",
        "Ġwait",
        ",",
        "ĠFrench",
        ".",
        "ĠWait",
        "Ġlet",
        "Ġme",
        "Ġcheck",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġagain",
        ".",
        "Ġ\"",
        "用户",
        "：“",
        "�",
        "�",
        "�",
        "”",
        "Ġ",
        "问",
        "：",
        "R",
        "acon",
        "ter",
        "ĠMo",
        "une",
        "ĠUn",
        "ĠBlonde",
        "\".",
        "ĠWait",
        "Ġwait",
        "Ġuser",
        "Ġjust",
        "Ġasked",
        "Ġme",
        ",",
        "Ġvia",
        "Ġmy",
        "ĠFrench",
        "Ġinstructions",
        ",",
        "Ġbut",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġthey",
        "Ġpast",
        "Ġin",
        "Ġhere",
        ":",
        "ĠUser",
        ":",
        "Ġ�",
        "�",
        "�",
        "Ġasking",
        "Ġ\""
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1,
        1389,
        3868,
        11,
        8753,
        13,
        14144,
        1095,
        757,
        1817,
        279,
        5150,
        1984,
        1578,
        13,
        330,
        20600,
        38087,
        162,
        229,
        113,
        863,
        220,
        57107,
        5232,
        49,
        23184,
        466,
        6178,
        2957,
        1252,
        53556,
        3343,
        14144,
        3868,
        1217,
        1120,
        4691,
        757,
        11,
        4669,
        856,
        8753,
        11470,
        11,
        719,
        279,
        5150,
        1984,
        814,
        3347,
        304,
        1618,
        25,
        2724,
        25,
        6704,
        229,
        123,
        10371,
        330
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "R",
            "token_id": 49,
            "logprob": -0.0212,
            "prob": 0.9805
          },
          {
            "token": "Ra",
            "token_id": 56635,
            "logprob": -5.2812,
            "prob": 0.0051
          },
          {
            "token": "ra",
            "token_id": 969,
            "logprob": -5.6562,
            "prob": 0.0035
          },
          {
            "token": "Tell",
            "token_id": 41551,
            "logprob": -5.9688,
            "prob": 0.0026
          },
          {
            "token": "Ġtell",
            "token_id": 3371,
            "logprob": -6.6562,
            "prob": 0.0013
          },
          {
            "token": "RA",
            "token_id": 5726,
            "logprob": -6.9062,
            "prob": 0.001
          },
          {
            "token": "Ġtelling",
            "token_id": 11890,
            "logprob": -7.0312,
            "prob": 0.0009
          },
          {
            "token": "ĠRa",
            "token_id": 18989,
            "logprob": -7.2188,
            "prob": 0.0007
          },
          {
            "token": "用户",
            "token_id": 20600,
            "logprob": -8.0625,
            "prob": 0.0003
          },
          {
            "token": "告",
            "token_id": 58655,
            "logprob": -8.0625,
            "prob": 0.0003
          }
        ]
      },
      "selected_token": {
        "token": "R",
        "token_id": 49,
        "selection_method": "sampling"
      }
    },
    {
      "step": 86,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\" – wait, French. Wait let me check the actual message again. \"用户：“懵” 问：Raconter Moune Un Blonde\". Wait wait user just asked me, via my French instructions, but the actual message they past in here: User: 懿 asking \"R",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\"",
        "Ġ–",
        "Ġwait",
        ",",
        "ĠFrench",
        ".",
        "ĠWait",
        "Ġlet",
        "Ġme",
        "Ġcheck",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġagain",
        ".",
        "Ġ\"",
        "用户",
        "：“",
        "�",
        "�",
        "�",
        "”",
        "Ġ",
        "问",
        "：",
        "R",
        "acon",
        "ter",
        "ĠMo",
        "une",
        "ĠUn",
        "ĠBlonde",
        "\".",
        "ĠWait",
        "Ġwait",
        "Ġuser",
        "Ġjust",
        "Ġasked",
        "Ġme",
        ",",
        "Ġvia",
        "Ġmy",
        "ĠFrench",
        "Ġinstructions",
        ",",
        "Ġbut",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġthey",
        "Ġpast",
        "Ġin",
        "Ġhere",
        ":",
        "ĠUser",
        ":",
        "Ġ�",
        "�",
        "�",
        "Ġasking",
        "Ġ\"",
        "R"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1,
        1389,
        3868,
        11,
        8753,
        13,
        14144,
        1095,
        757,
        1817,
        279,
        5150,
        1984,
        1578,
        13,
        330,
        20600,
        38087,
        162,
        229,
        113,
        863,
        220,
        57107,
        5232,
        49,
        23184,
        466,
        6178,
        2957,
        1252,
        53556,
        3343,
        14144,
        3868,
        1217,
        1120,
        4691,
        757,
        11,
        4669,
        856,
        8753,
        11470,
        11,
        719,
        279,
        5150,
        1984,
        814,
        3347,
        304,
        1618,
        25,
        2724,
        25,
        6704,
        229,
        123,
        10371,
        330,
        49
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "acon",
            "token_id": 23184,
            "logprob": -0.0005,
            "prob": 1.0
          },
          {
            "token": "ac",
            "token_id": 582,
            "logprob": -9.0,
            "prob": 0.0001
          },
          {
            "token": "conte",
            "token_id": 46452,
            "logprob": -11.3125,
            "prob": 0.0
          },
          {
            "token": "AC",
            "token_id": 1741,
            "logprob": -11.75,
            "prob": 0.0
          },
          {
            "token": "acock",
            "token_id": 81378,
            "logprob": -11.875,
            "prob": 0.0
          },
          {
            "token": "acad",
            "token_id": 50957,
            "logprob": -12.1875,
            "prob": 0.0
          },
          {
            "token": "acons",
            "token_id": 76544,
            "logprob": -12.25,
            "prob": 0.0
          },
          {
            "token": "acom",
            "token_id": 45670,
            "logprob": -12.25,
            "prob": 0.0
          },
          {
            "token": "acent",
            "token_id": 18680,
            "logprob": -12.5625,
            "prob": 0.0
          },
          {
            "token": "alcon",
            "token_id": 47314,
            "logprob": -12.625,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "acon",
        "token_id": 23184,
        "selection_method": "sampling"
      }
    },
    {
      "step": 87,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\" – wait, French. Wait let me check the actual message again. \"用户：“懵” 问：Raconter Moune Un Blonde\". Wait wait user just asked me, via my French instructions, but the actual message they past in here: User: 懿 asking \"Racon",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\"",
        "Ġ–",
        "Ġwait",
        ",",
        "ĠFrench",
        ".",
        "ĠWait",
        "Ġlet",
        "Ġme",
        "Ġcheck",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġagain",
        ".",
        "Ġ\"",
        "用户",
        "：“",
        "�",
        "�",
        "�",
        "”",
        "Ġ",
        "问",
        "：",
        "R",
        "acon",
        "ter",
        "ĠMo",
        "une",
        "ĠUn",
        "ĠBlonde",
        "\".",
        "ĠWait",
        "Ġwait",
        "Ġuser",
        "Ġjust",
        "Ġasked",
        "Ġme",
        ",",
        "Ġvia",
        "Ġmy",
        "ĠFrench",
        "Ġinstructions",
        ",",
        "Ġbut",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġthey",
        "Ġpast",
        "Ġin",
        "Ġhere",
        ":",
        "ĠUser",
        ":",
        "Ġ�",
        "�",
        "�",
        "Ġasking",
        "Ġ\"",
        "R",
        "acon"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1,
        1389,
        3868,
        11,
        8753,
        13,
        14144,
        1095,
        757,
        1817,
        279,
        5150,
        1984,
        1578,
        13,
        330,
        20600,
        38087,
        162,
        229,
        113,
        863,
        220,
        57107,
        5232,
        49,
        23184,
        466,
        6178,
        2957,
        1252,
        53556,
        3343,
        14144,
        3868,
        1217,
        1120,
        4691,
        757,
        11,
        4669,
        856,
        8753,
        11470,
        11,
        719,
        279,
        5150,
        1984,
        814,
        3347,
        304,
        1618,
        25,
        2724,
        25,
        6704,
        229,
        123,
        10371,
        330,
        49,
        23184
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "te",
            "token_id": 668,
            "logprob": -0.0708,
            "prob": 0.9336
          },
          {
            "token": "ter",
            "token_id": 466,
            "logprob": -2.7031,
            "prob": 0.0674
          },
          {
            "token": "tre",
            "token_id": 10367,
            "logprob": -9.3125,
            "prob": 0.0001
          },
          {
            "token": "t",
            "token_id": 83,
            "logprob": -9.4375,
            "prob": 0.0001
          },
          {
            "token": "тер",
            "token_id": 58060,
            "logprob": -10.1875,
            "prob": 0.0
          },
          {
            "token": "tell",
            "token_id": 73457,
            "logprob": -10.25,
            "prob": 0.0
          },
          {
            "token": "tem",
            "token_id": 880,
            "logprob": -10.5625,
            "prob": 0.0
          },
          {
            "token": "tier",
            "token_id": 50435,
            "logprob": -10.6875,
            "prob": 0.0
          },
          {
            "token": "to",
            "token_id": 998,
            "logprob": -10.8125,
            "prob": 0.0
          },
          {
            "token": "ten",
            "token_id": 2002,
            "logprob": -10.875,
            "prob": 0.0
          }
        ]
      },
      "selected_token": {
        "token": "тер",
        "token_id": 58060,
        "selection_method": "sampling"
      }
    },
    {
      "step": 88,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\" – wait, French. Wait let me check the actual message again. \"用户：“懵” 问：Raconter Moune Un Blonde\". Wait wait user just asked me, via my French instructions, but the actual message they past in here: User: 懿 asking \"Raconтер",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\"",
        "Ġ–",
        "Ġwait",
        ",",
        "ĠFrench",
        ".",
        "ĠWait",
        "Ġlet",
        "Ġme",
        "Ġcheck",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġagain",
        ".",
        "Ġ\"",
        "用户",
        "：“",
        "�",
        "�",
        "�",
        "”",
        "Ġ",
        "问",
        "：",
        "R",
        "acon",
        "ter",
        "ĠMo",
        "une",
        "ĠUn",
        "ĠBlonde",
        "\".",
        "ĠWait",
        "Ġwait",
        "Ġuser",
        "Ġjust",
        "Ġasked",
        "Ġme",
        ",",
        "Ġvia",
        "Ġmy",
        "ĠFrench",
        "Ġinstructions",
        ",",
        "Ġbut",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġthey",
        "Ġpast",
        "Ġin",
        "Ġhere",
        ":",
        "ĠUser",
        ":",
        "Ġ�",
        "�",
        "�",
        "Ġasking",
        "Ġ\"",
        "R",
        "acon",
        "тер"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1,
        1389,
        3868,
        11,
        8753,
        13,
        14144,
        1095,
        757,
        1817,
        279,
        5150,
        1984,
        1578,
        13,
        330,
        20600,
        38087,
        162,
        229,
        113,
        863,
        220,
        57107,
        5232,
        49,
        23184,
        466,
        6178,
        2957,
        1252,
        53556,
        3343,
        14144,
        3868,
        1217,
        1120,
        4691,
        757,
        11,
        4669,
        856,
        8753,
        11470,
        11,
        719,
        279,
        5150,
        1984,
        814,
        3347,
        304,
        1618,
        25,
        2724,
        25,
        6704,
        229,
        123,
        10371,
        330,
        49,
        23184,
        58060
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "ĠMo",
            "token_id": 6178,
            "logprob": -0.2373,
            "prob": 0.7891
          },
          {
            "token": "ĠМ",
            "token_id": 45458,
            "logprob": -2.7969,
            "prob": 0.0608
          },
          {
            "token": "Ġм",
            "token_id": 11562,
            "logprob": -2.9844,
            "prob": 0.0505
          },
          {
            "token": "Ġmo",
            "token_id": 4647,
            "logprob": -3.4844,
            "prob": 0.0306
          },
          {
            "token": "е",
            "token_id": 1532,
            "logprob": -4.375,
            "prob": 0.0128
          },
          {
            "token": "e",
            "token_id": 68,
            "logprob": -5.3125,
            "prob": 0.005
          },
          {
            "token": "Ġmoi",
            "token_id": 41091,
            "logprob": -5.5625,
            "prob": 0.0039
          },
          {
            "token": "Mo",
            "token_id": 26694,
            "logprob": -5.9688,
            "prob": 0.0026
          },
          {
            "token": "ĠM",
            "token_id": 386,
            "logprob": -6.0,
            "prob": 0.0025
          },
          {
            "token": "Ġme",
            "token_id": 757,
            "logprob": -6.0938,
            "prob": 0.0023
          }
        ]
      },
      "selected_token": {
        "token": "Ġм",
        "token_id": 11562,
        "selection_method": "sampling"
      }
    },
    {
      "step": 89,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\" – wait, French. Wait let me check the actual message again. \"用户：“懵” 问：Raconter Moune Un Blonde\". Wait wait user just asked me, via my French instructions, but the actual message they past in here: User: 懿 asking \"Raconтер м",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\"",
        "Ġ–",
        "Ġwait",
        ",",
        "ĠFrench",
        ".",
        "ĠWait",
        "Ġlet",
        "Ġme",
        "Ġcheck",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġagain",
        ".",
        "Ġ\"",
        "用户",
        "：“",
        "�",
        "�",
        "�",
        "”",
        "Ġ",
        "问",
        "：",
        "R",
        "acon",
        "ter",
        "ĠMo",
        "une",
        "ĠUn",
        "ĠBlonde",
        "\".",
        "ĠWait",
        "Ġwait",
        "Ġuser",
        "Ġjust",
        "Ġasked",
        "Ġme",
        ",",
        "Ġvia",
        "Ġmy",
        "ĠFrench",
        "Ġinstructions",
        ",",
        "Ġbut",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġthey",
        "Ġpast",
        "Ġin",
        "Ġhere",
        ":",
        "ĠUser",
        ":",
        "Ġ�",
        "�",
        "�",
        "Ġasking",
        "Ġ\"",
        "R",
        "acon",
        "тер",
        "Ġм"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1,
        1389,
        3868,
        11,
        8753,
        13,
        14144,
        1095,
        757,
        1817,
        279,
        5150,
        1984,
        1578,
        13,
        330,
        20600,
        38087,
        162,
        229,
        113,
        863,
        220,
        57107,
        5232,
        49,
        23184,
        466,
        6178,
        2957,
        1252,
        53556,
        3343,
        14144,
        3868,
        1217,
        1120,
        4691,
        757,
        11,
        4669,
        856,
        8753,
        11470,
        11,
        719,
        279,
        5150,
        1984,
        814,
        3347,
        304,
        1618,
        25,
        2724,
        25,
        6704,
        229,
        123,
        10371,
        330,
        49,
        23184,
        58060,
        11562
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "ой",
            "token_id": 16742,
            "logprob": -0.7148,
            "prob": 0.4883
          },
          {
            "token": "о",
            "token_id": 1482,
            "logprob": -1.2812,
            "prob": 0.2793
          },
          {
            "token": "не",
            "token_id": 79862,
            "logprob": -2.6562,
            "prob": 0.0703
          },
          {
            "token": "он",
            "token_id": 17766,
            "logprob": -3.6562,
            "prob": 0.0259
          },
          {
            "token": "ор",
            "token_id": 9239,
            "logprob": -4.4688,
            "prob": 0.0115
          },
          {
            "token": "ог",
            "token_id": 14837,
            "logprob": -4.5312,
            "prob": 0.0108
          },
          {
            "token": "е",
            "token_id": 1532,
            "logprob": -4.5938,
            "prob": 0.0101
          },
          {
            "token": "o",
            "token_id": 78,
            "logprob": -4.7812,
            "prob": 0.0084
          },
          {
            "token": "oi",
            "token_id": 6870,
            "logprob": -5.1562,
            "prob": 0.0058
          },
          {
            "token": "ун",
            "token_id": 35297,
            "logprob": -5.3438,
            "prob": 0.0048
          }
        ]
      },
      "selected_token": {
        "token": "o",
        "token_id": 78,
        "selection_method": "sampling"
      }
    },
    {
      "step": 90,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\" – wait, French. Wait let me check the actual message again. \"用户：“懵” 问：Raconter Moune Un Blonde\". Wait wait user just asked me, via my French instructions, but the actual message they past in here: User: 懿 asking \"Raconтер мo",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\"",
        "Ġ–",
        "Ġwait",
        ",",
        "ĠFrench",
        ".",
        "ĠWait",
        "Ġlet",
        "Ġme",
        "Ġcheck",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġagain",
        ".",
        "Ġ\"",
        "用户",
        "：“",
        "�",
        "�",
        "�",
        "”",
        "Ġ",
        "问",
        "：",
        "R",
        "acon",
        "ter",
        "ĠMo",
        "une",
        "ĠUn",
        "ĠBlonde",
        "\".",
        "ĠWait",
        "Ġwait",
        "Ġuser",
        "Ġjust",
        "Ġasked",
        "Ġme",
        ",",
        "Ġvia",
        "Ġmy",
        "ĠFrench",
        "Ġinstructions",
        ",",
        "Ġbut",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġthey",
        "Ġpast",
        "Ġin",
        "Ġhere",
        ":",
        "ĠUser",
        ":",
        "Ġ�",
        "�",
        "�",
        "Ġasking",
        "Ġ\"",
        "R",
        "acon",
        "тер",
        "Ġм",
        "o"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1,
        1389,
        3868,
        11,
        8753,
        13,
        14144,
        1095,
        757,
        1817,
        279,
        5150,
        1984,
        1578,
        13,
        330,
        20600,
        38087,
        162,
        229,
        113,
        863,
        220,
        57107,
        5232,
        49,
        23184,
        466,
        6178,
        2957,
        1252,
        53556,
        3343,
        14144,
        3868,
        1217,
        1120,
        4691,
        757,
        11,
        4669,
        856,
        8753,
        11470,
        11,
        719,
        279,
        5150,
        1984,
        814,
        3347,
        304,
        1618,
        25,
        2724,
        25,
        6704,
        229,
        123,
        10371,
        330,
        49,
        23184,
        58060,
        11562,
        78
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "une",
            "token_id": 2957,
            "logprob": -0.7031,
            "prob": 0.4961
          },
          {
            "token": "ун",
            "token_id": 35297,
            "logprob": -1.6406,
            "prob": 0.1943
          },
          {
            "token": "un",
            "token_id": 359,
            "logprob": -3.2656,
            "prob": 0.0381
          },
          {
            "token": "не",
            "token_id": 79862,
            "logprob": -3.3906,
            "prob": 0.0337
          },
          {
            "token": "ю",
            "token_id": 12182,
            "logprob": -3.6406,
            "prob": 0.0262
          },
          {
            "token": "у",
            "token_id": 3865,
            "logprob": -3.9531,
            "prob": 0.0192
          },
          {
            "token": "Ġune",
            "token_id": 6316,
            "logprob": -4.1875,
            "prob": 0.015
          },
          {
            "token": "Ġun",
            "token_id": 653,
            "logprob": -4.7812,
            "prob": 0.0082
          },
          {
            "token": "ü",
            "token_id": 2448,
            "logprob": -4.8438,
            "prob": 0.0078
          },
          {
            "token": "one",
            "token_id": 606,
            "logprob": -4.875,
            "prob": 0.0075
          }
        ]
      },
      "selected_token": {
        "token": "one",
        "token_id": 606,
        "selection_method": "sampling"
      }
    },
    {
      "step": 91,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\" – wait, French. Wait let me check the actual message again. \"用户：“懵” 问：Raconter Moune Un Blonde\". Wait wait user just asked me, via my French instructions, but the actual message they past in here: User: 懿 asking \"Raconтер мoone",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\"",
        "Ġ–",
        "Ġwait",
        ",",
        "ĠFrench",
        ".",
        "ĠWait",
        "Ġlet",
        "Ġme",
        "Ġcheck",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġagain",
        ".",
        "Ġ\"",
        "用户",
        "：“",
        "�",
        "�",
        "�",
        "”",
        "Ġ",
        "问",
        "：",
        "R",
        "acon",
        "ter",
        "ĠMo",
        "une",
        "ĠUn",
        "ĠBlonde",
        "\".",
        "ĠWait",
        "Ġwait",
        "Ġuser",
        "Ġjust",
        "Ġasked",
        "Ġme",
        ",",
        "Ġvia",
        "Ġmy",
        "ĠFrench",
        "Ġinstructions",
        ",",
        "Ġbut",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġthey",
        "Ġpast",
        "Ġin",
        "Ġhere",
        ":",
        "ĠUser",
        ":",
        "Ġ�",
        "�",
        "�",
        "Ġasking",
        "Ġ\"",
        "R",
        "acon",
        "тер",
        "Ġм",
        "o",
        "one"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1,
        1389,
        3868,
        11,
        8753,
        13,
        14144,
        1095,
        757,
        1817,
        279,
        5150,
        1984,
        1578,
        13,
        330,
        20600,
        38087,
        162,
        229,
        113,
        863,
        220,
        57107,
        5232,
        49,
        23184,
        466,
        6178,
        2957,
        1252,
        53556,
        3343,
        14144,
        3868,
        1217,
        1120,
        4691,
        757,
        11,
        4669,
        856,
        8753,
        11470,
        11,
        719,
        279,
        5150,
        1984,
        814,
        3347,
        304,
        1618,
        25,
        2724,
        25,
        6704,
        229,
        123,
        10371,
        330,
        49,
        23184,
        58060,
        11562,
        78,
        606
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġun",
            "token_id": 653,
            "logprob": -0.3887,
            "prob": 0.6797
          },
          {
            "token": "ĠUn",
            "token_id": 1252,
            "logprob": -1.5156,
            "prob": 0.2197
          },
          {
            "token": "Ġu",
            "token_id": 577,
            "logprob": -4.4062,
            "prob": 0.012
          },
          {
            "token": "Ġу",
            "token_id": 14257,
            "logprob": -5.125,
            "prob": 0.0059
          },
          {
            "token": "ĠBlonde",
            "token_id": 53556,
            "logprob": -5.4062,
            "prob": 0.0044
          },
          {
            "token": "Ġblonde",
            "token_id": 27117,
            "logprob": -5.5625,
            "prob": 0.0038
          },
          {
            "token": "Ġbl",
            "token_id": 1529,
            "logprob": -5.5938,
            "prob": 0.0037
          },
          {
            "token": "Ġuna",
            "token_id": 5203,
            "logprob": -5.6562,
            "prob": 0.0034
          },
          {
            "token": "ĠU",
            "token_id": 549,
            "logprob": -5.9062,
            "prob": 0.0027
          },
          {
            "token": "ĠBl",
            "token_id": 2563,
            "logprob": -5.9062,
            "prob": 0.0027
          }
        ]
      },
      "selected_token": {
        "token": "Ġun",
        "token_id": 653,
        "selection_method": "sampling"
      }
    },
    {
      "step": 92,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\" – wait, French. Wait let me check the actual message again. \"用户：“懵” 问：Raconter Moune Un Blonde\". Wait wait user just asked me, via my French instructions, but the actual message they past in here: User: 懿 asking \"Raconтер мoone un",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\"",
        "Ġ–",
        "Ġwait",
        ",",
        "ĠFrench",
        ".",
        "ĠWait",
        "Ġlet",
        "Ġme",
        "Ġcheck",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġagain",
        ".",
        "Ġ\"",
        "用户",
        "：“",
        "�",
        "�",
        "�",
        "”",
        "Ġ",
        "问",
        "：",
        "R",
        "acon",
        "ter",
        "ĠMo",
        "une",
        "ĠUn",
        "ĠBlonde",
        "\".",
        "ĠWait",
        "Ġwait",
        "Ġuser",
        "Ġjust",
        "Ġasked",
        "Ġme",
        ",",
        "Ġvia",
        "Ġmy",
        "ĠFrench",
        "Ġinstructions",
        ",",
        "Ġbut",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġthey",
        "Ġpast",
        "Ġin",
        "Ġhere",
        ":",
        "ĠUser",
        ":",
        "Ġ�",
        "�",
        "�",
        "Ġasking",
        "Ġ\"",
        "R",
        "acon",
        "тер",
        "Ġм",
        "o",
        "one",
        "Ġun"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1,
        1389,
        3868,
        11,
        8753,
        13,
        14144,
        1095,
        757,
        1817,
        279,
        5150,
        1984,
        1578,
        13,
        330,
        20600,
        38087,
        162,
        229,
        113,
        863,
        220,
        57107,
        5232,
        49,
        23184,
        466,
        6178,
        2957,
        1252,
        53556,
        3343,
        14144,
        3868,
        1217,
        1120,
        4691,
        757,
        11,
        4669,
        856,
        8753,
        11470,
        11,
        719,
        279,
        5150,
        1984,
        814,
        3347,
        304,
        1618,
        25,
        2724,
        25,
        6704,
        229,
        123,
        10371,
        330,
        49,
        23184,
        58060,
        11562,
        78,
        606,
        653
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġblonde",
            "token_id": 27117,
            "logprob": -0.7617,
            "prob": 0.4668
          },
          {
            "token": "ĠBlonde",
            "token_id": 53556,
            "logprob": -1.7656,
            "prob": 0.1719
          },
          {
            "token": "Ġbl",
            "token_id": 1529,
            "logprob": -2.2656,
            "prob": 0.104
          },
          {
            "token": "Ġblond",
            "token_id": 45469,
            "logprob": -2.7656,
            "prob": 0.063
          },
          {
            "token": "ĠBl",
            "token_id": 2563,
            "logprob": -3.0781,
            "prob": 0.0461
          },
          {
            "token": "Ġble",
            "token_id": 12704,
            "logprob": -4.0625,
            "prob": 0.017
          },
          {
            "token": "ĠBlond",
            "token_id": 93098,
            "logprob": -4.75,
            "prob": 0.0085
          },
          {
            "token": "Ġb",
            "token_id": 293,
            "logprob": -5.25,
            "prob": 0.0052
          },
          {
            "token": "ĠB",
            "token_id": 426,
            "logprob": -5.4062,
            "prob": 0.0044
          },
          {
            "token": "bl",
            "token_id": 2067,
            "logprob": -5.4062,
            "prob": 0.0044
          }
        ]
      },
      "selected_token": {
        "token": "ĠBl",
        "token_id": 2563,
        "selection_method": "sampling"
      }
    },
    {
      "step": 93,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\" – wait, French. Wait let me check the actual message again. \"用户：“懵” 问：Raconter Moune Un Blonde\". Wait wait user just asked me, via my French instructions, but the actual message they past in here: User: 懿 asking \"Raconтер мoone un Bl",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\"",
        "Ġ–",
        "Ġwait",
        ",",
        "ĠFrench",
        ".",
        "ĠWait",
        "Ġlet",
        "Ġme",
        "Ġcheck",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġagain",
        ".",
        "Ġ\"",
        "用户",
        "：“",
        "�",
        "�",
        "�",
        "”",
        "Ġ",
        "问",
        "：",
        "R",
        "acon",
        "ter",
        "ĠMo",
        "une",
        "ĠUn",
        "ĠBlonde",
        "\".",
        "ĠWait",
        "Ġwait",
        "Ġuser",
        "Ġjust",
        "Ġasked",
        "Ġme",
        ",",
        "Ġvia",
        "Ġmy",
        "ĠFrench",
        "Ġinstructions",
        ",",
        "Ġbut",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġthey",
        "Ġpast",
        "Ġin",
        "Ġhere",
        ":",
        "ĠUser",
        ":",
        "Ġ�",
        "�",
        "�",
        "Ġasking",
        "Ġ\"",
        "R",
        "acon",
        "тер",
        "Ġм",
        "o",
        "one",
        "Ġun",
        "ĠBl"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1,
        1389,
        3868,
        11,
        8753,
        13,
        14144,
        1095,
        757,
        1817,
        279,
        5150,
        1984,
        1578,
        13,
        330,
        20600,
        38087,
        162,
        229,
        113,
        863,
        220,
        57107,
        5232,
        49,
        23184,
        466,
        6178,
        2957,
        1252,
        53556,
        3343,
        14144,
        3868,
        1217,
        1120,
        4691,
        757,
        11,
        4669,
        856,
        8753,
        11470,
        11,
        719,
        279,
        5150,
        1984,
        814,
        3347,
        304,
        1618,
        25,
        2724,
        25,
        6704,
        229,
        123,
        10371,
        330,
        49,
        23184,
        58060,
        11562,
        78,
        606,
        653,
        2563
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "on",
            "token_id": 263,
            "logprob": -1.7031,
            "prob": 0.1826
          },
          {
            "token": "он",
            "token_id": 17766,
            "logprob": -1.7656,
            "prob": 0.1719
          },
          {
            "token": "onde",
            "token_id": 17675,
            "logprob": -2.0156,
            "prob": 0.1338
          },
          {
            "token": "a",
            "token_id": 64,
            "logprob": -2.5156,
            "prob": 0.0811
          },
          {
            "token": "onds",
            "token_id": 51058,
            "logprob": -3.1406,
            "prob": 0.0435
          },
          {
            "token": "ON",
            "token_id": 715,
            "logprob": -3.3906,
            "prob": 0.0337
          },
          {
            "token": "ons",
            "token_id": 2439,
            "logprob": -3.9531,
            "prob": 0.0193
          },
          {
            "token": "а",
            "token_id": 1506,
            "logprob": -4.0625,
            "prob": 0.017
          },
          {
            "token": "once",
            "token_id": 13486,
            "logprob": -4.0625,
            "prob": 0.017
          },
          {
            "token": "ond",
            "token_id": 2159,
            "logprob": -4.4375,
            "prob": 0.0117
          }
        ]
      },
      "selected_token": {
        "token": "ond",
        "token_id": 2159,
        "selection_method": "sampling"
      }
    },
    {
      "step": 94,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\" – wait, French. Wait let me check the actual message again. \"用户：“懵” 问：Raconter Moune Un Blonde\". Wait wait user just asked me, via my French instructions, but the actual message they past in here: User: 懿 asking \"Raconтер мoone un Blond",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\"",
        "Ġ–",
        "Ġwait",
        ",",
        "ĠFrench",
        ".",
        "ĠWait",
        "Ġlet",
        "Ġme",
        "Ġcheck",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġagain",
        ".",
        "Ġ\"",
        "用户",
        "：“",
        "�",
        "�",
        "�",
        "”",
        "Ġ",
        "问",
        "：",
        "R",
        "acon",
        "ter",
        "ĠMo",
        "une",
        "ĠUn",
        "ĠBlonde",
        "\".",
        "ĠWait",
        "Ġwait",
        "Ġuser",
        "Ġjust",
        "Ġasked",
        "Ġme",
        ",",
        "Ġvia",
        "Ġmy",
        "ĠFrench",
        "Ġinstructions",
        ",",
        "Ġbut",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġthey",
        "Ġpast",
        "Ġin",
        "Ġhere",
        ":",
        "ĠUser",
        ":",
        "Ġ�",
        "�",
        "�",
        "Ġasking",
        "Ġ\"",
        "R",
        "acon",
        "тер",
        "Ġм",
        "o",
        "one",
        "Ġun",
        "ĠBl",
        "ond"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1,
        1389,
        3868,
        11,
        8753,
        13,
        14144,
        1095,
        757,
        1817,
        279,
        5150,
        1984,
        1578,
        13,
        330,
        20600,
        38087,
        162,
        229,
        113,
        863,
        220,
        57107,
        5232,
        49,
        23184,
        466,
        6178,
        2957,
        1252,
        53556,
        3343,
        14144,
        3868,
        1217,
        1120,
        4691,
        757,
        11,
        4669,
        856,
        8753,
        11470,
        11,
        719,
        279,
        5150,
        1984,
        814,
        3347,
        304,
        1618,
        25,
        2724,
        25,
        6704,
        229,
        123,
        10371,
        330,
        49,
        23184,
        58060,
        11562,
        78,
        606,
        653,
        2563,
        2159
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "\"",
            "token_id": 1,
            "logprob": -1.2891,
            "prob": 0.2754
          },
          {
            "token": "é",
            "token_id": 978,
            "logprob": -1.5391,
            "prob": 0.2148
          },
          {
            "token": "е",
            "token_id": 1532,
            "logprob": -1.6641,
            "prob": 0.1895
          },
          {
            "token": "\".",
            "token_id": 3343,
            "logprob": -1.9141,
            "prob": 0.1475
          },
          {
            "token": "\"?",
            "token_id": 44969,
            "logprob": -2.1562,
            "prob": 0.1147
          },
          {
            "token": "es",
            "token_id": 288,
            "logprob": -3.9062,
            "prob": 0.02
          },
          {
            "token": "\",",
            "token_id": 498,
            "logprob": -5.0312,
            "prob": 0.0065
          },
          {
            "token": "?\"",
            "token_id": 7673,
            "logprob": -5.4688,
            "prob": 0.0042
          },
          {
            "token": "\"?\n\n",
            "token_id": 94770,
            "logprob": -5.7812,
            "prob": 0.0031
          },
          {
            "token": "\"—",
            "token_id": 66101,
            "logprob": -5.9062,
            "prob": 0.0027
          }
        ]
      },
      "selected_token": {
        "token": "é",
        "token_id": 978,
        "selection_method": "sampling"
      }
    },
    {
      "step": 95,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\" – wait, French. Wait let me check the actual message again. \"用户：“懵” 问：Raconter Moune Un Blonde\". Wait wait user just asked me, via my French instructions, but the actual message they past in here: User: 懿 asking \"Raconтер мoone un Blondé",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\"",
        "Ġ–",
        "Ġwait",
        ",",
        "ĠFrench",
        ".",
        "ĠWait",
        "Ġlet",
        "Ġme",
        "Ġcheck",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġagain",
        ".",
        "Ġ\"",
        "用户",
        "：“",
        "�",
        "�",
        "�",
        "”",
        "Ġ",
        "问",
        "：",
        "R",
        "acon",
        "ter",
        "ĠMo",
        "une",
        "ĠUn",
        "ĠBlonde",
        "\".",
        "ĠWait",
        "Ġwait",
        "Ġuser",
        "Ġjust",
        "Ġasked",
        "Ġme",
        ",",
        "Ġvia",
        "Ġmy",
        "ĠFrench",
        "Ġinstructions",
        ",",
        "Ġbut",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġthey",
        "Ġpast",
        "Ġin",
        "Ġhere",
        ":",
        "ĠUser",
        ":",
        "Ġ�",
        "�",
        "�",
        "Ġasking",
        "Ġ\"",
        "R",
        "acon",
        "тер",
        "Ġм",
        "o",
        "one",
        "Ġun",
        "ĠBl",
        "ond",
        "é"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1,
        1389,
        3868,
        11,
        8753,
        13,
        14144,
        1095,
        757,
        1817,
        279,
        5150,
        1984,
        1578,
        13,
        330,
        20600,
        38087,
        162,
        229,
        113,
        863,
        220,
        57107,
        5232,
        49,
        23184,
        466,
        6178,
        2957,
        1252,
        53556,
        3343,
        14144,
        3868,
        1217,
        1120,
        4691,
        757,
        11,
        4669,
        856,
        8753,
        11470,
        11,
        719,
        279,
        5150,
        1984,
        814,
        3347,
        304,
        1618,
        25,
        2724,
        25,
        6704,
        229,
        123,
        10371,
        330,
        49,
        23184,
        58060,
        11562,
        78,
        606,
        653,
        2563,
        2159,
        978
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "\"",
            "token_id": 1,
            "logprob": -0.5898,
            "prob": 0.5547
          },
          {
            "token": "\"?",
            "token_id": 44969,
            "logprob": -1.0938,
            "prob": 0.3359
          },
          {
            "token": "\".",
            "token_id": 3343,
            "logprob": -2.7188,
            "prob": 0.0659
          },
          {
            "token": "?\"",
            "token_id": 7673,
            "logprob": -3.4688,
            "prob": 0.0312
          },
          {
            "token": "\",",
            "token_id": 498,
            "logprob": -5.7188,
            "prob": 0.0033
          },
          {
            "token": "?",
            "token_id": 30,
            "logprob": -5.8438,
            "prob": 0.0029
          },
          {
            "token": "\"?\n\n",
            "token_id": 94770,
            "logprob": -6.4688,
            "prob": 0.0016
          },
          {
            "token": "”",
            "token_id": 863,
            "logprob": -6.9688,
            "prob": 0.0009
          },
          {
            "token": "Ġ?",
            "token_id": 949,
            "logprob": -7.2188,
            "prob": 0.0007
          },
          {
            "token": "\"...",
            "token_id": 53670,
            "logprob": -7.4688,
            "prob": 0.0006
          }
        ]
      },
      "selected_token": {
        "token": "Ġ?",
        "token_id": 949,
        "selection_method": "sampling"
      }
    },
    {
      "step": 96,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\" – wait, French. Wait let me check the actual message again. \"用户：“懵” 问：Raconter Moune Un Blonde\". Wait wait user just asked me, via my French instructions, but the actual message they past in here: User: 懿 asking \"Raconтер мoone un Blondé ?",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\"",
        "Ġ–",
        "Ġwait",
        ",",
        "ĠFrench",
        ".",
        "ĠWait",
        "Ġlet",
        "Ġme",
        "Ġcheck",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġagain",
        ".",
        "Ġ\"",
        "用户",
        "：“",
        "�",
        "�",
        "�",
        "”",
        "Ġ",
        "问",
        "：",
        "R",
        "acon",
        "ter",
        "ĠMo",
        "une",
        "ĠUn",
        "ĠBlonde",
        "\".",
        "ĠWait",
        "Ġwait",
        "Ġuser",
        "Ġjust",
        "Ġasked",
        "Ġme",
        ",",
        "Ġvia",
        "Ġmy",
        "ĠFrench",
        "Ġinstructions",
        ",",
        "Ġbut",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġthey",
        "Ġpast",
        "Ġin",
        "Ġhere",
        ":",
        "ĠUser",
        ":",
        "Ġ�",
        "�",
        "�",
        "Ġasking",
        "Ġ\"",
        "R",
        "acon",
        "тер",
        "Ġм",
        "o",
        "one",
        "Ġun",
        "ĠBl",
        "ond",
        "é",
        "Ġ?"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1,
        1389,
        3868,
        11,
        8753,
        13,
        14144,
        1095,
        757,
        1817,
        279,
        5150,
        1984,
        1578,
        13,
        330,
        20600,
        38087,
        162,
        229,
        113,
        863,
        220,
        57107,
        5232,
        49,
        23184,
        466,
        6178,
        2957,
        1252,
        53556,
        3343,
        14144,
        3868,
        1217,
        1120,
        4691,
        757,
        11,
        4669,
        856,
        8753,
        11470,
        11,
        719,
        279,
        5150,
        1984,
        814,
        3347,
        304,
        1618,
        25,
        2724,
        25,
        6704,
        229,
        123,
        10371,
        330,
        49,
        23184,
        58060,
        11562,
        78,
        606,
        653,
        2563,
        2159,
        978,
        949
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "\"",
            "token_id": 1,
            "logprob": -0.017,
            "prob": 0.9844
          },
          {
            "token": "\".",
            "token_id": 3343,
            "logprob": -5.1562,
            "prob": 0.0059
          },
          {
            "token": "”",
            "token_id": 863,
            "logprob": -5.4062,
            "prob": 0.0045
          },
          {
            "token": "\"\n\n",
            "token_id": 1875,
            "logprob": -6.0312,
            "prob": 0.0024
          },
          {
            "token": "Ġ\"",
            "token_id": 330,
            "logprob": -6.5938,
            "prob": 0.0014
          },
          {
            "token": "\"?",
            "token_id": 44969,
            "logprob": -7.5312,
            "prob": 0.0005
          },
          {
            "token": "ĠWait",
            "token_id": 14144,
            "logprob": -7.7188,
            "prob": 0.0005
          },
          {
            "token": "?\"",
            "token_id": 7673,
            "logprob": -8.5625,
            "prob": 0.0002
          },
          {
            "token": "!\"",
            "token_id": 9135,
            "logprob": -8.75,
            "prob": 0.0002
          },
          {
            "token": "\"—",
            "token_id": 66101,
            "logprob": -8.8125,
            "prob": 0.0001
          }
        ]
      },
      "selected_token": {
        "token": "Ġ\"",
        "token_id": 330,
        "selection_method": "sampling"
      }
    },
    {
      "step": 97,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\" – wait, French. Wait let me check the actual message again. \"用户：“懵” 问：Raconter Moune Un Blonde\". Wait wait user just asked me, via my French instructions, but the actual message they past in here: User: 懿 asking \"Raconтер мoone un Blondé ? \"",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\"",
        "Ġ–",
        "Ġwait",
        ",",
        "ĠFrench",
        ".",
        "ĠWait",
        "Ġlet",
        "Ġme",
        "Ġcheck",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġagain",
        ".",
        "Ġ\"",
        "用户",
        "：“",
        "�",
        "�",
        "�",
        "”",
        "Ġ",
        "问",
        "：",
        "R",
        "acon",
        "ter",
        "ĠMo",
        "une",
        "ĠUn",
        "ĠBlonde",
        "\".",
        "ĠWait",
        "Ġwait",
        "Ġuser",
        "Ġjust",
        "Ġasked",
        "Ġme",
        ",",
        "Ġvia",
        "Ġmy",
        "ĠFrench",
        "Ġinstructions",
        ",",
        "Ġbut",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġthey",
        "Ġpast",
        "Ġin",
        "Ġhere",
        ":",
        "ĠUser",
        ":",
        "Ġ�",
        "�",
        "�",
        "Ġasking",
        "Ġ\"",
        "R",
        "acon",
        "тер",
        "Ġм",
        "o",
        "one",
        "Ġun",
        "ĠBl",
        "ond",
        "é",
        "Ġ?",
        "Ġ\""
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1,
        1389,
        3868,
        11,
        8753,
        13,
        14144,
        1095,
        757,
        1817,
        279,
        5150,
        1984,
        1578,
        13,
        330,
        20600,
        38087,
        162,
        229,
        113,
        863,
        220,
        57107,
        5232,
        49,
        23184,
        466,
        6178,
        2957,
        1252,
        53556,
        3343,
        14144,
        3868,
        1217,
        1120,
        4691,
        757,
        11,
        4669,
        856,
        8753,
        11470,
        11,
        719,
        279,
        5150,
        1984,
        814,
        3347,
        304,
        1618,
        25,
        2724,
        25,
        6704,
        229,
        123,
        10371,
        330,
        49,
        23184,
        58060,
        11562,
        78,
        606,
        653,
        2563,
        2159,
        978,
        949,
        330
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "ĠHmm",
            "token_id": 89290,
            "logprob": -1.0625,
            "prob": 0.3457
          },
          {
            "token": "ĠWait",
            "token_id": 14144,
            "logprob": -1.5625,
            "prob": 0.209
          },
          {
            "token": "Ġwhich",
            "token_id": 902,
            "logprob": -1.5625,
            "prob": 0.209
          },
          {
            "token": "Ġ–",
            "token_id": 1389,
            "logprob": -2.8125,
            "prob": 0.0601
          },
          {
            "token": "ĠMaybe",
            "token_id": 10926,
            "logprob": -3.1875,
            "prob": 0.0413
          },
          {
            "token": "Ġor",
            "token_id": 477,
            "logprob": -3.4375,
            "prob": 0.032
          },
          {
            "token": "Ġ(",
            "token_id": 320,
            "logprob": -4.4375,
            "prob": 0.0118
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -4.6875,
            "prob": 0.0092
          },
          {
            "token": "ĠSo",
            "token_id": 2100,
            "logprob": -4.6875,
            "prob": 0.0092
          },
          {
            "token": "Ġ—",
            "token_id": 2001,
            "logprob": -4.8125,
            "prob": 0.0081
          }
        ]
      },
      "selected_token": {
        "token": "Ġwhich",
        "token_id": 902,
        "selection_method": "sampling"
      }
    },
    {
      "step": 98,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\" – wait, French. Wait let me check the actual message again. \"用户：“懵” 问：Raconter Moune Un Blonde\". Wait wait user just asked me, via my French instructions, but the actual message they past in here: User: 懿 asking \"Raconтер мoone un Blondé ? \" which",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\"",
        "Ġ–",
        "Ġwait",
        ",",
        "ĠFrench",
        ".",
        "ĠWait",
        "Ġlet",
        "Ġme",
        "Ġcheck",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġagain",
        ".",
        "Ġ\"",
        "用户",
        "：“",
        "�",
        "�",
        "�",
        "”",
        "Ġ",
        "问",
        "：",
        "R",
        "acon",
        "ter",
        "ĠMo",
        "une",
        "ĠUn",
        "ĠBlonde",
        "\".",
        "ĠWait",
        "Ġwait",
        "Ġuser",
        "Ġjust",
        "Ġasked",
        "Ġme",
        ",",
        "Ġvia",
        "Ġmy",
        "ĠFrench",
        "Ġinstructions",
        ",",
        "Ġbut",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġthey",
        "Ġpast",
        "Ġin",
        "Ġhere",
        ":",
        "ĠUser",
        ":",
        "Ġ�",
        "�",
        "�",
        "Ġasking",
        "Ġ\"",
        "R",
        "acon",
        "тер",
        "Ġм",
        "o",
        "one",
        "Ġun",
        "ĠBl",
        "ond",
        "é",
        "Ġ?",
        "Ġ\"",
        "Ġwhich"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1,
        1389,
        3868,
        11,
        8753,
        13,
        14144,
        1095,
        757,
        1817,
        279,
        5150,
        1984,
        1578,
        13,
        330,
        20600,
        38087,
        162,
        229,
        113,
        863,
        220,
        57107,
        5232,
        49,
        23184,
        466,
        6178,
        2957,
        1252,
        53556,
        3343,
        14144,
        3868,
        1217,
        1120,
        4691,
        757,
        11,
        4669,
        856,
        8753,
        11470,
        11,
        719,
        279,
        5150,
        1984,
        814,
        3347,
        304,
        1618,
        25,
        2724,
        25,
        6704,
        229,
        123,
        10371,
        330,
        49,
        23184,
        58060,
        11562,
        78,
        606,
        653,
        2563,
        2159,
        978,
        949,
        330,
        902
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġtranslates",
            "token_id": 48018,
            "logprob": -0.4512,
            "prob": 0.6367
          },
          {
            "token": "Ġis",
            "token_id": 374,
            "logprob": -1.8281,
            "prob": 0.1611
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -1.9531,
            "prob": 0.1426
          },
          {
            "token": "ĠI",
            "token_id": 358,
            "logprob": -3.9531,
            "prob": 0.0193
          },
          {
            "token": "Ġprobably",
            "token_id": 4762,
            "logprob": -4.1875,
            "prob": 0.015
          },
          {
            "token": "Ġseems",
            "token_id": 5084,
            "logprob": -4.8125,
            "prob": 0.008
          },
          {
            "token": "Ġmight",
            "token_id": 2643,
            "logprob": -4.9375,
            "prob": 0.0071
          },
          {
            "token": "Ġmeans",
            "token_id": 3445,
            "logprob": -6.0625,
            "prob": 0.0023
          },
          {
            "token": "Ġtranslated",
            "token_id": 25548,
            "logprob": -6.5625,
            "prob": 0.0014
          },
          {
            "token": "Ġtranslate",
            "token_id": 15025,
            "logprob": -6.6875,
            "prob": 0.0012
          }
        ]
      },
      "selected_token": {
        "token": "Ġtranslates",
        "token_id": 48018,
        "selection_method": "sampling"
      }
    },
    {
      "step": 99,
      "input_text": "Raconte-moi une blague<|im_end|>\n<|im_start|>assistant\n<think>Okay, il wants a riddle? Or an anecdote? The original query says \"Raconte moi une blonde\" – wait, French. Wait let me check the actual message again. \"用户：“懵” 问：Raconter Moune Un Blonde\". Wait wait user just asked me, via my French instructions, but the actual message they past in here: User: 懿 asking \"Raconтер мoone un Blondé ? \" which translates",
      "tokens": [
        "R",
        "acon",
        "te",
        "-m",
        "oi",
        "Ġune",
        "Ġbl",
        "ague",
        "<|im_end|>",
        "\n",
        "<|im_start|>",
        "assistant",
        "\n",
        "<th",
        "ink",
        ">",
        "Okay",
        ",",
        "Ġil",
        "Ġwants",
        "Ġa",
        "Ġr",
        "iddle",
        "?",
        "ĠOr",
        "Ġan",
        "Ġanecd",
        "ote",
        "?",
        "ĠThe",
        "Ġoriginal",
        "Ġquery",
        "Ġsays",
        "Ġ\"",
        "R",
        "acon",
        "te",
        "Ġmoi",
        "Ġune",
        "Ġblonde",
        "\"",
        "Ġ–",
        "Ġwait",
        ",",
        "ĠFrench",
        ".",
        "ĠWait",
        "Ġlet",
        "Ġme",
        "Ġcheck",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġagain",
        ".",
        "Ġ\"",
        "用户",
        "：“",
        "�",
        "�",
        "�",
        "”",
        "Ġ",
        "问",
        "：",
        "R",
        "acon",
        "ter",
        "ĠMo",
        "une",
        "ĠUn",
        "ĠBlonde",
        "\".",
        "ĠWait",
        "Ġwait",
        "Ġuser",
        "Ġjust",
        "Ġasked",
        "Ġme",
        ",",
        "Ġvia",
        "Ġmy",
        "ĠFrench",
        "Ġinstructions",
        ",",
        "Ġbut",
        "Ġthe",
        "Ġactual",
        "Ġmessage",
        "Ġthey",
        "Ġpast",
        "Ġin",
        "Ġhere",
        ":",
        "ĠUser",
        ":",
        "Ġ�",
        "�",
        "�",
        "Ġasking",
        "Ġ\"",
        "R",
        "acon",
        "тер",
        "Ġм",
        "o",
        "one",
        "Ġun",
        "ĠBl",
        "ond",
        "é",
        "Ġ?",
        "Ġ\"",
        "Ġwhich",
        "Ġtranslates"
      ],
      "token_ids": [
        49,
        23184,
        668,
        1474,
        6870,
        6316,
        1529,
        4768,
        100265,
        198,
        100264,
        78191,
        198,
        14023,
        771,
        29,
        33413,
        11,
        3900,
        6944,
        264,
        436,
        3390,
        30,
        2582,
        459,
        50750,
        1295,
        30,
        578,
        4113,
        3319,
        2795,
        330,
        49,
        23184,
        668,
        41091,
        6316,
        27117,
        1,
        1389,
        3868,
        11,
        8753,
        13,
        14144,
        1095,
        757,
        1817,
        279,
        5150,
        1984,
        1578,
        13,
        330,
        20600,
        38087,
        162,
        229,
        113,
        863,
        220,
        57107,
        5232,
        49,
        23184,
        466,
        6178,
        2957,
        1252,
        53556,
        3343,
        14144,
        3868,
        1217,
        1120,
        4691,
        757,
        11,
        4669,
        856,
        8753,
        11470,
        11,
        719,
        279,
        5150,
        1984,
        814,
        3347,
        304,
        1618,
        25,
        2724,
        25,
        6704,
        229,
        123,
        10371,
        330,
        49,
        23184,
        58060,
        11562,
        78,
        606,
        653,
        2563,
        2159,
        978,
        949,
        330,
        902,
        48018
      ],
      "output_distribution": {
        "top_k": 10,
        "candidates": [
          {
            "token": "Ġto",
            "token_id": 311,
            "logprob": -0.0437,
            "prob": 0.957
          },
          {
            "token": "Ġroughly",
            "token_id": 17715,
            "logprob": -3.7969,
            "prob": 0.0225
          },
          {
            "token": "Ġfrom",
            "token_id": 505,
            "logprob": -4.7812,
            "prob": 0.0083
          },
          {
            "token": "Ġas",
            "token_id": 439,
            "logprob": -4.9062,
            "prob": 0.0073
          },
          {
            "token": "Ġin",
            "token_id": 304,
            "logprob": -6.6562,
            "prob": 0.0013
          },
          {
            "token": "Ġvia",
            "token_id": 4669,
            "logprob": -7.1562,
            "prob": 0.0008
          },
          {
            "token": "Ġdirectly",
            "token_id": 6089,
            "logprob": -7.6562,
            "prob": 0.0005
          },
          {
            "token": "Ġliterally",
            "token_id": 16280,
            "logprob": -7.9062,
            "prob": 0.0004
          },
          {
            "token": "Ġapproximately",
            "token_id": 13489,
            "logprob": -7.9062,
            "prob": 0.0004
          },
          {
            "token": "Ġ\"",
            "token_id": 330,
            "logprob": -8.0625,
            "prob": 0.0003
          }
        ]
      },
      "selected_token": {
        "token": "Ġroughly",
        "token_id": 17715,
        "selection_method": "sampling"
      }
    }
  ]
}